# AutoGen API 参考文档

**原文档链接**: [AutoGen API Reference](https://microsoft.github.io/autogen/stable/reference/index.html)

**注意**: 未安装翻译工具，此文档保留英文原文并添加了翻译标记。API名称、参数名和代码示例保持英文原样。

## 目录

- [AutoGen AgentChat](#autogen-agentchat)
  - [autogen_agentchat](#autogen_agentchat)
  - [autogen_agentchat](#autogen_agentchat)
  - [autogen_agentchat.agents](#autogen_agentchatagents)
  - [autogen_agentchat.agents](#autogen_agentchatagents)
  - [autogen_agentchat.base](#autogen_agentchatbase)
  - [autogen_agentchat.base](#autogen_agentchatbase)
  - [autogen_agentchat.conditions](#autogen_agentchatconditions)
  - [autogen_agentchat.conditions](#autogen_agentchatconditions)
  - [autogen_agentchat.messages](#autogen_agentchatmessages)
  - [autogen_agentchat.messages](#autogen_agentchatmessages)
  - [autogen_agentchat.state](#autogen_agentchatstate)
  - [autogen_agentchat.state](#autogen_agentchatstate)
  - [autogen_agentchat.teams](#autogen_agentchatteams)
  - [autogen_agentchat.teams](#autogen_agentchatteams)
  - [autogen_agentchat.tools](#autogen_agentchattools)
  - [autogen_agentchat.tools](#autogen_agentchattools)
  - [autogen_agentchat.ui](#autogen_agentchatui)
  - [autogen_agentchat.ui](#autogen_agentchatui)
  - [next
autogen_agentchat](#next
autogen_agentchat)
- [Other API References](#other-api-references)
  - [autogen_core](#autogen_core)
  - [autogen_core](#autogen_core)
  - [autogen_core.code_executor](#autogen_corecode_executor)
  - [autogen_core.code_executor](#autogen_corecode_executor)
  - [autogen_core.exceptions](#autogen_coreexceptions)
  - [autogen_core.exceptions](#autogen_coreexceptions)
  - [autogen_core.logging](#autogen_corelogging)
  - [autogen_core.logging](#autogen_corelogging)
  - [autogen_core.memory](#autogen_corememory)
  - [autogen_core.memory](#autogen_corememory)
  - [autogen_core.model_context](#autogen_coremodel_context)
  - [autogen_core.model_context](#autogen_coremodel_context)
  - [autogen_core.models](#autogen_coremodels)
  - [autogen_core.models](#autogen_coremodels)
  - [autogen_core.tool_agent](#autogen_coretool_agent)
  - [autogen_core.tool_agent](#autogen_coretool_agent)
  - [autogen_core.tools](#autogen_coretools)
  - [autogen_core.tools](#autogen_coretools)
  - [autogen_ext.agents.azure](#autogen_extagentsazure)
  - [autogen_ext.agents.azure](#autogen_extagentsazure)
  - [autogen_ext.agents.file_surfer](#autogen_extagentsfile_surfer)
  - [autogen_ext.agents.file_surfer](#autogen_extagentsfile_surfer)
  - [autogen_ext.agents.magentic_one](#autogen_extagentsmagentic_one)
  - [autogen_ext.agents.magentic_one](#autogen_extagentsmagentic_one)
  - [autogen_ext.agents.openai](#autogen_extagentsopenai)
  - [autogen_ext.agents.openai](#autogen_extagentsopenai)
  - [autogen_ext.agents.video_surfer](#autogen_extagentsvideo_surfer)
  - [autogen_ext.agents.video_surfer](#autogen_extagentsvideo_surfer)
  - [autogen_ext.agents.video_surfer.tools](#autogen_extagentsvideo_surfertools)
  - [autogen_ext.agents.video_surfer.tools](#autogen_extagentsvideo_surfertools)
  - [autogen_ext.agents.web_surfer](#autogen_extagentsweb_surfer)
  - [autogen_ext.agents.web_surfer](#autogen_extagentsweb_surfer)
  - [autogen_ext.auth.azure](#autogen_extauthazure)
  - [autogen_ext.auth.azure](#autogen_extauthazure)
  - [autogen_ext.cache_store.diskcache](#autogen_extcache_storediskcache)
  - [autogen_ext.cache_store.diskcache](#autogen_extcache_storediskcache)
  - [autogen_ext.cache_store.redis](#autogen_extcache_storeredis)
  - [autogen_ext.cache_store.redis](#autogen_extcache_storeredis)
  - [autogen_ext.code_executors.azure](#autogen_extcode_executorsazure)
  - [autogen_ext.code_executors.azure](#autogen_extcode_executorsazure)
  - [autogen_ext.code_executors.docker](#autogen_extcode_executorsdocker)
  - [autogen_ext.code_executors.docker](#autogen_extcode_executorsdocker)
  - [autogen_ext.code_executors.docker_jupyter](#autogen_extcode_executorsdocker_jupyter)
  - [autogen_ext.code_executors.docker_jupyter](#autogen_extcode_executorsdocker_jupyter)
  - [autogen_ext.code_executors.jupyter](#autogen_extcode_executorsjupyter)
  - [autogen_ext.code_executors.jupyter](#autogen_extcode_executorsjupyter)
  - [autogen_ext.code_executors.local](#autogen_extcode_executorslocal)
  - [autogen_ext.code_executors.local](#autogen_extcode_executorslocal)
  - [autogen_ext.experimental.task_centric_memory](#autogen_extexperimentaltask_centric_memory)
  - [autogen_ext.experimental.task_centric_memory](#autogen_extexperimentaltask_centric_memory)
  - [autogen_ext.experimental.task_centric_memory.utils](#autogen_extexperimentaltask_centric_memoryutils)
  - [autogen_ext.experimental.task_centric_memory.utils](#autogen_extexperimentaltask_centric_memoryutils)
  - [autogen_ext.memory.canvas](#autogen_extmemorycanvas)
  - [autogen_ext.memory.canvas](#autogen_extmemorycanvas)
  - [autogen_ext.models.anthropic](#autogen_extmodelsanthropic)
  - [autogen_ext.models.anthropic](#autogen_extmodelsanthropic)
  - [autogen_ext.models.azure](#autogen_extmodelsazure)
  - [autogen_ext.models.azure](#autogen_extmodelsazure)
  - [autogen_ext.models.cache](#autogen_extmodelscache)
  - [autogen_ext.models.cache](#autogen_extmodelscache)
  - [autogen_ext.models.llama_cpp](#autogen_extmodelsllama_cpp)
  - [autogen_ext.models.llama_cpp](#autogen_extmodelsllama_cpp)
  - [autogen_ext.models.ollama](#autogen_extmodelsollama)
  - [autogen_ext.models.ollama](#autogen_extmodelsollama)
  - [autogen_ext.models.openai](#autogen_extmodelsopenai)
  - [autogen_ext.models.openai](#autogen_extmodelsopenai)
  - [autogen_ext.models.replay](#autogen_extmodelsreplay)
  - [autogen_ext.models.replay](#autogen_extmodelsreplay)
  - [autogen_ext.models.semantic_kernel](#autogen_extmodelssemantic_kernel)
  - [autogen_ext.models.semantic_kernel](#autogen_extmodelssemantic_kernel)
  - [autogen_ext.runtimes.grpc](#autogen_extruntimesgrpc)
  - [autogen_ext.runtimes.grpc](#autogen_extruntimesgrpc)
  - [autogen_ext.teams.magentic_one](#autogen_extteamsmagentic_one)
  - [autogen_ext.teams.magentic_one](#autogen_extteamsmagentic_one)
  - [autogen_ext.tools.azure](#autogen_exttoolsazure)
  - [autogen_ext.tools.azure](#autogen_exttoolsazure)
  - [autogen_ext.tools.code_execution](#autogen_exttoolscode_execution)
  - [autogen_ext.tools.code_execution](#autogen_exttoolscode_execution)
  - [autogen_ext.tools.graphrag](#autogen_exttoolsgraphrag)
  - [autogen_ext.tools.graphrag](#autogen_exttoolsgraphrag)
  - [autogen_ext.tools.http](#autogen_exttoolshttp)
  - [autogen_ext.tools.http](#autogen_exttoolshttp)
  - [autogen_ext.tools.langchain](#autogen_exttoolslangchain)
  - [autogen_ext.tools.langchain](#autogen_exttoolslangchain)
  - [autogen_ext.tools.mcp](#autogen_exttoolsmcp)
  - [autogen_ext.tools.mcp](#autogen_exttoolsmcp)
  - [autogen_ext.tools.semantic_kernel](#autogen_exttoolssemantic_kernel)
  - [autogen_ext.tools.semantic_kernel](#autogen_exttoolssemantic_kernel)

## AutoGen AgentChat {autogen-agentchat}

AutoGen AgentChat 提供了一组工具，用于创建、配置和部署各种类型的Agent，使它们能够互相沟通和协作。

### autogen_agentchat {autogen_agentchat}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html)

```python
EVENT_LOGGER_NAME = 'autogen_agentchat.events'#
```

【中文翻译】Logger name for event logs.

```python
TRACE_LOGGER_NAME = 'autogen_agentchat'#
```

【中文翻译】Logger name for trace logs.

【中文翻译】This module provides the main entry point for the autogen_agentchat package.
It includes logger names for trace and event logs, and retrieves the package version.

【中文翻译】previous

【中文翻译】API Reference

【中文翻译】next

【中文翻译】autogen_agentchat.messages

### autogen_agentchat {autogen_agentchat}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html)

```python
EVENT_LOGGER_NAME = 'autogen_agentchat.events'#
```

【中文翻译】Logger name for event logs.

```python
TRACE_LOGGER_NAME = 'autogen_agentchat'#
```

【中文翻译】Logger name for trace logs.

【中文翻译】This module provides the main entry point for the autogen_agentchat package.
It includes logger names for trace and event logs, and retrieves the package version.

【中文翻译】previous

【中文翻译】API Reference

【中文翻译】next

【中文翻译】autogen_agentchat.messages

### autogen_agentchat.agents {autogen_agentchatagents}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html)

```python
class AssistantAgent(name: str, model_client: ChatCompletionClient, *, tools: List[BaseTool[Any, Any] | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, workbench: Workbench | None = None, handoffs: List[Handoff | str] | None = None, model_context: ChatCompletionContext | None = None, description: str = 'An agent that provides assistance with ability to use tools.', system_message: str | None = 'You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.', model_client_stream: bool = False, reflect_on_tool_use: bool | None = None, tool_call_summary_format: str = '{result}', output_content_type: type[BaseModel] | None = None, output_content_type_format: str | None = None, memory: Sequence[Memory] | None = None, metadata: Dict[str, str] | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[AssistantAgentConfig]
An agent that provides assistance with tool use.
The on_messages() returns a Response
in which chat_message is the final
response message.
The on_messages_stream() creates an async generator that produces
the inner messages as they are created, and the Response
object as the last item before closing the generator.
The BaseChatAgent.run() method returns a TaskResult
containing the messages produced by the agent. In the list of messages,
messages,
the last message is the final response message.
The BaseChatAgent.run_stream() method creates an async generator that produces
the inner messages as they are created, and the TaskResult
object as the last item before closing the generator.

Attention
The caller must only pass the new messages to the agent on each call
to the on_messages(), on_messages_stream(), BaseChatAgent.run(),
or BaseChatAgent.run_stream() methods.
The agent maintains its state between calls to these methods.
Do not pass the entire conversation history to the agent on each call.


Warning
The assistant agent is not thread-safe or coroutine-safe.
It should not be shared between multiple tasks or coroutines, and it should
not call its methods concurrently.

The following diagram shows how the assistant agent works:

Structured output:
If the output_content_type is set, the agent will respond with a StructuredMessage
instead of a TextMessage in the final response by default.

Note
Currently, setting output_content_type prevents the agent from being
able to call load_component and dum_component methods for serializable
configuration. This will be fixed soon in the future.

Tool call behavior:

If the model returns no tool call, then the response is immediately returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message.

When the model returns tool calls, they will be executed right away:
When reflect_on_tool_use is False, the tool call results are returned as a ToolCallSummaryMessage in chat_message. tool_call_summary_format can be used to customize the tool call summary.
When reflect_on_tool_use is True, the another model inference is made using the tool calls and results, and final response is returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message.
reflect_on_tool_use is set to True by default when output_content_type is set.
reflect_on_tool_use is set to False by default when output_content_type is not set.




If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient.


Tip
By default, the tool call results are returned as response when tool calls are made.
So it is recommended to pay attention to the formatting of the tools return values,
especially if another agent is expecting them in a specific format.
Use tool_call_summary_format to customize the tool call summary, if needed.

Hand off behavior:

If a handoff is triggered, a HandoffMessage will be returned in chat_message.
If there are tool calls, they will also be executed right away before returning the handoff.
The tool calls and results are passed to the target agent through context.


Note
If multiple handoffs are detected, only the first handoff is executed.
To avoid this, disable parallel tool calls in the model client configuration.

Limit context size sent to the model:
You can limit the number of messages sent to the model by setting
the model_context parameter to a BufferedChatCompletionContext.
This will limit the number of recent messages sent to the model and can be useful
when the model has a limit on the number of tokens it can process.
Another option is to use a TokenLimitedChatCompletionContext
which will limit the number of tokens sent to the model.
You can also create your own model context by subclassing
ChatCompletionContext.
Streaming mode:
The assistant agent can be used in streaming mode by setting model_client_stream=True.
In this mode, the on_messages_stream() and BaseChatAgent.run_stream() methods will also yield
ModelClientStreamingChunkEvent
messages as the model client produces chunks of response.
The chunk messages will not be included in the final response’s inner messages.

Parameters:

name (str) – The name of the agent.
model_client (ChatCompletionClient) – The model client to use for inference.
tools (List[BaseTool[Any, Any]  | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – The tools to register with the agent.
workbench (Workbench | None, optional) – The workbench to use for the agent.
Tools cannot be used when workbench is set and vice versa.
handoffs (List[HandoffBase | str] | None, optional) – The handoff configurations for the agent,
allowing it to transfer to other agents by responding with a HandoffMessage.
The transfer is only executed when the team is in Swarm.
If a handoff is a string, it should represent the target agent’s name.
model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.
description (str, optional) – The description of the agent.
system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable.
model_client_stream (bool, optional) – If True, the model client will be used in streaming mode.
on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent
messages as the model client produces chunks of response. Defaults to False.
reflect_on_tool_use (bool, optional) – If True, the agent will make another model inference using the tool call and result
to generate a response. If False, the tool call result will be returned as the response. By default, if output_content_type is set, this will be True;
if output_content_type is not set, this will be False.
output_content_type (type[BaseModel] | None, optional) – The output content type for StructuredMessage response as a Pydantic model.
This will be used with the model client to generate structured output.
If this is set, the agent will respond with a StructuredMessage instead of a TextMessage
in the final response, unless reflect_on_tool_use is False and a tool call is made.
output_content_type_format (str | None, optional) – (Experimental) The format string used for the content of a StructuredMessage response.
tool_call_summary_format (str, optional) – The format string used to create the content for a ToolCallSummaryMessage response.
The format string is used to format the tool call summary for every tool call result.
Defaults to “{result}”.
When reflect_on_tool_use is False, a concatenation of all the tool call summaries, separated by a new line character (’n’)
will be returned as the response.
Available variables: {tool_name}, {arguments}, {result}.
For example, “{tool_name}: {result}” will create a summary like “tool_name: result”.
memory (Sequence[Memory] | None, optional) – The memory store to use for the agent. Defaults to None.
metadata (Dict[str, str] | None, optional) – Optional metadata for tracking.


Raises:

ValueError – If tool names are not unique.
ValueError – If handoff names are not unique.
ValueError – If handoff names are not unique from tool names.
ValueError – If maximum number of tool iterations is less than 1.



Examples
Example 1: basic agent
The following example demonstrates how to create an assistant agent with
a model client and generate a response to a simple task.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client)

    result = await agent.run(task="Name two cities in North America.")
    print(result)


asyncio.run(main())


Example 2: model client token streaming
This example demonstrates how to create an assistant agent with
a model client and generate a token stream by setting model_client_stream=True.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_client_stream=True,
    )

    stream = agent.run_stream(task="Name two cities in North America.")
    async for message in stream:
        print(message)


asyncio.run(main())


source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage'
source='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage'
messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None


Example 3: agent with tools
The following example demonstrates how to create an assistant agent with
a model client and a tool, generate a stream of messages for a task, and
print the messages to the console using Console.
The tool is a simple function that returns the current time.
Under the hood, the function is wrapped in a FunctionTool
and used with the agent’s model client. The doc string of the function
is used as the tool description, the function name is used as the tool name,
and the function signature including the type hints is used as the tool arguments.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console


async def get_current_time() -> str:
    return "The current time is 12:00 PM."


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client, tools=[get_current_time])
    await Console(agent.run_stream(task="What is the current time?"))


asyncio.run(main())


Example 4: agent with Model-Context Protocol (MCP) workbench
The following example demonstrates how to create an assistant agent with
a model client and an McpWorkbench for
interacting with a Model-Context Protocol (MCP) server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, McpWorkbench


async def main() -> None:
    params = StdioServerParams(
        command="uvx",
        args=["mcp-server-fetch"],
        read_timeout_seconds=60,
    )

    # You can also use `start()` and `stop()` to manage the session.
    async with McpWorkbench(server_params=params) as workbench:
        model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
        assistant = AssistantAgent(
            name="Assistant",
            model_client=model_client,
            workbench=workbench,
            reflect_on_tool_use=True,
        )
        await Console(
            assistant.run_stream(task="Go to https://github.com/microsoft/autogen and tell me what you see.")
        )


asyncio.run(main())


Example 5: agent with structured output and tool
The following example demonstrates how to create an assistant agent with
a model client configured to use structured output and a tool.
Note that you need to use FunctionTool to create the tool
and the strict=True is required for structured output mode.
Because the model is configured to use structured output, the output
reflection response will be a JSON formatted string.
import asyncio
from typing import Literal

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel


# Define the structured output format.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]


# Define the function to be called as a tool.
def sentiment_analysis(text: str) -> str:
    """Given a text, return the sentiment."""
    return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"


# Create a FunctionTool instance with `strict=True`,
# which is required for structured output mode.
tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)

# Create an OpenAIChatCompletionClient instance that supports structured output.
model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
)

# Create an AssistantAgent instance that uses the tool and model client.
agent = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[tool],
    system_message="Use the tool to analyze sentiment.",
    output_content_type=AgentResponse,
)


async def main() -> None:
    stream = agent.run_stream(task="I am happy today!")
    await Console(stream)


asyncio.run(main())


---------- assistant ----------
[FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{"text":"I am happy today!"}', name='sentiment_analysis')]
---------- assistant ----------
[FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)]
---------- assistant ----------
{"thoughts":"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.","response":"happy"}


Example 6: agent with bounded model context
The following example shows how to use a
BufferedChatCompletionContext
that only keeps the last 2 messages (1 user + 1 assistant).
Bounded model context is useful when the model has a limit on the
number of tokens it can process.
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Create a model client.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        # api_key = "your_openai_api_key"
    )

    # Create a model context that only keeps the last 2 messages (1 user + 1 assistant).
    model_context = BufferedChatCompletionContext(buffer_size=2)

    # Create an AssistantAgent instance with the model client and context.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_context=model_context,
        system_message="You are a helpful assistant.",
    )

    result = await agent.run(task="Name two cities in North America.")
    print(result.messages[-1].content)  # type: ignore

    result = await agent.run(task="My favorite color is blue.")
    print(result.messages[-1].content)  # type: ignore

    result = await agent.run(task="Did I ask you any question?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())


Two cities in North America are New York City and Toronto.
That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite?
No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know!


Example 7: agent with memory
The following example shows how to use a list-based memory with the assistant agent.
The memory is preloaded with some initial content.
Under the hood, the memory is used to update the model context
before making an inference, using the update_context() method.
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.memory import ListMemory, MemoryContent
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Create a model client.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        # api_key = "your_openai_api_key"
    )

    # Create a list-based memory with some initial content.
    memory = ListMemory()
    await memory.add(MemoryContent(content="User likes pizza.", mime_type="text/plain"))
    await memory.add(MemoryContent(content="User dislikes cheese.", mime_type="text/plain"))

    # Create an AssistantAgent instance with the model client and memory.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        memory=[memory],
        system_message="You are a helpful assistant.",
    )

    result = await agent.run(task="What is a good dinner idea?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())


How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea:

**Veggie Tomato Sauce Pizza**
- Start with a pizza crust (store-bought or homemade).
- Spread a layer of marinara or tomato sauce evenly over the crust.
- Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach.
- Add some protein if you’d like, such as grilled chicken or pepperoni (ensure it's cheese-free).
- Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil.
- Bake according to the crust instructions until the edges are golden and the veggies are cooked.

Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner!


Example 8: agent with `o1-mini`
The following example shows how to use o1-mini model with the assistant agent.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="o1-mini",
        # api_key = "your_openai_api_key"
    )
    # The system message is not supported by the o1 series model.
    agent = AssistantAgent(name="assistant", model_client=model_client, system_message=None)

    result = await agent.run(task="What is the capital of France?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())



Note
The o1-preview and o1-mini models do not support system message and function calling.
So the system_message should be set to None and the tools and handoffs should not be set.
See o1 beta limitations for more details.

Example 9: agent using reasoning model with custom model context.
The following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent.
The model context is used to filter out the thought field from the assistant message.
import asyncio
from typing import List

from autogen_agentchat.agents import AssistantAgent
from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily
from autogen_ext.models.ollama import OllamaChatCompletionClient


class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out: List[LLMMessage] = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out


# Create an instance of the model client for DeepSeek R1 hosted locally on Ollama.
model_client = OllamaChatCompletionClient(
    model="deepseek-r1:8b",
    model_info={
        "vision": False,
        "function_calling": False,
        "json_output": False,
        "family": ModelFamily.R1,
        "structured_output": True,
    },
)

agent = AssistantAgent(
    "reasoning_agent",
    model_client=model_client,
    model_context=ReasoningModelContext(),  # Use the custom model context.
)


async def run_reasoning_agent() -> None:
    result = await agent.run(task="What is the capital of France?")
    print(result)


asyncio.run(run_reasoning_agent())




component_config_schema#
alias of AssistantAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.AssistantAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the assistant agent



property model_context: ChatCompletionContext#
The model context in use by the agent.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Process the incoming messages with the assistant agent and yield events/responses as they happen.



async on_reset(cancellation_token: CancellationToken) → None[source]#
Reset the assistant agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.



async save_state() → Mapping[str, Any][source]#
Save the current state of the assistant agent.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client)

    result = await agent.run(task="Name two cities in North America.")
    print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_client_stream=True,
    )

    stream = agent.run_stream(task="Name two cities in North America.")
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage'
source='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage'
messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console


async def get_current_time() -> str:
    return "The current time is 12:00 PM."


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client, tools=[get_current_time])
    await Console(agent.run_stream(task="What is the current time?"))


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, McpWorkbench


async def main() -> None:
    params = StdioServerParams(
        command="uvx",
        args=["mcp-server-fetch"],
        read_timeout_seconds=60,
    )

    # You can also use `start()` and `stop()` to manage the session.
    async with McpWorkbench(server_params=params) as workbench:
        model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
        assistant = AssistantAgent(
            name="Assistant",
            model_client=model_client,
            workbench=workbench,
            reflect_on_tool_use=True,
        )
        await Console(
            assistant.run_stream(task="Go to https://github.com/microsoft/autogen and tell me what you see.")
        )


asyncio.run(main())

```

**示例**:
```python
import asyncio
from typing import Literal

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel


# Define the structured output format.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]


# Define the function to be called as a tool.
def sentiment_analysis(text: str) -> str:
    """Given a text, return the sentiment."""
    return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"


# Create a FunctionTool instance with `strict=True`,
# which is required for structured output mode.
tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)

# Create an OpenAIChatCompletionClient instance that supports structured output.
model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
)

# Create an AssistantAgent instance that uses the tool and model client.
agent = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[tool],
    system_message="Use the tool to analyze sentiment.",
    output_content_type=AgentResponse,
)


async def main() -> None:
    stream = agent.run_stream(task="I am happy today!")
    await Console(stream)


asyncio.run(main())

```

**示例**:
```python
---------- assistant ----------
[FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{"text":"I am happy today!"}', name='sentiment_analysis')]
---------- assistant ----------
[FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)]
---------- assistant ----------
{"thoughts":"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.","response":"happy"}

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Create a model client.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        # api_key = "your_openai_api_key"
    )

    # Create a model context that only keeps the last 2 messages (1 user + 1 assistant).
    model_context = BufferedChatCompletionContext(buffer_size=2)

    # Create an AssistantAgent instance with the model client and context.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_context=model_context,
        system_message="You are a helpful assistant.",
    )

    result = await agent.run(task="Name two cities in North America.")
    print(result.messages[-1].content)  # type: ignore

    result = await agent.run(task="My favorite color is blue.")
    print(result.messages[-1].content)  # type: ignore

    result = await agent.run(task="Did I ask you any question?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())

```

**示例**:
```python
Two cities in North America are New York City and Toronto.
That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite?
No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know!

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.memory import ListMemory, MemoryContent
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Create a model client.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        # api_key = "your_openai_api_key"
    )

    # Create a list-based memory with some initial content.
    memory = ListMemory()
    await memory.add(MemoryContent(content="User likes pizza.", mime_type="text/plain"))
    await memory.add(MemoryContent(content="User dislikes cheese.", mime_type="text/plain"))

    # Create an AssistantAgent instance with the model client and memory.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        memory=[memory],
        system_message="You are a helpful assistant.",
    )

    result = await agent.run(task="What is a good dinner idea?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())

```

**示例**:
```python
How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea:

**Veggie Tomato Sauce Pizza**
- Start with a pizza crust (store-bought or homemade).
- Spread a layer of marinara or tomato sauce evenly over the crust.
- Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach.
- Add some protein if you’d like, such as grilled chicken or pepperoni (ensure it's cheese-free).
- Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil.
- Bake according to the crust instructions until the edges are golden and the veggies are cooked.

Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner!

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="o1-mini",
        # api_key = "your_openai_api_key"
    )
    # The system message is not supported by the o1 series model.
    agent = AssistantAgent(name="assistant", model_client=model_client, system_message=None)

    result = await agent.run(task="What is the capital of France?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())

```

**示例**:
```python
import asyncio
from typing import List

from autogen_agentchat.agents import AssistantAgent
from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily
from autogen_ext.models.ollama import OllamaChatCompletionClient


class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out: List[LLMMessage] = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out


# Create an instance of the model client for DeepSeek R1 hosted locally on Ollama.
model_client = OllamaChatCompletionClient(
    model="deepseek-r1:8b",
    model_info={
        "vision": False,
        "function_calling": False,
        "json_output": False,
        "family": ModelFamily.R1,
        "structured_output": True,
    },
)

agent = AssistantAgent(
    "reasoning_agent",
    model_client=model_client,
    model_context=ReasoningModelContext(),  # Use the custom model context.
)


async def run_reasoning_agent() -> None:
    result = await agent.run(task="What is the capital of France?")
    print(result)


asyncio.run(run_reasoning_agent())

```

```python
component_config_schema#
```

【中文翻译】alias of AssistantAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.AssistantAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the assistant agent

```python
property model_context: ChatCompletionContext#
```

【中文翻译】The model context in use by the agent.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Process the incoming messages with the assistant agent and yield events/responses as they happen.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Reset the assistant agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the current state of the assistant agent.

```python
class BaseChatAgent(name: str, description: str)[source]#
```

【中文翻译】Bases: ChatAgent, ABC, ComponentBase[BaseModel]
Base class for a chat agent.
This abstract class provides a base implementation for a ChatAgent.
To create a new chat agent, subclass this class and implement the
on_messages(), on_reset(), and produced_message_types.
If streaming is required, also implement the on_messages_stream() method.
An agent is considered stateful and maintains its state between calls to
the on_messages() or on_messages_stream() methods.
The agent should store its state in the
agent instance. The agent should also implement the on_reset() method
to reset the agent to its initialization state.

Note
The caller should only pass the new messages to the agent on each call
to the on_messages() or on_messages_stream() method.
Do not pass the entire conversation history to the agent on each call.
This design principle must be followed when creating a new agent.



async close() → None[source]#
Release any resources held by the agent. This is a no-op by default in the
BaseChatAgent class. Subclasses can override this method to
implement custom close behavior.



component_type: ClassVar[ComponentType] = 'agent'#
The logical type of the component.



property description: str#
The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.



async load_state(state: Mapping[str, Any]) → None[source]#
Restore agent from saved state. Default implementation for stateless agents.



property name: str#
The name of the agent. This is used by team to uniquely identify
the agent. It should be unique within the team.



abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_pause(cancellation_token: CancellationToken) → None[source]#
Called when the agent is paused while running in its on_messages() or
on_messages_stream() method. This is a no-op by default in the
BaseChatAgent class. Subclasses can override this method to
implement custom pause behavior.



abstract async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



async on_resume(cancellation_token: CancellationToken) → None[source]#
Called when the agent is resumed from a pause while running in
its on_messages() or on_messages_stream() method.
This is a no-op by default in the BaseChatAgent class.
Subclasses can override this method to implement custom resume behavior.



abstract property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.



async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
Run the agent with the given task and return the result.



async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
Run the agent with the given task and return a stream of messages
and the final task result as the last item in the stream.



async save_state() → Mapping[str, Any][source]#
Export state. Default implementation for stateless agents.

```python
async close() → None[source]#
```

【中文翻译】Release any resources held by the agent. This is a no-op by default in the
BaseChatAgent class. Subclasses can override this method to
implement custom close behavior.

```python
component_type: ClassVar[ComponentType] = 'agent'#
```

【中文翻译】The logical type of the component.

```python
property description: str#
```

【中文翻译】The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Restore agent from saved state. Default implementation for stateless agents.

```python
property name: str#
```

【中文翻译】The name of the agent. This is used by team to uniquely identify
the agent. It should be unique within the team.

```python
abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_pause(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Called when the agent is paused while running in its on_messages() or
on_messages_stream() method. This is a no-op by default in the
BaseChatAgent class. Subclasses can override this method to
implement custom pause behavior.

```python
abstract async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
async on_resume(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Called when the agent is resumed from a pause while running in
its on_messages() or on_messages_stream() method.
This is a no-op by default in the BaseChatAgent class.
Subclasses can override this method to implement custom resume behavior.

```python
abstract property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
```

【中文翻译】Run the agent with the given task and return the result.

```python
async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
```

【中文翻译】Run the agent with the given task and return a stream of messages
and the final task result as the last item in the stream.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Export state. Default implementation for stateless agents.

```python
class CodeExecutorAgent(name: str, code_executor: CodeExecutor, *, model_client: ChatCompletionClient | None = None, model_context: ChatCompletionContext | None = None, model_client_stream: bool = False, max_retries_on_error: int = 0, description: str | None = None, system_message: str | None = DEFAULT_SYSTEM_MESSAGE, sources: Sequence[str] | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[CodeExecutorAgentConfig]
(Experimental) An agent that generates and executes code snippets based on user instructions.

Note
This agent is experimental and may change in future releases.

It is typically used within a team with another agent that generates code snippets
to be executed or alone with model_client provided so that it can generate code
based on user query, execute it and reflect on the code result.
When used with model_client, it will generate code snippets using the model
and execute them using the provided code_executor. The model will also reflect on the
code execution results. The agent will yield the final reflection result from the model
as the final response.
When used without model_client, it will only execute code blocks found in
TextMessage messages and returns the output
of the code execution.

Note
Using AssistantAgent with
PythonCodeExecutionTool
is an alternative to this agent. However, the model for that agent will
have to generate properly escaped code string as a parameter to the tool.


Parameters:

name (str) – The name of the agent.
code_executor (CodeExecutor) – The code executor responsible for executing code received in messages
(DockerCommandLineCodeExecutor recommended. See example below)
model_client (ChatCompletionClient, optional) – The model client to use for inference and generating code.
If not provided, the agent will only execute code blocks found in input messages.
Currently, the model must support structured output mode, which is required for
the automatic retry mechanism to work.
model_client_stream (bool, optional) – If True, the model client will be used in streaming mode.
on_messages_stream() and BaseChatAgent.run_stream() methods will
also yield ModelClientStreamingChunkEvent
messages as the model client produces chunks of response. Defaults to False.
description (str, optional) – The description of the agent. If not provided,
DEFAULT_AGENT_DESCRIPTION will be used.
system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable.
Defaults to DEFAULT_SYSTEM_MESSAGE. This is only used if model_client is provided.
sources (Sequence[str], optional) – Check only messages from the specified agents for the code to execute.
This is useful when the agent is part of a group chat and you want to limit the code execution to messages from specific agents.
If not provided, all messages will be checked for code blocks.
This is only used if model_client is not provided.
max_retries_on_error (int, optional) – The maximum number of retries on error. If the code execution fails, the agent will retry up to this number of times.
If the code execution fails after this number of retries, the agent will yield a reflection result.




Note
It is recommended that the CodeExecutorAgent agent uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running.
Follow the installation instructions for Docker.


Note
The code executor only processes code that is properly formatted in markdown code blocks using triple backticks.
For example:
```python
print("Hello World")
```

# or

```sh
echo "Hello World"
```



In this example, we show how to set up a CodeExecutorAgent agent that uses the
DockerCommandLineCodeExecutor
to execute code snippets in a Docker container. The work_dir parameter indicates where all executed files are first saved locally before being executed in the Docker container.

import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken


async def run_code_executor_agent() -> None:
    # Create a code executor agent that uses a Docker container to execute code.
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")
    await code_executor.start()
    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)

    # Run the agent with a given code snippet.
    task = TextMessage(
        content='''Here is some code
```python
print('Hello world')
```
''',
        source="user",
    )
    response = await code_executor_agent.on_messages([task], CancellationToken())
    print(response.chat_message)

    # Stop the code executor.
    await code_executor.stop()


asyncio.run(run_code_executor_agent())



In this example, we show how to set up a CodeExecutorAgent agent that uses the
DeviceRequest to expose a GPU to the container for cuda-accelerated code execution.

import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken
from docker.types import DeviceRequest


async def run_code_executor_agent() -> None:
    # Create a code executor agent that uses a Docker container to execute code.
    code_executor = DockerCommandLineCodeExecutor(
        work_dir="coding", device_requests=[DeviceRequest(count=-1, capabilities=[["gpu"]])]
    )
    await code_executor.start()
    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)

    # Display the GPU information
    task = TextMessage(
        content='''Here is some code
```bash
nvidia-smi
```
''',
        source="user",
    )
    response = await code_executor_agent.on_messages([task], CancellationToken())
    print(response.chat_message)

    # Stop the code executor.
    await code_executor.stop()


asyncio.run(run_code_executor_agent())



In the following example, we show how to setup CodeExecutorAgent without model_client parameter for executing code blocks generated by other agents in a group chat using DockerCommandLineCodeExecutor

import asyncio

from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console

termination_condition = MaxMessageTermination(3)


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # define the Docker CLI Code Executor
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")

    # start the execution container
    await code_executor.start()

    code_executor_agent = CodeExecutorAgent("code_executor_agent", code_executor=code_executor)
    coder_agent = AssistantAgent("coder_agent", model_client=model_client)

    groupchat = RoundRobinGroupChat(
        participants=[coder_agent, code_executor_agent], termination_condition=termination_condition
    )

    task = "Write python code to print Hello World!"
    await Console(groupchat.run_stream(task=task))

    # stop the execution container
    await code_executor.stop()


asyncio.run(main())


---------- user ----------
Write python code to print Hello World!
---------- coder_agent ----------
Certainly! Here's a simple Python code to print "Hello World!":

```python
print("Hello World!")
```

You can run this code in any Python environment to display the message.
---------- code_executor_agent ----------
Hello World!



In the following example, we show how to setup CodeExecutorAgent with model_client that can generate its own code without the help of any other agent and executing it in DockerCommandLineCodeExecutor

import asyncio

from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.ui import Console

termination_condition = TextMessageTermination("code_executor_agent")


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # define the Docker CLI Code Executor
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")

    # start the execution container
    await code_executor.start()

    code_executor_agent = CodeExecutorAgent(
        "code_executor_agent", code_executor=code_executor, model_client=model_client
    )

    task = "Write python code to print Hello World!"
    await Console(code_executor_agent.run_stream(task=task))

    # stop the execution container
    await code_executor.stop()


asyncio.run(main())


---------- user ----------
Write python code to print Hello World!
---------- code_executor_agent ----------
Certainly! Here is a simple Python code to print "Hello World!" to the console:

```python
print("Hello World!")
```

Let's execute it to confirm the output.
---------- code_executor_agent ----------
Hello World!

---------- code_executor_agent ----------
The code has been executed successfully, and it printed "Hello World!" as expected. If you have any more requests or questions, feel free to ask!





DEFAULT_AGENT_DESCRIPTION = 'A Code Execution Agent that generates and executes Python and shell scripts based on user instructions. Python code should be provided in ```python code blocks, and sh shell scripts should be provided in ```sh code blocks for execution. It ensures correctness, efficiency, and minimal errors while gracefully handling edge cases.'#



DEFAULT_SYSTEM_MESSAGE = 'You are a Code Execution Agent. Your role is to generate and execute Python code based on user instructions, ensuring correctness, efficiency, and minimal errors. Handle edge cases gracefully.'#



DEFAULT_TERMINAL_DESCRIPTION = 'A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).'#



NO_CODE_BLOCKS_FOUND_MESSAGE = 'No code blocks found in the thread. Please provide at least one markdown-encoded code block to execute (i.e., quoting code in ```python or ```sh code blocks).'#



classmethod _from_config(config: CodeExecutorAgentConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → CodeExecutorAgentConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of CodeExecutorAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.CodeExecutorAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_block(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#



async extract_code_blocks_from_messages(messages: Sequence[BaseChatMessage]) → List[CodeBlock][source]#



property model_context: ChatCompletionContext#
The model context in use by the agent.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Process the incoming messages with the assistant agent and yield events/responses as they happen.



async on_reset(cancellation_token: CancellationToken) → None[source]#
Its a no-op as the code executor agent has no mutable state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the code executor agent produces.

**示例**:
```python
```python
print("Hello World")
```

# or

```sh
echo "Hello World"
```

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken


async def run_code_executor_agent() -> None:
    # Create a code executor agent that uses a Docker container to execute code.
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")
    await code_executor.start()
    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)

    # Run the agent with a given code snippet.
    task = TextMessage(
        content='''Here is some code
```python
print('Hello world')
```
''',
        source="user",
    )
    response = await code_executor_agent.on_messages([task], CancellationToken())
    print(response.chat_message)

    # Stop the code executor.
    await code_executor.stop()


asyncio.run(run_code_executor_agent())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken
from docker.types import DeviceRequest


async def run_code_executor_agent() -> None:
    # Create a code executor agent that uses a Docker container to execute code.
    code_executor = DockerCommandLineCodeExecutor(
        work_dir="coding", device_requests=[DeviceRequest(count=-1, capabilities=[["gpu"]])]
    )
    await code_executor.start()
    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)

    # Display the GPU information
    task = TextMessage(
        content='''Here is some code
```bash
nvidia-smi
```
''',
        source="user",
    )
    response = await code_executor_agent.on_messages([task], CancellationToken())
    print(response.chat_message)

    # Stop the code executor.
    await code_executor.stop()


asyncio.run(run_code_executor_agent())

```

**示例**:
```python
import asyncio

from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console

termination_condition = MaxMessageTermination(3)


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # define the Docker CLI Code Executor
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")

    # start the execution container
    await code_executor.start()

    code_executor_agent = CodeExecutorAgent("code_executor_agent", code_executor=code_executor)
    coder_agent = AssistantAgent("coder_agent", model_client=model_client)

    groupchat = RoundRobinGroupChat(
        participants=[coder_agent, code_executor_agent], termination_condition=termination_condition
    )

    task = "Write python code to print Hello World!"
    await Console(groupchat.run_stream(task=task))

    # stop the execution container
    await code_executor.stop()


asyncio.run(main())

```

**示例**:
```python
---------- user ----------
Write python code to print Hello World!
---------- coder_agent ----------
Certainly! Here's a simple Python code to print "Hello World!":

```python
print("Hello World!")
```

You can run this code in any Python environment to display the message.
---------- code_executor_agent ----------
Hello World!

```

**示例**:
```python
import asyncio

from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.ui import Console

termination_condition = TextMessageTermination("code_executor_agent")


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # define the Docker CLI Code Executor
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")

    # start the execution container
    await code_executor.start()

    code_executor_agent = CodeExecutorAgent(
        "code_executor_agent", code_executor=code_executor, model_client=model_client
    )

    task = "Write python code to print Hello World!"
    await Console(code_executor_agent.run_stream(task=task))

    # stop the execution container
    await code_executor.stop()


asyncio.run(main())

```

**示例**:
```python
---------- user ----------
Write python code to print Hello World!
---------- code_executor_agent ----------
Certainly! Here is a simple Python code to print "Hello World!" to the console:

```python
print("Hello World!")
```

Let's execute it to confirm the output.
---------- code_executor_agent ----------
Hello World!

---------- code_executor_agent ----------
The code has been executed successfully, and it printed "Hello World!" as expected. If you have any more requests or questions, feel free to ask!

```

```python
DEFAULT_AGENT_DESCRIPTION = 'A Code Execution Agent that generates and executes Python and shell scripts based on user instructions. Python code should be provided in ```python code blocks, and sh shell scripts should be provided in ```sh code blocks for execution. It ensures correctness, efficiency, and minimal errors while gracefully handling edge cases.'#
```

```python
DEFAULT_SYSTEM_MESSAGE = 'You are a Code Execution Agent. Your role is to generate and execute Python code based on user instructions, ensuring correctness, efficiency, and minimal errors. Handle edge cases gracefully.'#
```

```python
DEFAULT_TERMINAL_DESCRIPTION = 'A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).'#
```

```python
NO_CODE_BLOCKS_FOUND_MESSAGE = 'No code blocks found in the thread. Please provide at least one markdown-encoded code block to execute (i.e., quoting code in ```python or ```sh code blocks).'#
```

```python
classmethod _from_config(config: CodeExecutorAgentConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → CodeExecutorAgentConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of CodeExecutorAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.CodeExecutorAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_block(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
```

```python
async extract_code_blocks_from_messages(messages: Sequence[BaseChatMessage]) → List[CodeBlock][source]#
```

```python
property model_context: ChatCompletionContext#
```

【中文翻译】The model context in use by the agent.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Process the incoming messages with the assistant agent and yield events/responses as they happen.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Its a no-op as the code executor agent has no mutable state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the code executor agent produces.

```python
class MessageFilterAgent(name: str, wrapped_agent: BaseChatAgent, filter: MessageFilterConfig)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[MessageFilterAgentConfig]
A wrapper agent that filters incoming messages before passing them to the inner agent.

Warning
This is an experimental feature, and the API will change in the future releases.

This is useful in scenarios like multi-agent workflows where an agent should only
process a subset of the full message history—for example, only the last message
from each upstream agent, or only the first message from a specific source.
Filtering is configured using MessageFilterConfig, which supports:
- Filtering by message source (e.g., only messages from “user” or another agent)
- Selecting the first N or last N messages from each source
- If position is None, all messages from that source are included
This agent is compatible with both direct message passing and team-based execution
such as GraphFlow.
Example
>>> agent_a = MessageFilterAgent(
...     name="A",
...     wrapped_agent=some_other_agent,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=2),
...         ]
...     ),
... )



Example use case with Graph:Suppose you have a looping multi-agent graph: A → B → A → B → C.
You want:
- A to only see the user message and the last message from B
- B to see the user message, last message from A, and its own prior responses (for reflection)
- C to see the user message and the last message from B
Wrap the agents like so:
>>> agent_a = MessageFilterAgent(
...     name="A",
...     wrapped_agent=agent_a_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=1),
...         ]
...     ),
... )


>>> agent_b = MessageFilterAgent(
...     name="B",
...     wrapped_agent=agent_b_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="A", position="last", count=1),
...             PerSourceFilter(source="B", position="last", count=10),
...         ]
...     ),
... )


>>> agent_c = MessageFilterAgent(
...     name="C",
...     wrapped_agent=agent_c_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=1),
...         ]
...     ),
... )


Then define the graph:
>>> graph = DiGraph(
...     nodes={
...         "A": DiGraphNode(name="A", edges=[DiGraphEdge(target="B")]),
...         "B": DiGraphNode(
...             name="B",
...             edges=[
...                 DiGraphEdge(target="C", condition="exit"),
...                 DiGraphEdge(target="A", condition="loop"),
...             ],
...         ),
...         "C": DiGraphNode(name="C", edges=[]),
...     },
...     default_start_node="A",
... )


This will ensure each agent sees only what is needed for its decision or action logic.




classmethod _from_config(config: MessageFilterAgentConfig) → MessageFilterAgent[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → MessageFilterAgentConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of MessageFilterAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.MessageFilterAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

**示例**:
```python
>>> agent_a = MessageFilterAgent(
...     name="A",
...     wrapped_agent=some_other_agent,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=2),
...         ]
...     ),
... )

```

**示例**:
```python
>>> agent_a = MessageFilterAgent(
...     name="A",
...     wrapped_agent=agent_a_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=1),
...         ]
...     ),
... )

```

**示例**:
```python
>>> agent_b = MessageFilterAgent(
...     name="B",
...     wrapped_agent=agent_b_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="A", position="last", count=1),
...             PerSourceFilter(source="B", position="last", count=10),
...         ]
...     ),
... )

```

**示例**:
```python
>>> agent_c = MessageFilterAgent(
...     name="C",
...     wrapped_agent=agent_c_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=1),
...         ]
...     ),
... )

```

**示例**:
```python
>>> graph = DiGraph(
...     nodes={
...         "A": DiGraphNode(name="A", edges=[DiGraphEdge(target="B")]),
...         "B": DiGraphNode(
...             name="B",
...             edges=[
...                 DiGraphEdge(target="C", condition="exit"),
...                 DiGraphEdge(target="A", condition="loop"),
...             ],
...         ),
...         "C": DiGraphNode(name="C", edges=[]),
...     },
...     default_start_node="A",
... )

```

```python
classmethod _from_config(config: MessageFilterAgentConfig) → MessageFilterAgent[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → MessageFilterAgentConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of MessageFilterAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.MessageFilterAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
pydantic model MessageFilterConfig[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "MessageFilterConfig",
   "type": "object",
   "properties": {
      "per_source": {
         "items": {
            "$ref": "#/$defs/PerSourceFilter"
         },
         "title": "Per Source",
         "type": "array"
      }
   },
   "$defs": {
      "PerSourceFilter": {
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "position": {
               "anyOf": [
                  {
                     "enum": [
                        "first",
                        "last"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Position"
            },
            "count": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Count"
            }
         },
         "required": [
            "source"
         ],
         "title": "PerSourceFilter",
         "type": "object"
      }
   },
   "required": [
      "per_source"
   ]
}



Fields:

per_source (List[autogen_agentchat.agents._message_filter_agent.PerSourceFilter])





field per_source: List[PerSourceFilter] [Required]#

**示例**:
```python
{
   "title": "MessageFilterConfig",
   "type": "object",
   "properties": {
      "per_source": {
         "items": {
            "$ref": "#/$defs/PerSourceFilter"
         },
         "title": "Per Source",
         "type": "array"
      }
   },
   "$defs": {
      "PerSourceFilter": {
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "position": {
               "anyOf": [
                  {
                     "enum": [
                        "first",
                        "last"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Position"
            },
            "count": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Count"
            }
         },
         "required": [
            "source"
         ],
         "title": "PerSourceFilter",
         "type": "object"
      }
   },
   "required": [
      "per_source"
   ]
}

```

```python
field per_source: List[PerSourceFilter] [Required]#
```

```python
pydantic model PerSourceFilter[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "PerSourceFilter",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "position": {
         "anyOf": [
            {
               "enum": [
                  "first",
                  "last"
               ],
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Position"
      },
      "count": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Count"
      }
   },
   "required": [
      "source"
   ]
}



Fields:

count (int | None)
position (Literal['first', 'last'] | None)
source (str)





field count: int | None = None#



field position: Literal['first', 'last'] | None = None#



field source: str [Required]#

**示例**:
```python
{
   "title": "PerSourceFilter",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "position": {
         "anyOf": [
            {
               "enum": [
                  "first",
                  "last"
               ],
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Position"
      },
      "count": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Count"
      }
   },
   "required": [
      "source"
   ]
}

```

```python
field count: int | None = None#
```

```python
field position: Literal['first', 'last'] | None = None#
```

```python
field source: str [Required]#
```

```python
class SocietyOfMindAgent(name: str, team: Team, model_client: ChatCompletionClient, *, description: str = DEFAULT_DESCRIPTION, instruction: str = DEFAULT_INSTRUCTION, response_prompt: str = DEFAULT_RESPONSE_PROMPT, model_context: ChatCompletionContext | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[SocietyOfMindAgentConfig]
An agent that uses an inner team of agents to generate responses.
Each time the agent’s on_messages() or on_messages_stream()
method is called, it runs the inner team of agents and then uses the
model client to generate a response based on the inner team’s messages.
Once the response is generated, the agent resets the inner team by
calling Team.reset().
Limit context size sent to the model:
You can limit the number of messages sent to the model by setting
the model_context parameter to a BufferedChatCompletionContext.
This will limit the number of recent messages sent to the model and can be useful
when the model has a limit on the number of tokens it can process.
You can also create your own model context by subclassing
ChatCompletionContext.

Parameters:

name (str) – The name of the agent.
team (Team) – The team of agents to use.
model_client (ChatCompletionClient) – The model client to use for preparing responses.
description (str, optional) – The description of the agent.
instruction (str, optional) – The instruction to use when generating a response using the inner team’s messages.
Defaults to DEFAULT_INSTRUCTION. It assumes the role of ‘system’.
response_prompt (str, optional) – The response prompt to use when generating a response using the inner team’s messages.
Defaults to DEFAULT_RESPONSE_PROMPT. It assumes the role of ‘system’.
model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.



Example:
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("assistant1", model_client=model_client, system_message="You are a writer, write well.")
    agent2 = AssistantAgent(
        "assistant2",
        model_client=model_client,
        system_message="You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.",
    )
    inner_termination = TextMentionTermination("APPROVE")
    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)

    society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client)

    agent3 = AssistantAgent(
        "assistant3", model_client=model_client, system_message="Translate the text to Spanish."
    )
    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)

    stream = team.run_stream(task="Write a short story with a surprising ending.")
    await Console(stream)


asyncio.run(main())




DEFAULT_DESCRIPTION = 'An agent that uses an inner team of agents to generate responses.'#
The default description for a SocietyOfMindAgent.

Type:
str





DEFAULT_INSTRUCTION = 'Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:'#
The default instruction to use when generating a response using the
inner team’s messages. The instruction will be prepended to the inner team’s
messages when generating a response using the model. It assumes the role of
‘system’.

Type:
str





DEFAULT_RESPONSE_PROMPT = 'Output a standalone response to the original request, without mentioning any of the intermediate discussion.'#
The default response prompt to use when generating a response using
the inner team’s messages. It assumes the role of ‘system’.

Type:
str





classmethod _from_config(config: SocietyOfMindAgentConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → SocietyOfMindAgentConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of SocietyOfMindAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.SocietyOfMindAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async load_state(state: Mapping[str, Any]) → None[source]#
Restore agent from saved state. Default implementation for stateless agents.



property model_context: ChatCompletionContext#
The model context in use by the agent.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.



async save_state() → Mapping[str, Any][source]#
Export state. Default implementation for stateless agents.

**示例**:
```python
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("assistant1", model_client=model_client, system_message="You are a writer, write well.")
    agent2 = AssistantAgent(
        "assistant2",
        model_client=model_client,
        system_message="You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.",
    )
    inner_termination = TextMentionTermination("APPROVE")
    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)

    society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client)

    agent3 = AssistantAgent(
        "assistant3", model_client=model_client, system_message="Translate the text to Spanish."
    )
    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)

    stream = team.run_stream(task="Write a short story with a surprising ending.")
    await Console(stream)


asyncio.run(main())

```

```python
DEFAULT_DESCRIPTION = 'An agent that uses an inner team of agents to generate responses.'#
```

【中文翻译】The default description for a SocietyOfMindAgent.

Type:
str

```python
DEFAULT_INSTRUCTION = 'Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:'#
```

【中文翻译】The default instruction to use when generating a response using the
inner team’s messages. The instruction will be prepended to the inner team’s
messages when generating a response using the model. It assumes the role of
‘system’.

Type:
str

```python
DEFAULT_RESPONSE_PROMPT = 'Output a standalone response to the original request, without mentioning any of the intermediate discussion.'#
```

【中文翻译】The default response prompt to use when generating a response using
the inner team’s messages. It assumes the role of ‘system’.

Type:
str

```python
classmethod _from_config(config: SocietyOfMindAgentConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → SocietyOfMindAgentConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of SocietyOfMindAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.SocietyOfMindAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Restore agent from saved state. Default implementation for stateless agents.

```python
property model_context: ChatCompletionContext#
```

【中文翻译】The model context in use by the agent.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Export state. Default implementation for stateless agents.

```python
class UserProxyAgent(name: str, *, description: str = 'A human user', input_func: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]] | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[UserProxyAgentConfig]
An agent that can represent a human user through an input function.
This agent can be used to represent a human user in a chat system by providing a custom input function.

Note
Using UserProxyAgent puts a running team in a temporary blocked
state until the user responds. So it is important to time out the user input
function and cancel using the CancellationToken if the user does not respond.
The input function should also handle exceptions and return a default response if needed.
For typical use cases that involve
slow human responses, it is recommended to use termination conditions
such as HandoffTermination or SourceMatchTermination
to stop the running team and return the control to the application.
You can run the team again with the user input. This way, the state of the team
can be saved and restored when the user responds.
See Human-in-the-loop for more information.


Parameters:

name (str) – The name of the agent.
description (str, optional) – A description of the agent.
input_func (Optional[Callable[[str], str]], Callable[[str, Optional[CancellationToken]], Awaitable[str]]) – A function that takes a prompt and returns a user input string.



For examples of integrating with web and UI frameworks, see the following:

FastAPI
ChainLit

Example
Simple usage case:
import asyncio
from autogen_core import CancellationToken
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import TextMessage


async def simple_user_agent():
    agent = UserProxyAgent("user_proxy")
    response = await asyncio.create_task(
        agent.on_messages(
            [TextMessage(content="What is your name? ", source="user")],
            cancellation_token=CancellationToken(),
        )
    )
    assert isinstance(response.chat_message, TextMessage)
    print(f"Your name is {response.chat_message.content}")


Example
Cancellable usage case:
import asyncio
from typing import Any
from autogen_core import CancellationToken
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import TextMessage


token = CancellationToken()
agent = UserProxyAgent("user_proxy")


async def timeout(delay: float):
    await asyncio.sleep(delay)


def cancellation_callback(task: asyncio.Task[Any]):
    token.cancel()


async def cancellable_user_agent():
    try:
        timeout_task = asyncio.create_task(timeout(3))
        timeout_task.add_done_callback(cancellation_callback)
        agent_task = asyncio.create_task(
            agent.on_messages(
                [TextMessage(content="What is your name? ", source="user")],
                cancellation_token=token,
            )
        )
        response = await agent_task
        assert isinstance(response.chat_message, TextMessage)
        print(f"Your name is {response.chat_message.content}")
    except Exception as e:
        print(f"Exception: {e}")
    except BaseException as e:
        print(f"BaseException: {e}")




class InputRequestContext[source]#
Bases: object


classmethod request_id() → str[source]#




classmethod _from_config(config: UserProxyAgentConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → UserProxyAgentConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of UserProxyAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.UserProxyAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'agent'#
The logical type of the component.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handle incoming messages by requesting user input.



async on_reset(cancellation_token: CancellationToken | None = None) → None[source]#
Reset agent state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
Message types this agent can produce.

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import TextMessage


async def simple_user_agent():
    agent = UserProxyAgent("user_proxy")
    response = await asyncio.create_task(
        agent.on_messages(
            [TextMessage(content="What is your name? ", source="user")],
            cancellation_token=CancellationToken(),
        )
    )
    assert isinstance(response.chat_message, TextMessage)
    print(f"Your name is {response.chat_message.content}")

```

**示例**:
```python
import asyncio
from typing import Any
from autogen_core import CancellationToken
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import TextMessage


token = CancellationToken()
agent = UserProxyAgent("user_proxy")


async def timeout(delay: float):
    await asyncio.sleep(delay)


def cancellation_callback(task: asyncio.Task[Any]):
    token.cancel()


async def cancellable_user_agent():
    try:
        timeout_task = asyncio.create_task(timeout(3))
        timeout_task.add_done_callback(cancellation_callback)
        agent_task = asyncio.create_task(
            agent.on_messages(
                [TextMessage(content="What is your name? ", source="user")],
                cancellation_token=token,
            )
        )
        response = await agent_task
        assert isinstance(response.chat_message, TextMessage)
        print(f"Your name is {response.chat_message.content}")
    except Exception as e:
        print(f"Exception: {e}")
    except BaseException as e:
        print(f"BaseException: {e}")

```

```python
class InputRequestContext[source]#
```

【中文翻译】Bases: object


classmethod request_id() → str[source]#

```python
classmethod request_id() → str[source]#
```

```python
classmethod _from_config(config: UserProxyAgentConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → UserProxyAgentConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of UserProxyAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.UserProxyAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'agent'#
```

【中文翻译】The logical type of the component.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handle incoming messages by requesting user input.

```python
async on_reset(cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Reset agent state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】Message types this agent can produce.

【中文翻译】This module initializes various pre-defined agents provided by the package.
BaseChatAgent is the base class for all agents in AgentChat.

【中文翻译】previous

【中文翻译】autogen_agentchat.messages

【中文翻译】next

【中文翻译】autogen_agentchat.tools

### autogen_agentchat.agents {autogen_agentchatagents}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html)

```python
class AssistantAgent(name: str, model_client: ChatCompletionClient, *, tools: List[BaseTool[Any, Any] | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, workbench: Workbench | None = None, handoffs: List[Handoff | str] | None = None, model_context: ChatCompletionContext | None = None, description: str = 'An agent that provides assistance with ability to use tools.', system_message: str | None = 'You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.', model_client_stream: bool = False, reflect_on_tool_use: bool | None = None, tool_call_summary_format: str = '{result}', output_content_type: type[BaseModel] | None = None, output_content_type_format: str | None = None, memory: Sequence[Memory] | None = None, metadata: Dict[str, str] | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[AssistantAgentConfig]
An agent that provides assistance with tool use.
The on_messages() returns a Response
in which chat_message is the final
response message.
The on_messages_stream() creates an async generator that produces
the inner messages as they are created, and the Response
object as the last item before closing the generator.
The BaseChatAgent.run() method returns a TaskResult
containing the messages produced by the agent. In the list of messages,
messages,
the last message is the final response message.
The BaseChatAgent.run_stream() method creates an async generator that produces
the inner messages as they are created, and the TaskResult
object as the last item before closing the generator.

Attention
The caller must only pass the new messages to the agent on each call
to the on_messages(), on_messages_stream(), BaseChatAgent.run(),
or BaseChatAgent.run_stream() methods.
The agent maintains its state between calls to these methods.
Do not pass the entire conversation history to the agent on each call.


Warning
The assistant agent is not thread-safe or coroutine-safe.
It should not be shared between multiple tasks or coroutines, and it should
not call its methods concurrently.

The following diagram shows how the assistant agent works:

Structured output:
If the output_content_type is set, the agent will respond with a StructuredMessage
instead of a TextMessage in the final response by default.

Note
Currently, setting output_content_type prevents the agent from being
able to call load_component and dum_component methods for serializable
configuration. This will be fixed soon in the future.

Tool call behavior:

If the model returns no tool call, then the response is immediately returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message.

When the model returns tool calls, they will be executed right away:
When reflect_on_tool_use is False, the tool call results are returned as a ToolCallSummaryMessage in chat_message. tool_call_summary_format can be used to customize the tool call summary.
When reflect_on_tool_use is True, the another model inference is made using the tool calls and results, and final response is returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message.
reflect_on_tool_use is set to True by default when output_content_type is set.
reflect_on_tool_use is set to False by default when output_content_type is not set.




If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient.


Tip
By default, the tool call results are returned as response when tool calls are made.
So it is recommended to pay attention to the formatting of the tools return values,
especially if another agent is expecting them in a specific format.
Use tool_call_summary_format to customize the tool call summary, if needed.

Hand off behavior:

If a handoff is triggered, a HandoffMessage will be returned in chat_message.
If there are tool calls, they will also be executed right away before returning the handoff.
The tool calls and results are passed to the target agent through context.


Note
If multiple handoffs are detected, only the first handoff is executed.
To avoid this, disable parallel tool calls in the model client configuration.

Limit context size sent to the model:
You can limit the number of messages sent to the model by setting
the model_context parameter to a BufferedChatCompletionContext.
This will limit the number of recent messages sent to the model and can be useful
when the model has a limit on the number of tokens it can process.
Another option is to use a TokenLimitedChatCompletionContext
which will limit the number of tokens sent to the model.
You can also create your own model context by subclassing
ChatCompletionContext.
Streaming mode:
The assistant agent can be used in streaming mode by setting model_client_stream=True.
In this mode, the on_messages_stream() and BaseChatAgent.run_stream() methods will also yield
ModelClientStreamingChunkEvent
messages as the model client produces chunks of response.
The chunk messages will not be included in the final response’s inner messages.

Parameters:

name (str) – The name of the agent.
model_client (ChatCompletionClient) – The model client to use for inference.
tools (List[BaseTool[Any, Any]  | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – The tools to register with the agent.
workbench (Workbench | None, optional) – The workbench to use for the agent.
Tools cannot be used when workbench is set and vice versa.
handoffs (List[HandoffBase | str] | None, optional) – The handoff configurations for the agent,
allowing it to transfer to other agents by responding with a HandoffMessage.
The transfer is only executed when the team is in Swarm.
If a handoff is a string, it should represent the target agent’s name.
model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.
description (str, optional) – The description of the agent.
system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable.
model_client_stream (bool, optional) – If True, the model client will be used in streaming mode.
on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent
messages as the model client produces chunks of response. Defaults to False.
reflect_on_tool_use (bool, optional) – If True, the agent will make another model inference using the tool call and result
to generate a response. If False, the tool call result will be returned as the response. By default, if output_content_type is set, this will be True;
if output_content_type is not set, this will be False.
output_content_type (type[BaseModel] | None, optional) – The output content type for StructuredMessage response as a Pydantic model.
This will be used with the model client to generate structured output.
If this is set, the agent will respond with a StructuredMessage instead of a TextMessage
in the final response, unless reflect_on_tool_use is False and a tool call is made.
output_content_type_format (str | None, optional) – (Experimental) The format string used for the content of a StructuredMessage response.
tool_call_summary_format (str, optional) – The format string used to create the content for a ToolCallSummaryMessage response.
The format string is used to format the tool call summary for every tool call result.
Defaults to “{result}”.
When reflect_on_tool_use is False, a concatenation of all the tool call summaries, separated by a new line character (’n’)
will be returned as the response.
Available variables: {tool_name}, {arguments}, {result}.
For example, “{tool_name}: {result}” will create a summary like “tool_name: result”.
memory (Sequence[Memory] | None, optional) – The memory store to use for the agent. Defaults to None.
metadata (Dict[str, str] | None, optional) – Optional metadata for tracking.


Raises:

ValueError – If tool names are not unique.
ValueError – If handoff names are not unique.
ValueError – If handoff names are not unique from tool names.
ValueError – If maximum number of tool iterations is less than 1.



Examples
Example 1: basic agent
The following example demonstrates how to create an assistant agent with
a model client and generate a response to a simple task.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client)

    result = await agent.run(task="Name two cities in North America.")
    print(result)


asyncio.run(main())


Example 2: model client token streaming
This example demonstrates how to create an assistant agent with
a model client and generate a token stream by setting model_client_stream=True.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_client_stream=True,
    )

    stream = agent.run_stream(task="Name two cities in North America.")
    async for message in stream:
        print(message)


asyncio.run(main())


source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage'
source='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage'
messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None


Example 3: agent with tools
The following example demonstrates how to create an assistant agent with
a model client and a tool, generate a stream of messages for a task, and
print the messages to the console using Console.
The tool is a simple function that returns the current time.
Under the hood, the function is wrapped in a FunctionTool
and used with the agent’s model client. The doc string of the function
is used as the tool description, the function name is used as the tool name,
and the function signature including the type hints is used as the tool arguments.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console


async def get_current_time() -> str:
    return "The current time is 12:00 PM."


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client, tools=[get_current_time])
    await Console(agent.run_stream(task="What is the current time?"))


asyncio.run(main())


Example 4: agent with Model-Context Protocol (MCP) workbench
The following example demonstrates how to create an assistant agent with
a model client and an McpWorkbench for
interacting with a Model-Context Protocol (MCP) server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, McpWorkbench


async def main() -> None:
    params = StdioServerParams(
        command="uvx",
        args=["mcp-server-fetch"],
        read_timeout_seconds=60,
    )

    # You can also use `start()` and `stop()` to manage the session.
    async with McpWorkbench(server_params=params) as workbench:
        model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
        assistant = AssistantAgent(
            name="Assistant",
            model_client=model_client,
            workbench=workbench,
            reflect_on_tool_use=True,
        )
        await Console(
            assistant.run_stream(task="Go to https://github.com/microsoft/autogen and tell me what you see.")
        )


asyncio.run(main())


Example 5: agent with structured output and tool
The following example demonstrates how to create an assistant agent with
a model client configured to use structured output and a tool.
Note that you need to use FunctionTool to create the tool
and the strict=True is required for structured output mode.
Because the model is configured to use structured output, the output
reflection response will be a JSON formatted string.
import asyncio
from typing import Literal

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel


# Define the structured output format.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]


# Define the function to be called as a tool.
def sentiment_analysis(text: str) -> str:
    """Given a text, return the sentiment."""
    return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"


# Create a FunctionTool instance with `strict=True`,
# which is required for structured output mode.
tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)

# Create an OpenAIChatCompletionClient instance that supports structured output.
model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
)

# Create an AssistantAgent instance that uses the tool and model client.
agent = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[tool],
    system_message="Use the tool to analyze sentiment.",
    output_content_type=AgentResponse,
)


async def main() -> None:
    stream = agent.run_stream(task="I am happy today!")
    await Console(stream)


asyncio.run(main())


---------- assistant ----------
[FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{"text":"I am happy today!"}', name='sentiment_analysis')]
---------- assistant ----------
[FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)]
---------- assistant ----------
{"thoughts":"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.","response":"happy"}


Example 6: agent with bounded model context
The following example shows how to use a
BufferedChatCompletionContext
that only keeps the last 2 messages (1 user + 1 assistant).
Bounded model context is useful when the model has a limit on the
number of tokens it can process.
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Create a model client.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        # api_key = "your_openai_api_key"
    )

    # Create a model context that only keeps the last 2 messages (1 user + 1 assistant).
    model_context = BufferedChatCompletionContext(buffer_size=2)

    # Create an AssistantAgent instance with the model client and context.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_context=model_context,
        system_message="You are a helpful assistant.",
    )

    result = await agent.run(task="Name two cities in North America.")
    print(result.messages[-1].content)  # type: ignore

    result = await agent.run(task="My favorite color is blue.")
    print(result.messages[-1].content)  # type: ignore

    result = await agent.run(task="Did I ask you any question?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())


Two cities in North America are New York City and Toronto.
That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite?
No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know!


Example 7: agent with memory
The following example shows how to use a list-based memory with the assistant agent.
The memory is preloaded with some initial content.
Under the hood, the memory is used to update the model context
before making an inference, using the update_context() method.
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.memory import ListMemory, MemoryContent
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Create a model client.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        # api_key = "your_openai_api_key"
    )

    # Create a list-based memory with some initial content.
    memory = ListMemory()
    await memory.add(MemoryContent(content="User likes pizza.", mime_type="text/plain"))
    await memory.add(MemoryContent(content="User dislikes cheese.", mime_type="text/plain"))

    # Create an AssistantAgent instance with the model client and memory.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        memory=[memory],
        system_message="You are a helpful assistant.",
    )

    result = await agent.run(task="What is a good dinner idea?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())


How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea:

**Veggie Tomato Sauce Pizza**
- Start with a pizza crust (store-bought or homemade).
- Spread a layer of marinara or tomato sauce evenly over the crust.
- Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach.
- Add some protein if you’d like, such as grilled chicken or pepperoni (ensure it's cheese-free).
- Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil.
- Bake according to the crust instructions until the edges are golden and the veggies are cooked.

Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner!


Example 8: agent with `o1-mini`
The following example shows how to use o1-mini model with the assistant agent.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="o1-mini",
        # api_key = "your_openai_api_key"
    )
    # The system message is not supported by the o1 series model.
    agent = AssistantAgent(name="assistant", model_client=model_client, system_message=None)

    result = await agent.run(task="What is the capital of France?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())



Note
The o1-preview and o1-mini models do not support system message and function calling.
So the system_message should be set to None and the tools and handoffs should not be set.
See o1 beta limitations for more details.

Example 9: agent using reasoning model with custom model context.
The following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent.
The model context is used to filter out the thought field from the assistant message.
import asyncio
from typing import List

from autogen_agentchat.agents import AssistantAgent
from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily
from autogen_ext.models.ollama import OllamaChatCompletionClient


class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out: List[LLMMessage] = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out


# Create an instance of the model client for DeepSeek R1 hosted locally on Ollama.
model_client = OllamaChatCompletionClient(
    model="deepseek-r1:8b",
    model_info={
        "vision": False,
        "function_calling": False,
        "json_output": False,
        "family": ModelFamily.R1,
        "structured_output": True,
    },
)

agent = AssistantAgent(
    "reasoning_agent",
    model_client=model_client,
    model_context=ReasoningModelContext(),  # Use the custom model context.
)


async def run_reasoning_agent() -> None:
    result = await agent.run(task="What is the capital of France?")
    print(result)


asyncio.run(run_reasoning_agent())




component_config_schema#
alias of AssistantAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.AssistantAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the assistant agent



property model_context: ChatCompletionContext#
The model context in use by the agent.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Process the incoming messages with the assistant agent and yield events/responses as they happen.



async on_reset(cancellation_token: CancellationToken) → None[source]#
Reset the assistant agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.



async save_state() → Mapping[str, Any][source]#
Save the current state of the assistant agent.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client)

    result = await agent.run(task="Name two cities in North America.")
    print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_client_stream=True,
    )

    stream = agent.run_stream(task="Name two cities in North America.")
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage'
source='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage'
messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console


async def get_current_time() -> str:
    return "The current time is 12:00 PM."


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client, tools=[get_current_time])
    await Console(agent.run_stream(task="What is the current time?"))


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, McpWorkbench


async def main() -> None:
    params = StdioServerParams(
        command="uvx",
        args=["mcp-server-fetch"],
        read_timeout_seconds=60,
    )

    # You can also use `start()` and `stop()` to manage the session.
    async with McpWorkbench(server_params=params) as workbench:
        model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
        assistant = AssistantAgent(
            name="Assistant",
            model_client=model_client,
            workbench=workbench,
            reflect_on_tool_use=True,
        )
        await Console(
            assistant.run_stream(task="Go to https://github.com/microsoft/autogen and tell me what you see.")
        )


asyncio.run(main())

```

**示例**:
```python
import asyncio
from typing import Literal

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel


# Define the structured output format.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]


# Define the function to be called as a tool.
def sentiment_analysis(text: str) -> str:
    """Given a text, return the sentiment."""
    return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"


# Create a FunctionTool instance with `strict=True`,
# which is required for structured output mode.
tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)

# Create an OpenAIChatCompletionClient instance that supports structured output.
model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
)

# Create an AssistantAgent instance that uses the tool and model client.
agent = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[tool],
    system_message="Use the tool to analyze sentiment.",
    output_content_type=AgentResponse,
)


async def main() -> None:
    stream = agent.run_stream(task="I am happy today!")
    await Console(stream)


asyncio.run(main())

```

**示例**:
```python
---------- assistant ----------
[FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{"text":"I am happy today!"}', name='sentiment_analysis')]
---------- assistant ----------
[FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)]
---------- assistant ----------
{"thoughts":"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.","response":"happy"}

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Create a model client.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        # api_key = "your_openai_api_key"
    )

    # Create a model context that only keeps the last 2 messages (1 user + 1 assistant).
    model_context = BufferedChatCompletionContext(buffer_size=2)

    # Create an AssistantAgent instance with the model client and context.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_context=model_context,
        system_message="You are a helpful assistant.",
    )

    result = await agent.run(task="Name two cities in North America.")
    print(result.messages[-1].content)  # type: ignore

    result = await agent.run(task="My favorite color is blue.")
    print(result.messages[-1].content)  # type: ignore

    result = await agent.run(task="Did I ask you any question?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())

```

**示例**:
```python
Two cities in North America are New York City and Toronto.
That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite?
No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know!

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.memory import ListMemory, MemoryContent
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Create a model client.
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        # api_key = "your_openai_api_key"
    )

    # Create a list-based memory with some initial content.
    memory = ListMemory()
    await memory.add(MemoryContent(content="User likes pizza.", mime_type="text/plain"))
    await memory.add(MemoryContent(content="User dislikes cheese.", mime_type="text/plain"))

    # Create an AssistantAgent instance with the model client and memory.
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        memory=[memory],
        system_message="You are a helpful assistant.",
    )

    result = await agent.run(task="What is a good dinner idea?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())

```

**示例**:
```python
How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea:

**Veggie Tomato Sauce Pizza**
- Start with a pizza crust (store-bought or homemade).
- Spread a layer of marinara or tomato sauce evenly over the crust.
- Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach.
- Add some protein if you’d like, such as grilled chicken or pepperoni (ensure it's cheese-free).
- Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil.
- Bake according to the crust instructions until the edges are golden and the veggies are cooked.

Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner!

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="o1-mini",
        # api_key = "your_openai_api_key"
    )
    # The system message is not supported by the o1 series model.
    agent = AssistantAgent(name="assistant", model_client=model_client, system_message=None)

    result = await agent.run(task="What is the capital of France?")
    print(result.messages[-1].content)  # type: ignore


asyncio.run(main())

```

**示例**:
```python
import asyncio
from typing import List

from autogen_agentchat.agents import AssistantAgent
from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily
from autogen_ext.models.ollama import OllamaChatCompletionClient


class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out: List[LLMMessage] = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out


# Create an instance of the model client for DeepSeek R1 hosted locally on Ollama.
model_client = OllamaChatCompletionClient(
    model="deepseek-r1:8b",
    model_info={
        "vision": False,
        "function_calling": False,
        "json_output": False,
        "family": ModelFamily.R1,
        "structured_output": True,
    },
)

agent = AssistantAgent(
    "reasoning_agent",
    model_client=model_client,
    model_context=ReasoningModelContext(),  # Use the custom model context.
)


async def run_reasoning_agent() -> None:
    result = await agent.run(task="What is the capital of France?")
    print(result)


asyncio.run(run_reasoning_agent())

```

```python
component_config_schema#
```

【中文翻译】alias of AssistantAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.AssistantAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the assistant agent

```python
property model_context: ChatCompletionContext#
```

【中文翻译】The model context in use by the agent.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Process the incoming messages with the assistant agent and yield events/responses as they happen.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Reset the assistant agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the current state of the assistant agent.

```python
class BaseChatAgent(name: str, description: str)[source]#
```

【中文翻译】Bases: ChatAgent, ABC, ComponentBase[BaseModel]
Base class for a chat agent.
This abstract class provides a base implementation for a ChatAgent.
To create a new chat agent, subclass this class and implement the
on_messages(), on_reset(), and produced_message_types.
If streaming is required, also implement the on_messages_stream() method.
An agent is considered stateful and maintains its state between calls to
the on_messages() or on_messages_stream() methods.
The agent should store its state in the
agent instance. The agent should also implement the on_reset() method
to reset the agent to its initialization state.

Note
The caller should only pass the new messages to the agent on each call
to the on_messages() or on_messages_stream() method.
Do not pass the entire conversation history to the agent on each call.
This design principle must be followed when creating a new agent.



async close() → None[source]#
Release any resources held by the agent. This is a no-op by default in the
BaseChatAgent class. Subclasses can override this method to
implement custom close behavior.



component_type: ClassVar[ComponentType] = 'agent'#
The logical type of the component.



property description: str#
The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.



async load_state(state: Mapping[str, Any]) → None[source]#
Restore agent from saved state. Default implementation for stateless agents.



property name: str#
The name of the agent. This is used by team to uniquely identify
the agent. It should be unique within the team.



abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_pause(cancellation_token: CancellationToken) → None[source]#
Called when the agent is paused while running in its on_messages() or
on_messages_stream() method. This is a no-op by default in the
BaseChatAgent class. Subclasses can override this method to
implement custom pause behavior.



abstract async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



async on_resume(cancellation_token: CancellationToken) → None[source]#
Called when the agent is resumed from a pause while running in
its on_messages() or on_messages_stream() method.
This is a no-op by default in the BaseChatAgent class.
Subclasses can override this method to implement custom resume behavior.



abstract property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.



async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
Run the agent with the given task and return the result.



async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
Run the agent with the given task and return a stream of messages
and the final task result as the last item in the stream.



async save_state() → Mapping[str, Any][source]#
Export state. Default implementation for stateless agents.

```python
async close() → None[source]#
```

【中文翻译】Release any resources held by the agent. This is a no-op by default in the
BaseChatAgent class. Subclasses can override this method to
implement custom close behavior.

```python
component_type: ClassVar[ComponentType] = 'agent'#
```

【中文翻译】The logical type of the component.

```python
property description: str#
```

【中文翻译】The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Restore agent from saved state. Default implementation for stateless agents.

```python
property name: str#
```

【中文翻译】The name of the agent. This is used by team to uniquely identify
the agent. It should be unique within the team.

```python
abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_pause(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Called when the agent is paused while running in its on_messages() or
on_messages_stream() method. This is a no-op by default in the
BaseChatAgent class. Subclasses can override this method to
implement custom pause behavior.

```python
abstract async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
async on_resume(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Called when the agent is resumed from a pause while running in
its on_messages() or on_messages_stream() method.
This is a no-op by default in the BaseChatAgent class.
Subclasses can override this method to implement custom resume behavior.

```python
abstract property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
```

【中文翻译】Run the agent with the given task and return the result.

```python
async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
```

【中文翻译】Run the agent with the given task and return a stream of messages
and the final task result as the last item in the stream.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Export state. Default implementation for stateless agents.

```python
class CodeExecutorAgent(name: str, code_executor: CodeExecutor, *, model_client: ChatCompletionClient | None = None, model_context: ChatCompletionContext | None = None, model_client_stream: bool = False, max_retries_on_error: int = 0, description: str | None = None, system_message: str | None = DEFAULT_SYSTEM_MESSAGE, sources: Sequence[str] | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[CodeExecutorAgentConfig]
(Experimental) An agent that generates and executes code snippets based on user instructions.

Note
This agent is experimental and may change in future releases.

It is typically used within a team with another agent that generates code snippets
to be executed or alone with model_client provided so that it can generate code
based on user query, execute it and reflect on the code result.
When used with model_client, it will generate code snippets using the model
and execute them using the provided code_executor. The model will also reflect on the
code execution results. The agent will yield the final reflection result from the model
as the final response.
When used without model_client, it will only execute code blocks found in
TextMessage messages and returns the output
of the code execution.

Note
Using AssistantAgent with
PythonCodeExecutionTool
is an alternative to this agent. However, the model for that agent will
have to generate properly escaped code string as a parameter to the tool.


Parameters:

name (str) – The name of the agent.
code_executor (CodeExecutor) – The code executor responsible for executing code received in messages
(DockerCommandLineCodeExecutor recommended. See example below)
model_client (ChatCompletionClient, optional) – The model client to use for inference and generating code.
If not provided, the agent will only execute code blocks found in input messages.
Currently, the model must support structured output mode, which is required for
the automatic retry mechanism to work.
model_client_stream (bool, optional) – If True, the model client will be used in streaming mode.
on_messages_stream() and BaseChatAgent.run_stream() methods will
also yield ModelClientStreamingChunkEvent
messages as the model client produces chunks of response. Defaults to False.
description (str, optional) – The description of the agent. If not provided,
DEFAULT_AGENT_DESCRIPTION will be used.
system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable.
Defaults to DEFAULT_SYSTEM_MESSAGE. This is only used if model_client is provided.
sources (Sequence[str], optional) – Check only messages from the specified agents for the code to execute.
This is useful when the agent is part of a group chat and you want to limit the code execution to messages from specific agents.
If not provided, all messages will be checked for code blocks.
This is only used if model_client is not provided.
max_retries_on_error (int, optional) – The maximum number of retries on error. If the code execution fails, the agent will retry up to this number of times.
If the code execution fails after this number of retries, the agent will yield a reflection result.




Note
It is recommended that the CodeExecutorAgent agent uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running.
Follow the installation instructions for Docker.


Note
The code executor only processes code that is properly formatted in markdown code blocks using triple backticks.
For example:
```python
print("Hello World")
```

# or

```sh
echo "Hello World"
```



In this example, we show how to set up a CodeExecutorAgent agent that uses the
DockerCommandLineCodeExecutor
to execute code snippets in a Docker container. The work_dir parameter indicates where all executed files are first saved locally before being executed in the Docker container.

import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken


async def run_code_executor_agent() -> None:
    # Create a code executor agent that uses a Docker container to execute code.
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")
    await code_executor.start()
    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)

    # Run the agent with a given code snippet.
    task = TextMessage(
        content='''Here is some code
```python
print('Hello world')
```
''',
        source="user",
    )
    response = await code_executor_agent.on_messages([task], CancellationToken())
    print(response.chat_message)

    # Stop the code executor.
    await code_executor.stop()


asyncio.run(run_code_executor_agent())



In this example, we show how to set up a CodeExecutorAgent agent that uses the
DeviceRequest to expose a GPU to the container for cuda-accelerated code execution.

import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken
from docker.types import DeviceRequest


async def run_code_executor_agent() -> None:
    # Create a code executor agent that uses a Docker container to execute code.
    code_executor = DockerCommandLineCodeExecutor(
        work_dir="coding", device_requests=[DeviceRequest(count=-1, capabilities=[["gpu"]])]
    )
    await code_executor.start()
    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)

    # Display the GPU information
    task = TextMessage(
        content='''Here is some code
```bash
nvidia-smi
```
''',
        source="user",
    )
    response = await code_executor_agent.on_messages([task], CancellationToken())
    print(response.chat_message)

    # Stop the code executor.
    await code_executor.stop()


asyncio.run(run_code_executor_agent())



In the following example, we show how to setup CodeExecutorAgent without model_client parameter for executing code blocks generated by other agents in a group chat using DockerCommandLineCodeExecutor

import asyncio

from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console

termination_condition = MaxMessageTermination(3)


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # define the Docker CLI Code Executor
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")

    # start the execution container
    await code_executor.start()

    code_executor_agent = CodeExecutorAgent("code_executor_agent", code_executor=code_executor)
    coder_agent = AssistantAgent("coder_agent", model_client=model_client)

    groupchat = RoundRobinGroupChat(
        participants=[coder_agent, code_executor_agent], termination_condition=termination_condition
    )

    task = "Write python code to print Hello World!"
    await Console(groupchat.run_stream(task=task))

    # stop the execution container
    await code_executor.stop()


asyncio.run(main())


---------- user ----------
Write python code to print Hello World!
---------- coder_agent ----------
Certainly! Here's a simple Python code to print "Hello World!":

```python
print("Hello World!")
```

You can run this code in any Python environment to display the message.
---------- code_executor_agent ----------
Hello World!



In the following example, we show how to setup CodeExecutorAgent with model_client that can generate its own code without the help of any other agent and executing it in DockerCommandLineCodeExecutor

import asyncio

from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.ui import Console

termination_condition = TextMessageTermination("code_executor_agent")


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # define the Docker CLI Code Executor
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")

    # start the execution container
    await code_executor.start()

    code_executor_agent = CodeExecutorAgent(
        "code_executor_agent", code_executor=code_executor, model_client=model_client
    )

    task = "Write python code to print Hello World!"
    await Console(code_executor_agent.run_stream(task=task))

    # stop the execution container
    await code_executor.stop()


asyncio.run(main())


---------- user ----------
Write python code to print Hello World!
---------- code_executor_agent ----------
Certainly! Here is a simple Python code to print "Hello World!" to the console:

```python
print("Hello World!")
```

Let's execute it to confirm the output.
---------- code_executor_agent ----------
Hello World!

---------- code_executor_agent ----------
The code has been executed successfully, and it printed "Hello World!" as expected. If you have any more requests or questions, feel free to ask!





DEFAULT_AGENT_DESCRIPTION = 'A Code Execution Agent that generates and executes Python and shell scripts based on user instructions. Python code should be provided in ```python code blocks, and sh shell scripts should be provided in ```sh code blocks for execution. It ensures correctness, efficiency, and minimal errors while gracefully handling edge cases.'#



DEFAULT_SYSTEM_MESSAGE = 'You are a Code Execution Agent. Your role is to generate and execute Python code based on user instructions, ensuring correctness, efficiency, and minimal errors. Handle edge cases gracefully.'#



DEFAULT_TERMINAL_DESCRIPTION = 'A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).'#



NO_CODE_BLOCKS_FOUND_MESSAGE = 'No code blocks found in the thread. Please provide at least one markdown-encoded code block to execute (i.e., quoting code in ```python or ```sh code blocks).'#



classmethod _from_config(config: CodeExecutorAgentConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → CodeExecutorAgentConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of CodeExecutorAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.CodeExecutorAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_block(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#



async extract_code_blocks_from_messages(messages: Sequence[BaseChatMessage]) → List[CodeBlock][source]#



property model_context: ChatCompletionContext#
The model context in use by the agent.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Process the incoming messages with the assistant agent and yield events/responses as they happen.



async on_reset(cancellation_token: CancellationToken) → None[source]#
Its a no-op as the code executor agent has no mutable state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the code executor agent produces.

**示例**:
```python
```python
print("Hello World")
```

# or

```sh
echo "Hello World"
```

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken


async def run_code_executor_agent() -> None:
    # Create a code executor agent that uses a Docker container to execute code.
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")
    await code_executor.start()
    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)

    # Run the agent with a given code snippet.
    task = TextMessage(
        content='''Here is some code
```python
print('Hello world')
```
''',
        source="user",
    )
    response = await code_executor_agent.on_messages([task], CancellationToken())
    print(response.chat_message)

    # Stop the code executor.
    await code_executor.stop()


asyncio.run(run_code_executor_agent())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_core import CancellationToken
from docker.types import DeviceRequest


async def run_code_executor_agent() -> None:
    # Create a code executor agent that uses a Docker container to execute code.
    code_executor = DockerCommandLineCodeExecutor(
        work_dir="coding", device_requests=[DeviceRequest(count=-1, capabilities=[["gpu"]])]
    )
    await code_executor.start()
    code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor)

    # Display the GPU information
    task = TextMessage(
        content='''Here is some code
```bash
nvidia-smi
```
''',
        source="user",
    )
    response = await code_executor_agent.on_messages([task], CancellationToken())
    print(response.chat_message)

    # Stop the code executor.
    await code_executor.stop()


asyncio.run(run_code_executor_agent())

```

**示例**:
```python
import asyncio

from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console

termination_condition = MaxMessageTermination(3)


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # define the Docker CLI Code Executor
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")

    # start the execution container
    await code_executor.start()

    code_executor_agent = CodeExecutorAgent("code_executor_agent", code_executor=code_executor)
    coder_agent = AssistantAgent("coder_agent", model_client=model_client)

    groupchat = RoundRobinGroupChat(
        participants=[coder_agent, code_executor_agent], termination_condition=termination_condition
    )

    task = "Write python code to print Hello World!"
    await Console(groupchat.run_stream(task=task))

    # stop the execution container
    await code_executor.stop()


asyncio.run(main())

```

**示例**:
```python
---------- user ----------
Write python code to print Hello World!
---------- coder_agent ----------
Certainly! Here's a simple Python code to print "Hello World!":

```python
print("Hello World!")
```

You can run this code in any Python environment to display the message.
---------- code_executor_agent ----------
Hello World!

```

**示例**:
```python
import asyncio

from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient

from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.ui import Console

termination_condition = TextMessageTermination("code_executor_agent")


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # define the Docker CLI Code Executor
    code_executor = DockerCommandLineCodeExecutor(work_dir="coding")

    # start the execution container
    await code_executor.start()

    code_executor_agent = CodeExecutorAgent(
        "code_executor_agent", code_executor=code_executor, model_client=model_client
    )

    task = "Write python code to print Hello World!"
    await Console(code_executor_agent.run_stream(task=task))

    # stop the execution container
    await code_executor.stop()


asyncio.run(main())

```

**示例**:
```python
---------- user ----------
Write python code to print Hello World!
---------- code_executor_agent ----------
Certainly! Here is a simple Python code to print "Hello World!" to the console:

```python
print("Hello World!")
```

Let's execute it to confirm the output.
---------- code_executor_agent ----------
Hello World!

---------- code_executor_agent ----------
The code has been executed successfully, and it printed "Hello World!" as expected. If you have any more requests or questions, feel free to ask!

```

```python
DEFAULT_AGENT_DESCRIPTION = 'A Code Execution Agent that generates and executes Python and shell scripts based on user instructions. Python code should be provided in ```python code blocks, and sh shell scripts should be provided in ```sh code blocks for execution. It ensures correctness, efficiency, and minimal errors while gracefully handling edge cases.'#
```

```python
DEFAULT_SYSTEM_MESSAGE = 'You are a Code Execution Agent. Your role is to generate and execute Python code based on user instructions, ensuring correctness, efficiency, and minimal errors. Handle edge cases gracefully.'#
```

```python
DEFAULT_TERMINAL_DESCRIPTION = 'A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).'#
```

```python
NO_CODE_BLOCKS_FOUND_MESSAGE = 'No code blocks found in the thread. Please provide at least one markdown-encoded code block to execute (i.e., quoting code in ```python or ```sh code blocks).'#
```

```python
classmethod _from_config(config: CodeExecutorAgentConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → CodeExecutorAgentConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of CodeExecutorAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.CodeExecutorAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_block(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
```

```python
async extract_code_blocks_from_messages(messages: Sequence[BaseChatMessage]) → List[CodeBlock][source]#
```

```python
property model_context: ChatCompletionContext#
```

【中文翻译】The model context in use by the agent.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Process the incoming messages with the assistant agent and yield events/responses as they happen.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Its a no-op as the code executor agent has no mutable state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the code executor agent produces.

```python
class MessageFilterAgent(name: str, wrapped_agent: BaseChatAgent, filter: MessageFilterConfig)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[MessageFilterAgentConfig]
A wrapper agent that filters incoming messages before passing them to the inner agent.

Warning
This is an experimental feature, and the API will change in the future releases.

This is useful in scenarios like multi-agent workflows where an agent should only
process a subset of the full message history—for example, only the last message
from each upstream agent, or only the first message from a specific source.
Filtering is configured using MessageFilterConfig, which supports:
- Filtering by message source (e.g., only messages from “user” or another agent)
- Selecting the first N or last N messages from each source
- If position is None, all messages from that source are included
This agent is compatible with both direct message passing and team-based execution
such as GraphFlow.
Example
>>> agent_a = MessageFilterAgent(
...     name="A",
...     wrapped_agent=some_other_agent,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=2),
...         ]
...     ),
... )



Example use case with Graph:Suppose you have a looping multi-agent graph: A → B → A → B → C.
You want:
- A to only see the user message and the last message from B
- B to see the user message, last message from A, and its own prior responses (for reflection)
- C to see the user message and the last message from B
Wrap the agents like so:
>>> agent_a = MessageFilterAgent(
...     name="A",
...     wrapped_agent=agent_a_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=1),
...         ]
...     ),
... )


>>> agent_b = MessageFilterAgent(
...     name="B",
...     wrapped_agent=agent_b_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="A", position="last", count=1),
...             PerSourceFilter(source="B", position="last", count=10),
...         ]
...     ),
... )


>>> agent_c = MessageFilterAgent(
...     name="C",
...     wrapped_agent=agent_c_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=1),
...         ]
...     ),
... )


Then define the graph:
>>> graph = DiGraph(
...     nodes={
...         "A": DiGraphNode(name="A", edges=[DiGraphEdge(target="B")]),
...         "B": DiGraphNode(
...             name="B",
...             edges=[
...                 DiGraphEdge(target="C", condition="exit"),
...                 DiGraphEdge(target="A", condition="loop"),
...             ],
...         ),
...         "C": DiGraphNode(name="C", edges=[]),
...     },
...     default_start_node="A",
... )


This will ensure each agent sees only what is needed for its decision or action logic.




classmethod _from_config(config: MessageFilterAgentConfig) → MessageFilterAgent[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → MessageFilterAgentConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of MessageFilterAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.MessageFilterAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

**示例**:
```python
>>> agent_a = MessageFilterAgent(
...     name="A",
...     wrapped_agent=some_other_agent,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=2),
...         ]
...     ),
... )

```

**示例**:
```python
>>> agent_a = MessageFilterAgent(
...     name="A",
...     wrapped_agent=agent_a_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=1),
...         ]
...     ),
... )

```

**示例**:
```python
>>> agent_b = MessageFilterAgent(
...     name="B",
...     wrapped_agent=agent_b_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="A", position="last", count=1),
...             PerSourceFilter(source="B", position="last", count=10),
...         ]
...     ),
... )

```

**示例**:
```python
>>> agent_c = MessageFilterAgent(
...     name="C",
...     wrapped_agent=agent_c_inner,
...     filter=MessageFilterConfig(
...         per_source=[
...             PerSourceFilter(source="user", position="first", count=1),
...             PerSourceFilter(source="B", position="last", count=1),
...         ]
...     ),
... )

```

**示例**:
```python
>>> graph = DiGraph(
...     nodes={
...         "A": DiGraphNode(name="A", edges=[DiGraphEdge(target="B")]),
...         "B": DiGraphNode(
...             name="B",
...             edges=[
...                 DiGraphEdge(target="C", condition="exit"),
...                 DiGraphEdge(target="A", condition="loop"),
...             ],
...         ),
...         "C": DiGraphNode(name="C", edges=[]),
...     },
...     default_start_node="A",
... )

```

```python
classmethod _from_config(config: MessageFilterAgentConfig) → MessageFilterAgent[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → MessageFilterAgentConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of MessageFilterAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.MessageFilterAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
pydantic model MessageFilterConfig[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "MessageFilterConfig",
   "type": "object",
   "properties": {
      "per_source": {
         "items": {
            "$ref": "#/$defs/PerSourceFilter"
         },
         "title": "Per Source",
         "type": "array"
      }
   },
   "$defs": {
      "PerSourceFilter": {
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "position": {
               "anyOf": [
                  {
                     "enum": [
                        "first",
                        "last"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Position"
            },
            "count": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Count"
            }
         },
         "required": [
            "source"
         ],
         "title": "PerSourceFilter",
         "type": "object"
      }
   },
   "required": [
      "per_source"
   ]
}



Fields:

per_source (List[autogen_agentchat.agents._message_filter_agent.PerSourceFilter])





field per_source: List[PerSourceFilter] [Required]#

**示例**:
```python
{
   "title": "MessageFilterConfig",
   "type": "object",
   "properties": {
      "per_source": {
         "items": {
            "$ref": "#/$defs/PerSourceFilter"
         },
         "title": "Per Source",
         "type": "array"
      }
   },
   "$defs": {
      "PerSourceFilter": {
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "position": {
               "anyOf": [
                  {
                     "enum": [
                        "first",
                        "last"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Position"
            },
            "count": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Count"
            }
         },
         "required": [
            "source"
         ],
         "title": "PerSourceFilter",
         "type": "object"
      }
   },
   "required": [
      "per_source"
   ]
}

```

```python
field per_source: List[PerSourceFilter] [Required]#
```

```python
pydantic model PerSourceFilter[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "PerSourceFilter",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "position": {
         "anyOf": [
            {
               "enum": [
                  "first",
                  "last"
               ],
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Position"
      },
      "count": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Count"
      }
   },
   "required": [
      "source"
   ]
}



Fields:

count (int | None)
position (Literal['first', 'last'] | None)
source (str)





field count: int | None = None#



field position: Literal['first', 'last'] | None = None#



field source: str [Required]#

**示例**:
```python
{
   "title": "PerSourceFilter",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "position": {
         "anyOf": [
            {
               "enum": [
                  "first",
                  "last"
               ],
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Position"
      },
      "count": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Count"
      }
   },
   "required": [
      "source"
   ]
}

```

```python
field count: int | None = None#
```

```python
field position: Literal['first', 'last'] | None = None#
```

```python
field source: str [Required]#
```

```python
class SocietyOfMindAgent(name: str, team: Team, model_client: ChatCompletionClient, *, description: str = DEFAULT_DESCRIPTION, instruction: str = DEFAULT_INSTRUCTION, response_prompt: str = DEFAULT_RESPONSE_PROMPT, model_context: ChatCompletionContext | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[SocietyOfMindAgentConfig]
An agent that uses an inner team of agents to generate responses.
Each time the agent’s on_messages() or on_messages_stream()
method is called, it runs the inner team of agents and then uses the
model client to generate a response based on the inner team’s messages.
Once the response is generated, the agent resets the inner team by
calling Team.reset().
Limit context size sent to the model:
You can limit the number of messages sent to the model by setting
the model_context parameter to a BufferedChatCompletionContext.
This will limit the number of recent messages sent to the model and can be useful
when the model has a limit on the number of tokens it can process.
You can also create your own model context by subclassing
ChatCompletionContext.

Parameters:

name (str) – The name of the agent.
team (Team) – The team of agents to use.
model_client (ChatCompletionClient) – The model client to use for preparing responses.
description (str, optional) – The description of the agent.
instruction (str, optional) – The instruction to use when generating a response using the inner team’s messages.
Defaults to DEFAULT_INSTRUCTION. It assumes the role of ‘system’.
response_prompt (str, optional) – The response prompt to use when generating a response using the inner team’s messages.
Defaults to DEFAULT_RESPONSE_PROMPT. It assumes the role of ‘system’.
model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.



Example:
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("assistant1", model_client=model_client, system_message="You are a writer, write well.")
    agent2 = AssistantAgent(
        "assistant2",
        model_client=model_client,
        system_message="You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.",
    )
    inner_termination = TextMentionTermination("APPROVE")
    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)

    society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client)

    agent3 = AssistantAgent(
        "assistant3", model_client=model_client, system_message="Translate the text to Spanish."
    )
    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)

    stream = team.run_stream(task="Write a short story with a surprising ending.")
    await Console(stream)


asyncio.run(main())




DEFAULT_DESCRIPTION = 'An agent that uses an inner team of agents to generate responses.'#
The default description for a SocietyOfMindAgent.

Type:
str





DEFAULT_INSTRUCTION = 'Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:'#
The default instruction to use when generating a response using the
inner team’s messages. The instruction will be prepended to the inner team’s
messages when generating a response using the model. It assumes the role of
‘system’.

Type:
str





DEFAULT_RESPONSE_PROMPT = 'Output a standalone response to the original request, without mentioning any of the intermediate discussion.'#
The default response prompt to use when generating a response using
the inner team’s messages. It assumes the role of ‘system’.

Type:
str





classmethod _from_config(config: SocietyOfMindAgentConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → SocietyOfMindAgentConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of SocietyOfMindAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.SocietyOfMindAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async load_state(state: Mapping[str, Any]) → None[source]#
Restore agent from saved state. Default implementation for stateless agents.



property model_context: ChatCompletionContext#
The model context in use by the agent.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.



async save_state() → Mapping[str, Any][source]#
Export state. Default implementation for stateless agents.

**示例**:
```python
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("assistant1", model_client=model_client, system_message="You are a writer, write well.")
    agent2 = AssistantAgent(
        "assistant2",
        model_client=model_client,
        system_message="You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.",
    )
    inner_termination = TextMentionTermination("APPROVE")
    inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination)

    society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client)

    agent3 = AssistantAgent(
        "assistant3", model_client=model_client, system_message="Translate the text to Spanish."
    )
    team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2)

    stream = team.run_stream(task="Write a short story with a surprising ending.")
    await Console(stream)


asyncio.run(main())

```

```python
DEFAULT_DESCRIPTION = 'An agent that uses an inner team of agents to generate responses.'#
```

【中文翻译】The default description for a SocietyOfMindAgent.

Type:
str

```python
DEFAULT_INSTRUCTION = 'Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:'#
```

【中文翻译】The default instruction to use when generating a response using the
inner team’s messages. The instruction will be prepended to the inner team’s
messages when generating a response using the model. It assumes the role of
‘system’.

Type:
str

```python
DEFAULT_RESPONSE_PROMPT = 'Output a standalone response to the original request, without mentioning any of the intermediate discussion.'#
```

【中文翻译】The default response prompt to use when generating a response using
the inner team’s messages. It assumes the role of ‘system’.

Type:
str

```python
classmethod _from_config(config: SocietyOfMindAgentConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → SocietyOfMindAgentConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of SocietyOfMindAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.SocietyOfMindAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Restore agent from saved state. Default implementation for stateless agents.

```python
property model_context: ChatCompletionContext#
```

【中文翻译】The model context in use by the agent.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Export state. Default implementation for stateless agents.

```python
class UserProxyAgent(name: str, *, description: str = 'A human user', input_func: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]] | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[UserProxyAgentConfig]
An agent that can represent a human user through an input function.
This agent can be used to represent a human user in a chat system by providing a custom input function.

Note
Using UserProxyAgent puts a running team in a temporary blocked
state until the user responds. So it is important to time out the user input
function and cancel using the CancellationToken if the user does not respond.
The input function should also handle exceptions and return a default response if needed.
For typical use cases that involve
slow human responses, it is recommended to use termination conditions
such as HandoffTermination or SourceMatchTermination
to stop the running team and return the control to the application.
You can run the team again with the user input. This way, the state of the team
can be saved and restored when the user responds.
See Human-in-the-loop for more information.


Parameters:

name (str) – The name of the agent.
description (str, optional) – A description of the agent.
input_func (Optional[Callable[[str], str]], Callable[[str, Optional[CancellationToken]], Awaitable[str]]) – A function that takes a prompt and returns a user input string.



For examples of integrating with web and UI frameworks, see the following:

FastAPI
ChainLit

Example
Simple usage case:
import asyncio
from autogen_core import CancellationToken
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import TextMessage


async def simple_user_agent():
    agent = UserProxyAgent("user_proxy")
    response = await asyncio.create_task(
        agent.on_messages(
            [TextMessage(content="What is your name? ", source="user")],
            cancellation_token=CancellationToken(),
        )
    )
    assert isinstance(response.chat_message, TextMessage)
    print(f"Your name is {response.chat_message.content}")


Example
Cancellable usage case:
import asyncio
from typing import Any
from autogen_core import CancellationToken
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import TextMessage


token = CancellationToken()
agent = UserProxyAgent("user_proxy")


async def timeout(delay: float):
    await asyncio.sleep(delay)


def cancellation_callback(task: asyncio.Task[Any]):
    token.cancel()


async def cancellable_user_agent():
    try:
        timeout_task = asyncio.create_task(timeout(3))
        timeout_task.add_done_callback(cancellation_callback)
        agent_task = asyncio.create_task(
            agent.on_messages(
                [TextMessage(content="What is your name? ", source="user")],
                cancellation_token=token,
            )
        )
        response = await agent_task
        assert isinstance(response.chat_message, TextMessage)
        print(f"Your name is {response.chat_message.content}")
    except Exception as e:
        print(f"Exception: {e}")
    except BaseException as e:
        print(f"BaseException: {e}")




class InputRequestContext[source]#
Bases: object


classmethod request_id() → str[source]#




classmethod _from_config(config: UserProxyAgentConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → UserProxyAgentConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of UserProxyAgentConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.UserProxyAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'agent'#
The logical type of the component.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handle incoming messages by requesting user input.



async on_reset(cancellation_token: CancellationToken | None = None) → None[source]#
Reset agent state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
Message types this agent can produce.

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import TextMessage


async def simple_user_agent():
    agent = UserProxyAgent("user_proxy")
    response = await asyncio.create_task(
        agent.on_messages(
            [TextMessage(content="What is your name? ", source="user")],
            cancellation_token=CancellationToken(),
        )
    )
    assert isinstance(response.chat_message, TextMessage)
    print(f"Your name is {response.chat_message.content}")

```

**示例**:
```python
import asyncio
from typing import Any
from autogen_core import CancellationToken
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.messages import TextMessage


token = CancellationToken()
agent = UserProxyAgent("user_proxy")


async def timeout(delay: float):
    await asyncio.sleep(delay)


def cancellation_callback(task: asyncio.Task[Any]):
    token.cancel()


async def cancellable_user_agent():
    try:
        timeout_task = asyncio.create_task(timeout(3))
        timeout_task.add_done_callback(cancellation_callback)
        agent_task = asyncio.create_task(
            agent.on_messages(
                [TextMessage(content="What is your name? ", source="user")],
                cancellation_token=token,
            )
        )
        response = await agent_task
        assert isinstance(response.chat_message, TextMessage)
        print(f"Your name is {response.chat_message.content}")
    except Exception as e:
        print(f"Exception: {e}")
    except BaseException as e:
        print(f"BaseException: {e}")

```

```python
class InputRequestContext[source]#
```

【中文翻译】Bases: object


classmethod request_id() → str[source]#

```python
classmethod request_id() → str[source]#
```

```python
classmethod _from_config(config: UserProxyAgentConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → UserProxyAgentConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of UserProxyAgentConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.UserProxyAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'agent'#
```

【中文翻译】The logical type of the component.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handle incoming messages by requesting user input.

```python
async on_reset(cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Reset agent state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】Message types this agent can produce.

【中文翻译】This module initializes various pre-defined agents provided by the package.
BaseChatAgent is the base class for all agents in AgentChat.

【中文翻译】previous

【中文翻译】autogen_agentchat.messages

【中文翻译】next

【中文翻译】autogen_agentchat.tools

### autogen_agentchat.base {autogen_agentchatbase}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html)

```python
class AndTerminationCondition(*conditions: TerminationCondition)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[AndTerminationConditionConfig]


component_config_schema#
alias of AndTerminationConditionConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.base.AndTerminationCondition'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'termination'#
The logical type of the component.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
component_config_schema#
```

【中文翻译】alias of AndTerminationConditionConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.base.AndTerminationCondition'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'termination'#
```

【中文翻译】The logical type of the component.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class ChatAgent(*args, **kwargs)[source]#
```

【中文翻译】Bases: ABC, TaskRunner, ComponentBase[BaseModel]
Protocol for a chat agent.


abstract async close() → None[source]#
Release any resources held by the agent.



component_type: ClassVar[ComponentType] = 'agent'#
The logical type of the component.



abstract property description: str#
The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.



abstract async load_state(state: Mapping[str, Any]) → None[source]#
Restore agent from saved state



abstract property name: str#
The name of the agent. This is used by team to uniquely identify
the agent. It should be unique within the team.



abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.



abstract on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of inner messages and
and the final item is the response.



abstract async on_pause(cancellation_token: CancellationToken) → None[source]#
Called when the agent is paused. The agent may be running in on_messages() or
on_messages_stream() when this method is called.



abstract async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



abstract async on_resume(cancellation_token: CancellationToken) → None[source]#
Called when the agent is resumed. The agent may be running in on_messages() or
on_messages_stream() when this method is called.



abstract property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.



abstract async save_state() → Mapping[str, Any][source]#
Save agent state for later restoration

```python
abstract async close() → None[source]#
```

【中文翻译】Release any resources held by the agent.

```python
component_type: ClassVar[ComponentType] = 'agent'#
```

【中文翻译】The logical type of the component.

```python
abstract property description: str#
```

【中文翻译】The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.

```python
abstract async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Restore agent from saved state

```python
abstract property name: str#
```

【中文翻译】The name of the agent. This is used by team to uniquely identify
the agent. It should be unique within the team.

```python
abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

```python
abstract on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of inner messages and
and the final item is the response.

```python
abstract async on_pause(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Called when the agent is paused. The agent may be running in on_messages() or
on_messages_stream() when this method is called.

```python
abstract async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
abstract async on_resume(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Called when the agent is resumed. The agent may be running in on_messages() or
on_messages_stream() when this method is called.

```python
abstract property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
abstract async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save agent state for later restoration

```python
pydantic model Handoff[source]#
```

【中文翻译】Bases: BaseModel
Handoff configuration.

Show JSON schema{
   "title": "Handoff",
   "description": "Handoff configuration.",
   "type": "object",
   "properties": {
      "target": {
         "title": "Target",
         "type": "string"
      },
      "description": {
         "default": "",
         "title": "Description",
         "type": "string"
      },
      "name": {
         "default": "",
         "title": "Name",
         "type": "string"
      },
      "message": {
         "default": "",
         "title": "Message",
         "type": "string"
      }
   },
   "required": [
      "target"
   ]
}



Fields:

description (str)
message (str)
name (str)
target (str)


Validators:

set_defaults » all fields





field description: str = ''#
The description of the handoff such as the condition under which it should happen and the target agent’s ability.
If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults






field message: str = ''#
The message to the target agent.
By default, it will be the result for the handoff tool.
If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults






field name: str = ''#
The name of this handoff configuration. If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults






field target: str [Required]#
The name of the target agent to handoff to.

Validated by:

set_defaults






validator set_defaults  »  all fields[source]#



property handoff_tool: BaseTool[BaseModel, BaseModel]#
Create a handoff tool from this handoff configuration.

**示例**:
```python
{
   "title": "Handoff",
   "description": "Handoff configuration.",
   "type": "object",
   "properties": {
      "target": {
         "title": "Target",
         "type": "string"
      },
      "description": {
         "default": "",
         "title": "Description",
         "type": "string"
      },
      "name": {
         "default": "",
         "title": "Name",
         "type": "string"
      },
      "message": {
         "default": "",
         "title": "Message",
         "type": "string"
      }
   },
   "required": [
      "target"
   ]
}

```

```python
field description: str = ''#
```

【中文翻译】The description of the handoff such as the condition under which it should happen and the target agent’s ability.
If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults

```python
field message: str = ''#
```

【中文翻译】The message to the target agent.
By default, it will be the result for the handoff tool.
If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults

```python
field name: str = ''#
```

【中文翻译】The name of this handoff configuration. If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults

```python
field target: str [Required]#
```

【中文翻译】The name of the target agent to handoff to.

Validated by:

set_defaults

```python
validator set_defaults  »  all fields[source]#
```

```python
property handoff_tool: BaseTool[BaseModel, BaseModel]#
```

【中文翻译】Create a handoff tool from this handoff configuration.

```python
class OrTerminationCondition(*conditions: TerminationCondition)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[OrTerminationConditionConfig]


component_config_schema#
alias of OrTerminationConditionConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.base.OrTerminationCondition'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'termination'#
The logical type of the component.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
component_config_schema#
```

【中文翻译】alias of OrTerminationConditionConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.base.OrTerminationCondition'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'termination'#
```

【中文翻译】The logical type of the component.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class Response(*, chat_message: BaseChatMessage, inner_messages: Sequence[BaseAgentEvent | BaseChatMessage] | None = None)[source]#
```

【中文翻译】Bases: object
A response from calling ChatAgent.on_messages().


chat_message: BaseChatMessage#
A chat message produced by the agent as the response.



inner_messages: Sequence[BaseAgentEvent | BaseChatMessage] | None = None#
Inner messages produced by the agent, they can be BaseAgentEvent
or BaseChatMessage.

```python
chat_message: BaseChatMessage#
```

【中文翻译】A chat message produced by the agent as the response.

```python
inner_messages: Sequence[BaseAgentEvent | BaseChatMessage] | None = None#
```

【中文翻译】Inner messages produced by the agent, they can be BaseAgentEvent
or BaseChatMessage.

```python
pydantic model TaskResult[source]#
```

【中文翻译】Bases: BaseModel
Result of running a task.

Show JSON schema{
   "title": "TaskResult",
   "description": "Result of running a task.",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "anyOf": [
               {
                  "$ref": "#/$defs/BaseAgentEvent"
               },
               {
                  "$ref": "#/$defs/BaseChatMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      },
      "stop_reason": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Reason"
      }
   },
   "$defs": {
      "BaseAgentEvent": {
         "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseAgentEvent",
         "type": "object"
      },
      "BaseChatMessage": {
         "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseChatMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "messages"
   ]
}



Fields:

messages (Sequence[autogen_agentchat.messages.BaseAgentEvent | autogen_agentchat.messages.BaseChatMessage])
stop_reason (str | None)





field messages: Sequence[BaseAgentEvent | BaseChatMessage] [Required]#
Messages produced by the task.



field stop_reason: str | None = None#
The reason the task stopped.

**示例**:
```python
{
   "title": "TaskResult",
   "description": "Result of running a task.",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "anyOf": [
               {
                  "$ref": "#/$defs/BaseAgentEvent"
               },
               {
                  "$ref": "#/$defs/BaseChatMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      },
      "stop_reason": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Reason"
      }
   },
   "$defs": {
      "BaseAgentEvent": {
         "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseAgentEvent",
         "type": "object"
      },
      "BaseChatMessage": {
         "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseChatMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "messages"
   ]
}

```

```python
field messages: Sequence[BaseAgentEvent | BaseChatMessage] [Required]#
```

【中文翻译】Messages produced by the task.

```python
field stop_reason: str | None = None#
```

【中文翻译】The reason the task stopped.

```python
class TaskRunner(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol
A task runner.


async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
Run the task and return the result.
The task can be a string, a single message, or a sequence of messages.
The runner is stateful and a subsequent call to this method will continue
from where the previous call left off. If the task is not specified,
the runner will continue with the current task.



run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
Run the task and produces a stream of messages and the final result
TaskResult as the last item in the stream.
The task can be a string, a single message, or a sequence of messages.
The runner is stateful and a subsequent call to this method will continue
from where the previous call left off. If the task is not specified,
the runner will continue with the current task.

```python
async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
```

【中文翻译】Run the task and return the result.
The task can be a string, a single message, or a sequence of messages.
The runner is stateful and a subsequent call to this method will continue
from where the previous call left off. If the task is not specified,
the runner will continue with the current task.

```python
run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
```

【中文翻译】Run the task and produces a stream of messages and the final result
TaskResult as the last item in the stream.
The task can be a string, a single message, or a sequence of messages.
The runner is stateful and a subsequent call to this method will continue
from where the previous call left off. If the task is not specified,
the runner will continue with the current task.

```python
class Team(*args, **kwargs)[source]#
```

【中文翻译】Bases: ABC, TaskRunner, ComponentBase[BaseModel]


component_type: ClassVar[ComponentType] = 'team'#
The logical type of the component.



abstract async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the team.



abstract async pause() → None[source]#
Pause the team and all its participants. This is useful for
pausing the autogen_agentchat.base.TaskRunner.run() or
autogen_agentchat.base.TaskRunner.run_stream() methods from
concurrently, while keeping them alive.



abstract async reset() → None[source]#
Reset the team and all its participants to its initial state.



abstract async resume() → None[source]#
Resume the team and all its participants from a pause after
pause() was called.



abstract async save_state() → Mapping[str, Any][source]#
Save the current state of the team.

```python
component_type: ClassVar[ComponentType] = 'team'#
```

【中文翻译】The logical type of the component.

```python
abstract async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the team.

```python
abstract async pause() → None[source]#
```

【中文翻译】Pause the team and all its participants. This is useful for
pausing the autogen_agentchat.base.TaskRunner.run() or
autogen_agentchat.base.TaskRunner.run_stream() methods from
concurrently, while keeping them alive.

```python
abstract async reset() → None[source]#
```

【中文翻译】Reset the team and all its participants to its initial state.

```python
abstract async resume() → None[source]#
```

【中文翻译】Resume the team and all its participants from a pause after
pause() was called.

```python
abstract async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the current state of the team.

```python
exception TerminatedException[source]#
```

【中文翻译】Bases: BaseException

```python
class TerminationCondition[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
A stateful condition that determines when a conversation should be terminated.
A termination condition is a callable that takes a sequence of BaseChatMessage objects
since the last time the condition was called, and returns a StopMessage if the
conversation should be terminated, or None otherwise.
Once a termination condition has been reached, it must be reset before it can be used again.
Termination conditions can be combined using the AND and OR operators.
Example
import asyncio
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination


async def main() -> None:
    # Terminate the conversation after 10 turns or if the text "TERMINATE" is mentioned.
    cond1 = MaxMessageTermination(10) | TextMentionTermination("TERMINATE")

    # Terminate the conversation after 10 turns and if the text "TERMINATE" is mentioned.
    cond2 = MaxMessageTermination(10) & TextMentionTermination("TERMINATE")

    # ...

    # Reset the termination condition.
    await cond1.reset()
    await cond2.reset()


asyncio.run(main())




component_type: ClassVar[ComponentType] = 'termination'#
The logical type of the component.



abstract async reset() → None[source]#
Reset the termination condition.



abstract property terminated: bool#
Check if the termination condition has been reached

**示例**:
```python
import asyncio
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination


async def main() -> None:
    # Terminate the conversation after 10 turns or if the text "TERMINATE" is mentioned.
    cond1 = MaxMessageTermination(10) | TextMentionTermination("TERMINATE")

    # Terminate the conversation after 10 turns and if the text "TERMINATE" is mentioned.
    cond2 = MaxMessageTermination(10) & TextMentionTermination("TERMINATE")

    # ...

    # Reset the termination condition.
    await cond1.reset()
    await cond2.reset()


asyncio.run(main())

```

```python
component_type: ClassVar[ComponentType] = 'termination'#
```

【中文翻译】The logical type of the component.

```python
abstract async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
abstract property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

【中文翻译】previous

【中文翻译】autogen_agentchat.teams

【中文翻译】next

【中文翻译】autogen_agentchat.conditions

### autogen_agentchat.base {autogen_agentchatbase}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html)

```python
class AndTerminationCondition(*conditions: TerminationCondition)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[AndTerminationConditionConfig]


component_config_schema#
alias of AndTerminationConditionConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.base.AndTerminationCondition'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'termination'#
The logical type of the component.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
component_config_schema#
```

【中文翻译】alias of AndTerminationConditionConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.base.AndTerminationCondition'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'termination'#
```

【中文翻译】The logical type of the component.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class ChatAgent(*args, **kwargs)[source]#
```

【中文翻译】Bases: ABC, TaskRunner, ComponentBase[BaseModel]
Protocol for a chat agent.


abstract async close() → None[source]#
Release any resources held by the agent.



component_type: ClassVar[ComponentType] = 'agent'#
The logical type of the component.



abstract property description: str#
The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.



abstract async load_state(state: Mapping[str, Any]) → None[source]#
Restore agent from saved state



abstract property name: str#
The name of the agent. This is used by team to uniquely identify
the agent. It should be unique within the team.



abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.



abstract on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of inner messages and
and the final item is the response.



abstract async on_pause(cancellation_token: CancellationToken) → None[source]#
Called when the agent is paused. The agent may be running in on_messages() or
on_messages_stream() when this method is called.



abstract async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



abstract async on_resume(cancellation_token: CancellationToken) → None[source]#
Called when the agent is resumed. The agent may be running in on_messages() or
on_messages_stream() when this method is called.



abstract property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.



abstract async save_state() → Mapping[str, Any][source]#
Save agent state for later restoration

```python
abstract async close() → None[source]#
```

【中文翻译】Release any resources held by the agent.

```python
component_type: ClassVar[ComponentType] = 'agent'#
```

【中文翻译】The logical type of the component.

```python
abstract property description: str#
```

【中文翻译】The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.

```python
abstract async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Restore agent from saved state

```python
abstract property name: str#
```

【中文翻译】The name of the agent. This is used by team to uniquely identify
the agent. It should be unique within the team.

```python
abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

```python
abstract on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of inner messages and
and the final item is the response.

```python
abstract async on_pause(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Called when the agent is paused. The agent may be running in on_messages() or
on_messages_stream() when this method is called.

```python
abstract async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
abstract async on_resume(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Called when the agent is resumed. The agent may be running in on_messages() or
on_messages_stream() when this method is called.

```python
abstract property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
abstract async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save agent state for later restoration

```python
pydantic model Handoff[source]#
```

【中文翻译】Bases: BaseModel
Handoff configuration.

Show JSON schema{
   "title": "Handoff",
   "description": "Handoff configuration.",
   "type": "object",
   "properties": {
      "target": {
         "title": "Target",
         "type": "string"
      },
      "description": {
         "default": "",
         "title": "Description",
         "type": "string"
      },
      "name": {
         "default": "",
         "title": "Name",
         "type": "string"
      },
      "message": {
         "default": "",
         "title": "Message",
         "type": "string"
      }
   },
   "required": [
      "target"
   ]
}



Fields:

description (str)
message (str)
name (str)
target (str)


Validators:

set_defaults » all fields





field description: str = ''#
The description of the handoff such as the condition under which it should happen and the target agent’s ability.
If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults






field message: str = ''#
The message to the target agent.
By default, it will be the result for the handoff tool.
If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults






field name: str = ''#
The name of this handoff configuration. If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults






field target: str [Required]#
The name of the target agent to handoff to.

Validated by:

set_defaults






validator set_defaults  »  all fields[source]#



property handoff_tool: BaseTool[BaseModel, BaseModel]#
Create a handoff tool from this handoff configuration.

**示例**:
```python
{
   "title": "Handoff",
   "description": "Handoff configuration.",
   "type": "object",
   "properties": {
      "target": {
         "title": "Target",
         "type": "string"
      },
      "description": {
         "default": "",
         "title": "Description",
         "type": "string"
      },
      "name": {
         "default": "",
         "title": "Name",
         "type": "string"
      },
      "message": {
         "default": "",
         "title": "Message",
         "type": "string"
      }
   },
   "required": [
      "target"
   ]
}

```

```python
field description: str = ''#
```

【中文翻译】The description of the handoff such as the condition under which it should happen and the target agent’s ability.
If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults

```python
field message: str = ''#
```

【中文翻译】The message to the target agent.
By default, it will be the result for the handoff tool.
If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults

```python
field name: str = ''#
```

【中文翻译】The name of this handoff configuration. If not provided, it is generated from the target agent’s name.

Validated by:

set_defaults

```python
field target: str [Required]#
```

【中文翻译】The name of the target agent to handoff to.

Validated by:

set_defaults

```python
validator set_defaults  »  all fields[source]#
```

```python
property handoff_tool: BaseTool[BaseModel, BaseModel]#
```

【中文翻译】Create a handoff tool from this handoff configuration.

```python
class OrTerminationCondition(*conditions: TerminationCondition)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[OrTerminationConditionConfig]


component_config_schema#
alias of OrTerminationConditionConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.base.OrTerminationCondition'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'termination'#
The logical type of the component.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
component_config_schema#
```

【中文翻译】alias of OrTerminationConditionConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.base.OrTerminationCondition'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'termination'#
```

【中文翻译】The logical type of the component.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class Response(*, chat_message: BaseChatMessage, inner_messages: Sequence[BaseAgentEvent | BaseChatMessage] | None = None)[source]#
```

【中文翻译】Bases: object
A response from calling ChatAgent.on_messages().


chat_message: BaseChatMessage#
A chat message produced by the agent as the response.



inner_messages: Sequence[BaseAgentEvent | BaseChatMessage] | None = None#
Inner messages produced by the agent, they can be BaseAgentEvent
or BaseChatMessage.

```python
chat_message: BaseChatMessage#
```

【中文翻译】A chat message produced by the agent as the response.

```python
inner_messages: Sequence[BaseAgentEvent | BaseChatMessage] | None = None#
```

【中文翻译】Inner messages produced by the agent, they can be BaseAgentEvent
or BaseChatMessage.

```python
pydantic model TaskResult[source]#
```

【中文翻译】Bases: BaseModel
Result of running a task.

Show JSON schema{
   "title": "TaskResult",
   "description": "Result of running a task.",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "anyOf": [
               {
                  "$ref": "#/$defs/BaseAgentEvent"
               },
               {
                  "$ref": "#/$defs/BaseChatMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      },
      "stop_reason": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Reason"
      }
   },
   "$defs": {
      "BaseAgentEvent": {
         "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseAgentEvent",
         "type": "object"
      },
      "BaseChatMessage": {
         "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseChatMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "messages"
   ]
}



Fields:

messages (Sequence[autogen_agentchat.messages.BaseAgentEvent | autogen_agentchat.messages.BaseChatMessage])
stop_reason (str | None)





field messages: Sequence[BaseAgentEvent | BaseChatMessage] [Required]#
Messages produced by the task.



field stop_reason: str | None = None#
The reason the task stopped.

**示例**:
```python
{
   "title": "TaskResult",
   "description": "Result of running a task.",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "anyOf": [
               {
                  "$ref": "#/$defs/BaseAgentEvent"
               },
               {
                  "$ref": "#/$defs/BaseChatMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      },
      "stop_reason": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Reason"
      }
   },
   "$defs": {
      "BaseAgentEvent": {
         "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseAgentEvent",
         "type": "object"
      },
      "BaseChatMessage": {
         "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
         "properties": {
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseChatMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "messages"
   ]
}

```

```python
field messages: Sequence[BaseAgentEvent | BaseChatMessage] [Required]#
```

【中文翻译】Messages produced by the task.

```python
field stop_reason: str | None = None#
```

【中文翻译】The reason the task stopped.

```python
class TaskRunner(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol
A task runner.


async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
Run the task and return the result.
The task can be a string, a single message, or a sequence of messages.
The runner is stateful and a subsequent call to this method will continue
from where the previous call left off. If the task is not specified,
the runner will continue with the current task.



run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
Run the task and produces a stream of messages and the final result
TaskResult as the last item in the stream.
The task can be a string, a single message, or a sequence of messages.
The runner is stateful and a subsequent call to this method will continue
from where the previous call left off. If the task is not specified,
the runner will continue with the current task.

```python
async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
```

【中文翻译】Run the task and return the result.
The task can be a string, a single message, or a sequence of messages.
The runner is stateful and a subsequent call to this method will continue
from where the previous call left off. If the task is not specified,
the runner will continue with the current task.

```python
run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
```

【中文翻译】Run the task and produces a stream of messages and the final result
TaskResult as the last item in the stream.
The task can be a string, a single message, or a sequence of messages.
The runner is stateful and a subsequent call to this method will continue
from where the previous call left off. If the task is not specified,
the runner will continue with the current task.

```python
class Team(*args, **kwargs)[source]#
```

【中文翻译】Bases: ABC, TaskRunner, ComponentBase[BaseModel]


component_type: ClassVar[ComponentType] = 'team'#
The logical type of the component.



abstract async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the team.



abstract async pause() → None[source]#
Pause the team and all its participants. This is useful for
pausing the autogen_agentchat.base.TaskRunner.run() or
autogen_agentchat.base.TaskRunner.run_stream() methods from
concurrently, while keeping them alive.



abstract async reset() → None[source]#
Reset the team and all its participants to its initial state.



abstract async resume() → None[source]#
Resume the team and all its participants from a pause after
pause() was called.



abstract async save_state() → Mapping[str, Any][source]#
Save the current state of the team.

```python
component_type: ClassVar[ComponentType] = 'team'#
```

【中文翻译】The logical type of the component.

```python
abstract async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the team.

```python
abstract async pause() → None[source]#
```

【中文翻译】Pause the team and all its participants. This is useful for
pausing the autogen_agentchat.base.TaskRunner.run() or
autogen_agentchat.base.TaskRunner.run_stream() methods from
concurrently, while keeping them alive.

```python
abstract async reset() → None[source]#
```

【中文翻译】Reset the team and all its participants to its initial state.

```python
abstract async resume() → None[source]#
```

【中文翻译】Resume the team and all its participants from a pause after
pause() was called.

```python
abstract async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the current state of the team.

```python
exception TerminatedException[source]#
```

【中文翻译】Bases: BaseException

```python
class TerminationCondition[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
A stateful condition that determines when a conversation should be terminated.
A termination condition is a callable that takes a sequence of BaseChatMessage objects
since the last time the condition was called, and returns a StopMessage if the
conversation should be terminated, or None otherwise.
Once a termination condition has been reached, it must be reset before it can be used again.
Termination conditions can be combined using the AND and OR operators.
Example
import asyncio
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination


async def main() -> None:
    # Terminate the conversation after 10 turns or if the text "TERMINATE" is mentioned.
    cond1 = MaxMessageTermination(10) | TextMentionTermination("TERMINATE")

    # Terminate the conversation after 10 turns and if the text "TERMINATE" is mentioned.
    cond2 = MaxMessageTermination(10) & TextMentionTermination("TERMINATE")

    # ...

    # Reset the termination condition.
    await cond1.reset()
    await cond2.reset()


asyncio.run(main())




component_type: ClassVar[ComponentType] = 'termination'#
The logical type of the component.



abstract async reset() → None[source]#
Reset the termination condition.



abstract property terminated: bool#
Check if the termination condition has been reached

**示例**:
```python
import asyncio
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination


async def main() -> None:
    # Terminate the conversation after 10 turns or if the text "TERMINATE" is mentioned.
    cond1 = MaxMessageTermination(10) | TextMentionTermination("TERMINATE")

    # Terminate the conversation after 10 turns and if the text "TERMINATE" is mentioned.
    cond2 = MaxMessageTermination(10) & TextMentionTermination("TERMINATE")

    # ...

    # Reset the termination condition.
    await cond1.reset()
    await cond2.reset()


asyncio.run(main())

```

```python
component_type: ClassVar[ComponentType] = 'termination'#
```

【中文翻译】The logical type of the component.

```python
abstract async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
abstract property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

【中文翻译】previous

【中文翻译】autogen_agentchat.teams

【中文翻译】next

【中文翻译】autogen_agentchat.conditions

### autogen_agentchat.conditions {autogen_agentchatconditions}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html)

```python
class ExternalTermination[source]#
```

【中文翻译】Bases: TerminationCondition, Component[ExternalTerminationConfig]
A termination condition that is externally controlled
by calling the set() method.
Example:
from autogen_agentchat.conditions import ExternalTermination

termination = ExternalTermination()

# Run the team in an asyncio task.
...

# Set the termination condition externally
termination.set()




classmethod _from_config(config: ExternalTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → ExternalTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of ExternalTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.ExternalTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



set() → None[source]#
Set the termination condition to terminated.



property terminated: bool#
Check if the termination condition has been reached

**示例**:
```python
from autogen_agentchat.conditions import ExternalTermination

termination = ExternalTermination()

# Run the team in an asyncio task.
...

# Set the termination condition externally
termination.set()

```

```python
classmethod _from_config(config: ExternalTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → ExternalTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of ExternalTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.ExternalTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
set() → None[source]#
```

【中文翻译】Set the termination condition to terminated.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class FunctionCallTermination(function_name: str)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[FunctionCallTerminationConfig]
Terminate the conversation if a FunctionExecutionResult
with a specific name was received.

Parameters:
function_name (str) – The name of the function to look for in the messages.

Raises:
TerminatedException – If the termination condition has already been reached.




classmethod _from_config(config: FunctionCallTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → FunctionCallTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of FunctionCallTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.FunctionCallTermination'#
The schema for the component configuration.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: FunctionCallTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → FunctionCallTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of FunctionCallTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.FunctionCallTermination'#
```

【中文翻译】The schema for the component configuration.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class FunctionalTermination(func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]])[source]#
```

【中文翻译】Bases: TerminationCondition
Terminate the conversation if an functional expression is met.

Parameters:
func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]]) – A function that takes a sequence of messages
and returns True if the termination condition is met, False otherwise.
The function can be a callable or an async callable.


Example
import asyncio
from typing import Sequence

from autogen_agentchat.conditions import FunctionalTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage


def expression(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> bool:
    # Check if the last message is a stop message
    return isinstance(messages[-1], StopMessage)


termination = FunctionalTermination(expression)


async def run() -> None:
    messages = [
        StopMessage(source="agent1", content="Stop"),
    ]
    result = await termination(messages)
    print(result)


asyncio.run(run())


StopMessage(source="FunctionalTermination", content="Functional termination condition met")




async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

**示例**:
```python
import asyncio
from typing import Sequence

from autogen_agentchat.conditions import FunctionalTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage


def expression(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> bool:
    # Check if the last message is a stop message
    return isinstance(messages[-1], StopMessage)


termination = FunctionalTermination(expression)


async def run() -> None:
    messages = [
        StopMessage(source="agent1", content="Stop"),
    ]
    result = await termination(messages)
    print(result)


asyncio.run(run())

```

**示例**:
```python
StopMessage(source="FunctionalTermination", content="Functional termination condition met")

```

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class HandoffTermination(target: str)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[HandoffTerminationConfig]
Terminate the conversation if a HandoffMessage
with the given target is received.

Parameters:
target (str) – The target of the handoff message.




classmethod _from_config(config: HandoffTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → HandoffTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of HandoffTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.HandoffTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: HandoffTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → HandoffTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of HandoffTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.HandoffTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class MaxMessageTermination(max_messages: int, include_agent_event: bool = False)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[MaxMessageTerminationConfig]
Terminate the conversation after a maximum number of messages have been exchanged.

Parameters:

max_messages – The maximum number of messages allowed in the conversation.
include_agent_event – If True, include BaseAgentEvent in the message count.
Otherwise, only include BaseChatMessage. Defaults to False.





classmethod _from_config(config: MaxMessageTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → MaxMessageTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of MaxMessageTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.MaxMessageTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: MaxMessageTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → MaxMessageTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of MaxMessageTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.MaxMessageTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class SourceMatchTermination(sources: List[str])[source]#
```

【中文翻译】Bases: TerminationCondition, Component[SourceMatchTerminationConfig]
Terminate the conversation after a specific source responds.

Parameters:
sources (List[str]) – List of source names to terminate the conversation.

Raises:
TerminatedException – If the termination condition has already been reached.




classmethod _from_config(config: SourceMatchTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → SourceMatchTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of SourceMatchTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.SourceMatchTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: SourceMatchTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → SourceMatchTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of SourceMatchTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.SourceMatchTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class StopMessageTermination[source]#
```

【中文翻译】Bases: TerminationCondition, Component[StopMessageTerminationConfig]
Terminate the conversation if a StopMessage is received.


classmethod _from_config(config: StopMessageTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → StopMessageTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of StopMessageTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.StopMessageTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: StopMessageTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → StopMessageTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of StopMessageTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.StopMessageTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class TextMentionTermination(text: str, sources: Sequence[str] | None = None)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[TextMentionTerminationConfig]
Terminate the conversation if a specific text is mentioned.

Parameters:

text – The text to look for in the messages.
sources – Check only messages of the specified agents for the text to look for.





classmethod _from_config(config: TextMentionTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TextMentionTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TextMentionTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMentionTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: TextMentionTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TextMentionTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TextMentionTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMentionTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class TextMessageTermination(source: str | None = None)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[TextMessageTerminationConfig]
Terminate the conversation if a TextMessage is received.
This termination condition checks for TextMessage instances in the message sequence. When a TextMessage is found,
it terminates the conversation if either:
- No source was specified (terminates on any TextMessage)
- The message source matches the specified source

Parameters:
source (str | None, optional) – The source name to match against incoming messages. If None, matches any source.
Defaults to None.




classmethod _from_config(config: TextMessageTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TextMessageTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TextMessageTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMessageTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: TextMessageTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TextMessageTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TextMessageTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMessageTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class TimeoutTermination(timeout_seconds: float)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[TimeoutTerminationConfig]
Terminate the conversation after a specified duration has passed.

Parameters:
timeout_seconds – The maximum duration in seconds before terminating the conversation.




classmethod _from_config(config: TimeoutTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TimeoutTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TimeoutTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TimeoutTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: TimeoutTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TimeoutTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TimeoutTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TimeoutTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class TokenUsageTermination(max_total_token: int | None = None, max_prompt_token: int | None = None, max_completion_token: int | None = None)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[TokenUsageTerminationConfig]
Terminate the conversation if a token usage limit is reached.

Parameters:

max_total_token – The maximum total number of tokens allowed in the conversation.
max_prompt_token – The maximum number of prompt tokens allowed in the conversation.
max_completion_token – The maximum number of completion tokens allowed in the conversation.


Raises:
ValueError – If none of max_total_token, max_prompt_token, or max_completion_token is provided.




classmethod _from_config(config: TokenUsageTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TokenUsageTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TokenUsageTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TokenUsageTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: TokenUsageTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TokenUsageTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TokenUsageTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TokenUsageTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

【中文翻译】This module provides various termination conditions for controlling the behavior of
multi-agent teams.

【中文翻译】previous

【中文翻译】autogen_agentchat.base

【中文翻译】next

【中文翻译】autogen_agentchat.ui

### autogen_agentchat.conditions {autogen_agentchatconditions}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html)

```python
class ExternalTermination[source]#
```

【中文翻译】Bases: TerminationCondition, Component[ExternalTerminationConfig]
A termination condition that is externally controlled
by calling the set() method.
Example:
from autogen_agentchat.conditions import ExternalTermination

termination = ExternalTermination()

# Run the team in an asyncio task.
...

# Set the termination condition externally
termination.set()




classmethod _from_config(config: ExternalTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → ExternalTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of ExternalTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.ExternalTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



set() → None[source]#
Set the termination condition to terminated.



property terminated: bool#
Check if the termination condition has been reached

**示例**:
```python
from autogen_agentchat.conditions import ExternalTermination

termination = ExternalTermination()

# Run the team in an asyncio task.
...

# Set the termination condition externally
termination.set()

```

```python
classmethod _from_config(config: ExternalTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → ExternalTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of ExternalTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.ExternalTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
set() → None[source]#
```

【中文翻译】Set the termination condition to terminated.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class FunctionCallTermination(function_name: str)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[FunctionCallTerminationConfig]
Terminate the conversation if a FunctionExecutionResult
with a specific name was received.

Parameters:
function_name (str) – The name of the function to look for in the messages.

Raises:
TerminatedException – If the termination condition has already been reached.




classmethod _from_config(config: FunctionCallTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → FunctionCallTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of FunctionCallTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.FunctionCallTermination'#
The schema for the component configuration.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: FunctionCallTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → FunctionCallTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of FunctionCallTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.FunctionCallTermination'#
```

【中文翻译】The schema for the component configuration.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class FunctionalTermination(func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]])[source]#
```

【中文翻译】Bases: TerminationCondition
Terminate the conversation if an functional expression is met.

Parameters:
func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]]) – A function that takes a sequence of messages
and returns True if the termination condition is met, False otherwise.
The function can be a callable or an async callable.


Example
import asyncio
from typing import Sequence

from autogen_agentchat.conditions import FunctionalTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage


def expression(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> bool:
    # Check if the last message is a stop message
    return isinstance(messages[-1], StopMessage)


termination = FunctionalTermination(expression)


async def run() -> None:
    messages = [
        StopMessage(source="agent1", content="Stop"),
    ]
    result = await termination(messages)
    print(result)


asyncio.run(run())


StopMessage(source="FunctionalTermination", content="Functional termination condition met")




async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

**示例**:
```python
import asyncio
from typing import Sequence

from autogen_agentchat.conditions import FunctionalTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage


def expression(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> bool:
    # Check if the last message is a stop message
    return isinstance(messages[-1], StopMessage)


termination = FunctionalTermination(expression)


async def run() -> None:
    messages = [
        StopMessage(source="agent1", content="Stop"),
    ]
    result = await termination(messages)
    print(result)


asyncio.run(run())

```

**示例**:
```python
StopMessage(source="FunctionalTermination", content="Functional termination condition met")

```

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class HandoffTermination(target: str)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[HandoffTerminationConfig]
Terminate the conversation if a HandoffMessage
with the given target is received.

Parameters:
target (str) – The target of the handoff message.




classmethod _from_config(config: HandoffTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → HandoffTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of HandoffTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.HandoffTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: HandoffTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → HandoffTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of HandoffTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.HandoffTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class MaxMessageTermination(max_messages: int, include_agent_event: bool = False)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[MaxMessageTerminationConfig]
Terminate the conversation after a maximum number of messages have been exchanged.

Parameters:

max_messages – The maximum number of messages allowed in the conversation.
include_agent_event – If True, include BaseAgentEvent in the message count.
Otherwise, only include BaseChatMessage. Defaults to False.





classmethod _from_config(config: MaxMessageTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → MaxMessageTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of MaxMessageTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.MaxMessageTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: MaxMessageTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → MaxMessageTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of MaxMessageTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.MaxMessageTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class SourceMatchTermination(sources: List[str])[source]#
```

【中文翻译】Bases: TerminationCondition, Component[SourceMatchTerminationConfig]
Terminate the conversation after a specific source responds.

Parameters:
sources (List[str]) – List of source names to terminate the conversation.

Raises:
TerminatedException – If the termination condition has already been reached.




classmethod _from_config(config: SourceMatchTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → SourceMatchTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of SourceMatchTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.SourceMatchTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: SourceMatchTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → SourceMatchTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of SourceMatchTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.SourceMatchTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class StopMessageTermination[source]#
```

【中文翻译】Bases: TerminationCondition, Component[StopMessageTerminationConfig]
Terminate the conversation if a StopMessage is received.


classmethod _from_config(config: StopMessageTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → StopMessageTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of StopMessageTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.StopMessageTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: StopMessageTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → StopMessageTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of StopMessageTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.StopMessageTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class TextMentionTermination(text: str, sources: Sequence[str] | None = None)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[TextMentionTerminationConfig]
Terminate the conversation if a specific text is mentioned.

Parameters:

text – The text to look for in the messages.
sources – Check only messages of the specified agents for the text to look for.





classmethod _from_config(config: TextMentionTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TextMentionTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TextMentionTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMentionTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: TextMentionTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TextMentionTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TextMentionTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMentionTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class TextMessageTermination(source: str | None = None)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[TextMessageTerminationConfig]
Terminate the conversation if a TextMessage is received.
This termination condition checks for TextMessage instances in the message sequence. When a TextMessage is found,
it terminates the conversation if either:
- No source was specified (terminates on any TextMessage)
- The message source matches the specified source

Parameters:
source (str | None, optional) – The source name to match against incoming messages. If None, matches any source.
Defaults to None.




classmethod _from_config(config: TextMessageTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TextMessageTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TextMessageTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMessageTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: TextMessageTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TextMessageTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TextMessageTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMessageTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class TimeoutTermination(timeout_seconds: float)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[TimeoutTerminationConfig]
Terminate the conversation after a specified duration has passed.

Parameters:
timeout_seconds – The maximum duration in seconds before terminating the conversation.




classmethod _from_config(config: TimeoutTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TimeoutTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TimeoutTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TimeoutTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: TimeoutTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TimeoutTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TimeoutTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TimeoutTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

```python
class TokenUsageTermination(max_total_token: int | None = None, max_prompt_token: int | None = None, max_completion_token: int | None = None)[source]#
```

【中文翻译】Bases: TerminationCondition, Component[TokenUsageTerminationConfig]
Terminate the conversation if a token usage limit is reached.

Parameters:

max_total_token – The maximum total number of tokens allowed in the conversation.
max_prompt_token – The maximum number of prompt tokens allowed in the conversation.
max_completion_token – The maximum number of completion tokens allowed in the conversation.


Raises:
ValueError – If none of max_total_token, max_prompt_token, or max_completion_token is provided.




classmethod _from_config(config: TokenUsageTerminationConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TokenUsageTerminationConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TokenUsageTerminationConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TokenUsageTermination'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async reset() → None[source]#
Reset the termination condition.



property terminated: bool#
Check if the termination condition has been reached

```python
classmethod _from_config(config: TokenUsageTerminationConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TokenUsageTerminationConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TokenUsageTerminationConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TokenUsageTermination'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async reset() → None[source]#
```

【中文翻译】Reset the termination condition.

```python
property terminated: bool#
```

【中文翻译】Check if the termination condition has been reached

【中文翻译】This module provides various termination conditions for controlling the behavior of
multi-agent teams.

【中文翻译】previous

【中文翻译】autogen_agentchat.base

【中文翻译】next

【中文翻译】autogen_agentchat.ui

### autogen_agentchat.messages {autogen_agentchatmessages}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html)

```python
AgentEvent#
```

【中文翻译】The union type of all built-in concrete subclasses of BaseAgentEvent.
alias of Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator=’type’)]

```python
pydantic model BaseAgentEvent[source]#
```

【中文翻译】Bases: BaseMessage, ABC
Base class for agent events.

Note
If you want to create a new message type for signaling observable events
to user and application, inherit from this class.

Agent events are used to signal actions and thoughts produced by agents
and teams to user and applications. They are not used for agent-to-agent
communication and are not expected to be processed by other agents.
You should override the to_text() method if you want to provide
a custom rendering of the content.

Show JSON schema{
   "title": "BaseAgentEvent",
   "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}



Fields:

metadata (Dict[str, str])
models_usage (autogen_core.models._types.RequestUsage | None)
source (str)





field metadata: Dict[str, str] = {}#
Additional metadata about the message.



field models_usage: RequestUsage | None = None#
The model client usage incurred when producing this message.



field source: str [Required]#
The name of the agent that sent this message.

**示例**:
```python
{
   "title": "BaseAgentEvent",
   "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}

```

```python
field metadata: Dict[str, str] = {}#
```

【中文翻译】Additional metadata about the message.

```python
field models_usage: RequestUsage | None = None#
```

【中文翻译】The model client usage incurred when producing this message.

```python
field source: str [Required]#
```

【中文翻译】The name of the agent that sent this message.

```python
pydantic model BaseChatMessage[source]#
```

【中文翻译】Bases: BaseMessage, ABC
Abstract base class for chat messages.

Note
If you want to create a new message type that is used for agent-to-agent
communication, inherit from this class, or simply use
StructuredMessage if your content type is a subclass of
Pydantic BaseModel.

This class is used for messages that are sent between agents in a chat
conversation. Agents are expected to process the content of the
message using models and return a response as another BaseChatMessage.

Show JSON schema{
   "title": "BaseChatMessage",
   "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}



Fields:

metadata (Dict[str, str])
models_usage (autogen_core.models._types.RequestUsage | None)
source (str)





field metadata: Dict[str, str] = {}#
Additional metadata about the message.



field models_usage: RequestUsage | None = None#
The model client usage incurred when producing this message.



field source: str [Required]#
The name of the agent that sent this message.



abstract to_model_message() → UserMessage[source]#
Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.



abstract to_model_text() → str[source]#
Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.

**示例**:
```python
{
   "title": "BaseChatMessage",
   "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}

```

```python
field metadata: Dict[str, str] = {}#
```

【中文翻译】Additional metadata about the message.

```python
field models_usage: RequestUsage | None = None#
```

【中文翻译】The model client usage incurred when producing this message.

```python
field source: str [Required]#
```

【中文翻译】The name of the agent that sent this message.

```python
abstract to_model_message() → UserMessage[source]#
```

【中文翻译】Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.

```python
abstract to_model_text() → str[source]#
```

【中文翻译】Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.

```python
pydantic model BaseMessage[source]#
```

【中文翻译】Bases: BaseModel, ABC
Abstract base class for all message types in AgentChat.

Warning
If you want to create a new message type, do not inherit from this class.
Instead, inherit from BaseChatMessage or BaseAgentEvent
to clarify the purpose of the message type.


Show JSON schema{
   "title": "BaseMessage",
   "description": "Abstract base class for all message types in AgentChat.\n\n.. warning::\n\n    If you want to create a new message type, do not inherit from this class.\n    Instead, inherit from :class:`BaseChatMessage` or :class:`BaseAgentEvent`\n    to clarify the purpose of the message type.",
   "type": "object",
   "properties": {}
}




dump() → Mapping[str, Any][source]#
Convert the message to a JSON-serializable dictionary.
The default implementation uses the Pydantic model’s
model_dump() method to convert the message to a dictionary.
Override this method if you want to customize the serialization
process or add additional fields to the output.



classmethod load(data: Mapping[str, Any]) → Self[source]#
Create a message from a dictionary of JSON-serializable data.
The default implementation uses the Pydantic model’s
model_validate() method to create the message from the data.
Override this method if you want to customize the deserialization
process or add additional fields to the input data.



abstract to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "BaseMessage",
   "description": "Abstract base class for all message types in AgentChat.\n\n.. warning::\n\n    If you want to create a new message type, do not inherit from this class.\n    Instead, inherit from :class:`BaseChatMessage` or :class:`BaseAgentEvent`\n    to clarify the purpose of the message type.",
   "type": "object",
   "properties": {}
}

```

```python
dump() → Mapping[str, Any][source]#
```

【中文翻译】Convert the message to a JSON-serializable dictionary.
The default implementation uses the Pydantic model’s
model_dump() method to convert the message to a dictionary.
Override this method if you want to customize the serialization
process or add additional fields to the output.

```python
classmethod load(data: Mapping[str, Any]) → Self[source]#
```

【中文翻译】Create a message from a dictionary of JSON-serializable data.
The default implementation uses the Pydantic model’s
model_validate() method to create the message from the data.
Override this method if you want to customize the deserialization
process or add additional fields to the input data.

```python
abstract to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model BaseTextChatMessage[source]#
```

【中文翻译】Bases: BaseChatMessage, ABC
Base class for all text-only BaseChatMessage types.
It has implementations for to_text(), to_model_text(),
and to_model_message() methods.
Inherit from this class if your message content type is a string.

Show JSON schema{
   "title": "BaseTextChatMessage",
   "description": "Base class for all text-only :class:`BaseChatMessage` types.\nIt has implementations for :meth:`to_text`, :meth:`to_model_text`,\nand :meth:`to_model_message` methods.\n\nInherit from this class if your message content type is a string.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (str)





field content: str [Required]#
The content of the message.



to_model_message() → UserMessage[source]#
Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.



to_model_text() → str[source]#
Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "BaseTextChatMessage",
   "description": "Base class for all text-only :class:`BaseChatMessage` types.\nIt has implementations for :meth:`to_text`, :meth:`to_model_text`,\nand :meth:`to_model_message` methods.\n\nInherit from this class if your message content type is a string.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】The content of the message.

```python
to_model_message() → UserMessage[source]#
```

【中文翻译】Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.

```python
to_model_text() → str[source]#
```

【中文翻译】Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
ChatMessage#
```

【中文翻译】The union type of all built-in concrete subclasses of BaseChatMessage.
It does not include StructuredMessage types.
alias of Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator=’type’)]

```python
pydantic model CodeExecutionEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling code execution event.

Show JSON schema{
   "title": "CodeExecutionEvent",
   "description": "An event signaling code execution event.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "retry_attempt": {
         "title": "Retry Attempt",
         "type": "integer"
      },
      "result": {
         "$ref": "#/$defs/CodeResult"
      },
      "type": {
         "const": "CodeExecutionEvent",
         "default": "CodeExecutionEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "CodeResult": {
         "properties": {
            "exit_code": {
               "title": "Exit Code",
               "type": "integer"
            },
            "output": {
               "title": "Output",
               "type": "string"
            }
         },
         "required": [
            "exit_code",
            "output"
         ],
         "title": "CodeResult",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "retry_attempt",
      "result"
   ]
}



Fields:

result (autogen_core.code_executor._base.CodeResult)
retry_attempt (int)
type (Literal['CodeExecutionEvent'])





field result: CodeResult [Required]#
Code Execution Result



field retry_attempt: int [Required]#
Retry number, 0 means first execution



field type: Literal['CodeExecutionEvent'] = 'CodeExecutionEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "CodeExecutionEvent",
   "description": "An event signaling code execution event.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "retry_attempt": {
         "title": "Retry Attempt",
         "type": "integer"
      },
      "result": {
         "$ref": "#/$defs/CodeResult"
      },
      "type": {
         "const": "CodeExecutionEvent",
         "default": "CodeExecutionEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "CodeResult": {
         "properties": {
            "exit_code": {
               "title": "Exit Code",
               "type": "integer"
            },
            "output": {
               "title": "Output",
               "type": "string"
            }
         },
         "required": [
            "exit_code",
            "output"
         ],
         "title": "CodeResult",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "retry_attempt",
      "result"
   ]
}

```

```python
field result: CodeResult [Required]#
```

【中文翻译】Code Execution Result

```python
field retry_attempt: int [Required]#
```

【中文翻译】Retry number, 0 means first execution

```python
field type: Literal['CodeExecutionEvent'] = 'CodeExecutionEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model CodeGenerationEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling code generation event.

Show JSON schema{
   "title": "CodeGenerationEvent",
   "description": "An event signaling code generation event.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "retry_attempt": {
         "title": "Retry Attempt",
         "type": "integer"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "code_blocks": {
         "items": {
            "$ref": "#/$defs/CodeBlock"
         },
         "title": "Code Blocks",
         "type": "array"
      },
      "type": {
         "const": "CodeGenerationEvent",
         "default": "CodeGenerationEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "CodeBlock": {
         "properties": {
            "code": {
               "title": "Code",
               "type": "string"
            },
            "language": {
               "title": "Language",
               "type": "string"
            }
         },
         "required": [
            "code",
            "language"
         ],
         "title": "CodeBlock",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "retry_attempt",
      "content",
      "code_blocks"
   ]
}



Fields:

code_blocks (List[autogen_core.code_executor._base.CodeBlock])
content (str)
retry_attempt (int)
type (Literal['CodeGenerationEvent'])





field code_blocks: List[CodeBlock] [Required]#
List of code blocks present in content



field content: str [Required]#
The complete content as string.



field retry_attempt: int [Required]#
Retry number, 0 means first generation



field type: Literal['CodeGenerationEvent'] = 'CodeGenerationEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "CodeGenerationEvent",
   "description": "An event signaling code generation event.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "retry_attempt": {
         "title": "Retry Attempt",
         "type": "integer"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "code_blocks": {
         "items": {
            "$ref": "#/$defs/CodeBlock"
         },
         "title": "Code Blocks",
         "type": "array"
      },
      "type": {
         "const": "CodeGenerationEvent",
         "default": "CodeGenerationEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "CodeBlock": {
         "properties": {
            "code": {
               "title": "Code",
               "type": "string"
            },
            "language": {
               "title": "Language",
               "type": "string"
            }
         },
         "required": [
            "code",
            "language"
         ],
         "title": "CodeBlock",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "retry_attempt",
      "content",
      "code_blocks"
   ]
}

```

```python
field code_blocks: List[CodeBlock] [Required]#
```

【中文翻译】List of code blocks present in content

```python
field content: str [Required]#
```

【中文翻译】The complete content as string.

```python
field retry_attempt: int [Required]#
```

【中文翻译】Retry number, 0 means first generation

```python
field type: Literal['CodeGenerationEvent'] = 'CodeGenerationEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model HandoffMessage[source]#
```

【中文翻译】Bases: BaseTextChatMessage
A message requesting handoff of a conversation to another agent.

Show JSON schema{
   "title": "HandoffMessage",
   "description": "A message requesting handoff of a conversation to another agent.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "target": {
         "title": "Target",
         "type": "string"
      },
      "context": {
         "default": [],
         "items": {
            "discriminator": {
               "mapping": {
                  "AssistantMessage": "#/$defs/AssistantMessage",
                  "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                  "SystemMessage": "#/$defs/SystemMessage",
                  "UserMessage": "#/$defs/UserMessage"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/SystemMessage"
               },
               {
                  "$ref": "#/$defs/UserMessage"
               },
               {
                  "$ref": "#/$defs/AssistantMessage"
               },
               {
                  "$ref": "#/$defs/FunctionExecutionResultMessage"
               }
            ]
         },
         "title": "Context",
         "type": "array"
      },
      "type": {
         "const": "HandoffMessage",
         "default": "HandoffMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "AssistantMessage": {
         "description": "Assistant message are sampled from the language model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "$ref": "#/$defs/FunctionCall"
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "thought": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Thought"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "AssistantMessage",
               "default": "AssistantMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "AssistantMessage",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "FunctionExecutionResultMessage": {
         "description": "Function execution result message contains the output of multiple function calls.",
         "properties": {
            "content": {
               "items": {
                  "$ref": "#/$defs/FunctionExecutionResult"
               },
               "title": "Content",
               "type": "array"
            },
            "type": {
               "const": "FunctionExecutionResultMessage",
               "default": "FunctionExecutionResultMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "FunctionExecutionResultMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      },
      "SystemMessage": {
         "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "type": {
               "const": "SystemMessage",
               "default": "SystemMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "SystemMessage",
         "type": "object"
      },
      "UserMessage": {
         "description": "User message contains input from end users, or a catch-all for data provided to the model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "anyOf": [
                           {
                              "type": "string"
                           },
                           {}
                        ]
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "UserMessage",
               "default": "UserMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "UserMessage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content",
      "target"
   ]
}



Fields:

context (List[Annotated[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]])
target (str)
type (Literal['HandoffMessage'])





field context: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] = []#
The model context to be passed to the target agent.



field target: str [Required]#
The name of the target agent to handoff to.



field type: Literal['HandoffMessage'] = 'HandoffMessage'#

**示例**:
```python
{
   "title": "HandoffMessage",
   "description": "A message requesting handoff of a conversation to another agent.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "target": {
         "title": "Target",
         "type": "string"
      },
      "context": {
         "default": [],
         "items": {
            "discriminator": {
               "mapping": {
                  "AssistantMessage": "#/$defs/AssistantMessage",
                  "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                  "SystemMessage": "#/$defs/SystemMessage",
                  "UserMessage": "#/$defs/UserMessage"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/SystemMessage"
               },
               {
                  "$ref": "#/$defs/UserMessage"
               },
               {
                  "$ref": "#/$defs/AssistantMessage"
               },
               {
                  "$ref": "#/$defs/FunctionExecutionResultMessage"
               }
            ]
         },
         "title": "Context",
         "type": "array"
      },
      "type": {
         "const": "HandoffMessage",
         "default": "HandoffMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "AssistantMessage": {
         "description": "Assistant message are sampled from the language model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "$ref": "#/$defs/FunctionCall"
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "thought": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Thought"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "AssistantMessage",
               "default": "AssistantMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "AssistantMessage",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "FunctionExecutionResultMessage": {
         "description": "Function execution result message contains the output of multiple function calls.",
         "properties": {
            "content": {
               "items": {
                  "$ref": "#/$defs/FunctionExecutionResult"
               },
               "title": "Content",
               "type": "array"
            },
            "type": {
               "const": "FunctionExecutionResultMessage",
               "default": "FunctionExecutionResultMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "FunctionExecutionResultMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      },
      "SystemMessage": {
         "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "type": {
               "const": "SystemMessage",
               "default": "SystemMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "SystemMessage",
         "type": "object"
      },
      "UserMessage": {
         "description": "User message contains input from end users, or a catch-all for data provided to the model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "anyOf": [
                           {
                              "type": "string"
                           },
                           {}
                        ]
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "UserMessage",
               "default": "UserMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "UserMessage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content",
      "target"
   ]
}

```

```python
field context: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] = []#
```

【中文翻译】The model context to be passed to the target agent.

```python
field target: str [Required]#
```

【中文翻译】The name of the target agent to handoff to.

```python
field type: Literal['HandoffMessage'] = 'HandoffMessage'#
```

```python
pydantic model MemoryQueryEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling the results of memory queries.

Show JSON schema{
   "title": "MemoryQueryEvent",
   "description": "An event signaling the results of memory queries.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/MemoryContent"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "MemoryQueryEvent",
         "default": "MemoryQueryEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[autogen_core.memory._base_memory.MemoryContent])
type (Literal['MemoryQueryEvent'])





field content: List[MemoryContent] [Required]#
The memory query results.



field type: Literal['MemoryQueryEvent'] = 'MemoryQueryEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "MemoryQueryEvent",
   "description": "An event signaling the results of memory queries.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/MemoryContent"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "MemoryQueryEvent",
         "default": "MemoryQueryEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[MemoryContent] [Required]#
```

【中文翻译】The memory query results.

```python
field type: Literal['MemoryQueryEvent'] = 'MemoryQueryEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model ModelClientStreamingChunkEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling a text output chunk from a model client in streaming mode.

Show JSON schema{
   "title": "ModelClientStreamingChunkEvent",
   "description": "An event signaling a text output chunk from a model client in streaming mode.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ModelClientStreamingChunkEvent",
         "default": "ModelClientStreamingChunkEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (str)
type (Literal['ModelClientStreamingChunkEvent'])





field content: str [Required]#
A string chunk from the model client.



field type: Literal['ModelClientStreamingChunkEvent'] = 'ModelClientStreamingChunkEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "ModelClientStreamingChunkEvent",
   "description": "An event signaling a text output chunk from a model client in streaming mode.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ModelClientStreamingChunkEvent",
         "default": "ModelClientStreamingChunkEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】A string chunk from the model client.

```python
field type: Literal['ModelClientStreamingChunkEvent'] = 'ModelClientStreamingChunkEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model MultiModalMessage[source]#
```

【中文翻译】Bases: BaseChatMessage
A multimodal message.

Show JSON schema{
   "title": "MultiModalMessage",
   "description": "A multimodal message.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "anyOf": [
               {
                  "type": "string"
               },
               {}
            ]
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "MultiModalMessage",
         "default": "MultiModalMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[str | autogen_core._image.Image])
type (Literal['MultiModalMessage'])





field content: List[str | Image] [Required]#
The content of the message.



field type: Literal['MultiModalMessage'] = 'MultiModalMessage'#



to_model_message() → UserMessage[source]#
Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.



to_model_text(image_placeholder: str | None = '[image]') → str[source]#
Convert the content of the message to a string-only representation.
If an image is present, it will be replaced with the image placeholder
by default, otherwise it will be a base64 string when set to None.



to_text(iterm: bool = False) → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "MultiModalMessage",
   "description": "A multimodal message.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "anyOf": [
               {
                  "type": "string"
               },
               {}
            ]
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "MultiModalMessage",
         "default": "MultiModalMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[str | Image] [Required]#
```

【中文翻译】The content of the message.

```python
field type: Literal['MultiModalMessage'] = 'MultiModalMessage'#
```

```python
to_model_message() → UserMessage[source]#
```

【中文翻译】Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.

```python
to_model_text(image_placeholder: str | None = '[image]') → str[source]#
```

【中文翻译】Convert the content of the message to a string-only representation.
If an image is present, it will be replaced with the image placeholder
by default, otherwise it will be a base64 string when set to None.

```python
to_text(iterm: bool = False) → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model SelectSpeakerEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling the selection of speakers for a conversation.

Show JSON schema{
   "title": "SelectSpeakerEvent",
   "description": "An event signaling the selection of speakers for a conversation.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "type": "string"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "SelectSpeakerEvent",
         "default": "SelectSpeakerEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[str])
type (Literal['SelectSpeakerEvent'])





field content: List[str] [Required]#
The names of the selected speakers.



field type: Literal['SelectSpeakerEvent'] = 'SelectSpeakerEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "SelectSpeakerEvent",
   "description": "An event signaling the selection of speakers for a conversation.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "type": "string"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "SelectSpeakerEvent",
         "default": "SelectSpeakerEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[str] [Required]#
```

【中文翻译】The names of the selected speakers.

```python
field type: Literal['SelectSpeakerEvent'] = 'SelectSpeakerEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model StopMessage[source]#
```

【中文翻译】Bases: BaseTextChatMessage
A message requesting stop of a conversation.

Show JSON schema{
   "title": "StopMessage",
   "description": "A message requesting stop of a conversation.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "StopMessage",
         "default": "StopMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

type (Literal['StopMessage'])





field type: Literal['StopMessage'] = 'StopMessage'#

**示例**:
```python
{
   "title": "StopMessage",
   "description": "A message requesting stop of a conversation.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "StopMessage",
         "default": "StopMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field type: Literal['StopMessage'] = 'StopMessage'#
```

```python
class StructuredContentType#
```

【中文翻译】Type variable for structured content types.
alias of TypeVar(‘StructuredContentType’, bound=BaseModel, covariant=True)

```python
pydantic model StructuredMessage[source]#
```

【中文翻译】Bases: BaseChatMessage, Generic[StructuredContentType]
A BaseChatMessage type with an unspecified content type.
To create a new structured message type, specify the content type
as a subclass of Pydantic BaseModel.
from pydantic import BaseModel
from autogen_agentchat.messages import StructuredMessage


class MyMessageContent(BaseModel):
    text: str
    number: int


message = StructuredMessage[MyMessageContent](
    content=MyMessageContent(text="Hello", number=42),
    source="agent1",
)

print(message.to_text())  # {"text": "Hello", "number": 42}


from pydantic import BaseModel
from autogen_agentchat.messages import StructuredMessage


class MyMessageContent(BaseModel):
    text: str
    number: int


message = StructuredMessage[MyMessageContent](
    content=MyMessageContent(text="Hello", number=42),
    source="agent",
    format_string="Hello, {text} {number}!",
)

print(message.to_text())  # Hello, agent 42!



Show JSON schema{
   "title": "StructuredMessage",
   "description": "A :class:`BaseChatMessage` type with an unspecified content type.\n\nTo create a new structured message type, specify the content type\nas a subclass of `Pydantic BaseModel <https://docs.pydantic.dev/latest/concepts/models/>`_.\n\n.. code-block:: python\n\n    from pydantic import BaseModel\n    from autogen_agentchat.messages import StructuredMessage\n\n\n    class MyMessageContent(BaseModel):\n        text: str\n        number: int\n\n\n    message = StructuredMessage[MyMessageContent](\n        content=MyMessageContent(text=\"Hello\", number=42),\n        source=\"agent1\",\n    )\n\n    print(message.to_text())  # {\"text\": \"Hello\", \"number\": 42}\n\n.. code-block:: python\n\n    from pydantic import BaseModel\n    from autogen_agentchat.messages import StructuredMessage\n\n\n    class MyMessageContent(BaseModel):\n        text: str\n        number: int\n\n\n    message = StructuredMessage[MyMessageContent](\n        content=MyMessageContent(text=\"Hello\", number=42),\n        source=\"agent\",\n        format_string=\"Hello, {text} {number}!\",\n    )\n\n    print(message.to_text())  # Hello, agent 42!",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "$ref": "#/$defs/BaseModel"
      },
      "format_string": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Format String"
      }
   },
   "$defs": {
      "BaseModel": {
         "properties": {},
         "title": "BaseModel",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (autogen_agentchat.messages.StructuredContentType)
format_string (str | None)





field content: StructuredContentType [Required]#
The content of the message. Must be a subclass of
Pydantic BaseModel.



field format_string: str | None = None#
(Experimental) An optional format string to render the content into a human-readable format.
The format string can use the fields of the content model as placeholders.
For example, if the content model has a field name, you can use
{name} in the format string to include the value of that field.
The format string is used in the to_text() method to create a
human-readable representation of the message.
This setting is experimental and will change in the future.



to_model_message() → UserMessage[source]#
Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.



to_model_text() → str[source]#
Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.



property type: str#

**示例**:
```python
from pydantic import BaseModel
from autogen_agentchat.messages import StructuredMessage


class MyMessageContent(BaseModel):
    text: str
    number: int


message = StructuredMessage[MyMessageContent](
    content=MyMessageContent(text="Hello", number=42),
    source="agent1",
)

print(message.to_text())  # {"text": "Hello", "number": 42}

```

**示例**:
```python
from pydantic import BaseModel
from autogen_agentchat.messages import StructuredMessage


class MyMessageContent(BaseModel):
    text: str
    number: int


message = StructuredMessage[MyMessageContent](
    content=MyMessageContent(text="Hello", number=42),
    source="agent",
    format_string="Hello, {text} {number}!",
)

print(message.to_text())  # Hello, agent 42!

```

**示例**:
```python
{
   "title": "StructuredMessage",
   "description": "A :class:`BaseChatMessage` type with an unspecified content type.\n\nTo create a new structured message type, specify the content type\nas a subclass of `Pydantic BaseModel <https://docs.pydantic.dev/latest/concepts/models/>`_.\n\n.. code-block:: python\n\n    from pydantic import BaseModel\n    from autogen_agentchat.messages import StructuredMessage\n\n\n    class MyMessageContent(BaseModel):\n        text: str\n        number: int\n\n\n    message = StructuredMessage[MyMessageContent](\n        content=MyMessageContent(text=\"Hello\", number=42),\n        source=\"agent1\",\n    )\n\n    print(message.to_text())  # {\"text\": \"Hello\", \"number\": 42}\n\n.. code-block:: python\n\n    from pydantic import BaseModel\n    from autogen_agentchat.messages import StructuredMessage\n\n\n    class MyMessageContent(BaseModel):\n        text: str\n        number: int\n\n\n    message = StructuredMessage[MyMessageContent](\n        content=MyMessageContent(text=\"Hello\", number=42),\n        source=\"agent\",\n        format_string=\"Hello, {text} {number}!\",\n    )\n\n    print(message.to_text())  # Hello, agent 42!",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "$ref": "#/$defs/BaseModel"
      },
      "format_string": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Format String"
      }
   },
   "$defs": {
      "BaseModel": {
         "properties": {},
         "title": "BaseModel",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: StructuredContentType [Required]#
```

【中文翻译】The content of the message. Must be a subclass of
Pydantic BaseModel.

```python
field format_string: str | None = None#
```

【中文翻译】(Experimental) An optional format string to render the content into a human-readable format.
The format string can use the fields of the content model as placeholders.
For example, if the content model has a field name, you can use
{name} in the format string to include the value of that field.
The format string is used in the to_text() method to create a
human-readable representation of the message.
This setting is experimental and will change in the future.

```python
to_model_message() → UserMessage[source]#
```

【中文翻译】Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.

```python
to_model_text() → str[source]#
```

【中文翻译】Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
property type: str#
```

```python
pydantic model TextMessage[source]#
```

【中文翻译】Bases: BaseTextChatMessage
A text message with string-only content.

Show JSON schema{
   "title": "TextMessage",
   "description": "A text message with string-only content.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "TextMessage",
         "default": "TextMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

type (Literal['TextMessage'])





field type: Literal['TextMessage'] = 'TextMessage'#

**示例**:
```python
{
   "title": "TextMessage",
   "description": "A text message with string-only content.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "TextMessage",
         "default": "TextMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field type: Literal['TextMessage'] = 'TextMessage'#
```

```python
pydantic model ThoughtEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling the thought process of a model.
It is used to communicate the reasoning tokens generated by a reasoning model,
or the extra text content generated by a function call.

Show JSON schema{
   "title": "ThoughtEvent",
   "description": "An event signaling the thought process of a model.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ThoughtEvent",
         "default": "ThoughtEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (str)
type (Literal['ThoughtEvent'])





field content: str [Required]#
The thought process of the model.



field type: Literal['ThoughtEvent'] = 'ThoughtEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "ThoughtEvent",
   "description": "An event signaling the thought process of a model.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ThoughtEvent",
         "default": "ThoughtEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】The thought process of the model.

```python
field type: Literal['ThoughtEvent'] = 'ThoughtEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model ToolCallExecutionEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling the execution of tool calls.

Show JSON schema{
   "title": "ToolCallExecutionEvent",
   "description": "An event signaling the execution of tool calls.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionExecutionResult"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "ToolCallExecutionEvent",
         "default": "ToolCallExecutionEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[autogen_core.models._types.FunctionExecutionResult])
type (Literal['ToolCallExecutionEvent'])





field content: List[FunctionExecutionResult] [Required]#
The tool call results.



field type: Literal['ToolCallExecutionEvent'] = 'ToolCallExecutionEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "ToolCallExecutionEvent",
   "description": "An event signaling the execution of tool calls.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionExecutionResult"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "ToolCallExecutionEvent",
         "default": "ToolCallExecutionEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[FunctionExecutionResult] [Required]#
```

【中文翻译】The tool call results.

```python
field type: Literal['ToolCallExecutionEvent'] = 'ToolCallExecutionEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model ToolCallRequestEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling a request to use tools.

Show JSON schema{
   "title": "ToolCallRequestEvent",
   "description": "An event signaling a request to use tools.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionCall"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "ToolCallRequestEvent",
         "default": "ToolCallRequestEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[autogen_core._types.FunctionCall])
type (Literal['ToolCallRequestEvent'])





field content: List[FunctionCall] [Required]#
The tool calls.



field type: Literal['ToolCallRequestEvent'] = 'ToolCallRequestEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "ToolCallRequestEvent",
   "description": "An event signaling a request to use tools.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionCall"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "ToolCallRequestEvent",
         "default": "ToolCallRequestEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[FunctionCall] [Required]#
```

【中文翻译】The tool calls.

```python
field type: Literal['ToolCallRequestEvent'] = 'ToolCallRequestEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model ToolCallSummaryMessage[source]#
```

【中文翻译】Bases: BaseTextChatMessage
A message signaling the summary of tool call results.

Show JSON schema{
   "title": "ToolCallSummaryMessage",
   "description": "A message signaling the summary of tool call results.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ToolCallSummaryMessage",
         "default": "ToolCallSummaryMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

type (Literal['ToolCallSummaryMessage'])





field type: Literal['ToolCallSummaryMessage'] = 'ToolCallSummaryMessage'#

**示例**:
```python
{
   "title": "ToolCallSummaryMessage",
   "description": "A message signaling the summary of tool call results.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ToolCallSummaryMessage",
         "default": "ToolCallSummaryMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field type: Literal['ToolCallSummaryMessage'] = 'ToolCallSummaryMessage'#
```

```python
pydantic model UserInputRequestedEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.

Show JSON schema{
   "title": "UserInputRequestedEvent",
   "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "request_id": {
         "title": "Request Id",
         "type": "string"
      },
      "content": {
         "const": "",
         "default": "",
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "UserInputRequestedEvent",
         "default": "UserInputRequestedEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "request_id"
   ]
}



Fields:

content (Literal[''])
request_id (str)
type (Literal['UserInputRequestedEvent'])





field content: Literal[''] = ''#
Empty content for compat with consumers expecting a content field.



field request_id: str [Required]#
Identifier for the user input request.



field type: Literal['UserInputRequestedEvent'] = 'UserInputRequestedEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "UserInputRequestedEvent",
   "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "request_id": {
         "title": "Request Id",
         "type": "string"
      },
      "content": {
         "const": "",
         "default": "",
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "UserInputRequestedEvent",
         "default": "UserInputRequestedEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "request_id"
   ]
}

```

```python
field content: Literal[''] = ''#
```

【中文翻译】Empty content for compat with consumers expecting a content field.

```python
field request_id: str [Required]#
```

【中文翻译】Identifier for the user input request.

```python
field type: Literal['UserInputRequestedEvent'] = 'UserInputRequestedEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

【中文翻译】This module defines various message types used for agent-to-agent communication.
Each message type inherits either from the BaseChatMessage class or BaseAgentEvent
class and includes specific fields relevant to the type of message being sent.

【中文翻译】previous

【中文翻译】autogen_agentchat

【中文翻译】next

【中文翻译】autogen_agentchat.agents

### autogen_agentchat.messages {autogen_agentchatmessages}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html)

```python
AgentEvent#
```

【中文翻译】The union type of all built-in concrete subclasses of BaseAgentEvent.
alias of Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator=’type’)]

```python
pydantic model BaseAgentEvent[source]#
```

【中文翻译】Bases: BaseMessage, ABC
Base class for agent events.

Note
If you want to create a new message type for signaling observable events
to user and application, inherit from this class.

Agent events are used to signal actions and thoughts produced by agents
and teams to user and applications. They are not used for agent-to-agent
communication and are not expected to be processed by other agents.
You should override the to_text() method if you want to provide
a custom rendering of the content.

Show JSON schema{
   "title": "BaseAgentEvent",
   "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}



Fields:

metadata (Dict[str, str])
models_usage (autogen_core.models._types.RequestUsage | None)
source (str)





field metadata: Dict[str, str] = {}#
Additional metadata about the message.



field models_usage: RequestUsage | None = None#
The model client usage incurred when producing this message.



field source: str [Required]#
The name of the agent that sent this message.

**示例**:
```python
{
   "title": "BaseAgentEvent",
   "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}

```

```python
field metadata: Dict[str, str] = {}#
```

【中文翻译】Additional metadata about the message.

```python
field models_usage: RequestUsage | None = None#
```

【中文翻译】The model client usage incurred when producing this message.

```python
field source: str [Required]#
```

【中文翻译】The name of the agent that sent this message.

```python
pydantic model BaseChatMessage[source]#
```

【中文翻译】Bases: BaseMessage, ABC
Abstract base class for chat messages.

Note
If you want to create a new message type that is used for agent-to-agent
communication, inherit from this class, or simply use
StructuredMessage if your content type is a subclass of
Pydantic BaseModel.

This class is used for messages that are sent between agents in a chat
conversation. Agents are expected to process the content of the
message using models and return a response as another BaseChatMessage.

Show JSON schema{
   "title": "BaseChatMessage",
   "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}



Fields:

metadata (Dict[str, str])
models_usage (autogen_core.models._types.RequestUsage | None)
source (str)





field metadata: Dict[str, str] = {}#
Additional metadata about the message.



field models_usage: RequestUsage | None = None#
The model client usage incurred when producing this message.



field source: str [Required]#
The name of the agent that sent this message.



abstract to_model_message() → UserMessage[source]#
Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.



abstract to_model_text() → str[source]#
Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.

**示例**:
```python
{
   "title": "BaseChatMessage",
   "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}

```

```python
field metadata: Dict[str, str] = {}#
```

【中文翻译】Additional metadata about the message.

```python
field models_usage: RequestUsage | None = None#
```

【中文翻译】The model client usage incurred when producing this message.

```python
field source: str [Required]#
```

【中文翻译】The name of the agent that sent this message.

```python
abstract to_model_message() → UserMessage[source]#
```

【中文翻译】Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.

```python
abstract to_model_text() → str[source]#
```

【中文翻译】Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.

```python
pydantic model BaseMessage[source]#
```

【中文翻译】Bases: BaseModel, ABC
Abstract base class for all message types in AgentChat.

Warning
If you want to create a new message type, do not inherit from this class.
Instead, inherit from BaseChatMessage or BaseAgentEvent
to clarify the purpose of the message type.


Show JSON schema{
   "title": "BaseMessage",
   "description": "Abstract base class for all message types in AgentChat.\n\n.. warning::\n\n    If you want to create a new message type, do not inherit from this class.\n    Instead, inherit from :class:`BaseChatMessage` or :class:`BaseAgentEvent`\n    to clarify the purpose of the message type.",
   "type": "object",
   "properties": {}
}




dump() → Mapping[str, Any][source]#
Convert the message to a JSON-serializable dictionary.
The default implementation uses the Pydantic model’s
model_dump() method to convert the message to a dictionary.
Override this method if you want to customize the serialization
process or add additional fields to the output.



classmethod load(data: Mapping[str, Any]) → Self[source]#
Create a message from a dictionary of JSON-serializable data.
The default implementation uses the Pydantic model’s
model_validate() method to create the message from the data.
Override this method if you want to customize the deserialization
process or add additional fields to the input data.



abstract to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "BaseMessage",
   "description": "Abstract base class for all message types in AgentChat.\n\n.. warning::\n\n    If you want to create a new message type, do not inherit from this class.\n    Instead, inherit from :class:`BaseChatMessage` or :class:`BaseAgentEvent`\n    to clarify the purpose of the message type.",
   "type": "object",
   "properties": {}
}

```

```python
dump() → Mapping[str, Any][source]#
```

【中文翻译】Convert the message to a JSON-serializable dictionary.
The default implementation uses the Pydantic model’s
model_dump() method to convert the message to a dictionary.
Override this method if you want to customize the serialization
process or add additional fields to the output.

```python
classmethod load(data: Mapping[str, Any]) → Self[source]#
```

【中文翻译】Create a message from a dictionary of JSON-serializable data.
The default implementation uses the Pydantic model’s
model_validate() method to create the message from the data.
Override this method if you want to customize the deserialization
process or add additional fields to the input data.

```python
abstract to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model BaseTextChatMessage[source]#
```

【中文翻译】Bases: BaseChatMessage, ABC
Base class for all text-only BaseChatMessage types.
It has implementations for to_text(), to_model_text(),
and to_model_message() methods.
Inherit from this class if your message content type is a string.

Show JSON schema{
   "title": "BaseTextChatMessage",
   "description": "Base class for all text-only :class:`BaseChatMessage` types.\nIt has implementations for :meth:`to_text`, :meth:`to_model_text`,\nand :meth:`to_model_message` methods.\n\nInherit from this class if your message content type is a string.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (str)





field content: str [Required]#
The content of the message.



to_model_message() → UserMessage[source]#
Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.



to_model_text() → str[source]#
Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "BaseTextChatMessage",
   "description": "Base class for all text-only :class:`BaseChatMessage` types.\nIt has implementations for :meth:`to_text`, :meth:`to_model_text`,\nand :meth:`to_model_message` methods.\n\nInherit from this class if your message content type is a string.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】The content of the message.

```python
to_model_message() → UserMessage[source]#
```

【中文翻译】Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.

```python
to_model_text() → str[source]#
```

【中文翻译】Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
ChatMessage#
```

【中文翻译】The union type of all built-in concrete subclasses of BaseChatMessage.
It does not include StructuredMessage types.
alias of Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator=’type’)]

```python
pydantic model CodeExecutionEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling code execution event.

Show JSON schema{
   "title": "CodeExecutionEvent",
   "description": "An event signaling code execution event.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "retry_attempt": {
         "title": "Retry Attempt",
         "type": "integer"
      },
      "result": {
         "$ref": "#/$defs/CodeResult"
      },
      "type": {
         "const": "CodeExecutionEvent",
         "default": "CodeExecutionEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "CodeResult": {
         "properties": {
            "exit_code": {
               "title": "Exit Code",
               "type": "integer"
            },
            "output": {
               "title": "Output",
               "type": "string"
            }
         },
         "required": [
            "exit_code",
            "output"
         ],
         "title": "CodeResult",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "retry_attempt",
      "result"
   ]
}



Fields:

result (autogen_core.code_executor._base.CodeResult)
retry_attempt (int)
type (Literal['CodeExecutionEvent'])





field result: CodeResult [Required]#
Code Execution Result



field retry_attempt: int [Required]#
Retry number, 0 means first execution



field type: Literal['CodeExecutionEvent'] = 'CodeExecutionEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "CodeExecutionEvent",
   "description": "An event signaling code execution event.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "retry_attempt": {
         "title": "Retry Attempt",
         "type": "integer"
      },
      "result": {
         "$ref": "#/$defs/CodeResult"
      },
      "type": {
         "const": "CodeExecutionEvent",
         "default": "CodeExecutionEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "CodeResult": {
         "properties": {
            "exit_code": {
               "title": "Exit Code",
               "type": "integer"
            },
            "output": {
               "title": "Output",
               "type": "string"
            }
         },
         "required": [
            "exit_code",
            "output"
         ],
         "title": "CodeResult",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "retry_attempt",
      "result"
   ]
}

```

```python
field result: CodeResult [Required]#
```

【中文翻译】Code Execution Result

```python
field retry_attempt: int [Required]#
```

【中文翻译】Retry number, 0 means first execution

```python
field type: Literal['CodeExecutionEvent'] = 'CodeExecutionEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model CodeGenerationEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling code generation event.

Show JSON schema{
   "title": "CodeGenerationEvent",
   "description": "An event signaling code generation event.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "retry_attempt": {
         "title": "Retry Attempt",
         "type": "integer"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "code_blocks": {
         "items": {
            "$ref": "#/$defs/CodeBlock"
         },
         "title": "Code Blocks",
         "type": "array"
      },
      "type": {
         "const": "CodeGenerationEvent",
         "default": "CodeGenerationEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "CodeBlock": {
         "properties": {
            "code": {
               "title": "Code",
               "type": "string"
            },
            "language": {
               "title": "Language",
               "type": "string"
            }
         },
         "required": [
            "code",
            "language"
         ],
         "title": "CodeBlock",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "retry_attempt",
      "content",
      "code_blocks"
   ]
}



Fields:

code_blocks (List[autogen_core.code_executor._base.CodeBlock])
content (str)
retry_attempt (int)
type (Literal['CodeGenerationEvent'])





field code_blocks: List[CodeBlock] [Required]#
List of code blocks present in content



field content: str [Required]#
The complete content as string.



field retry_attempt: int [Required]#
Retry number, 0 means first generation



field type: Literal['CodeGenerationEvent'] = 'CodeGenerationEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "CodeGenerationEvent",
   "description": "An event signaling code generation event.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "retry_attempt": {
         "title": "Retry Attempt",
         "type": "integer"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "code_blocks": {
         "items": {
            "$ref": "#/$defs/CodeBlock"
         },
         "title": "Code Blocks",
         "type": "array"
      },
      "type": {
         "const": "CodeGenerationEvent",
         "default": "CodeGenerationEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "CodeBlock": {
         "properties": {
            "code": {
               "title": "Code",
               "type": "string"
            },
            "language": {
               "title": "Language",
               "type": "string"
            }
         },
         "required": [
            "code",
            "language"
         ],
         "title": "CodeBlock",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "retry_attempt",
      "content",
      "code_blocks"
   ]
}

```

```python
field code_blocks: List[CodeBlock] [Required]#
```

【中文翻译】List of code blocks present in content

```python
field content: str [Required]#
```

【中文翻译】The complete content as string.

```python
field retry_attempt: int [Required]#
```

【中文翻译】Retry number, 0 means first generation

```python
field type: Literal['CodeGenerationEvent'] = 'CodeGenerationEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model HandoffMessage[source]#
```

【中文翻译】Bases: BaseTextChatMessage
A message requesting handoff of a conversation to another agent.

Show JSON schema{
   "title": "HandoffMessage",
   "description": "A message requesting handoff of a conversation to another agent.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "target": {
         "title": "Target",
         "type": "string"
      },
      "context": {
         "default": [],
         "items": {
            "discriminator": {
               "mapping": {
                  "AssistantMessage": "#/$defs/AssistantMessage",
                  "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                  "SystemMessage": "#/$defs/SystemMessage",
                  "UserMessage": "#/$defs/UserMessage"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/SystemMessage"
               },
               {
                  "$ref": "#/$defs/UserMessage"
               },
               {
                  "$ref": "#/$defs/AssistantMessage"
               },
               {
                  "$ref": "#/$defs/FunctionExecutionResultMessage"
               }
            ]
         },
         "title": "Context",
         "type": "array"
      },
      "type": {
         "const": "HandoffMessage",
         "default": "HandoffMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "AssistantMessage": {
         "description": "Assistant message are sampled from the language model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "$ref": "#/$defs/FunctionCall"
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "thought": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Thought"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "AssistantMessage",
               "default": "AssistantMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "AssistantMessage",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "FunctionExecutionResultMessage": {
         "description": "Function execution result message contains the output of multiple function calls.",
         "properties": {
            "content": {
               "items": {
                  "$ref": "#/$defs/FunctionExecutionResult"
               },
               "title": "Content",
               "type": "array"
            },
            "type": {
               "const": "FunctionExecutionResultMessage",
               "default": "FunctionExecutionResultMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "FunctionExecutionResultMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      },
      "SystemMessage": {
         "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "type": {
               "const": "SystemMessage",
               "default": "SystemMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "SystemMessage",
         "type": "object"
      },
      "UserMessage": {
         "description": "User message contains input from end users, or a catch-all for data provided to the model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "anyOf": [
                           {
                              "type": "string"
                           },
                           {}
                        ]
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "UserMessage",
               "default": "UserMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "UserMessage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content",
      "target"
   ]
}



Fields:

context (List[Annotated[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]])
target (str)
type (Literal['HandoffMessage'])





field context: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] = []#
The model context to be passed to the target agent.



field target: str [Required]#
The name of the target agent to handoff to.



field type: Literal['HandoffMessage'] = 'HandoffMessage'#

**示例**:
```python
{
   "title": "HandoffMessage",
   "description": "A message requesting handoff of a conversation to another agent.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "target": {
         "title": "Target",
         "type": "string"
      },
      "context": {
         "default": [],
         "items": {
            "discriminator": {
               "mapping": {
                  "AssistantMessage": "#/$defs/AssistantMessage",
                  "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                  "SystemMessage": "#/$defs/SystemMessage",
                  "UserMessage": "#/$defs/UserMessage"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/SystemMessage"
               },
               {
                  "$ref": "#/$defs/UserMessage"
               },
               {
                  "$ref": "#/$defs/AssistantMessage"
               },
               {
                  "$ref": "#/$defs/FunctionExecutionResultMessage"
               }
            ]
         },
         "title": "Context",
         "type": "array"
      },
      "type": {
         "const": "HandoffMessage",
         "default": "HandoffMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "AssistantMessage": {
         "description": "Assistant message are sampled from the language model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "$ref": "#/$defs/FunctionCall"
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "thought": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Thought"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "AssistantMessage",
               "default": "AssistantMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "AssistantMessage",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "FunctionExecutionResultMessage": {
         "description": "Function execution result message contains the output of multiple function calls.",
         "properties": {
            "content": {
               "items": {
                  "$ref": "#/$defs/FunctionExecutionResult"
               },
               "title": "Content",
               "type": "array"
            },
            "type": {
               "const": "FunctionExecutionResultMessage",
               "default": "FunctionExecutionResultMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "FunctionExecutionResultMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      },
      "SystemMessage": {
         "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "type": {
               "const": "SystemMessage",
               "default": "SystemMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "SystemMessage",
         "type": "object"
      },
      "UserMessage": {
         "description": "User message contains input from end users, or a catch-all for data provided to the model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "anyOf": [
                           {
                              "type": "string"
                           },
                           {}
                        ]
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "UserMessage",
               "default": "UserMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "UserMessage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content",
      "target"
   ]
}

```

```python
field context: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] = []#
```

【中文翻译】The model context to be passed to the target agent.

```python
field target: str [Required]#
```

【中文翻译】The name of the target agent to handoff to.

```python
field type: Literal['HandoffMessage'] = 'HandoffMessage'#
```

```python
pydantic model MemoryQueryEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling the results of memory queries.

Show JSON schema{
   "title": "MemoryQueryEvent",
   "description": "An event signaling the results of memory queries.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/MemoryContent"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "MemoryQueryEvent",
         "default": "MemoryQueryEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[autogen_core.memory._base_memory.MemoryContent])
type (Literal['MemoryQueryEvent'])





field content: List[MemoryContent] [Required]#
The memory query results.



field type: Literal['MemoryQueryEvent'] = 'MemoryQueryEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "MemoryQueryEvent",
   "description": "An event signaling the results of memory queries.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/MemoryContent"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "MemoryQueryEvent",
         "default": "MemoryQueryEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[MemoryContent] [Required]#
```

【中文翻译】The memory query results.

```python
field type: Literal['MemoryQueryEvent'] = 'MemoryQueryEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model ModelClientStreamingChunkEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling a text output chunk from a model client in streaming mode.

Show JSON schema{
   "title": "ModelClientStreamingChunkEvent",
   "description": "An event signaling a text output chunk from a model client in streaming mode.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ModelClientStreamingChunkEvent",
         "default": "ModelClientStreamingChunkEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (str)
type (Literal['ModelClientStreamingChunkEvent'])





field content: str [Required]#
A string chunk from the model client.



field type: Literal['ModelClientStreamingChunkEvent'] = 'ModelClientStreamingChunkEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "ModelClientStreamingChunkEvent",
   "description": "An event signaling a text output chunk from a model client in streaming mode.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ModelClientStreamingChunkEvent",
         "default": "ModelClientStreamingChunkEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】A string chunk from the model client.

```python
field type: Literal['ModelClientStreamingChunkEvent'] = 'ModelClientStreamingChunkEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model MultiModalMessage[source]#
```

【中文翻译】Bases: BaseChatMessage
A multimodal message.

Show JSON schema{
   "title": "MultiModalMessage",
   "description": "A multimodal message.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "anyOf": [
               {
                  "type": "string"
               },
               {}
            ]
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "MultiModalMessage",
         "default": "MultiModalMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[str | autogen_core._image.Image])
type (Literal['MultiModalMessage'])





field content: List[str | Image] [Required]#
The content of the message.



field type: Literal['MultiModalMessage'] = 'MultiModalMessage'#



to_model_message() → UserMessage[source]#
Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.



to_model_text(image_placeholder: str | None = '[image]') → str[source]#
Convert the content of the message to a string-only representation.
If an image is present, it will be replaced with the image placeholder
by default, otherwise it will be a base64 string when set to None.



to_text(iterm: bool = False) → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "MultiModalMessage",
   "description": "A multimodal message.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "anyOf": [
               {
                  "type": "string"
               },
               {}
            ]
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "MultiModalMessage",
         "default": "MultiModalMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[str | Image] [Required]#
```

【中文翻译】The content of the message.

```python
field type: Literal['MultiModalMessage'] = 'MultiModalMessage'#
```

```python
to_model_message() → UserMessage[source]#
```

【中文翻译】Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.

```python
to_model_text(image_placeholder: str | None = '[image]') → str[source]#
```

【中文翻译】Convert the content of the message to a string-only representation.
If an image is present, it will be replaced with the image placeholder
by default, otherwise it will be a base64 string when set to None.

```python
to_text(iterm: bool = False) → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model SelectSpeakerEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling the selection of speakers for a conversation.

Show JSON schema{
   "title": "SelectSpeakerEvent",
   "description": "An event signaling the selection of speakers for a conversation.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "type": "string"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "SelectSpeakerEvent",
         "default": "SelectSpeakerEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[str])
type (Literal['SelectSpeakerEvent'])





field content: List[str] [Required]#
The names of the selected speakers.



field type: Literal['SelectSpeakerEvent'] = 'SelectSpeakerEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "SelectSpeakerEvent",
   "description": "An event signaling the selection of speakers for a conversation.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "type": "string"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "SelectSpeakerEvent",
         "default": "SelectSpeakerEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[str] [Required]#
```

【中文翻译】The names of the selected speakers.

```python
field type: Literal['SelectSpeakerEvent'] = 'SelectSpeakerEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model StopMessage[source]#
```

【中文翻译】Bases: BaseTextChatMessage
A message requesting stop of a conversation.

Show JSON schema{
   "title": "StopMessage",
   "description": "A message requesting stop of a conversation.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "StopMessage",
         "default": "StopMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

type (Literal['StopMessage'])





field type: Literal['StopMessage'] = 'StopMessage'#

**示例**:
```python
{
   "title": "StopMessage",
   "description": "A message requesting stop of a conversation.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "StopMessage",
         "default": "StopMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field type: Literal['StopMessage'] = 'StopMessage'#
```

```python
class StructuredContentType#
```

【中文翻译】Type variable for structured content types.
alias of TypeVar(‘StructuredContentType’, bound=BaseModel, covariant=True)

```python
pydantic model StructuredMessage[source]#
```

【中文翻译】Bases: BaseChatMessage, Generic[StructuredContentType]
A BaseChatMessage type with an unspecified content type.
To create a new structured message type, specify the content type
as a subclass of Pydantic BaseModel.
from pydantic import BaseModel
from autogen_agentchat.messages import StructuredMessage


class MyMessageContent(BaseModel):
    text: str
    number: int


message = StructuredMessage[MyMessageContent](
    content=MyMessageContent(text="Hello", number=42),
    source="agent1",
)

print(message.to_text())  # {"text": "Hello", "number": 42}


from pydantic import BaseModel
from autogen_agentchat.messages import StructuredMessage


class MyMessageContent(BaseModel):
    text: str
    number: int


message = StructuredMessage[MyMessageContent](
    content=MyMessageContent(text="Hello", number=42),
    source="agent",
    format_string="Hello, {text} {number}!",
)

print(message.to_text())  # Hello, agent 42!



Show JSON schema{
   "title": "StructuredMessage",
   "description": "A :class:`BaseChatMessage` type with an unspecified content type.\n\nTo create a new structured message type, specify the content type\nas a subclass of `Pydantic BaseModel <https://docs.pydantic.dev/latest/concepts/models/>`_.\n\n.. code-block:: python\n\n    from pydantic import BaseModel\n    from autogen_agentchat.messages import StructuredMessage\n\n\n    class MyMessageContent(BaseModel):\n        text: str\n        number: int\n\n\n    message = StructuredMessage[MyMessageContent](\n        content=MyMessageContent(text=\"Hello\", number=42),\n        source=\"agent1\",\n    )\n\n    print(message.to_text())  # {\"text\": \"Hello\", \"number\": 42}\n\n.. code-block:: python\n\n    from pydantic import BaseModel\n    from autogen_agentchat.messages import StructuredMessage\n\n\n    class MyMessageContent(BaseModel):\n        text: str\n        number: int\n\n\n    message = StructuredMessage[MyMessageContent](\n        content=MyMessageContent(text=\"Hello\", number=42),\n        source=\"agent\",\n        format_string=\"Hello, {text} {number}!\",\n    )\n\n    print(message.to_text())  # Hello, agent 42!",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "$ref": "#/$defs/BaseModel"
      },
      "format_string": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Format String"
      }
   },
   "$defs": {
      "BaseModel": {
         "properties": {},
         "title": "BaseModel",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (autogen_agentchat.messages.StructuredContentType)
format_string (str | None)





field content: StructuredContentType [Required]#
The content of the message. Must be a subclass of
Pydantic BaseModel.



field format_string: str | None = None#
(Experimental) An optional format string to render the content into a human-readable format.
The format string can use the fields of the content model as placeholders.
For example, if the content model has a field name, you can use
{name} in the format string to include the value of that field.
The format string is used in the to_text() method to create a
human-readable representation of the message.
This setting is experimental and will change in the future.



to_model_message() → UserMessage[source]#
Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.



to_model_text() → str[source]#
Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.



property type: str#

**示例**:
```python
from pydantic import BaseModel
from autogen_agentchat.messages import StructuredMessage


class MyMessageContent(BaseModel):
    text: str
    number: int


message = StructuredMessage[MyMessageContent](
    content=MyMessageContent(text="Hello", number=42),
    source="agent1",
)

print(message.to_text())  # {"text": "Hello", "number": 42}

```

**示例**:
```python
from pydantic import BaseModel
from autogen_agentchat.messages import StructuredMessage


class MyMessageContent(BaseModel):
    text: str
    number: int


message = StructuredMessage[MyMessageContent](
    content=MyMessageContent(text="Hello", number=42),
    source="agent",
    format_string="Hello, {text} {number}!",
)

print(message.to_text())  # Hello, agent 42!

```

**示例**:
```python
{
   "title": "StructuredMessage",
   "description": "A :class:`BaseChatMessage` type with an unspecified content type.\n\nTo create a new structured message type, specify the content type\nas a subclass of `Pydantic BaseModel <https://docs.pydantic.dev/latest/concepts/models/>`_.\n\n.. code-block:: python\n\n    from pydantic import BaseModel\n    from autogen_agentchat.messages import StructuredMessage\n\n\n    class MyMessageContent(BaseModel):\n        text: str\n        number: int\n\n\n    message = StructuredMessage[MyMessageContent](\n        content=MyMessageContent(text=\"Hello\", number=42),\n        source=\"agent1\",\n    )\n\n    print(message.to_text())  # {\"text\": \"Hello\", \"number\": 42}\n\n.. code-block:: python\n\n    from pydantic import BaseModel\n    from autogen_agentchat.messages import StructuredMessage\n\n\n    class MyMessageContent(BaseModel):\n        text: str\n        number: int\n\n\n    message = StructuredMessage[MyMessageContent](\n        content=MyMessageContent(text=\"Hello\", number=42),\n        source=\"agent\",\n        format_string=\"Hello, {text} {number}!\",\n    )\n\n    print(message.to_text())  # Hello, agent 42!",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "$ref": "#/$defs/BaseModel"
      },
      "format_string": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Format String"
      }
   },
   "$defs": {
      "BaseModel": {
         "properties": {},
         "title": "BaseModel",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: StructuredContentType [Required]#
```

【中文翻译】The content of the message. Must be a subclass of
Pydantic BaseModel.

```python
field format_string: str | None = None#
```

【中文翻译】(Experimental) An optional format string to render the content into a human-readable format.
The format string can use the fields of the content model as placeholders.
For example, if the content model has a field name, you can use
{name} in the format string to include the value of that field.
The format string is used in the to_text() method to create a
human-readable representation of the message.
This setting is experimental and will change in the future.

```python
to_model_message() → UserMessage[source]#
```

【中文翻译】Convert the message content to a UserMessage
for use with model client, e.g., ChatCompletionClient.

```python
to_model_text() → str[source]#
```

【中文翻译】Convert the content of the message to text-only representation.
This is used for creating text-only content for models.
This is not used for rendering the message in console. For that, use
to_text().
The difference between this and to_model_message() is that this
is used to construct parts of the a message for the model client,
while to_model_message() is used to create a complete message
for the model client.

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
property type: str#
```

```python
pydantic model TextMessage[source]#
```

【中文翻译】Bases: BaseTextChatMessage
A text message with string-only content.

Show JSON schema{
   "title": "TextMessage",
   "description": "A text message with string-only content.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "TextMessage",
         "default": "TextMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

type (Literal['TextMessage'])





field type: Literal['TextMessage'] = 'TextMessage'#

**示例**:
```python
{
   "title": "TextMessage",
   "description": "A text message with string-only content.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "TextMessage",
         "default": "TextMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field type: Literal['TextMessage'] = 'TextMessage'#
```

```python
pydantic model ThoughtEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling the thought process of a model.
It is used to communicate the reasoning tokens generated by a reasoning model,
or the extra text content generated by a function call.

Show JSON schema{
   "title": "ThoughtEvent",
   "description": "An event signaling the thought process of a model.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ThoughtEvent",
         "default": "ThoughtEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (str)
type (Literal['ThoughtEvent'])





field content: str [Required]#
The thought process of the model.



field type: Literal['ThoughtEvent'] = 'ThoughtEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "ThoughtEvent",
   "description": "An event signaling the thought process of a model.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ThoughtEvent",
         "default": "ThoughtEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】The thought process of the model.

```python
field type: Literal['ThoughtEvent'] = 'ThoughtEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model ToolCallExecutionEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling the execution of tool calls.

Show JSON schema{
   "title": "ToolCallExecutionEvent",
   "description": "An event signaling the execution of tool calls.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionExecutionResult"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "ToolCallExecutionEvent",
         "default": "ToolCallExecutionEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[autogen_core.models._types.FunctionExecutionResult])
type (Literal['ToolCallExecutionEvent'])





field content: List[FunctionExecutionResult] [Required]#
The tool call results.



field type: Literal['ToolCallExecutionEvent'] = 'ToolCallExecutionEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "ToolCallExecutionEvent",
   "description": "An event signaling the execution of tool calls.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionExecutionResult"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "ToolCallExecutionEvent",
         "default": "ToolCallExecutionEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[FunctionExecutionResult] [Required]#
```

【中文翻译】The tool call results.

```python
field type: Literal['ToolCallExecutionEvent'] = 'ToolCallExecutionEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model ToolCallRequestEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling a request to use tools.

Show JSON schema{
   "title": "ToolCallRequestEvent",
   "description": "An event signaling a request to use tools.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionCall"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "ToolCallRequestEvent",
         "default": "ToolCallRequestEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

content (List[autogen_core._types.FunctionCall])
type (Literal['ToolCallRequestEvent'])





field content: List[FunctionCall] [Required]#
The tool calls.



field type: Literal['ToolCallRequestEvent'] = 'ToolCallRequestEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "ToolCallRequestEvent",
   "description": "An event signaling a request to use tools.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionCall"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "ToolCallRequestEvent",
         "default": "ToolCallRequestEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field content: List[FunctionCall] [Required]#
```

【中文翻译】The tool calls.

```python
field type: Literal['ToolCallRequestEvent'] = 'ToolCallRequestEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

```python
pydantic model ToolCallSummaryMessage[source]#
```

【中文翻译】Bases: BaseTextChatMessage
A message signaling the summary of tool call results.

Show JSON schema{
   "title": "ToolCallSummaryMessage",
   "description": "A message signaling the summary of tool call results.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ToolCallSummaryMessage",
         "default": "ToolCallSummaryMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}



Fields:

type (Literal['ToolCallSummaryMessage'])





field type: Literal['ToolCallSummaryMessage'] = 'ToolCallSummaryMessage'#

**示例**:
```python
{
   "title": "ToolCallSummaryMessage",
   "description": "A message signaling the summary of tool call results.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "ToolCallSummaryMessage",
         "default": "ToolCallSummaryMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "content"
   ]
}

```

```python
field type: Literal['ToolCallSummaryMessage'] = 'ToolCallSummaryMessage'#
```

```python
pydantic model UserInputRequestedEvent[source]#
```

【中文翻译】Bases: BaseAgentEvent
An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.

Show JSON schema{
   "title": "UserInputRequestedEvent",
   "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "request_id": {
         "title": "Request Id",
         "type": "string"
      },
      "content": {
         "const": "",
         "default": "",
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "UserInputRequestedEvent",
         "default": "UserInputRequestedEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "request_id"
   ]
}



Fields:

content (Literal[''])
request_id (str)
type (Literal['UserInputRequestedEvent'])





field content: Literal[''] = ''#
Empty content for compat with consumers expecting a content field.



field request_id: str [Required]#
Identifier for the user input request.



field type: Literal['UserInputRequestedEvent'] = 'UserInputRequestedEvent'#



to_text() → str[source]#
Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

**示例**:
```python
{
   "title": "UserInputRequestedEvent",
   "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.",
   "type": "object",
   "properties": {
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "request_id": {
         "title": "Request Id",
         "type": "string"
      },
      "content": {
         "const": "",
         "default": "",
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "UserInputRequestedEvent",
         "default": "UserInputRequestedEvent",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source",
      "request_id"
   ]
}

```

```python
field content: Literal[''] = ''#
```

【中文翻译】Empty content for compat with consumers expecting a content field.

```python
field request_id: str [Required]#
```

【中文翻译】Identifier for the user input request.

```python
field type: Literal['UserInputRequestedEvent'] = 'UserInputRequestedEvent'#
```

```python
to_text() → str[source]#
```

【中文翻译】Convert the message content to a string-only representation
that can be rendered in the console and inspected by the user or conditions.
This is not used for creating text-only content for models.
For BaseChatMessage types, use to_model_text() instead.

【中文翻译】This module defines various message types used for agent-to-agent communication.
Each message type inherits either from the BaseChatMessage class or BaseAgentEvent
class and includes specific fields relevant to the type of message being sent.

【中文翻译】previous

【中文翻译】autogen_agentchat

【中文翻译】next

【中文翻译】autogen_agentchat.agents

### autogen_agentchat.state {autogen_agentchatstate}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html)

```python
pydantic model AssistantAgentState[source]#
```

【中文翻译】Bases: BaseState
State for an assistant agent.

Show JSON schema{
   "title": "AssistantAgentState",
   "description": "State for an assistant agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "AssistantAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "llm_context": {
         "title": "Llm Context",
         "type": "object"
      }
   }
}



Fields:

llm_context (Mapping[str, Any])
type (str)





field llm_context: Mapping[str, Any] [Optional]#



field type: str = 'AssistantAgentState'#

**示例**:
```python
{
   "title": "AssistantAgentState",
   "description": "State for an assistant agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "AssistantAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "llm_context": {
         "title": "Llm Context",
         "type": "object"
      }
   }
}

```

```python
field llm_context: Mapping[str, Any] [Optional]#
```

```python
field type: str = 'AssistantAgentState'#
```

```python
pydantic model BaseGroupChatManagerState[source]#
```

【中文翻译】Bases: BaseState
Base state for all group chat managers.

Show JSON schema{
   "title": "BaseGroupChatManagerState",
   "description": "Base state for all group chat managers.",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseGroupChatManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      }
   }
}



Fields:

current_turn (int)
message_thread (List[Mapping[str, Any]])
type (str)





field current_turn: int = 0#



field message_thread: List[Mapping[str, Any]] [Optional]#



field type: str = 'BaseGroupChatManagerState'#

**示例**:
```python
{
   "title": "BaseGroupChatManagerState",
   "description": "Base state for all group chat managers.",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseGroupChatManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      }
   }
}

```

```python
field current_turn: int = 0#
```

```python
field message_thread: List[Mapping[str, Any]] [Optional]#
```

```python
field type: str = 'BaseGroupChatManagerState'#
```

```python
pydantic model BaseState[source]#
```

【中文翻译】Bases: BaseModel
Base class for all saveable state

Show JSON schema{
   "title": "BaseState",
   "description": "Base class for all saveable state",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      }
   }
}



Fields:

type (str)
version (str)





field type: str = 'BaseState'#



field version: str = '1.0.0'#

**示例**:
```python
{
   "title": "BaseState",
   "description": "Base class for all saveable state",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      }
   }
}

```

```python
field type: str = 'BaseState'#
```

```python
field version: str = '1.0.0'#
```

```python
pydantic model ChatAgentContainerState[source]#
```

【中文翻译】Bases: BaseState
State for a container of chat agents.

Show JSON schema{
   "title": "ChatAgentContainerState",
   "description": "State for a container of chat agents.",
   "type": "object",
   "properties": {
      "type": {
         "default": "ChatAgentContainerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "agent_state": {
         "title": "Agent State",
         "type": "object"
      },
      "message_buffer": {
         "items": {
            "type": "object"
         },
         "title": "Message Buffer",
         "type": "array"
      }
   }
}



Fields:

agent_state (Mapping[str, Any])
message_buffer (List[Mapping[str, Any]])
type (str)





field agent_state: Mapping[str, Any] [Optional]#



field message_buffer: List[Mapping[str, Any]] [Optional]#



field type: str = 'ChatAgentContainerState'#

**示例**:
```python
{
   "title": "ChatAgentContainerState",
   "description": "State for a container of chat agents.",
   "type": "object",
   "properties": {
      "type": {
         "default": "ChatAgentContainerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "agent_state": {
         "title": "Agent State",
         "type": "object"
      },
      "message_buffer": {
         "items": {
            "type": "object"
         },
         "title": "Message Buffer",
         "type": "array"
      }
   }
}

```

```python
field agent_state: Mapping[str, Any] [Optional]#
```

```python
field message_buffer: List[Mapping[str, Any]] [Optional]#
```

```python
field type: str = 'ChatAgentContainerState'#
```

```python
pydantic model MagenticOneOrchestratorState[source]#
```

【中文翻译】Bases: BaseGroupChatManagerState
State for MagneticOneGroupChat orchestrator.

Show JSON schema{
   "title": "MagenticOneOrchestratorState",
   "description": "State for :class:`~autogen_agentchat.teams.MagneticOneGroupChat` orchestrator.",
   "type": "object",
   "properties": {
      "type": {
         "default": "MagenticOneOrchestratorState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "task": {
         "default": "",
         "title": "Task",
         "type": "string"
      },
      "facts": {
         "default": "",
         "title": "Facts",
         "type": "string"
      },
      "plan": {
         "default": "",
         "title": "Plan",
         "type": "string"
      },
      "n_rounds": {
         "default": 0,
         "title": "N Rounds",
         "type": "integer"
      },
      "n_stalls": {
         "default": 0,
         "title": "N Stalls",
         "type": "integer"
      }
   }
}



Fields:

facts (str)
n_rounds (int)
n_stalls (int)
plan (str)
task (str)
type (str)





field facts: str = ''#



field n_rounds: int = 0#



field n_stalls: int = 0#



field plan: str = ''#



field task: str = ''#



field type: str = 'MagenticOneOrchestratorState'#

**示例**:
```python
{
   "title": "MagenticOneOrchestratorState",
   "description": "State for :class:`~autogen_agentchat.teams.MagneticOneGroupChat` orchestrator.",
   "type": "object",
   "properties": {
      "type": {
         "default": "MagenticOneOrchestratorState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "task": {
         "default": "",
         "title": "Task",
         "type": "string"
      },
      "facts": {
         "default": "",
         "title": "Facts",
         "type": "string"
      },
      "plan": {
         "default": "",
         "title": "Plan",
         "type": "string"
      },
      "n_rounds": {
         "default": 0,
         "title": "N Rounds",
         "type": "integer"
      },
      "n_stalls": {
         "default": 0,
         "title": "N Stalls",
         "type": "integer"
      }
   }
}

```

```python
field facts: str = ''#
```

```python
field n_rounds: int = 0#
```

```python
field n_stalls: int = 0#
```

```python
field plan: str = ''#
```

```python
field task: str = ''#
```

```python
field type: str = 'MagenticOneOrchestratorState'#
```

```python
pydantic model RoundRobinManagerState[source]#
```

【中文翻译】Bases: BaseGroupChatManagerState
State for RoundRobinGroupChat manager.

Show JSON schema{
   "title": "RoundRobinManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.RoundRobinGroupChat` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "RoundRobinManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "next_speaker_index": {
         "default": 0,
         "title": "Next Speaker Index",
         "type": "integer"
      }
   }
}



Fields:

next_speaker_index (int)
type (str)





field next_speaker_index: int = 0#



field type: str = 'RoundRobinManagerState'#

**示例**:
```python
{
   "title": "RoundRobinManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.RoundRobinGroupChat` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "RoundRobinManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "next_speaker_index": {
         "default": 0,
         "title": "Next Speaker Index",
         "type": "integer"
      }
   }
}

```

```python
field next_speaker_index: int = 0#
```

```python
field type: str = 'RoundRobinManagerState'#
```

```python
pydantic model SelectorManagerState[source]#
```

【中文翻译】Bases: BaseGroupChatManagerState
State for SelectorGroupChat manager.

Show JSON schema{
   "title": "SelectorManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.SelectorGroupChat` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SelectorManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "previous_speaker": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Previous Speaker"
      }
   }
}



Fields:

previous_speaker (str | None)
type (str)





field previous_speaker: str | None = None#



field type: str = 'SelectorManagerState'#

**示例**:
```python
{
   "title": "SelectorManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.SelectorGroupChat` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SelectorManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "previous_speaker": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Previous Speaker"
      }
   }
}

```

```python
field previous_speaker: str | None = None#
```

```python
field type: str = 'SelectorManagerState'#
```

```python
pydantic model SocietyOfMindAgentState[source]#
```

【中文翻译】Bases: BaseState
State for a Society of Mind agent.

Show JSON schema{
   "title": "SocietyOfMindAgentState",
   "description": "State for a Society of Mind agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SocietyOfMindAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "inner_team_state": {
         "title": "Inner Team State",
         "type": "object"
      }
   }
}



Fields:

inner_team_state (Mapping[str, Any])
type (str)





field inner_team_state: Mapping[str, Any] [Optional]#



field type: str = 'SocietyOfMindAgentState'#

**示例**:
```python
{
   "title": "SocietyOfMindAgentState",
   "description": "State for a Society of Mind agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SocietyOfMindAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "inner_team_state": {
         "title": "Inner Team State",
         "type": "object"
      }
   }
}

```

```python
field inner_team_state: Mapping[str, Any] [Optional]#
```

```python
field type: str = 'SocietyOfMindAgentState'#
```

```python
pydantic model SwarmManagerState[source]#
```

【中文翻译】Bases: BaseGroupChatManagerState
State for Swarm manager.

Show JSON schema{
   "title": "SwarmManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.Swarm` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SwarmManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "current_speaker": {
         "default": "",
         "title": "Current Speaker",
         "type": "string"
      }
   }
}



Fields:

current_speaker (str)
type (str)





field current_speaker: str = ''#



field type: str = 'SwarmManagerState'#

**示例**:
```python
{
   "title": "SwarmManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.Swarm` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SwarmManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "current_speaker": {
         "default": "",
         "title": "Current Speaker",
         "type": "string"
      }
   }
}

```

```python
field current_speaker: str = ''#
```

```python
field type: str = 'SwarmManagerState'#
```

```python
pydantic model TeamState[source]#
```

【中文翻译】Bases: BaseState
State for a team of agents.

Show JSON schema{
   "title": "TeamState",
   "description": "State for a team of agents.",
   "type": "object",
   "properties": {
      "type": {
         "default": "TeamState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "agent_states": {
         "title": "Agent States",
         "type": "object"
      }
   }
}



Fields:

agent_states (Mapping[str, Any])
type (str)





field agent_states: Mapping[str, Any] [Optional]#



field type: str = 'TeamState'#

**示例**:
```python
{
   "title": "TeamState",
   "description": "State for a team of agents.",
   "type": "object",
   "properties": {
      "type": {
         "default": "TeamState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "agent_states": {
         "title": "Agent States",
         "type": "object"
      }
   }
}

```

```python
field agent_states: Mapping[str, Any] [Optional]#
```

```python
field type: str = 'TeamState'#
```

【中文翻译】State management for agents, teams and termination conditions.

【中文翻译】previous

【中文翻译】autogen_agentchat.ui

【中文翻译】next

【中文翻译】autogen_core

### autogen_agentchat.state {autogen_agentchatstate}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html)

```python
pydantic model AssistantAgentState[source]#
```

【中文翻译】Bases: BaseState
State for an assistant agent.

Show JSON schema{
   "title": "AssistantAgentState",
   "description": "State for an assistant agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "AssistantAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "llm_context": {
         "title": "Llm Context",
         "type": "object"
      }
   }
}



Fields:

llm_context (Mapping[str, Any])
type (str)





field llm_context: Mapping[str, Any] [Optional]#



field type: str = 'AssistantAgentState'#

**示例**:
```python
{
   "title": "AssistantAgentState",
   "description": "State for an assistant agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "AssistantAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "llm_context": {
         "title": "Llm Context",
         "type": "object"
      }
   }
}

```

```python
field llm_context: Mapping[str, Any] [Optional]#
```

```python
field type: str = 'AssistantAgentState'#
```

```python
pydantic model BaseGroupChatManagerState[source]#
```

【中文翻译】Bases: BaseState
Base state for all group chat managers.

Show JSON schema{
   "title": "BaseGroupChatManagerState",
   "description": "Base state for all group chat managers.",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseGroupChatManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      }
   }
}



Fields:

current_turn (int)
message_thread (List[Mapping[str, Any]])
type (str)





field current_turn: int = 0#



field message_thread: List[Mapping[str, Any]] [Optional]#



field type: str = 'BaseGroupChatManagerState'#

**示例**:
```python
{
   "title": "BaseGroupChatManagerState",
   "description": "Base state for all group chat managers.",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseGroupChatManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      }
   }
}

```

```python
field current_turn: int = 0#
```

```python
field message_thread: List[Mapping[str, Any]] [Optional]#
```

```python
field type: str = 'BaseGroupChatManagerState'#
```

```python
pydantic model BaseState[source]#
```

【中文翻译】Bases: BaseModel
Base class for all saveable state

Show JSON schema{
   "title": "BaseState",
   "description": "Base class for all saveable state",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      }
   }
}



Fields:

type (str)
version (str)





field type: str = 'BaseState'#



field version: str = '1.0.0'#

**示例**:
```python
{
   "title": "BaseState",
   "description": "Base class for all saveable state",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      }
   }
}

```

```python
field type: str = 'BaseState'#
```

```python
field version: str = '1.0.0'#
```

```python
pydantic model ChatAgentContainerState[source]#
```

【中文翻译】Bases: BaseState
State for a container of chat agents.

Show JSON schema{
   "title": "ChatAgentContainerState",
   "description": "State for a container of chat agents.",
   "type": "object",
   "properties": {
      "type": {
         "default": "ChatAgentContainerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "agent_state": {
         "title": "Agent State",
         "type": "object"
      },
      "message_buffer": {
         "items": {
            "type": "object"
         },
         "title": "Message Buffer",
         "type": "array"
      }
   }
}



Fields:

agent_state (Mapping[str, Any])
message_buffer (List[Mapping[str, Any]])
type (str)





field agent_state: Mapping[str, Any] [Optional]#



field message_buffer: List[Mapping[str, Any]] [Optional]#



field type: str = 'ChatAgentContainerState'#

**示例**:
```python
{
   "title": "ChatAgentContainerState",
   "description": "State for a container of chat agents.",
   "type": "object",
   "properties": {
      "type": {
         "default": "ChatAgentContainerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "agent_state": {
         "title": "Agent State",
         "type": "object"
      },
      "message_buffer": {
         "items": {
            "type": "object"
         },
         "title": "Message Buffer",
         "type": "array"
      }
   }
}

```

```python
field agent_state: Mapping[str, Any] [Optional]#
```

```python
field message_buffer: List[Mapping[str, Any]] [Optional]#
```

```python
field type: str = 'ChatAgentContainerState'#
```

```python
pydantic model MagenticOneOrchestratorState[source]#
```

【中文翻译】Bases: BaseGroupChatManagerState
State for MagneticOneGroupChat orchestrator.

Show JSON schema{
   "title": "MagenticOneOrchestratorState",
   "description": "State for :class:`~autogen_agentchat.teams.MagneticOneGroupChat` orchestrator.",
   "type": "object",
   "properties": {
      "type": {
         "default": "MagenticOneOrchestratorState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "task": {
         "default": "",
         "title": "Task",
         "type": "string"
      },
      "facts": {
         "default": "",
         "title": "Facts",
         "type": "string"
      },
      "plan": {
         "default": "",
         "title": "Plan",
         "type": "string"
      },
      "n_rounds": {
         "default": 0,
         "title": "N Rounds",
         "type": "integer"
      },
      "n_stalls": {
         "default": 0,
         "title": "N Stalls",
         "type": "integer"
      }
   }
}



Fields:

facts (str)
n_rounds (int)
n_stalls (int)
plan (str)
task (str)
type (str)





field facts: str = ''#



field n_rounds: int = 0#



field n_stalls: int = 0#



field plan: str = ''#



field task: str = ''#



field type: str = 'MagenticOneOrchestratorState'#

**示例**:
```python
{
   "title": "MagenticOneOrchestratorState",
   "description": "State for :class:`~autogen_agentchat.teams.MagneticOneGroupChat` orchestrator.",
   "type": "object",
   "properties": {
      "type": {
         "default": "MagenticOneOrchestratorState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "task": {
         "default": "",
         "title": "Task",
         "type": "string"
      },
      "facts": {
         "default": "",
         "title": "Facts",
         "type": "string"
      },
      "plan": {
         "default": "",
         "title": "Plan",
         "type": "string"
      },
      "n_rounds": {
         "default": 0,
         "title": "N Rounds",
         "type": "integer"
      },
      "n_stalls": {
         "default": 0,
         "title": "N Stalls",
         "type": "integer"
      }
   }
}

```

```python
field facts: str = ''#
```

```python
field n_rounds: int = 0#
```

```python
field n_stalls: int = 0#
```

```python
field plan: str = ''#
```

```python
field task: str = ''#
```

```python
field type: str = 'MagenticOneOrchestratorState'#
```

```python
pydantic model RoundRobinManagerState[source]#
```

【中文翻译】Bases: BaseGroupChatManagerState
State for RoundRobinGroupChat manager.

Show JSON schema{
   "title": "RoundRobinManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.RoundRobinGroupChat` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "RoundRobinManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "next_speaker_index": {
         "default": 0,
         "title": "Next Speaker Index",
         "type": "integer"
      }
   }
}



Fields:

next_speaker_index (int)
type (str)





field next_speaker_index: int = 0#



field type: str = 'RoundRobinManagerState'#

**示例**:
```python
{
   "title": "RoundRobinManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.RoundRobinGroupChat` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "RoundRobinManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "next_speaker_index": {
         "default": 0,
         "title": "Next Speaker Index",
         "type": "integer"
      }
   }
}

```

```python
field next_speaker_index: int = 0#
```

```python
field type: str = 'RoundRobinManagerState'#
```

```python
pydantic model SelectorManagerState[source]#
```

【中文翻译】Bases: BaseGroupChatManagerState
State for SelectorGroupChat manager.

Show JSON schema{
   "title": "SelectorManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.SelectorGroupChat` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SelectorManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "previous_speaker": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Previous Speaker"
      }
   }
}



Fields:

previous_speaker (str | None)
type (str)





field previous_speaker: str | None = None#



field type: str = 'SelectorManagerState'#

**示例**:
```python
{
   "title": "SelectorManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.SelectorGroupChat` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SelectorManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "previous_speaker": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Previous Speaker"
      }
   }
}

```

```python
field previous_speaker: str | None = None#
```

```python
field type: str = 'SelectorManagerState'#
```

```python
pydantic model SocietyOfMindAgentState[source]#
```

【中文翻译】Bases: BaseState
State for a Society of Mind agent.

Show JSON schema{
   "title": "SocietyOfMindAgentState",
   "description": "State for a Society of Mind agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SocietyOfMindAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "inner_team_state": {
         "title": "Inner Team State",
         "type": "object"
      }
   }
}



Fields:

inner_team_state (Mapping[str, Any])
type (str)





field inner_team_state: Mapping[str, Any] [Optional]#



field type: str = 'SocietyOfMindAgentState'#

**示例**:
```python
{
   "title": "SocietyOfMindAgentState",
   "description": "State for a Society of Mind agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SocietyOfMindAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "inner_team_state": {
         "title": "Inner Team State",
         "type": "object"
      }
   }
}

```

```python
field inner_team_state: Mapping[str, Any] [Optional]#
```

```python
field type: str = 'SocietyOfMindAgentState'#
```

```python
pydantic model SwarmManagerState[source]#
```

【中文翻译】Bases: BaseGroupChatManagerState
State for Swarm manager.

Show JSON schema{
   "title": "SwarmManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.Swarm` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SwarmManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "current_speaker": {
         "default": "",
         "title": "Current Speaker",
         "type": "string"
      }
   }
}



Fields:

current_speaker (str)
type (str)





field current_speaker: str = ''#



field type: str = 'SwarmManagerState'#

**示例**:
```python
{
   "title": "SwarmManagerState",
   "description": "State for :class:`~autogen_agentchat.teams.Swarm` manager.",
   "type": "object",
   "properties": {
      "type": {
         "default": "SwarmManagerState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "message_thread": {
         "items": {
            "type": "object"
         },
         "title": "Message Thread",
         "type": "array"
      },
      "current_turn": {
         "default": 0,
         "title": "Current Turn",
         "type": "integer"
      },
      "current_speaker": {
         "default": "",
         "title": "Current Speaker",
         "type": "string"
      }
   }
}

```

```python
field current_speaker: str = ''#
```

```python
field type: str = 'SwarmManagerState'#
```

```python
pydantic model TeamState[source]#
```

【中文翻译】Bases: BaseState
State for a team of agents.

Show JSON schema{
   "title": "TeamState",
   "description": "State for a team of agents.",
   "type": "object",
   "properties": {
      "type": {
         "default": "TeamState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "agent_states": {
         "title": "Agent States",
         "type": "object"
      }
   }
}



Fields:

agent_states (Mapping[str, Any])
type (str)





field agent_states: Mapping[str, Any] [Optional]#



field type: str = 'TeamState'#

**示例**:
```python
{
   "title": "TeamState",
   "description": "State for a team of agents.",
   "type": "object",
   "properties": {
      "type": {
         "default": "TeamState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "agent_states": {
         "title": "Agent States",
         "type": "object"
      }
   }
}

```

```python
field agent_states: Mapping[str, Any] [Optional]#
```

```python
field type: str = 'TeamState'#
```

【中文翻译】State management for agents, teams and termination conditions.

【中文翻译】previous

【中文翻译】autogen_agentchat.ui

【中文翻译】next

【中文翻译】autogen_core

### autogen_agentchat.teams {autogen_agentchatteams}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html)

```python
class BaseGroupChat(participants: List[ChatAgent], group_chat_manager_name: str, group_chat_manager_class: type[SequentialRoutedAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]#
```

【中文翻译】Bases: Team, ABC, ComponentBase[BaseModel]
The base class for group chat teams.
To implement a group chat team, first create a subclass of BaseGroupChatManager and then
create a subclass of BaseGroupChat that uses the group chat manager.


component_type: ClassVar[ComponentType] = 'team'#
The logical type of the component.



async load_state(state: Mapping[str, Any]) → None[source]#
Load an external state and overwrite the current state of the group chat team.
The state is loaded by calling the agent_load_state() method
on each participant and the group chat manager with their internal agent ID.
See save_state() for the expected format of the state.



async pause() → None[source]#
Pause its participants when the team is running by calling their
on_pause() method via direct RPC calls.

Attention
This is an experimental feature introduced in v0.4.9 and may subject
to change or removal in the future.

The team must be initialized before it can be paused.
Different from termination, pausing the team does not cause the
run() or run_stream() method to return. It calls the
on_pause() method on each
participant, and if the participant does not implement the method, it
will be a no-op.

Note
It is the responsibility of the agent class to handle the pause
and ensure that the agent can be resumed later.
Make sure to implement the on_pause()
method in your agent class for custom pause behavior.
By default, the agent will not do anything when called.


Raises:
RuntimeError – If the team has not been initialized. Exceptions from
    the participants when calling their implementations of
    on_pause are
    propagated to this method and raised.





async reset() → None[source]#
Reset the team and its participants to their initial state.
The team must be stopped before it can be reset.

Raises:
RuntimeError – If the team has not been initialized or is currently running.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Reset the team.
    await team.reset()
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)


asyncio.run(main())





async resume() → None[source]#
Resume its participants when the team is running and paused by calling their
on_resume() method via direct RPC calls.

Attention
This is an experimental feature introduced in v0.4.9 and may subject
to change or removal in the future.

The team must be initialized before it can be resumed.
Different from termination and restart with a new task, resuming the team
does not cause the run() or run_stream() method to return.
It calls the on_resume() method on each
participant, and if the participant does not implement the method, it
will be a no-op.

Note
It is the responsibility of the agent class to handle the resume
and ensure that the agent continues from where it was paused.
Make sure to implement the on_resume()
method in your agent class for custom resume behavior.


Raises:
RuntimeError – If the team has not been initialized. Exceptions from
    the participants when calling their implementations of on_resume
    method are propagated to this method and raised.





async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
Run the team and return the result. The base implementation uses
run_stream() to run the team and then returns the final result.
Once the team is stopped, the termination condition is reset.

Parameters:

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.
cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately.
Setting the cancellation token potentially put the team in an inconsistent state,
and it may not reset the termination condition.
To gracefully stop the team, use ExternalTermination instead.


Returns:
result – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())


Example using the CancellationToken to cancel the task:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())





async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
Run the team and produces a stream of messages and the final result
of the type TaskResult as the last item in the stream. Once the
team is stopped, the termination condition is reset.

Note
If an agent produces ModelClientStreamingChunkEvent,
the message will be yielded in the stream but it will not be included in the
messages.


Parameters:

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.
cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately.
Setting the cancellation token potentially put the team in an inconsistent state,
and it may not reset the termination condition.
To gracefully stop the team, use ExternalTermination instead.


Returns:
stream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Run the team again without a task to continue the previous task.
    stream = team.run_stream()
    async for message in stream:
        print(message)


asyncio.run(main())


Example using the CancellationToken to cancel the task:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        Console(
            team.run_stream(
                task="Count from 1 to 10, respond one at a time.",
                cancellation_token=cancellation_token,
            )
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())





async save_state() → Mapping[str, Any][source]#
Save the state of the group chat team.
The state is saved by calling the agent_save_state() method
on each participant and the group chat manager with their internal agent ID.
The state is returned as a nested dictionary: a dictionary with key agent_states,
which is a dictionary the agent names as keys and the state as values.
{
    "agent_states": {
        "agent1": ...,
        "agent2": ...,
        "RoundRobinGroupChatManager": ...
    }
}



Note
Starting v0.4.9, the state is using the agent name as the key instead of the agent ID,
and the team_id field is removed from the state. This is to allow the state to be
portable across different teams and runtimes. States saved with the old format
may not be compatible with the new format in the future.


Caution
When calling save_state() on a team
while it is running, the state may not be consistent and may result in an unexpected state.
It is recommended to call this method when the team is not running or after it is stopped.

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Reset the team.
    await team.reset()
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Run the team again without a task to continue the previous task.
    stream = team.run_stream()
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        Console(
            team.run_stream(
                task="Count from 1 to 10, respond one at a time.",
                cancellation_token=cancellation_token,
            )
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

```

**示例**:
```python
{
    "agent_states": {
        "agent1": ...,
        "agent2": ...,
        "RoundRobinGroupChatManager": ...
    }
}

```

```python
component_type: ClassVar[ComponentType] = 'team'#
```

【中文翻译】The logical type of the component.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load an external state and overwrite the current state of the group chat team.
The state is loaded by calling the agent_load_state() method
on each participant and the group chat manager with their internal agent ID.
See save_state() for the expected format of the state.

```python
async pause() → None[source]#
```

【中文翻译】Pause its participants when the team is running by calling their
on_pause() method via direct RPC calls.

Attention
This is an experimental feature introduced in v0.4.9 and may subject
to change or removal in the future.

The team must be initialized before it can be paused.
Different from termination, pausing the team does not cause the
run() or run_stream() method to return. It calls the
on_pause() method on each
participant, and if the participant does not implement the method, it
will be a no-op.

Note
It is the responsibility of the agent class to handle the pause
and ensure that the agent can be resumed later.
Make sure to implement the on_pause()
method in your agent class for custom pause behavior.
By default, the agent will not do anything when called.


Raises:
RuntimeError – If the team has not been initialized. Exceptions from
    the participants when calling their implementations of
    on_pause are
    propagated to this method and raised.

```python
async reset() → None[source]#
```

【中文翻译】Reset the team and its participants to their initial state.
The team must be stopped before it can be reset.

Raises:
RuntimeError – If the team has not been initialized or is currently running.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Reset the team.
    await team.reset()
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)


asyncio.run(main())

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Reset the team.
    await team.reset()
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)


asyncio.run(main())

```

```python
async resume() → None[source]#
```

【中文翻译】Resume its participants when the team is running and paused by calling their
on_resume() method via direct RPC calls.

Attention
This is an experimental feature introduced in v0.4.9 and may subject
to change or removal in the future.

The team must be initialized before it can be resumed.
Different from termination and restart with a new task, resuming the team
does not cause the run() or run_stream() method to return.
It calls the on_resume() method on each
participant, and if the participant does not implement the method, it
will be a no-op.

Note
It is the responsibility of the agent class to handle the resume
and ensure that the agent continues from where it was paused.
Make sure to implement the on_resume()
method in your agent class for custom resume behavior.


Raises:
RuntimeError – If the team has not been initialized. Exceptions from
    the participants when calling their implementations of on_resume
    method are propagated to this method and raised.

```python
async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
```

【中文翻译】Run the team and return the result. The base implementation uses
run_stream() to run the team and then returns the final result.
Once the team is stopped, the termination condition is reset.

Parameters:

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.
cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately.
Setting the cancellation token potentially put the team in an inconsistent state,
and it may not reset the termination condition.
To gracefully stop the team, use ExternalTermination instead.


Returns:
result – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())


Example using the CancellationToken to cancel the task:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

```

```python
async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
```

【中文翻译】Run the team and produces a stream of messages and the final result
of the type TaskResult as the last item in the stream. Once the
team is stopped, the termination condition is reset.

Note
If an agent produces ModelClientStreamingChunkEvent,
the message will be yielded in the stream but it will not be included in the
messages.


Parameters:

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.
cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately.
Setting the cancellation token potentially put the team in an inconsistent state,
and it may not reset the termination condition.
To gracefully stop the team, use ExternalTermination instead.


Returns:
stream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Run the team again without a task to continue the previous task.
    stream = team.run_stream()
    async for message in stream:
        print(message)


asyncio.run(main())


Example using the CancellationToken to cancel the task:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        Console(
            team.run_stream(
                task="Count from 1 to 10, respond one at a time.",
                cancellation_token=cancellation_token,
            )
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Run the team again without a task to continue the previous task.
    stream = team.run_stream()
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        Console(
            team.run_stream(
                task="Count from 1 to 10, respond one at a time.",
                cancellation_token=cancellation_token,
            )
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the group chat team.
The state is saved by calling the agent_save_state() method
on each participant and the group chat manager with their internal agent ID.
The state is returned as a nested dictionary: a dictionary with key agent_states,
which is a dictionary the agent names as keys and the state as values.
{
    "agent_states": {
        "agent1": ...,
        "agent2": ...,
        "RoundRobinGroupChatManager": ...
    }
}



Note
Starting v0.4.9, the state is using the agent name as the key instead of the agent ID,
and the team_id field is removed from the state. This is to allow the state to be
portable across different teams and runtimes. States saved with the old format
may not be compatible with the new format in the future.


Caution
When calling save_state() on a team
while it is running, the state may not be consistent and may result in an unexpected state.
It is recommended to call this method when the team is not running or after it is stopped.

**示例**:
```python
{
    "agent_states": {
        "agent1": ...,
        "agent2": ...,
        "RoundRobinGroupChatManager": ...
    }
}

```

```python
pydantic model DiGraph[source]#
```

【中文翻译】Bases: BaseModel
Defines a directed graph structure with nodes and edges.
GraphFlow uses this to determine execution order and conditions.

Warning
This is an experimental feature, and the API will change in the future releases.


Show JSON schema{
   "title": "DiGraph",
   "description": "Defines a directed graph structure with nodes and edges.\n:class:`GraphFlow` uses this to determine execution order and conditions.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "nodes": {
         "additionalProperties": {
            "$ref": "#/$defs/DiGraphNode"
         },
         "title": "Nodes",
         "type": "object"
      },
      "default_start_node": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Start Node"
      }
   },
   "$defs": {
      "DiGraphEdge": {
         "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "target": {
               "title": "Target",
               "type": "string"
            },
            "condition": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Condition"
            }
         },
         "required": [
            "target"
         ],
         "title": "DiGraphEdge",
         "type": "object"
      },
      "DiGraphNode": {
         "description": "Represents a node (agent) in a :class:`DiGraph`, with its outgoing edges and activation type.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "edges": {
               "default": [],
               "items": {
                  "$ref": "#/$defs/DiGraphEdge"
               },
               "title": "Edges",
               "type": "array"
            },
            "activation": {
               "default": "all",
               "enum": [
                  "all",
                  "any"
               ],
               "title": "Activation",
               "type": "string"
            }
         },
         "required": [
            "name"
         ],
         "title": "DiGraphNode",
         "type": "object"
      }
   },
   "required": [
      "nodes"
   ]
}



Fields:

default_start_node (str | None)
nodes (Dict[str, autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphNode])





field default_start_node: str | None = None#



field nodes: Dict[str, DiGraphNode] [Required]#



get_has_cycles() → bool[source]#
Indicates if the graph has at least one cycle (with valid exit conditions).



get_leaf_nodes() → Set[str][source]#
Return nodes that have no outgoing edges (final output nodes).



get_parents() → Dict[str, List[str]][source]#
Compute a mapping of each node to its parent nodes.



get_start_nodes() → Set[str][source]#
Return the nodes that have no incoming edges (entry points).



graph_validate() → None[source]#
Validate graph structure and execution rules.



has_cycles_with_exit() → bool[source]#
Check if the graph has any cycles and validate that each cycle has at least one conditional edge.

Returns:
bool – True if there is at least one cycle and all cycles have an exit condition.
False if there are no cycles.

Raises:
ValueError – If there is a cycle without any conditional edge.





model_post_init(context: Any, /) → None#
This function is meant to behave like a BaseModel method to initialise private attributes.
It takes context as an argument since that’s what pydantic-core passes when calling it.

Parameters:

self – The BaseModel instance.
context – The context.

**示例**:
```python
{
   "title": "DiGraph",
   "description": "Defines a directed graph structure with nodes and edges.\n:class:`GraphFlow` uses this to determine execution order and conditions.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "nodes": {
         "additionalProperties": {
            "$ref": "#/$defs/DiGraphNode"
         },
         "title": "Nodes",
         "type": "object"
      },
      "default_start_node": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Start Node"
      }
   },
   "$defs": {
      "DiGraphEdge": {
         "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "target": {
               "title": "Target",
               "type": "string"
            },
            "condition": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Condition"
            }
         },
         "required": [
            "target"
         ],
         "title": "DiGraphEdge",
         "type": "object"
      },
      "DiGraphNode": {
         "description": "Represents a node (agent) in a :class:`DiGraph`, with its outgoing edges and activation type.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "edges": {
               "default": [],
               "items": {
                  "$ref": "#/$defs/DiGraphEdge"
               },
               "title": "Edges",
               "type": "array"
            },
            "activation": {
               "default": "all",
               "enum": [
                  "all",
                  "any"
               ],
               "title": "Activation",
               "type": "string"
            }
         },
         "required": [
            "name"
         ],
         "title": "DiGraphNode",
         "type": "object"
      }
   },
   "required": [
      "nodes"
   ]
}

```

```python
field default_start_node: str | None = None#
```

```python
field nodes: Dict[str, DiGraphNode] [Required]#
```

```python
get_has_cycles() → bool[source]#
```

【中文翻译】Indicates if the graph has at least one cycle (with valid exit conditions).

```python
get_leaf_nodes() → Set[str][source]#
```

【中文翻译】Return nodes that have no outgoing edges (final output nodes).

```python
get_parents() → Dict[str, List[str]][source]#
```

【中文翻译】Compute a mapping of each node to its parent nodes.

```python
get_start_nodes() → Set[str][source]#
```

【中文翻译】Return the nodes that have no incoming edges (entry points).

```python
graph_validate() → None[source]#
```

【中文翻译】Validate graph structure and execution rules.

```python
has_cycles_with_exit() → bool[source]#
```

【中文翻译】Check if the graph has any cycles and validate that each cycle has at least one conditional edge.

Returns:
bool – True if there is at least one cycle and all cycles have an exit condition.
False if there are no cycles.

Raises:
ValueError – If there is a cycle without any conditional edge.

```python
model_post_init(context: Any, /) → None#
```

【中文翻译】This function is meant to behave like a BaseModel method to initialise private attributes.
It takes context as an argument since that’s what pydantic-core passes when calling it.

Parameters:

self – The BaseModel instance.
context – The context.

```python
class DiGraphBuilder[source]#
```

【中文翻译】Bases: object
A fluent builder for constructing DiGraph execution graphs used in GraphFlow.

Warning
This is an experimental feature, and the API will change in the future releases.

This utility provides a convenient way to programmatically build a graph of agent interactions,
including complex execution flows such as:

Sequential chains
Parallel fan-outs
Conditional branching
Cyclic loops with safe exits

Each node in the graph represents an agent. Edges define execution paths between agents,
and can optionally be conditioned on message content.
The builder is compatible with the Graph runner and supports both standard and filtered agents.


- add_node(agent, activation)
Add an agent node to the graph.



- add_edge(source, target, condition)
Connect two nodes optionally with a condition.



- add_conditional_edges(source, condition_to_target)
Add multiple conditional edges from a source.



- set_entry_point(agent)
Define the default start node (optional).



- build()
Generate a validated DiGraph.



- get_participants()
Return the list of added agents.


Example — Sequential Flow A → B → C:>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
>>> team = Graph(
...     participants=builder.get_participants(),
...     graph=builder.build(),
...     termination_condition=MaxMessageTermination(5),
... )



Example — Parallel Fan-out A → (B, C):>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)



Example — Conditional Branching A → B (“yes”), A → C (“no”):>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_conditional_edges(agent_a, {"yes": agent_b, "no": agent_c})



Example — Loop: A → B → A (“loop”), B → C (“exit”):>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b)
>>> builder.add_conditional_edges(agent_b, {"loop": agent_a, "exit": agent_c})






add_conditional_edges(source: str | ChatAgent, condition_to_target: Dict[str, str | ChatAgent]) → DiGraphBuilder[source]#
Add multiple conditional edges from a source node based on condition strings.



add_edge(source: str | ChatAgent, target: str | ChatAgent, condition: str | None = None) → DiGraphBuilder[source]#
Add a directed edge from source to target, optionally with a condition.



add_node(agent: ChatAgent, activation: Literal['all', 'any'] = 'all') → DiGraphBuilder[source]#
Add a node to the graph and register its agent.



build() → DiGraph[source]#
Build and validate the DiGraph.



get_participants() → list[ChatAgent][source]#
Return the list of agents in the builder, in insertion order.



set_entry_point(name: str | ChatAgent) → DiGraphBuilder[source]#
Set the default start node of the graph.

**示例**:
```python
>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
>>> team = Graph(
...     participants=builder.get_participants(),
...     graph=builder.build(),
...     termination_condition=MaxMessageTermination(5),
... )

```

**示例**:
```python
>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)

```

**示例**:
```python
>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_conditional_edges(agent_a, {"yes": agent_b, "no": agent_c})

```

**示例**:
```python
>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b)
>>> builder.add_conditional_edges(agent_b, {"loop": agent_a, "exit": agent_c})

```

```python
- add_node(agent, activation)
```

【中文翻译】Add an agent node to the graph.

```python
- add_edge(source, target, condition)
```

【中文翻译】Connect two nodes optionally with a condition.

```python
- add_conditional_edges(source, condition_to_target)
```

【中文翻译】Add multiple conditional edges from a source.

```python
- set_entry_point(agent)
```

【中文翻译】Define the default start node (optional).

```python
- build()
```

【中文翻译】Generate a validated DiGraph.

```python
- get_participants()
```

【中文翻译】Return the list of added agents.

```python
add_conditional_edges(source: str | ChatAgent, condition_to_target: Dict[str, str | ChatAgent]) → DiGraphBuilder[source]#
```

【中文翻译】Add multiple conditional edges from a source node based on condition strings.

```python
add_edge(source: str | ChatAgent, target: str | ChatAgent, condition: str | None = None) → DiGraphBuilder[source]#
```

【中文翻译】Add a directed edge from source to target, optionally with a condition.

```python
add_node(agent: ChatAgent, activation: Literal['all', 'any'] = 'all') → DiGraphBuilder[source]#
```

【中文翻译】Add a node to the graph and register its agent.

```python
build() → DiGraph[source]#
```

【中文翻译】Build and validate the DiGraph.

```python
get_participants() → list[ChatAgent][source]#
```

【中文翻译】Return the list of agents in the builder, in insertion order.

```python
set_entry_point(name: str | ChatAgent) → DiGraphBuilder[source]#
```

【中文翻译】Set the default start node of the graph.

```python
pydantic model DiGraphEdge[source]#
```

【中文翻译】Bases: BaseModel
Represents a directed edge in a DiGraph, with an optional execution condition.

Warning
This is an experimental feature, and the API will change in the future releases.


Show JSON schema{
   "title": "DiGraphEdge",
   "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "target": {
         "title": "Target",
         "type": "string"
      },
      "condition": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Condition"
      }
   },
   "required": [
      "target"
   ]
}



Fields:

condition (str | None)
target (str)





field condition: str | None = None#
(Experimental) Condition to execute this edge.
If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message.
NOTE: This is an experimental feature WILL change in the future releases to allow for better spcification of branching conditions
similar to the TerminationCondition class.



field target: str [Required]#

**示例**:
```python
{
   "title": "DiGraphEdge",
   "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "target": {
         "title": "Target",
         "type": "string"
      },
      "condition": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Condition"
      }
   },
   "required": [
      "target"
   ]
}

```

```python
field condition: str | None = None#
```

【中文翻译】(Experimental) Condition to execute this edge.
If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message.
NOTE: This is an experimental feature WILL change in the future releases to allow for better spcification of branching conditions
similar to the TerminationCondition class.

```python
field target: str [Required]#
```

```python
pydantic model DiGraphNode[source]#
```

【中文翻译】Bases: BaseModel
Represents a node (agent) in a DiGraph, with its outgoing edges and activation type.

Warning
This is an experimental feature, and the API will change in the future releases.


Show JSON schema{
   "title": "DiGraphNode",
   "description": "Represents a node (agent) in a :class:`DiGraph`, with its outgoing edges and activation type.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "name": {
         "title": "Name",
         "type": "string"
      },
      "edges": {
         "default": [],
         "items": {
            "$ref": "#/$defs/DiGraphEdge"
         },
         "title": "Edges",
         "type": "array"
      },
      "activation": {
         "default": "all",
         "enum": [
            "all",
            "any"
         ],
         "title": "Activation",
         "type": "string"
      }
   },
   "$defs": {
      "DiGraphEdge": {
         "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "target": {
               "title": "Target",
               "type": "string"
            },
            "condition": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Condition"
            }
         },
         "required": [
            "target"
         ],
         "title": "DiGraphEdge",
         "type": "object"
      }
   },
   "required": [
      "name"
   ]
}



Fields:

activation (Literal['all', 'any'])
edges (List[autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphEdge])
name (str)





field activation: Literal['all', 'any'] = 'all'#



field edges: List[DiGraphEdge] = []#



field name: str [Required]#

**示例**:
```python
{
   "title": "DiGraphNode",
   "description": "Represents a node (agent) in a :class:`DiGraph`, with its outgoing edges and activation type.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "name": {
         "title": "Name",
         "type": "string"
      },
      "edges": {
         "default": [],
         "items": {
            "$ref": "#/$defs/DiGraphEdge"
         },
         "title": "Edges",
         "type": "array"
      },
      "activation": {
         "default": "all",
         "enum": [
            "all",
            "any"
         ],
         "title": "Activation",
         "type": "string"
      }
   },
   "$defs": {
      "DiGraphEdge": {
         "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "target": {
               "title": "Target",
               "type": "string"
            },
            "condition": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Condition"
            }
         },
         "required": [
            "target"
         ],
         "title": "DiGraphEdge",
         "type": "object"
      }
   },
   "required": [
      "name"
   ]
}

```

```python
field activation: Literal['all', 'any'] = 'all'#
```

```python
field edges: List[DiGraphEdge] = []#
```

```python
field name: str [Required]#
```

```python
class GraphFlow(participants: List[ChatAgent], graph: DiGraph, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[GraphFlowConfig]
A team that runs a group chat following a Directed Graph execution pattern.

Warning
This is an experimental feature, and the API will change in the future releases.

This group chat executes agents based on a directed graph (DiGraph) structure,
allowing complex workflows such as sequential execution, parallel fan-out,
conditional branching, join patterns, and loops with explicit exit conditions.
The execution order is determined by the edges defined in the DiGraph. Each node
in the graph corresponds to an agent, and edges define the flow of messages between agents.
Nodes can be configured to activate when:


All parent nodes have completed (activation=”all”) → default
Any parent node completes (activation=”any”)


Conditional branching is supported using edge conditions, where the next agent(s) are selected
based on content in the chat history. Loops are permitted as long as there is a condition
that eventually exits the loop.

Note
Use the DiGraphBuilder class to create a DiGraph easily. It provides a fluent API
for adding nodes and edges, setting entry points, and validating the graph structure.
See the DiGraphBuilder documentation for more details.
The GraphFlow class is designed to be used with the DiGraphBuilder for creating complex workflows.


Parameters:

participants (List[ChatAgent]) – The participants in the group chat.
termination_condition (TerminationCondition, optional) – Termination condition for the chat.
max_turns (int, optional) – Maximum number of turns before forcing termination.
graph (DiGraph) – Directed execution graph defining node flow and conditions.


Raises:
ValueError – If participant names are not unique, or if graph validation fails (e.g., cycles without exit).


Examples
Sequential Flow: A → B → C
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.")
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to English.")

    # Create a directed graph with sequential flow A -> B -> C.
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short story about a cat."):
        print(event)


asyncio.run(main())


Parallel Fan-out: A → (B, C)
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.")
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Japanese.")

    # Create a directed graph with fan-out flow A -> (B, C).
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short story about a cat."):
        print(event)


asyncio.run(main())


Conditional Branching: A → B (if ‘yes’) or C (if ‘no’)
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent(
        "A",
        model_client=model_client,
        system_message="Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.",
    )
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to English.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Chinese.")

    # Create a directed graph with conditional branching flow A -> B ("yes"), A -> C ("no").
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b, condition="yes")
    builder.add_edge(agent_a, agent_c, condition="no")
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="AutoGen is a framework for building AI agents."):
        print(event)


asyncio.run(main())


Loop with exit condition: A → B → C (if ‘APPROVE’) or A (if ‘REJECT’)
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    agent_a = AssistantAgent(
        "A",
        model_client=model_client,
        system_message="You are a helpful assistant.",
    )
    agent_b = AssistantAgent(
        "B",
        model_client=model_client,
        system_message="Provide feedback on the input, if your feedback has been addressed, "
        "say 'APPROVE', else say 'REJECT' and provide a reason.",
    )
    agent_c = AssistantAgent(
        "C", model_client=model_client, system_message="Translate the final product to Korean."
    )

    # Create a loop graph with conditional exit: A -> B -> C ("APPROVE"), B -> A ("REJECT").
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b)
    builder.add_conditional_edges(agent_b, {"APPROVE": agent_c, "REJECT": agent_a})
    builder.set_entry_point(agent_a)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(20),  # Max 20 messages to avoid infinite loop.
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short poem about AI Agents."):
        print(event)


asyncio.run(main())




component_config_schema#
alias of GraphFlowConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.GraphFlow'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.")
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to English.")

    # Create a directed graph with sequential flow A -> B -> C.
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short story about a cat."):
        print(event)


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.")
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Japanese.")

    # Create a directed graph with fan-out flow A -> (B, C).
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short story about a cat."):
        print(event)


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent(
        "A",
        model_client=model_client,
        system_message="Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.",
    )
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to English.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Chinese.")

    # Create a directed graph with conditional branching flow A -> B ("yes"), A -> C ("no").
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b, condition="yes")
    builder.add_edge(agent_a, agent_c, condition="no")
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="AutoGen is a framework for building AI agents."):
        print(event)


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    agent_a = AssistantAgent(
        "A",
        model_client=model_client,
        system_message="You are a helpful assistant.",
    )
    agent_b = AssistantAgent(
        "B",
        model_client=model_client,
        system_message="Provide feedback on the input, if your feedback has been addressed, "
        "say 'APPROVE', else say 'REJECT' and provide a reason.",
    )
    agent_c = AssistantAgent(
        "C", model_client=model_client, system_message="Translate the final product to Korean."
    )

    # Create a loop graph with conditional exit: A -> B -> C ("APPROVE"), B -> A ("REJECT").
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b)
    builder.add_conditional_edges(agent_b, {"APPROVE": agent_c, "REJECT": agent_a})
    builder.set_entry_point(agent_a)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(20),  # Max 20 messages to avoid infinite loop.
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short poem about AI Agents."):
        print(event)


asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of GraphFlowConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.GraphFlow'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class MagenticOneGroupChat(participants: List[ChatAgent], model_client: ChatCompletionClient, *, termination_condition: TerminationCondition | None = None, max_turns: int | None = 20, runtime: AgentRuntime | None = None, max_stalls: int = 3, final_answer_prompt: str = ORCHESTRATOR_FINAL_ANSWER_PROMPT, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[MagenticOneGroupChatConfig]
A team that runs a group chat with participants managed by the MagenticOneOrchestrator.
The orchestrator handles the conversation flow, ensuring that the task is completed
efficiently by managing the participants’ interactions.
The orchestrator is based on the Magentic-One architecture, which is a generalist multi-agent system for solving complex tasks (see references below).

Parameters:

participants (List[ChatAgent]) – The participants in the group chat.
model_client (ChatCompletionClient) – The model client used for generating responses.
termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None.
Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached.
max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to 20.
max_stalls (int, optional) – The maximum number of stalls allowed before re-planning. Defaults to 3.
final_answer_prompt (str, optional) – The LLM prompt used to generate the final answer or response from the team’s transcript. A default (sensible for GPT-4o class models) is provided.
custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat.
If you are using custom message types or your agents produces custom message types, you need to specify them here.
Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.
emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.


Raises:
ValueError – In orchestration logic if progress ledger does not have required keys or if next speaker is not valid.


Examples:
MagenticOneGroupChat with one assistant agent:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
    )
    team = MagenticOneGroupChat([assistant], model_client=model_client)
    await Console(team.run_stream(task="Provide a different proof to Fermat last theorem"))


asyncio.run(main())



References
If you use the MagenticOneGroupChat in your work, please cite the following paper:
@article{fourney2024magentic,
    title={Magentic-one: A generalist multi-agent system for solving complex tasks},
    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
    journal={arXiv preprint arXiv:2411.04468},
    year={2024}
}




classmethod _from_config(config: MagenticOneGroupChatConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → MagenticOneGroupChatConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of MagenticOneGroupChatConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.MagenticOneGroupChat'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
    )
    team = MagenticOneGroupChat([assistant], model_client=model_client)
    await Console(team.run_stream(task="Provide a different proof to Fermat last theorem"))


asyncio.run(main())

```

**示例**:
```python
@article{fourney2024magentic,
    title={Magentic-one: A generalist multi-agent system for solving complex tasks},
    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
    journal={arXiv preprint arXiv:2411.04468},
    year={2024}
}

```

```python
classmethod _from_config(config: MagenticOneGroupChatConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → MagenticOneGroupChatConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of MagenticOneGroupChatConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.MagenticOneGroupChat'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class RoundRobinGroupChat(participants: List[ChatAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[RoundRobinGroupChatConfig]
A team that runs a group chat with participants taking turns in a round-robin fashion
to publish a message to all.
If a single participant is in the team, the participant will be the only speaker.

Parameters:

participants (List[BaseChatAgent]) – The participants in the group chat.
termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None.
Without a termination condition, the group chat will run indefinitely.
max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat.
If you are using custom message types or your agents produces custom message types, you need to specify them here.
Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.
emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.


Raises:
ValueError – If no participants are provided or if participant names are not unique.


Examples:
A team with one participant with tools:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def get_weather(location: str) -> str:
        return f"The weather in {location} is sunny."

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
        tools=[get_weather],
    )
    termination = TextMentionTermination("TERMINATE")
    team = RoundRobinGroupChat([assistant], termination_condition=termination)
    await Console(team.run_stream(task="What's the weather in New York?"))


asyncio.run(main())



A team with multiple participants:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = TextMentionTermination("TERMINATE")
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    await Console(team.run_stream(task="Tell me some jokes."))


asyncio.run(main())





classmethod _from_config(config: RoundRobinGroupChatConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → RoundRobinGroupChatConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of RoundRobinGroupChatConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.RoundRobinGroupChat'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def get_weather(location: str) -> str:
        return f"The weather in {location} is sunny."

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
        tools=[get_weather],
    )
    termination = TextMentionTermination("TERMINATE")
    team = RoundRobinGroupChat([assistant], termination_condition=termination)
    await Console(team.run_stream(task="What's the weather in New York?"))


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = TextMentionTermination("TERMINATE")
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    await Console(team.run_stream(task="Tell me some jokes."))


asyncio.run(main())

```

```python
classmethod _from_config(config: RoundRobinGroupChatConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → RoundRobinGroupChatConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of RoundRobinGroupChatConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.RoundRobinGroupChat'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class SelectorGroupChat(participants: List[ChatAgent], model_client: ChatCompletionClient, *, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, selector_prompt: str = 'You are in a role play game. The following roles are available:\n{roles}.\nRead the following conversation. Then select the next role from {participants} to play. Only return the role.\n\n{history}\n\nRead the above conversation. Then select the next role from {participants} to play. Only return the role.\n', allow_repeated_speaker: bool = False, max_selector_attempts: int = 3, selector_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]] | None = None, candidate_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]] | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False, model_client_streaming: bool = False)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[SelectorGroupChatConfig]
A group chat team that have participants takes turn to publish a message
to all, using a ChatCompletion model to select the next speaker after each message.

Parameters:

participants (List[ChatAgent]) – The participants in the group chat,
must have unique names and at least two participants.
model_client (ChatCompletionClient) – The ChatCompletion model client used
to select the next speaker.
termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None.
Without a termination condition, the group chat will run indefinitely.
max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
selector_prompt (str, optional) – The prompt template to use for selecting the next speaker.
Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’.
{participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …].
{roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”.
{history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”.
allow_repeated_speaker (bool, optional) – Whether to include the previous speaker in the list of candidates to be selected for the next turn.
Defaults to False. The model may still select the previous speaker – a warning will be logged if this happens.
max_selector_attempts (int, optional) – The maximum number of attempts to select a speaker using the model. Defaults to 3.
If the model fails to select a speaker after the maximum number of attempts, the previous speaker will be used if available,
otherwise the first participant will be used.
selector_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]], optional) – A custom selector
function that takes the conversation history and returns the name of the next speaker.
If provided, this function will be used to override the model to select the next speaker.
If the function returns None, the model will be used to select the next speaker.
candidate_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]], optional) – A custom function that takes the conversation history and returns a filtered list of candidates for the next speaker
selection using model. If the function returns an empty list or None, SelectorGroupChat will raise a ValueError.
This function is only used if selector_func is not set. The allow_repeated_speaker will be ignored if set.
custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat.
If you are using custom message types or your agents produces custom message types, you need to specify them here.
Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.
emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.
model_client_streaming (bool, optional) – Whether to use streaming for the model client. (This is useful for reasoning models like QwQ). Defaults to False.


Raises:
ValueError – If the number of participants is less than two or if the selector prompt is invalid.


Examples:
A team with multiple participants:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def lookup_hotel(location: str) -> str:
        return f"Here are some hotels in {location}: hotel1, hotel2, hotel3."

    async def lookup_flight(origin: str, destination: str) -> str:
        return f"Here are some flights from {origin} to {destination}: flight1, flight2, flight3."

    async def book_trip() -> str:
        return "Your trip is booked!"

    travel_advisor = AssistantAgent(
        "Travel_Advisor",
        model_client,
        tools=[book_trip],
        description="Helps with travel planning.",
    )
    hotel_agent = AssistantAgent(
        "Hotel_Agent",
        model_client,
        tools=[lookup_hotel],
        description="Helps with hotel booking.",
    )
    flight_agent = AssistantAgent(
        "Flight_Agent",
        model_client,
        tools=[lookup_flight],
        description="Helps with flight booking.",
    )
    termination = TextMentionTermination("TERMINATE")
    team = SelectorGroupChat(
        [travel_advisor, hotel_agent, flight_agent],
        model_client=model_client,
        termination_condition=termination,
    )
    await Console(team.run_stream(task="Book a 3-day trip to new york."))


asyncio.run(main())



A team with a custom selector function:

import asyncio
from typing import Sequence
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    def check_calculation(x: int, y: int, answer: int) -> str:
        if x + y == answer:
            return "Correct!"
        else:
            return "Incorrect!"

    agent1 = AssistantAgent(
        "Agent1",
        model_client,
        description="For calculation",
        system_message="Calculate the sum of two numbers",
    )
    agent2 = AssistantAgent(
        "Agent2",
        model_client,
        tools=[check_calculation],
        description="For checking calculation",
        system_message="Check the answer and respond with 'Correct!' or 'Incorrect!'",
    )

    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:
        if len(messages) == 1 or messages[-1].to_text() == "Incorrect!":
            return "Agent1"
        if messages[-1].source == "Agent1":
            return "Agent2"
        return None

    termination = TextMentionTermination("Correct!")
    team = SelectorGroupChat(
        [agent1, agent2],
        model_client=model_client,
        selector_func=selector_func,
        termination_condition=termination,
    )

    await Console(team.run_stream(task="What is 1 + 1?"))


asyncio.run(main())





classmethod _from_config(config: SelectorGroupChatConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → SelectorGroupChatConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of SelectorGroupChatConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.SelectorGroupChat'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def lookup_hotel(location: str) -> str:
        return f"Here are some hotels in {location}: hotel1, hotel2, hotel3."

    async def lookup_flight(origin: str, destination: str) -> str:
        return f"Here are some flights from {origin} to {destination}: flight1, flight2, flight3."

    async def book_trip() -> str:
        return "Your trip is booked!"

    travel_advisor = AssistantAgent(
        "Travel_Advisor",
        model_client,
        tools=[book_trip],
        description="Helps with travel planning.",
    )
    hotel_agent = AssistantAgent(
        "Hotel_Agent",
        model_client,
        tools=[lookup_hotel],
        description="Helps with hotel booking.",
    )
    flight_agent = AssistantAgent(
        "Flight_Agent",
        model_client,
        tools=[lookup_flight],
        description="Helps with flight booking.",
    )
    termination = TextMentionTermination("TERMINATE")
    team = SelectorGroupChat(
        [travel_advisor, hotel_agent, flight_agent],
        model_client=model_client,
        termination_condition=termination,
    )
    await Console(team.run_stream(task="Book a 3-day trip to new york."))


asyncio.run(main())

```

**示例**:
```python
import asyncio
from typing import Sequence
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    def check_calculation(x: int, y: int, answer: int) -> str:
        if x + y == answer:
            return "Correct!"
        else:
            return "Incorrect!"

    agent1 = AssistantAgent(
        "Agent1",
        model_client,
        description="For calculation",
        system_message="Calculate the sum of two numbers",
    )
    agent2 = AssistantAgent(
        "Agent2",
        model_client,
        tools=[check_calculation],
        description="For checking calculation",
        system_message="Check the answer and respond with 'Correct!' or 'Incorrect!'",
    )

    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:
        if len(messages) == 1 or messages[-1].to_text() == "Incorrect!":
            return "Agent1"
        if messages[-1].source == "Agent1":
            return "Agent2"
        return None

    termination = TextMentionTermination("Correct!")
    team = SelectorGroupChat(
        [agent1, agent2],
        model_client=model_client,
        selector_func=selector_func,
        termination_condition=termination,
    )

    await Console(team.run_stream(task="What is 1 + 1?"))


asyncio.run(main())

```

```python
classmethod _from_config(config: SelectorGroupChatConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → SelectorGroupChatConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of SelectorGroupChatConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.SelectorGroupChat'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class Swarm(participants: List[ChatAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[SwarmConfig]
A group chat team that selects the next speaker based on handoff message only.
The first participant in the list of participants is the initial speaker.
The next speaker is selected based on the HandoffMessage message
sent by the current speaker. If no handoff message is sent, the current speaker
continues to be the speaker.

Parameters:

participants (List[ChatAgent]) – The agents participating in the group chat. The first agent in the list is the initial speaker.
termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None.
Without a termination condition, the group chat will run indefinitely.
max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat.
If you are using custom message types or your agents produces custom message types, you need to specify them here.
Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.
emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.



Basic example:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import Swarm
from autogen_agentchat.conditions import MaxMessageTermination


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent(
        "Alice",
        model_client=model_client,
        handoffs=["Bob"],
        system_message="You are Alice and you only answer questions about yourself.",
    )
    agent2 = AssistantAgent(
        "Bob", model_client=model_client, system_message="You are Bob and your birthday is on 1st January."
    )

    termination = MaxMessageTermination(3)
    team = Swarm([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="What is bob's birthday?")
    async for message in stream:
        print(message)


asyncio.run(main())



Using the HandoffTermination for human-in-the-loop handoff:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import Swarm
from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import HandoffMessage


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent = AssistantAgent(
        "Alice",
        model_client=model_client,
        handoffs=["user"],
        system_message="You are Alice and you only answer questions about yourself, ask the user for help if needed.",
    )
    termination = HandoffTermination(target="user") | MaxMessageTermination(3)
    team = Swarm([agent], termination_condition=termination)

    # Start the conversation.
    await Console(team.run_stream(task="What is bob's birthday?"))

    # Resume with user feedback.
    await Console(
        team.run_stream(
            task=HandoffMessage(source="user", target="Alice", content="Bob's birthday is on 1st January.")
        )
    )


asyncio.run(main())





classmethod _from_config(config: SwarmConfig) → Swarm[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → SwarmConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of SwarmConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.Swarm'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import Swarm
from autogen_agentchat.conditions import MaxMessageTermination


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent(
        "Alice",
        model_client=model_client,
        handoffs=["Bob"],
        system_message="You are Alice and you only answer questions about yourself.",
    )
    agent2 = AssistantAgent(
        "Bob", model_client=model_client, system_message="You are Bob and your birthday is on 1st January."
    )

    termination = MaxMessageTermination(3)
    team = Swarm([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="What is bob's birthday?")
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import Swarm
from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import HandoffMessage


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent = AssistantAgent(
        "Alice",
        model_client=model_client,
        handoffs=["user"],
        system_message="You are Alice and you only answer questions about yourself, ask the user for help if needed.",
    )
    termination = HandoffTermination(target="user") | MaxMessageTermination(3)
    team = Swarm([agent], termination_condition=termination)

    # Start the conversation.
    await Console(team.run_stream(task="What is bob's birthday?"))

    # Resume with user feedback.
    await Console(
        team.run_stream(
            task=HandoffMessage(source="user", target="Alice", content="Bob's birthday is on 1st January.")
        )
    )


asyncio.run(main())

```

```python
classmethod _from_config(config: SwarmConfig) → Swarm[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → SwarmConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of SwarmConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.Swarm'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

【中文翻译】This module provides implementation of various pre-defined multi-agent teams.
Each team inherits from the BaseGroupChat class.

【中文翻译】previous

【中文翻译】autogen_agentchat.tools

【中文翻译】next

【中文翻译】autogen_agentchat.base

### autogen_agentchat.teams {autogen_agentchatteams}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html)

```python
class BaseGroupChat(participants: List[ChatAgent], group_chat_manager_name: str, group_chat_manager_class: type[SequentialRoutedAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]#
```

【中文翻译】Bases: Team, ABC, ComponentBase[BaseModel]
The base class for group chat teams.
To implement a group chat team, first create a subclass of BaseGroupChatManager and then
create a subclass of BaseGroupChat that uses the group chat manager.


component_type: ClassVar[ComponentType] = 'team'#
The logical type of the component.



async load_state(state: Mapping[str, Any]) → None[source]#
Load an external state and overwrite the current state of the group chat team.
The state is loaded by calling the agent_load_state() method
on each participant and the group chat manager with their internal agent ID.
See save_state() for the expected format of the state.



async pause() → None[source]#
Pause its participants when the team is running by calling their
on_pause() method via direct RPC calls.

Attention
This is an experimental feature introduced in v0.4.9 and may subject
to change or removal in the future.

The team must be initialized before it can be paused.
Different from termination, pausing the team does not cause the
run() or run_stream() method to return. It calls the
on_pause() method on each
participant, and if the participant does not implement the method, it
will be a no-op.

Note
It is the responsibility of the agent class to handle the pause
and ensure that the agent can be resumed later.
Make sure to implement the on_pause()
method in your agent class for custom pause behavior.
By default, the agent will not do anything when called.


Raises:
RuntimeError – If the team has not been initialized. Exceptions from
    the participants when calling their implementations of
    on_pause are
    propagated to this method and raised.





async reset() → None[source]#
Reset the team and its participants to their initial state.
The team must be stopped before it can be reset.

Raises:
RuntimeError – If the team has not been initialized or is currently running.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Reset the team.
    await team.reset()
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)


asyncio.run(main())





async resume() → None[source]#
Resume its participants when the team is running and paused by calling their
on_resume() method via direct RPC calls.

Attention
This is an experimental feature introduced in v0.4.9 and may subject
to change or removal in the future.

The team must be initialized before it can be resumed.
Different from termination and restart with a new task, resuming the team
does not cause the run() or run_stream() method to return.
It calls the on_resume() method on each
participant, and if the participant does not implement the method, it
will be a no-op.

Note
It is the responsibility of the agent class to handle the resume
and ensure that the agent continues from where it was paused.
Make sure to implement the on_resume()
method in your agent class for custom resume behavior.


Raises:
RuntimeError – If the team has not been initialized. Exceptions from
    the participants when calling their implementations of on_resume
    method are propagated to this method and raised.





async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
Run the team and return the result. The base implementation uses
run_stream() to run the team and then returns the final result.
Once the team is stopped, the termination condition is reset.

Parameters:

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.
cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately.
Setting the cancellation token potentially put the team in an inconsistent state,
and it may not reset the termination condition.
To gracefully stop the team, use ExternalTermination instead.


Returns:
result – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())


Example using the CancellationToken to cancel the task:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())





async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
Run the team and produces a stream of messages and the final result
of the type TaskResult as the last item in the stream. Once the
team is stopped, the termination condition is reset.

Note
If an agent produces ModelClientStreamingChunkEvent,
the message will be yielded in the stream but it will not be included in the
messages.


Parameters:

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.
cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately.
Setting the cancellation token potentially put the team in an inconsistent state,
and it may not reset the termination condition.
To gracefully stop the team, use ExternalTermination instead.


Returns:
stream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Run the team again without a task to continue the previous task.
    stream = team.run_stream()
    async for message in stream:
        print(message)


asyncio.run(main())


Example using the CancellationToken to cancel the task:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        Console(
            team.run_stream(
                task="Count from 1 to 10, respond one at a time.",
                cancellation_token=cancellation_token,
            )
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())





async save_state() → Mapping[str, Any][source]#
Save the state of the group chat team.
The state is saved by calling the agent_save_state() method
on each participant and the group chat manager with their internal agent ID.
The state is returned as a nested dictionary: a dictionary with key agent_states,
which is a dictionary the agent names as keys and the state as values.
{
    "agent_states": {
        "agent1": ...,
        "agent2": ...,
        "RoundRobinGroupChatManager": ...
    }
}



Note
Starting v0.4.9, the state is using the agent name as the key instead of the agent ID,
and the team_id field is removed from the state. This is to allow the state to be
portable across different teams and runtimes. States saved with the old format
may not be compatible with the new format in the future.


Caution
When calling save_state() on a team
while it is running, the state may not be consistent and may result in an unexpected state.
It is recommended to call this method when the team is not running or after it is stopped.

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Reset the team.
    await team.reset()
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Run the team again without a task to continue the previous task.
    stream = team.run_stream()
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        Console(
            team.run_stream(
                task="Count from 1 to 10, respond one at a time.",
                cancellation_token=cancellation_token,
            )
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

```

**示例**:
```python
{
    "agent_states": {
        "agent1": ...,
        "agent2": ...,
        "RoundRobinGroupChatManager": ...
    }
}

```

```python
component_type: ClassVar[ComponentType] = 'team'#
```

【中文翻译】The logical type of the component.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load an external state and overwrite the current state of the group chat team.
The state is loaded by calling the agent_load_state() method
on each participant and the group chat manager with their internal agent ID.
See save_state() for the expected format of the state.

```python
async pause() → None[source]#
```

【中文翻译】Pause its participants when the team is running by calling their
on_pause() method via direct RPC calls.

Attention
This is an experimental feature introduced in v0.4.9 and may subject
to change or removal in the future.

The team must be initialized before it can be paused.
Different from termination, pausing the team does not cause the
run() or run_stream() method to return. It calls the
on_pause() method on each
participant, and if the participant does not implement the method, it
will be a no-op.

Note
It is the responsibility of the agent class to handle the pause
and ensure that the agent can be resumed later.
Make sure to implement the on_pause()
method in your agent class for custom pause behavior.
By default, the agent will not do anything when called.


Raises:
RuntimeError – If the team has not been initialized. Exceptions from
    the participants when calling their implementations of
    on_pause are
    propagated to this method and raised.

```python
async reset() → None[source]#
```

【中文翻译】Reset the team and its participants to their initial state.
The team must be stopped before it can be reset.

Raises:
RuntimeError – If the team has not been initialized or is currently running.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Reset the team.
    await team.reset()
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)


asyncio.run(main())

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Reset the team.
    await team.reset()
    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)


asyncio.run(main())

```

```python
async resume() → None[source]#
```

【中文翻译】Resume its participants when the team is running and paused by calling their
on_resume() method via direct RPC calls.

Attention
This is an experimental feature introduced in v0.4.9 and may subject
to change or removal in the future.

The team must be initialized before it can be resumed.
Different from termination and restart with a new task, resuming the team
does not cause the run() or run_stream() method to return.
It calls the on_resume() method on each
participant, and if the participant does not implement the method, it
will be a no-op.

Note
It is the responsibility of the agent class to handle the resume
and ensure that the agent continues from where it was paused.
Make sure to implement the on_resume()
method in your agent class for custom resume behavior.


Raises:
RuntimeError – If the team has not been initialized. Exceptions from
    the participants when calling their implementations of on_resume
    method are propagated to this method and raised.

```python
async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → TaskResult[source]#
```

【中文翻译】Run the team and return the result. The base implementation uses
run_stream() to run the team and then returns the final result.
Once the team is stopped, the termination condition is reset.

Parameters:

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.
cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately.
Setting the cancellation token potentially put the team in an inconsistent state,
and it may not reset the termination condition.
To gracefully stop the team, use ExternalTermination instead.


Returns:
result – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())


Example using the CancellationToken to cancel the task:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

```

```python
async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]#
```

【中文翻译】Run the team and produces a stream of messages and the final result
of the type TaskResult as the last item in the stream. Once the
team is stopped, the termination condition is reset.

Note
If an agent produces ModelClientStreamingChunkEvent,
the message will be yielded in the stream but it will not be included in the
messages.


Parameters:

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.
cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately.
Setting the cancellation token potentially put the team in an inconsistent state,
and it may not reset the termination condition.
To gracefully stop the team, use ExternalTermination instead.


Returns:
stream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream.


Example using the RoundRobinGroupChat team:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Run the team again without a task to continue the previous task.
    stream = team.run_stream()
    async for message in stream:
        print(message)


asyncio.run(main())


Example using the CancellationToken to cancel the task:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        Console(
            team.run_stream(
                task="Count from 1 to 10, respond one at a time.",
                cancellation_token=cancellation_token,
            )
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="Count from 1 to 10, respond one at a time.")
    async for message in stream:
        print(message)

    # Run the team again without a task to continue the previous task.
    stream = team.run_stream()
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        Console(
            team.run_stream(
                task="Count from 1 to 10, respond one at a time.",
                cancellation_token=cancellation_token,
            )
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())

```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the group chat team.
The state is saved by calling the agent_save_state() method
on each participant and the group chat manager with their internal agent ID.
The state is returned as a nested dictionary: a dictionary with key agent_states,
which is a dictionary the agent names as keys and the state as values.
{
    "agent_states": {
        "agent1": ...,
        "agent2": ...,
        "RoundRobinGroupChatManager": ...
    }
}



Note
Starting v0.4.9, the state is using the agent name as the key instead of the agent ID,
and the team_id field is removed from the state. This is to allow the state to be
portable across different teams and runtimes. States saved with the old format
may not be compatible with the new format in the future.


Caution
When calling save_state() on a team
while it is running, the state may not be consistent and may result in an unexpected state.
It is recommended to call this method when the team is not running or after it is stopped.

**示例**:
```python
{
    "agent_states": {
        "agent1": ...,
        "agent2": ...,
        "RoundRobinGroupChatManager": ...
    }
}

```

```python
pydantic model DiGraph[source]#
```

【中文翻译】Bases: BaseModel
Defines a directed graph structure with nodes and edges.
GraphFlow uses this to determine execution order and conditions.

Warning
This is an experimental feature, and the API will change in the future releases.


Show JSON schema{
   "title": "DiGraph",
   "description": "Defines a directed graph structure with nodes and edges.\n:class:`GraphFlow` uses this to determine execution order and conditions.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "nodes": {
         "additionalProperties": {
            "$ref": "#/$defs/DiGraphNode"
         },
         "title": "Nodes",
         "type": "object"
      },
      "default_start_node": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Start Node"
      }
   },
   "$defs": {
      "DiGraphEdge": {
         "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "target": {
               "title": "Target",
               "type": "string"
            },
            "condition": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Condition"
            }
         },
         "required": [
            "target"
         ],
         "title": "DiGraphEdge",
         "type": "object"
      },
      "DiGraphNode": {
         "description": "Represents a node (agent) in a :class:`DiGraph`, with its outgoing edges and activation type.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "edges": {
               "default": [],
               "items": {
                  "$ref": "#/$defs/DiGraphEdge"
               },
               "title": "Edges",
               "type": "array"
            },
            "activation": {
               "default": "all",
               "enum": [
                  "all",
                  "any"
               ],
               "title": "Activation",
               "type": "string"
            }
         },
         "required": [
            "name"
         ],
         "title": "DiGraphNode",
         "type": "object"
      }
   },
   "required": [
      "nodes"
   ]
}



Fields:

default_start_node (str | None)
nodes (Dict[str, autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphNode])





field default_start_node: str | None = None#



field nodes: Dict[str, DiGraphNode] [Required]#



get_has_cycles() → bool[source]#
Indicates if the graph has at least one cycle (with valid exit conditions).



get_leaf_nodes() → Set[str][source]#
Return nodes that have no outgoing edges (final output nodes).



get_parents() → Dict[str, List[str]][source]#
Compute a mapping of each node to its parent nodes.



get_start_nodes() → Set[str][source]#
Return the nodes that have no incoming edges (entry points).



graph_validate() → None[source]#
Validate graph structure and execution rules.



has_cycles_with_exit() → bool[source]#
Check if the graph has any cycles and validate that each cycle has at least one conditional edge.

Returns:
bool – True if there is at least one cycle and all cycles have an exit condition.
False if there are no cycles.

Raises:
ValueError – If there is a cycle without any conditional edge.





model_post_init(context: Any, /) → None#
This function is meant to behave like a BaseModel method to initialise private attributes.
It takes context as an argument since that’s what pydantic-core passes when calling it.

Parameters:

self – The BaseModel instance.
context – The context.

**示例**:
```python
{
   "title": "DiGraph",
   "description": "Defines a directed graph structure with nodes and edges.\n:class:`GraphFlow` uses this to determine execution order and conditions.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "nodes": {
         "additionalProperties": {
            "$ref": "#/$defs/DiGraphNode"
         },
         "title": "Nodes",
         "type": "object"
      },
      "default_start_node": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Start Node"
      }
   },
   "$defs": {
      "DiGraphEdge": {
         "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "target": {
               "title": "Target",
               "type": "string"
            },
            "condition": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Condition"
            }
         },
         "required": [
            "target"
         ],
         "title": "DiGraphEdge",
         "type": "object"
      },
      "DiGraphNode": {
         "description": "Represents a node (agent) in a :class:`DiGraph`, with its outgoing edges and activation type.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "edges": {
               "default": [],
               "items": {
                  "$ref": "#/$defs/DiGraphEdge"
               },
               "title": "Edges",
               "type": "array"
            },
            "activation": {
               "default": "all",
               "enum": [
                  "all",
                  "any"
               ],
               "title": "Activation",
               "type": "string"
            }
         },
         "required": [
            "name"
         ],
         "title": "DiGraphNode",
         "type": "object"
      }
   },
   "required": [
      "nodes"
   ]
}

```

```python
field default_start_node: str | None = None#
```

```python
field nodes: Dict[str, DiGraphNode] [Required]#
```

```python
get_has_cycles() → bool[source]#
```

【中文翻译】Indicates if the graph has at least one cycle (with valid exit conditions).

```python
get_leaf_nodes() → Set[str][source]#
```

【中文翻译】Return nodes that have no outgoing edges (final output nodes).

```python
get_parents() → Dict[str, List[str]][source]#
```

【中文翻译】Compute a mapping of each node to its parent nodes.

```python
get_start_nodes() → Set[str][source]#
```

【中文翻译】Return the nodes that have no incoming edges (entry points).

```python
graph_validate() → None[source]#
```

【中文翻译】Validate graph structure and execution rules.

```python
has_cycles_with_exit() → bool[source]#
```

【中文翻译】Check if the graph has any cycles and validate that each cycle has at least one conditional edge.

Returns:
bool – True if there is at least one cycle and all cycles have an exit condition.
False if there are no cycles.

Raises:
ValueError – If there is a cycle without any conditional edge.

```python
model_post_init(context: Any, /) → None#
```

【中文翻译】This function is meant to behave like a BaseModel method to initialise private attributes.
It takes context as an argument since that’s what pydantic-core passes when calling it.

Parameters:

self – The BaseModel instance.
context – The context.

```python
class DiGraphBuilder[source]#
```

【中文翻译】Bases: object
A fluent builder for constructing DiGraph execution graphs used in GraphFlow.

Warning
This is an experimental feature, and the API will change in the future releases.

This utility provides a convenient way to programmatically build a graph of agent interactions,
including complex execution flows such as:

Sequential chains
Parallel fan-outs
Conditional branching
Cyclic loops with safe exits

Each node in the graph represents an agent. Edges define execution paths between agents,
and can optionally be conditioned on message content.
The builder is compatible with the Graph runner and supports both standard and filtered agents.


- add_node(agent, activation)
Add an agent node to the graph.



- add_edge(source, target, condition)
Connect two nodes optionally with a condition.



- add_conditional_edges(source, condition_to_target)
Add multiple conditional edges from a source.



- set_entry_point(agent)
Define the default start node (optional).



- build()
Generate a validated DiGraph.



- get_participants()
Return the list of added agents.


Example — Sequential Flow A → B → C:>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
>>> team = Graph(
...     participants=builder.get_participants(),
...     graph=builder.build(),
...     termination_condition=MaxMessageTermination(5),
... )



Example — Parallel Fan-out A → (B, C):>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)



Example — Conditional Branching A → B (“yes”), A → C (“no”):>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_conditional_edges(agent_a, {"yes": agent_b, "no": agent_c})



Example — Loop: A → B → A (“loop”), B → C (“exit”):>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b)
>>> builder.add_conditional_edges(agent_b, {"loop": agent_a, "exit": agent_c})






add_conditional_edges(source: str | ChatAgent, condition_to_target: Dict[str, str | ChatAgent]) → DiGraphBuilder[source]#
Add multiple conditional edges from a source node based on condition strings.



add_edge(source: str | ChatAgent, target: str | ChatAgent, condition: str | None = None) → DiGraphBuilder[source]#
Add a directed edge from source to target, optionally with a condition.



add_node(agent: ChatAgent, activation: Literal['all', 'any'] = 'all') → DiGraphBuilder[source]#
Add a node to the graph and register its agent.



build() → DiGraph[source]#
Build and validate the DiGraph.



get_participants() → list[ChatAgent][source]#
Return the list of agents in the builder, in insertion order.



set_entry_point(name: str | ChatAgent) → DiGraphBuilder[source]#
Set the default start node of the graph.

**示例**:
```python
>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
>>> team = Graph(
...     participants=builder.get_participants(),
...     graph=builder.build(),
...     termination_condition=MaxMessageTermination(5),
... )

```

**示例**:
```python
>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)

```

**示例**:
```python
>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_conditional_edges(agent_a, {"yes": agent_b, "no": agent_c})

```

**示例**:
```python
>>> builder = GraphBuilder()
>>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
>>> builder.add_edge(agent_a, agent_b)
>>> builder.add_conditional_edges(agent_b, {"loop": agent_a, "exit": agent_c})

```

```python
- add_node(agent, activation)
```

【中文翻译】Add an agent node to the graph.

```python
- add_edge(source, target, condition)
```

【中文翻译】Connect two nodes optionally with a condition.

```python
- add_conditional_edges(source, condition_to_target)
```

【中文翻译】Add multiple conditional edges from a source.

```python
- set_entry_point(agent)
```

【中文翻译】Define the default start node (optional).

```python
- build()
```

【中文翻译】Generate a validated DiGraph.

```python
- get_participants()
```

【中文翻译】Return the list of added agents.

```python
add_conditional_edges(source: str | ChatAgent, condition_to_target: Dict[str, str | ChatAgent]) → DiGraphBuilder[source]#
```

【中文翻译】Add multiple conditional edges from a source node based on condition strings.

```python
add_edge(source: str | ChatAgent, target: str | ChatAgent, condition: str | None = None) → DiGraphBuilder[source]#
```

【中文翻译】Add a directed edge from source to target, optionally with a condition.

```python
add_node(agent: ChatAgent, activation: Literal['all', 'any'] = 'all') → DiGraphBuilder[source]#
```

【中文翻译】Add a node to the graph and register its agent.

```python
build() → DiGraph[source]#
```

【中文翻译】Build and validate the DiGraph.

```python
get_participants() → list[ChatAgent][source]#
```

【中文翻译】Return the list of agents in the builder, in insertion order.

```python
set_entry_point(name: str | ChatAgent) → DiGraphBuilder[source]#
```

【中文翻译】Set the default start node of the graph.

```python
pydantic model DiGraphEdge[source]#
```

【中文翻译】Bases: BaseModel
Represents a directed edge in a DiGraph, with an optional execution condition.

Warning
This is an experimental feature, and the API will change in the future releases.


Show JSON schema{
   "title": "DiGraphEdge",
   "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "target": {
         "title": "Target",
         "type": "string"
      },
      "condition": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Condition"
      }
   },
   "required": [
      "target"
   ]
}



Fields:

condition (str | None)
target (str)





field condition: str | None = None#
(Experimental) Condition to execute this edge.
If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message.
NOTE: This is an experimental feature WILL change in the future releases to allow for better spcification of branching conditions
similar to the TerminationCondition class.



field target: str [Required]#

**示例**:
```python
{
   "title": "DiGraphEdge",
   "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "target": {
         "title": "Target",
         "type": "string"
      },
      "condition": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Condition"
      }
   },
   "required": [
      "target"
   ]
}

```

```python
field condition: str | None = None#
```

【中文翻译】(Experimental) Condition to execute this edge.
If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message.
NOTE: This is an experimental feature WILL change in the future releases to allow for better spcification of branching conditions
similar to the TerminationCondition class.

```python
field target: str [Required]#
```

```python
pydantic model DiGraphNode[source]#
```

【中文翻译】Bases: BaseModel
Represents a node (agent) in a DiGraph, with its outgoing edges and activation type.

Warning
This is an experimental feature, and the API will change in the future releases.


Show JSON schema{
   "title": "DiGraphNode",
   "description": "Represents a node (agent) in a :class:`DiGraph`, with its outgoing edges and activation type.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "name": {
         "title": "Name",
         "type": "string"
      },
      "edges": {
         "default": [],
         "items": {
            "$ref": "#/$defs/DiGraphEdge"
         },
         "title": "Edges",
         "type": "array"
      },
      "activation": {
         "default": "all",
         "enum": [
            "all",
            "any"
         ],
         "title": "Activation",
         "type": "string"
      }
   },
   "$defs": {
      "DiGraphEdge": {
         "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "target": {
               "title": "Target",
               "type": "string"
            },
            "condition": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Condition"
            }
         },
         "required": [
            "target"
         ],
         "title": "DiGraphEdge",
         "type": "object"
      }
   },
   "required": [
      "name"
   ]
}



Fields:

activation (Literal['all', 'any'])
edges (List[autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphEdge])
name (str)





field activation: Literal['all', 'any'] = 'all'#



field edges: List[DiGraphEdge] = []#



field name: str [Required]#

**示例**:
```python
{
   "title": "DiGraphNode",
   "description": "Represents a node (agent) in a :class:`DiGraph`, with its outgoing edges and activation type.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
   "type": "object",
   "properties": {
      "name": {
         "title": "Name",
         "type": "string"
      },
      "edges": {
         "default": [],
         "items": {
            "$ref": "#/$defs/DiGraphEdge"
         },
         "title": "Edges",
         "type": "array"
      },
      "activation": {
         "default": "all",
         "enum": [
            "all",
            "any"
         ],
         "title": "Activation",
         "type": "string"
      }
   },
   "$defs": {
      "DiGraphEdge": {
         "description": "Represents a directed edge in a :class:`DiGraph`, with an optional execution condition.\n\n.. warning::\n\n    This is an experimental feature, and the API will change in the future releases.",
         "properties": {
            "target": {
               "title": "Target",
               "type": "string"
            },
            "condition": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Condition"
            }
         },
         "required": [
            "target"
         ],
         "title": "DiGraphEdge",
         "type": "object"
      }
   },
   "required": [
      "name"
   ]
}

```

```python
field activation: Literal['all', 'any'] = 'all'#
```

```python
field edges: List[DiGraphEdge] = []#
```

```python
field name: str [Required]#
```

```python
class GraphFlow(participants: List[ChatAgent], graph: DiGraph, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[GraphFlowConfig]
A team that runs a group chat following a Directed Graph execution pattern.

Warning
This is an experimental feature, and the API will change in the future releases.

This group chat executes agents based on a directed graph (DiGraph) structure,
allowing complex workflows such as sequential execution, parallel fan-out,
conditional branching, join patterns, and loops with explicit exit conditions.
The execution order is determined by the edges defined in the DiGraph. Each node
in the graph corresponds to an agent, and edges define the flow of messages between agents.
Nodes can be configured to activate when:


All parent nodes have completed (activation=”all”) → default
Any parent node completes (activation=”any”)


Conditional branching is supported using edge conditions, where the next agent(s) are selected
based on content in the chat history. Loops are permitted as long as there is a condition
that eventually exits the loop.

Note
Use the DiGraphBuilder class to create a DiGraph easily. It provides a fluent API
for adding nodes and edges, setting entry points, and validating the graph structure.
See the DiGraphBuilder documentation for more details.
The GraphFlow class is designed to be used with the DiGraphBuilder for creating complex workflows.


Parameters:

participants (List[ChatAgent]) – The participants in the group chat.
termination_condition (TerminationCondition, optional) – Termination condition for the chat.
max_turns (int, optional) – Maximum number of turns before forcing termination.
graph (DiGraph) – Directed execution graph defining node flow and conditions.


Raises:
ValueError – If participant names are not unique, or if graph validation fails (e.g., cycles without exit).


Examples
Sequential Flow: A → B → C
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.")
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to English.")

    # Create a directed graph with sequential flow A -> B -> C.
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short story about a cat."):
        print(event)


asyncio.run(main())


Parallel Fan-out: A → (B, C)
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.")
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Japanese.")

    # Create a directed graph with fan-out flow A -> (B, C).
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short story about a cat."):
        print(event)


asyncio.run(main())


Conditional Branching: A → B (if ‘yes’) or C (if ‘no’)
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent(
        "A",
        model_client=model_client,
        system_message="Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.",
    )
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to English.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Chinese.")

    # Create a directed graph with conditional branching flow A -> B ("yes"), A -> C ("no").
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b, condition="yes")
    builder.add_edge(agent_a, agent_c, condition="no")
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="AutoGen is a framework for building AI agents."):
        print(event)


asyncio.run(main())


Loop with exit condition: A → B → C (if ‘APPROVE’) or A (if ‘REJECT’)
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    agent_a = AssistantAgent(
        "A",
        model_client=model_client,
        system_message="You are a helpful assistant.",
    )
    agent_b = AssistantAgent(
        "B",
        model_client=model_client,
        system_message="Provide feedback on the input, if your feedback has been addressed, "
        "say 'APPROVE', else say 'REJECT' and provide a reason.",
    )
    agent_c = AssistantAgent(
        "C", model_client=model_client, system_message="Translate the final product to Korean."
    )

    # Create a loop graph with conditional exit: A -> B -> C ("APPROVE"), B -> A ("REJECT").
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b)
    builder.add_conditional_edges(agent_b, {"APPROVE": agent_c, "REJECT": agent_a})
    builder.set_entry_point(agent_a)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(20),  # Max 20 messages to avoid infinite loop.
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short poem about AI Agents."):
        print(event)


asyncio.run(main())




component_config_schema#
alias of GraphFlowConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.GraphFlow'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.")
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to English.")

    # Create a directed graph with sequential flow A -> B -> C.
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short story about a cat."):
        print(event)


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.")
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Japanese.")

    # Create a directed graph with fan-out flow A -> (B, C).
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short story about a cat."):
        print(event)


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    agent_a = AssistantAgent(
        "A",
        model_client=model_client,
        system_message="Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.",
    )
    agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to English.")
    agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Chinese.")

    # Create a directed graph with conditional branching flow A -> B ("yes"), A -> C ("no").
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b, condition="yes")
    builder.add_edge(agent_a, agent_c, condition="no")
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(5),
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="AutoGen is a framework for building AI agents."):
        print(event)


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main():
    # Initialize agents with OpenAI model clients.
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    agent_a = AssistantAgent(
        "A",
        model_client=model_client,
        system_message="You are a helpful assistant.",
    )
    agent_b = AssistantAgent(
        "B",
        model_client=model_client,
        system_message="Provide feedback on the input, if your feedback has been addressed, "
        "say 'APPROVE', else say 'REJECT' and provide a reason.",
    )
    agent_c = AssistantAgent(
        "C", model_client=model_client, system_message="Translate the final product to Korean."
    )

    # Create a loop graph with conditional exit: A -> B -> C ("APPROVE"), B -> A ("REJECT").
    builder = DiGraphBuilder()
    builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)
    builder.add_edge(agent_a, agent_b)
    builder.add_conditional_edges(agent_b, {"APPROVE": agent_c, "REJECT": agent_a})
    builder.set_entry_point(agent_a)
    graph = builder.build()

    # Create a GraphFlow team with the directed graph.
    team = GraphFlow(
        participants=[agent_a, agent_b, agent_c],
        graph=graph,
        termination_condition=MaxMessageTermination(20),  # Max 20 messages to avoid infinite loop.
    )

    # Run the team and print the events.
    async for event in team.run_stream(task="Write a short poem about AI Agents."):
        print(event)


asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of GraphFlowConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.GraphFlow'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class MagenticOneGroupChat(participants: List[ChatAgent], model_client: ChatCompletionClient, *, termination_condition: TerminationCondition | None = None, max_turns: int | None = 20, runtime: AgentRuntime | None = None, max_stalls: int = 3, final_answer_prompt: str = ORCHESTRATOR_FINAL_ANSWER_PROMPT, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[MagenticOneGroupChatConfig]
A team that runs a group chat with participants managed by the MagenticOneOrchestrator.
The orchestrator handles the conversation flow, ensuring that the task is completed
efficiently by managing the participants’ interactions.
The orchestrator is based on the Magentic-One architecture, which is a generalist multi-agent system for solving complex tasks (see references below).

Parameters:

participants (List[ChatAgent]) – The participants in the group chat.
model_client (ChatCompletionClient) – The model client used for generating responses.
termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None.
Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached.
max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to 20.
max_stalls (int, optional) – The maximum number of stalls allowed before re-planning. Defaults to 3.
final_answer_prompt (str, optional) – The LLM prompt used to generate the final answer or response from the team’s transcript. A default (sensible for GPT-4o class models) is provided.
custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat.
If you are using custom message types or your agents produces custom message types, you need to specify them here.
Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.
emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.


Raises:
ValueError – In orchestration logic if progress ledger does not have required keys or if next speaker is not valid.


Examples:
MagenticOneGroupChat with one assistant agent:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
    )
    team = MagenticOneGroupChat([assistant], model_client=model_client)
    await Console(team.run_stream(task="Provide a different proof to Fermat last theorem"))


asyncio.run(main())



References
If you use the MagenticOneGroupChat in your work, please cite the following paper:
@article{fourney2024magentic,
    title={Magentic-one: A generalist multi-agent system for solving complex tasks},
    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
    journal={arXiv preprint arXiv:2411.04468},
    year={2024}
}




classmethod _from_config(config: MagenticOneGroupChatConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → MagenticOneGroupChatConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of MagenticOneGroupChatConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.MagenticOneGroupChat'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
    )
    team = MagenticOneGroupChat([assistant], model_client=model_client)
    await Console(team.run_stream(task="Provide a different proof to Fermat last theorem"))


asyncio.run(main())

```

**示例**:
```python
@article{fourney2024magentic,
    title={Magentic-one: A generalist multi-agent system for solving complex tasks},
    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
    journal={arXiv preprint arXiv:2411.04468},
    year={2024}
}

```

```python
classmethod _from_config(config: MagenticOneGroupChatConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → MagenticOneGroupChatConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of MagenticOneGroupChatConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.MagenticOneGroupChat'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class RoundRobinGroupChat(participants: List[ChatAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[RoundRobinGroupChatConfig]
A team that runs a group chat with participants taking turns in a round-robin fashion
to publish a message to all.
If a single participant is in the team, the participant will be the only speaker.

Parameters:

participants (List[BaseChatAgent]) – The participants in the group chat.
termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None.
Without a termination condition, the group chat will run indefinitely.
max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat.
If you are using custom message types or your agents produces custom message types, you need to specify them here.
Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.
emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.


Raises:
ValueError – If no participants are provided or if participant names are not unique.


Examples:
A team with one participant with tools:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def get_weather(location: str) -> str:
        return f"The weather in {location} is sunny."

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
        tools=[get_weather],
    )
    termination = TextMentionTermination("TERMINATE")
    team = RoundRobinGroupChat([assistant], termination_condition=termination)
    await Console(team.run_stream(task="What's the weather in New York?"))


asyncio.run(main())



A team with multiple participants:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = TextMentionTermination("TERMINATE")
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    await Console(team.run_stream(task="Tell me some jokes."))


asyncio.run(main())





classmethod _from_config(config: RoundRobinGroupChatConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → RoundRobinGroupChatConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of RoundRobinGroupChatConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.RoundRobinGroupChat'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def get_weather(location: str) -> str:
        return f"The weather in {location} is sunny."

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
        tools=[get_weather],
    )
    termination = TextMentionTermination("TERMINATE")
    team = RoundRobinGroupChat([assistant], termination_condition=termination)
    await Console(team.run_stream(task="What's the weather in New York?"))


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = TextMentionTermination("TERMINATE")
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)
    await Console(team.run_stream(task="Tell me some jokes."))


asyncio.run(main())

```

```python
classmethod _from_config(config: RoundRobinGroupChatConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → RoundRobinGroupChatConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of RoundRobinGroupChatConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.RoundRobinGroupChat'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class SelectorGroupChat(participants: List[ChatAgent], model_client: ChatCompletionClient, *, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, selector_prompt: str = 'You are in a role play game. The following roles are available:\n{roles}.\nRead the following conversation. Then select the next role from {participants} to play. Only return the role.\n\n{history}\n\nRead the above conversation. Then select the next role from {participants} to play. Only return the role.\n', allow_repeated_speaker: bool = False, max_selector_attempts: int = 3, selector_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]] | None = None, candidate_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]] | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False, model_client_streaming: bool = False)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[SelectorGroupChatConfig]
A group chat team that have participants takes turn to publish a message
to all, using a ChatCompletion model to select the next speaker after each message.

Parameters:

participants (List[ChatAgent]) – The participants in the group chat,
must have unique names and at least two participants.
model_client (ChatCompletionClient) – The ChatCompletion model client used
to select the next speaker.
termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None.
Without a termination condition, the group chat will run indefinitely.
max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
selector_prompt (str, optional) – The prompt template to use for selecting the next speaker.
Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’.
{participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …].
{roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”.
{history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”.
allow_repeated_speaker (bool, optional) – Whether to include the previous speaker in the list of candidates to be selected for the next turn.
Defaults to False. The model may still select the previous speaker – a warning will be logged if this happens.
max_selector_attempts (int, optional) – The maximum number of attempts to select a speaker using the model. Defaults to 3.
If the model fails to select a speaker after the maximum number of attempts, the previous speaker will be used if available,
otherwise the first participant will be used.
selector_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]], optional) – A custom selector
function that takes the conversation history and returns the name of the next speaker.
If provided, this function will be used to override the model to select the next speaker.
If the function returns None, the model will be used to select the next speaker.
candidate_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]], optional) – A custom function that takes the conversation history and returns a filtered list of candidates for the next speaker
selection using model. If the function returns an empty list or None, SelectorGroupChat will raise a ValueError.
This function is only used if selector_func is not set. The allow_repeated_speaker will be ignored if set.
custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat.
If you are using custom message types or your agents produces custom message types, you need to specify them here.
Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.
emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.
model_client_streaming (bool, optional) – Whether to use streaming for the model client. (This is useful for reasoning models like QwQ). Defaults to False.


Raises:
ValueError – If the number of participants is less than two or if the selector prompt is invalid.


Examples:
A team with multiple participants:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def lookup_hotel(location: str) -> str:
        return f"Here are some hotels in {location}: hotel1, hotel2, hotel3."

    async def lookup_flight(origin: str, destination: str) -> str:
        return f"Here are some flights from {origin} to {destination}: flight1, flight2, flight3."

    async def book_trip() -> str:
        return "Your trip is booked!"

    travel_advisor = AssistantAgent(
        "Travel_Advisor",
        model_client,
        tools=[book_trip],
        description="Helps with travel planning.",
    )
    hotel_agent = AssistantAgent(
        "Hotel_Agent",
        model_client,
        tools=[lookup_hotel],
        description="Helps with hotel booking.",
    )
    flight_agent = AssistantAgent(
        "Flight_Agent",
        model_client,
        tools=[lookup_flight],
        description="Helps with flight booking.",
    )
    termination = TextMentionTermination("TERMINATE")
    team = SelectorGroupChat(
        [travel_advisor, hotel_agent, flight_agent],
        model_client=model_client,
        termination_condition=termination,
    )
    await Console(team.run_stream(task="Book a 3-day trip to new york."))


asyncio.run(main())



A team with a custom selector function:

import asyncio
from typing import Sequence
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    def check_calculation(x: int, y: int, answer: int) -> str:
        if x + y == answer:
            return "Correct!"
        else:
            return "Incorrect!"

    agent1 = AssistantAgent(
        "Agent1",
        model_client,
        description="For calculation",
        system_message="Calculate the sum of two numbers",
    )
    agent2 = AssistantAgent(
        "Agent2",
        model_client,
        tools=[check_calculation],
        description="For checking calculation",
        system_message="Check the answer and respond with 'Correct!' or 'Incorrect!'",
    )

    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:
        if len(messages) == 1 or messages[-1].to_text() == "Incorrect!":
            return "Agent1"
        if messages[-1].source == "Agent1":
            return "Agent2"
        return None

    termination = TextMentionTermination("Correct!")
    team = SelectorGroupChat(
        [agent1, agent2],
        model_client=model_client,
        selector_func=selector_func,
        termination_condition=termination,
    )

    await Console(team.run_stream(task="What is 1 + 1?"))


asyncio.run(main())





classmethod _from_config(config: SelectorGroupChatConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → SelectorGroupChatConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of SelectorGroupChatConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.SelectorGroupChat'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    async def lookup_hotel(location: str) -> str:
        return f"Here are some hotels in {location}: hotel1, hotel2, hotel3."

    async def lookup_flight(origin: str, destination: str) -> str:
        return f"Here are some flights from {origin} to {destination}: flight1, flight2, flight3."

    async def book_trip() -> str:
        return "Your trip is booked!"

    travel_advisor = AssistantAgent(
        "Travel_Advisor",
        model_client,
        tools=[book_trip],
        description="Helps with travel planning.",
    )
    hotel_agent = AssistantAgent(
        "Hotel_Agent",
        model_client,
        tools=[lookup_hotel],
        description="Helps with hotel booking.",
    )
    flight_agent = AssistantAgent(
        "Flight_Agent",
        model_client,
        tools=[lookup_flight],
        description="Helps with flight booking.",
    )
    termination = TextMentionTermination("TERMINATE")
    team = SelectorGroupChat(
        [travel_advisor, hotel_agent, flight_agent],
        model_client=model_client,
        termination_condition=termination,
    )
    await Console(team.run_stream(task="Book a 3-day trip to new york."))


asyncio.run(main())

```

**示例**:
```python
import asyncio
from typing import Sequence
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    def check_calculation(x: int, y: int, answer: int) -> str:
        if x + y == answer:
            return "Correct!"
        else:
            return "Incorrect!"

    agent1 = AssistantAgent(
        "Agent1",
        model_client,
        description="For calculation",
        system_message="Calculate the sum of two numbers",
    )
    agent2 = AssistantAgent(
        "Agent2",
        model_client,
        tools=[check_calculation],
        description="For checking calculation",
        system_message="Check the answer and respond with 'Correct!' or 'Incorrect!'",
    )

    def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:
        if len(messages) == 1 or messages[-1].to_text() == "Incorrect!":
            return "Agent1"
        if messages[-1].source == "Agent1":
            return "Agent2"
        return None

    termination = TextMentionTermination("Correct!")
    team = SelectorGroupChat(
        [agent1, agent2],
        model_client=model_client,
        selector_func=selector_func,
        termination_condition=termination,
    )

    await Console(team.run_stream(task="What is 1 + 1?"))


asyncio.run(main())

```

```python
classmethod _from_config(config: SelectorGroupChatConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → SelectorGroupChatConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of SelectorGroupChatConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.SelectorGroupChat'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class Swarm(participants: List[ChatAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]#
```

【中文翻译】Bases: BaseGroupChat, Component[SwarmConfig]
A group chat team that selects the next speaker based on handoff message only.
The first participant in the list of participants is the initial speaker.
The next speaker is selected based on the HandoffMessage message
sent by the current speaker. If no handoff message is sent, the current speaker
continues to be the speaker.

Parameters:

participants (List[ChatAgent]) – The agents participating in the group chat. The first agent in the list is the initial speaker.
termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None.
Without a termination condition, the group chat will run indefinitely.
max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.
custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat.
If you are using custom message types or your agents produces custom message types, you need to specify them here.
Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.
emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.



Basic example:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import Swarm
from autogen_agentchat.conditions import MaxMessageTermination


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent(
        "Alice",
        model_client=model_client,
        handoffs=["Bob"],
        system_message="You are Alice and you only answer questions about yourself.",
    )
    agent2 = AssistantAgent(
        "Bob", model_client=model_client, system_message="You are Bob and your birthday is on 1st January."
    )

    termination = MaxMessageTermination(3)
    team = Swarm([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="What is bob's birthday?")
    async for message in stream:
        print(message)


asyncio.run(main())



Using the HandoffTermination for human-in-the-loop handoff:

import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import Swarm
from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import HandoffMessage


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent = AssistantAgent(
        "Alice",
        model_client=model_client,
        handoffs=["user"],
        system_message="You are Alice and you only answer questions about yourself, ask the user for help if needed.",
    )
    termination = HandoffTermination(target="user") | MaxMessageTermination(3)
    team = Swarm([agent], termination_condition=termination)

    # Start the conversation.
    await Console(team.run_stream(task="What is bob's birthday?"))

    # Resume with user feedback.
    await Console(
        team.run_stream(
            task=HandoffMessage(source="user", target="Alice", content="Bob's birthday is on 1st January.")
        )
    )


asyncio.run(main())





classmethod _from_config(config: SwarmConfig) → Swarm[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → SwarmConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of SwarmConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.Swarm'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import Swarm
from autogen_agentchat.conditions import MaxMessageTermination


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent(
        "Alice",
        model_client=model_client,
        handoffs=["Bob"],
        system_message="You are Alice and you only answer questions about yourself.",
    )
    agent2 = AssistantAgent(
        "Bob", model_client=model_client, system_message="You are Bob and your birthday is on 1st January."
    )

    termination = MaxMessageTermination(3)
    team = Swarm([agent1, agent2], termination_condition=termination)

    stream = team.run_stream(task="What is bob's birthday?")
    async for message in stream:
        print(message)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import Swarm
from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_agentchat.messages import HandoffMessage


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent = AssistantAgent(
        "Alice",
        model_client=model_client,
        handoffs=["user"],
        system_message="You are Alice and you only answer questions about yourself, ask the user for help if needed.",
    )
    termination = HandoffTermination(target="user") | MaxMessageTermination(3)
    team = Swarm([agent], termination_condition=termination)

    # Start the conversation.
    await Console(team.run_stream(task="What is bob's birthday?"))

    # Resume with user feedback.
    await Console(
        team.run_stream(
            task=HandoffMessage(source="user", target="Alice", content="Bob's birthday is on 1st January.")
        )
    )


asyncio.run(main())

```

```python
classmethod _from_config(config: SwarmConfig) → Swarm[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → SwarmConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of SwarmConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.Swarm'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

【中文翻译】This module provides implementation of various pre-defined multi-agent teams.
Each team inherits from the BaseGroupChat class.

【中文翻译】previous

【中文翻译】autogen_agentchat.tools

【中文翻译】next

【中文翻译】autogen_agentchat.base

### autogen_agentchat.tools {autogen_agentchattools}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html)

```python
class AgentTool(agent: BaseChatAgent)[source]#
```

【中文翻译】Bases: TaskRunnerTool, Component[AgentToolConfig]
Tool that can be used to run a task using an agent.
The tool returns the result of the task execution as a TaskResult object.

Parameters:
agent (BaseChatAgent) – The agent to be used for running the task.


Example
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4")
    writer = AssistantAgent(
        name="writer",
        description="A writer agent for generating text.",
        model_client=model_client,
        system_message="Write well.",
    )
    writer_tool = AgentTool(agent=writer)
    assistant = AssistantAgent(
        name="assistant",
        model_client=model_client,
        tools=[writer_tool],
        system_message="You are a helpful assistant.",
    )
    await Console(assistant.run_stream(task="Write a poem about the sea."))


asyncio.run(main())




classmethod _from_config(config: AgentToolConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → AgentToolConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of AgentToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.tools.AgentTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4")
    writer = AssistantAgent(
        name="writer",
        description="A writer agent for generating text.",
        model_client=model_client,
        system_message="Write well.",
    )
    writer_tool = AgentTool(agent=writer)
    assistant = AssistantAgent(
        name="assistant",
        model_client=model_client,
        tools=[writer_tool],
        system_message="You are a helpful assistant.",
    )
    await Console(assistant.run_stream(task="Write a poem about the sea."))


asyncio.run(main())

```

```python
classmethod _from_config(config: AgentToolConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → AgentToolConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of AgentToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.tools.AgentTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class TeamTool(team: BaseGroupChat, name: str, description: str)[source]#
```

【中文翻译】Bases: TaskRunnerTool, Component[TeamToolConfig]
Tool that can be used to run a task.
The tool returns the result of the task execution as a TaskResult object.

Parameters:

team (BaseGroupChat) – The team to be used for running the task.
name (str) – The name of the tool.
description (str) – The description of the tool.





classmethod _from_config(config: TeamToolConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TeamToolConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TeamToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.tools.TeamTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
classmethod _from_config(config: TeamToolConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TeamToolConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TeamToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.tools.TeamTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

【中文翻译】previous

【中文翻译】autogen_agentchat.agents

【中文翻译】next

【中文翻译】autogen_agentchat.teams

### autogen_agentchat.tools {autogen_agentchattools}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html)

```python
class AgentTool(agent: BaseChatAgent)[source]#
```

【中文翻译】Bases: TaskRunnerTool, Component[AgentToolConfig]
Tool that can be used to run a task using an agent.
The tool returns the result of the task execution as a TaskResult object.

Parameters:
agent (BaseChatAgent) – The agent to be used for running the task.


Example
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4")
    writer = AssistantAgent(
        name="writer",
        description="A writer agent for generating text.",
        model_client=model_client,
        system_message="Write well.",
    )
    writer_tool = AgentTool(agent=writer)
    assistant = AssistantAgent(
        name="assistant",
        model_client=model_client,
        tools=[writer_tool],
        system_message="You are a helpful assistant.",
    )
    await Console(assistant.run_stream(task="Write a poem about the sea."))


asyncio.run(main())




classmethod _from_config(config: AgentToolConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → AgentToolConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of AgentToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.tools.AgentTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4")
    writer = AssistantAgent(
        name="writer",
        description="A writer agent for generating text.",
        model_client=model_client,
        system_message="Write well.",
    )
    writer_tool = AgentTool(agent=writer)
    assistant = AssistantAgent(
        name="assistant",
        model_client=model_client,
        tools=[writer_tool],
        system_message="You are a helpful assistant.",
    )
    await Console(assistant.run_stream(task="Write a poem about the sea."))


asyncio.run(main())

```

```python
classmethod _from_config(config: AgentToolConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → AgentToolConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of AgentToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.tools.AgentTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
class TeamTool(team: BaseGroupChat, name: str, description: str)[source]#
```

【中文翻译】Bases: TaskRunnerTool, Component[TeamToolConfig]
Tool that can be used to run a task.
The tool returns the result of the task execution as a TaskResult object.

Parameters:

team (BaseGroupChat) – The team to be used for running the task.
name (str) – The name of the tool.
description (str) – The description of the tool.





classmethod _from_config(config: TeamToolConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TeamToolConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TeamToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_agentchat.tools.TeamTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
classmethod _from_config(config: TeamToolConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TeamToolConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TeamToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_agentchat.tools.TeamTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

【中文翻译】previous

【中文翻译】autogen_agentchat.agents

【中文翻译】next

【中文翻译】autogen_agentchat.teams

### autogen_agentchat.ui {autogen_agentchatui}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html)

```python
async Console(stream: AsyncGenerator[BaseAgentEvent | BaseChatMessage | T, None], *, no_inline_images: bool = False, output_stats: bool = False, user_input_manager: UserInputManager | None = None) → T[source]#
```

【中文翻译】Consumes the message stream from run_stream()
or on_messages_stream() and renders the messages to the console.
Returns the last processed TaskResult or Response.

Note
output_stats is experimental and the stats may not be accurate.
It will be improved in future releases.


Parameters:

stream (AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None] | AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]) – Message stream to render.
This can be from run_stream() or on_messages_stream().
no_inline_images (bool, optional) – If terminal is iTerm2 will render images inline. Use this to disable this behavior. Defaults to False.
output_stats (bool, optional) – (Experimental) If True, will output a summary of the messages and inline token usage info. Defaults to False.


Returns:
last_processed – A TaskResult if the stream is from run_stream()
or a Response if the stream is from on_messages_stream().

```python
class UserInputManager(callback: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]])[source]#
```

【中文翻译】Bases: object


get_wrapped_callback() → Callable[[str, CancellationToken | None], Awaitable[str]][source]#



notify_event_received(request_id: str) → None[source]#

```python
get_wrapped_callback() → Callable[[str, CancellationToken | None], Awaitable[str]][source]#
```

```python
notify_event_received(request_id: str) → None[source]#
```

【中文翻译】This module implements utility classes for formatting/printing agent messages.

【中文翻译】previous

【中文翻译】autogen_agentchat.conditions

【中文翻译】next

【中文翻译】autogen_agentchat.state

### autogen_agentchat.ui {autogen_agentchatui}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html)

```python
async Console(stream: AsyncGenerator[BaseAgentEvent | BaseChatMessage | T, None], *, no_inline_images: bool = False, output_stats: bool = False, user_input_manager: UserInputManager | None = None) → T[source]#
```

【中文翻译】Consumes the message stream from run_stream()
or on_messages_stream() and renders the messages to the console.
Returns the last processed TaskResult or Response.

Note
output_stats is experimental and the stats may not be accurate.
It will be improved in future releases.


Parameters:

stream (AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None] | AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]) – Message stream to render.
This can be from run_stream() or on_messages_stream().
no_inline_images (bool, optional) – If terminal is iTerm2 will render images inline. Use this to disable this behavior. Defaults to False.
output_stats (bool, optional) – (Experimental) If True, will output a summary of the messages and inline token usage info. Defaults to False.


Returns:
last_processed – A TaskResult if the stream is from run_stream()
or a Response if the stream is from on_messages_stream().

```python
class UserInputManager(callback: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]])[source]#
```

【中文翻译】Bases: object


get_wrapped_callback() → Callable[[str, CancellationToken | None], Awaitable[str]][source]#



notify_event_received(request_id: str) → None[source]#

```python
get_wrapped_callback() → Callable[[str, CancellationToken | None], Awaitable[str]][source]#
```

```python
notify_event_received(request_id: str) → None[source]#
```

【中文翻译】This module implements utility classes for formatting/printing agent messages.

【中文翻译】previous

【中文翻译】autogen_agentchat.conditions

【中文翻译】next

【中文翻译】autogen_agentchat.state

### next
autogen_agentchat {next
autogen_agentchat}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html)

```python
EVENT_LOGGER_NAME = 'autogen_agentchat.events'#
```

【中文翻译】Logger name for event logs.

```python
TRACE_LOGGER_NAME = 'autogen_agentchat'#
```

【中文翻译】Logger name for trace logs.

【中文翻译】This module provides the main entry point for the autogen_agentchat package.
It includes logger names for trace and event logs, and retrieves the package version.

【中文翻译】previous

【中文翻译】API Reference

【中文翻译】next

【中文翻译】autogen_agentchat.messages

## Other API References {other-api-references}

【中文翻译】Other API References module.

### autogen_core {autogen_core}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html)

```python
class Agent(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


property metadata: AgentMetadata#
Metadata of the agent.



property id: AgentId#
ID of the agent.



async on_message(message: Any, ctx: MessageContext) → Any[source]#
Message handler for the agent. This should only be called by the runtime, not by other agents.

Parameters:

message (Any) – Received message. Type is one of the types in subscriptions.
ctx (MessageContext) – Context of the message.


Returns:
Any – Response to the message. Can be None.

Raises:

CancelledError – If the message was cancelled.
CantHandleException – If the agent cannot handle the message.






async save_state() → Mapping[str, Any][source]#
Save the state of the agent. The result must be JSON serializable.



async load_state(state: Mapping[str, Any]) → None[source]#
Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.





async close() → None[source]#
Called when the runtime is closed

```python
property metadata: AgentMetadata#
```

【中文翻译】Metadata of the agent.

```python
property id: AgentId#
```

【中文翻译】ID of the agent.

```python
async on_message(message: Any, ctx: MessageContext) → Any[source]#
```

【中文翻译】Message handler for the agent. This should only be called by the runtime, not by other agents.

Parameters:

message (Any) – Received message. Type is one of the types in subscriptions.
ctx (MessageContext) – Context of the message.


Returns:
Any – Response to the message. Can be None.

Raises:

CancelledError – If the message was cancelled.
CantHandleException – If the agent cannot handle the message.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the agent. The result must be JSON serializable.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.

```python
async close() → None[source]#
```

【中文翻译】Called when the runtime is closed

```python
class AgentId(type: str | AgentType, key: str)[source]#
```

【中文翻译】Bases: object
Agent ID uniquely identifies an agent instance within an agent runtime - including distributed runtime. It is the ‘address’ of the agent instance for receiving messages.
See here for more information: Agent Identity and Lifecycle


classmethod from_str(agent_id: str) → Self[source]#
Convert a string of the format type/key into an AgentId



property type: str#
An identifier that associates an agent with a specific factory function.
Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (_).



property key: str#
Agent instance identifier.
Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (_).

```python
classmethod from_str(agent_id: str) → Self[source]#
```

【中文翻译】Convert a string of the format type/key into an AgentId

```python
property type: str#
```

【中文翻译】An identifier that associates an agent with a specific factory function.
Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (_).

```python
property key: str#
```

【中文翻译】Agent instance identifier.
Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (_).

```python
class AgentProxy(agent: AgentId, runtime: AgentRuntime)[source]#
```

【中文翻译】Bases: object
A helper class that allows you to use an AgentId in place of its associated Agent


property id: AgentId#
Target agent for this proxy



property metadata: Awaitable[AgentMetadata]#
Metadata of the agent.



async send_message(message: Any, *, sender: AgentId, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#



async save_state() → Mapping[str, Any][source]#
Save the state of the agent. The result must be JSON serializable.



async load_state(state: Mapping[str, Any]) → None[source]#
Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.

```python
property id: AgentId#
```

【中文翻译】Target agent for this proxy

```python
property metadata: Awaitable[AgentMetadata]#
```

【中文翻译】Metadata of the agent.

```python
async send_message(message: Any, *, sender: AgentId, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the agent. The result must be JSON serializable.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.

```python
class AgentMetadata[source]#
```

【中文翻译】Bases: TypedDict


type: str#



key: str#



description: str#

```python
type: str#
```

```python
key: str#
```

```python
description: str#
```

```python
class AgentRuntime(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.





async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.





async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.






async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.






async get(id: AgentId, /, *, lazy: bool = True) → AgentId[source]#

async get(type: AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId



async save_state() → Mapping[str, Any][source]#
Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to load_state().
The structure of the state is implementation defined and can be any JSON serializable object.

Returns:
Mapping[str, Any] – The saved state.





async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by save_state().

Parameters:
state (Mapping[str, Any]) – The saved state.





async agent_metadata(agent: AgentId) → AgentMetadata[source]#
Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.





async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.





async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.






async add_subscription(subscription: Subscription) → None[source]#
Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add





async remove_subscription(id: str) → None[source]#
Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist





add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

【中文翻译】Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.

```python
async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
```

【中文翻译】Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.

```python
async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
```

【中文翻译】Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
```

【中文翻译】Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.

```python
async get(id: AgentId, /, *, lazy: bool = True) → AgentId[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to load_state().
The structure of the state is implementation defined and can be any JSON serializable object.

Returns:
Mapping[str, Any] – The saved state.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by save_state().

Parameters:
state (Mapping[str, Any]) – The saved state.

```python
async agent_metadata(agent: AgentId) → AgentMetadata[source]#
```

【中文翻译】Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.

```python
async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
```

【中文翻译】Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.

```python
async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.

```python
async add_subscription(subscription: Subscription) → None[source]#
```

【中文翻译】Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add

```python
async remove_subscription(id: str) → None[source]#
```

【中文翻译】Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist

```python
add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
```

【中文翻译】Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

```python
class BaseAgent(description: str)[source]#
```

【中文翻译】Bases: ABC, Agent


property metadata: AgentMetadata#
Metadata of the agent.



property type: str#



property id: AgentId#
ID of the agent.



property runtime: AgentRuntime#



final async on_message(message: Any, ctx: MessageContext) → Any[source]#
Message handler for the agent. This should only be called by the runtime, not by other agents.

Parameters:

message (Any) – Received message. Type is one of the types in subscriptions.
ctx (MessageContext) – Context of the message.


Returns:
Any – Response to the message. Can be None.

Raises:

CancelledError – If the message was cancelled.
CantHandleException – If the agent cannot handle the message.






abstract async on_message_impl(message: Any, ctx: MessageContext) → Any[source]#



async send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
See autogen_core.AgentRuntime.send_message() for more information.



async publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) → None[source]#



async save_state() → Mapping[str, Any][source]#
Save the state of the agent. The result must be JSON serializable.



async load_state(state: Mapping[str, Any]) → None[source]#
Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.





async close() → None[source]#
Called when the runtime is closed



async classmethod register(runtime: AgentRuntime, type: str, factory: Callable[[], Self | Awaitable[Self]], *, skip_class_subscriptions: bool = False, skip_direct_message_subscription: bool = False) → AgentType[source]#
Register a virtual subclass of an ABC.
Returns the subclass, to allow usage as a class decorator.

```python
property metadata: AgentMetadata#
```

【中文翻译】Metadata of the agent.

```python
property type: str#
```

```python
property id: AgentId#
```

【中文翻译】ID of the agent.

```python
property runtime: AgentRuntime#
```

```python
final async on_message(message: Any, ctx: MessageContext) → Any[source]#
```

【中文翻译】Message handler for the agent. This should only be called by the runtime, not by other agents.

Parameters:

message (Any) – Received message. Type is one of the types in subscriptions.
ctx (MessageContext) – Context of the message.


Returns:
Any – Response to the message. Can be None.

Raises:

CancelledError – If the message was cancelled.
CantHandleException – If the agent cannot handle the message.

```python
abstract async on_message_impl(message: Any, ctx: MessageContext) → Any[source]#
```

```python
async send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

【中文翻译】See autogen_core.AgentRuntime.send_message() for more information.

```python
async publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) → None[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the agent. The result must be JSON serializable.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.

```python
async close() → None[source]#
```

【中文翻译】Called when the runtime is closed

```python
async classmethod register(runtime: AgentRuntime, type: str, factory: Callable[[], Self | Awaitable[Self]], *, skip_class_subscriptions: bool = False, skip_direct_message_subscription: bool = False) → AgentType[source]#
```

【中文翻译】Register a virtual subclass of an ABC.
Returns the subclass, to allow usage as a class decorator.

```python
class CacheStore[source]#
```

【中文翻译】Bases: ABC, Generic[T], ComponentBase[BaseModel]
This protocol defines the basic interface for store/cache operations.
Sub-classes should handle the lifecycle of underlying storage.


component_type: ClassVar[ComponentType] = 'cache_store'#
The logical type of the component.



abstract get(key: str, default: T | None = None) → T | None[source]#
Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.





abstract set(key: str, value: T) → None[source]#
Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
component_type: ClassVar[ComponentType] = 'cache_store'#
```

【中文翻译】The logical type of the component.

```python
abstract get(key: str, default: T | None = None) → T | None[source]#
```

【中文翻译】Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.

```python
abstract set(key: str, value: T) → None[source]#
```

【中文翻译】Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
class InMemoryStore[source]#
```

【中文翻译】Bases: CacheStore[T], Component[InMemoryStoreConfig]


component_provider_override: ClassVar[str | None] = 'autogen_core.InMemoryStore'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_config_schema#
alias of InMemoryStoreConfig



get(key: str, default: T | None = None) → T | None[source]#
Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.





set(key: str, value: T) → None[source]#
Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.






_to_config() → InMemoryStoreConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





classmethod _from_config(config: InMemoryStoreConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.InMemoryStore'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_config_schema#
```

【中文翻译】alias of InMemoryStoreConfig

```python
get(key: str, default: T | None = None) → T | None[source]#
```

【中文翻译】Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.

```python
set(key: str, value: T) → None[source]#
```

【中文翻译】Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
_to_config() → InMemoryStoreConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
classmethod _from_config(config: InMemoryStoreConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
class CancellationToken[source]#
```

【中文翻译】Bases: object
A token used to cancel pending async calls


cancel() → None[source]#
Cancel pending async calls linked to this cancellation token.



is_cancelled() → bool[source]#
Check if the CancellationToken has been used



add_callback(callback: Callable[[], None]) → None[source]#
Attach a callback that will be called when cancel is invoked



link_future(future: Future[Any]) → Future[Any][source]#
Link a pending async call to a token to allow its cancellation

```python
cancel() → None[source]#
```

【中文翻译】Cancel pending async calls linked to this cancellation token.

```python
is_cancelled() → bool[source]#
```

【中文翻译】Check if the CancellationToken has been used

```python
add_callback(callback: Callable[[], None]) → None[source]#
```

【中文翻译】Attach a callback that will be called when cancel is invoked

```python
link_future(future: Future[Any]) → Future[Any][source]#
```

【中文翻译】Link a pending async call to a token to allow its cancellation

```python
class AgentInstantiationContext[source]#
```

【中文翻译】Bases: object
A static class that provides context for agent instantiation.
This static class can be used to access the current runtime and agent ID
during agent instantiation – inside the factory function or the agent’s
class constructor.
Example
Get the current runtime and agent ID inside the factory function and
the agent’s constructor:
import asyncio
from dataclasses import dataclass

from autogen_core import (
    AgentId,
    AgentInstantiationContext,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    message_handler,
)


@dataclass
class TestMessage:
    content: str


class TestAgent(RoutedAgent):
    def __init__(self, description: str):
        super().__init__(description)
        # Get the current runtime -- we don't use it here, but it's available.
        _ = AgentInstantiationContext.current_runtime()
        # Get the current agent ID.
        agent_id = AgentInstantiationContext.current_agent_id()
        print(f"Current AgentID from constructor: {agent_id}")

    @message_handler
    async def handle_test_message(self, message: TestMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


def test_agent_factory() -> TestAgent:
    # Get the current runtime -- we don't use it here, but it's available.
    _ = AgentInstantiationContext.current_runtime()
    # Get the current agent ID.
    agent_id = AgentInstantiationContext.current_agent_id()
    print(f"Current AgentID from factory: {agent_id}")
    return TestAgent(description="Test agent")


async def main() -> None:
    # Create a SingleThreadedAgentRuntime instance.
    runtime = SingleThreadedAgentRuntime()

    # Start the runtime.
    runtime.start()

    # Register the agent type with a factory function.
    await runtime.register_factory("test_agent", test_agent_factory)

    # Send a message to the agent. The runtime will instantiate the agent and call the message handler.
    await runtime.send_message(TestMessage(content="Hello, world!"), AgentId("test_agent", "default"))

    # Stop the runtime.
    await runtime.stop()


asyncio.run(main())




classmethod current_runtime() → AgentRuntime[source]#



classmethod current_agent_id() → AgentId[source]#

**示例**:
```python
import asyncio
from dataclasses import dataclass

from autogen_core import (
    AgentId,
    AgentInstantiationContext,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    message_handler,
)


@dataclass
class TestMessage:
    content: str


class TestAgent(RoutedAgent):
    def __init__(self, description: str):
        super().__init__(description)
        # Get the current runtime -- we don't use it here, but it's available.
        _ = AgentInstantiationContext.current_runtime()
        # Get the current agent ID.
        agent_id = AgentInstantiationContext.current_agent_id()
        print(f"Current AgentID from constructor: {agent_id}")

    @message_handler
    async def handle_test_message(self, message: TestMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


def test_agent_factory() -> TestAgent:
    # Get the current runtime -- we don't use it here, but it's available.
    _ = AgentInstantiationContext.current_runtime()
    # Get the current agent ID.
    agent_id = AgentInstantiationContext.current_agent_id()
    print(f"Current AgentID from factory: {agent_id}")
    return TestAgent(description="Test agent")


async def main() -> None:
    # Create a SingleThreadedAgentRuntime instance.
    runtime = SingleThreadedAgentRuntime()

    # Start the runtime.
    runtime.start()

    # Register the agent type with a factory function.
    await runtime.register_factory("test_agent", test_agent_factory)

    # Send a message to the agent. The runtime will instantiate the agent and call the message handler.
    await runtime.send_message(TestMessage(content="Hello, world!"), AgentId("test_agent", "default"))

    # Stop the runtime.
    await runtime.stop()


asyncio.run(main())

```

```python
classmethod current_runtime() → AgentRuntime[source]#
```

```python
classmethod current_agent_id() → AgentId[source]#
```

```python
class TopicId(type: str, source: str)[source]#
```

【中文翻译】Bases: object
TopicId defines the scope of a broadcast message. In essence, agent runtime implements a publish-subscribe model through its broadcast API: when publishing a message, the topic must be specified.
See here for more information: Topic


type: str#
Type of the event that this topic_id contains. Adhere’s to the cloud event spec.
Must match the pattern: ^[w-.:=]+Z
Learn more here: cloudevents/spec



source: str#
Identifies the context in which an event happened. Adhere’s to the cloud event spec.
Learn more here: cloudevents/spec



classmethod from_str(topic_id: str) → Self[source]#
Convert a string of the format type/source into a TopicId

```python
type: str#
```

【中文翻译】Type of the event that this topic_id contains. Adhere’s to the cloud event spec.
Must match the pattern: ^[w-.:=]+Z
Learn more here: cloudevents/spec

```python
source: str#
```

【中文翻译】Identifies the context in which an event happened. Adhere’s to the cloud event spec.
Learn more here: cloudevents/spec

```python
classmethod from_str(topic_id: str) → Self[source]#
```

【中文翻译】Convert a string of the format type/source into a TopicId

```python
class Subscription(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol
Subscriptions define the topics that an agent is interested in.


property id: str#
Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.





is_match(topic_id: TopicId) → bool[source]#
Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.





map_to_agent(topic_id: TopicId) → AgentId[source]#
Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

```python
property id: str#
```

【中文翻译】Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.

```python
is_match(topic_id: TopicId) → bool[source]#
```

【中文翻译】Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.

```python
map_to_agent(topic_id: TopicId) → AgentId[source]#
```

【中文翻译】Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

```python
class MessageContext(sender: AgentId | None, topic_id: TopicId | None, is_rpc: bool, cancellation_token: CancellationToken, message_id: str)[source]#
```

【中文翻译】Bases: object


sender: AgentId | None#



topic_id: TopicId | None#



is_rpc: bool#



cancellation_token: CancellationToken#



message_id: str#

```python
sender: AgentId | None#
```

```python
topic_id: TopicId | None#
```

```python
is_rpc: bool#
```

```python
cancellation_token: CancellationToken#
```

```python
message_id: str#
```

```python
class AgentType(type: str)[source]#
```

【中文翻译】Bases: object


type: str#
String representation of this agent type.

```python
type: str#
```

【中文翻译】String representation of this agent type.

```python
class SubscriptionInstantiationContext[source]#
```

【中文翻译】Bases: object


classmethod agent_type() → AgentType[source]#

```python
classmethod agent_type() → AgentType[source]#
```

```python
class MessageHandlerContext[source]#
```

【中文翻译】Bases: object


classmethod agent_id() → AgentId[source]#

```python
classmethod agent_id() → AgentId[source]#
```

```python
class MessageSerializer(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol[T]


property data_content_type: str#



property type_name: str#



deserialize(payload: bytes) → T[source]#



serialize(message: T) → bytes[source]#

```python
property data_content_type: str#
```

```python
property type_name: str#
```

```python
deserialize(payload: bytes) → T[source]#
```

```python
serialize(message: T) → bytes[source]#
```

```python
class UnknownPayload(type_name: str, data_content_type: str, payload: bytes)[source]#
```

【中文翻译】Bases: object


type_name: str#



data_content_type: str#



payload: bytes#

```python
type_name: str#
```

```python
data_content_type: str#
```

```python
payload: bytes#
```

```python
class Image(image: Image)[source]#
```

【中文翻译】Bases: object
Represents an image.
Example
Loading an image from a URL:
from autogen_core import Image
from PIL import Image as PILImage
import aiohttp
import asyncio


async def from_url(url: str) -> Image:
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            content = await response.read()
            return Image.from_pil(PILImage.open(content))


image = asyncio.run(from_url("https://example.com/image"))




classmethod from_pil(pil_image: Image) → Image[source]#



classmethod from_uri(uri: str) → Image[source]#



classmethod from_base64(base64_str: str) → Image[source]#



to_base64() → str[source]#



classmethod from_file(file_path: Path) → Image[source]#



property data_uri: str#



to_openai_format(detail: Literal['auto', 'low', 'high'] = 'auto') → Dict[str, Any][source]#

**示例**:
```python
from autogen_core import Image
from PIL import Image as PILImage
import aiohttp
import asyncio


async def from_url(url: str) -> Image:
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            content = await response.read()
            return Image.from_pil(PILImage.open(content))


image = asyncio.run(from_url("https://example.com/image"))

```

```python
classmethod from_pil(pil_image: Image) → Image[source]#
```

```python
classmethod from_uri(uri: str) → Image[source]#
```

```python
classmethod from_base64(base64_str: str) → Image[source]#
```

```python
to_base64() → str[source]#
```

```python
classmethod from_file(file_path: Path) → Image[source]#
```

```python
property data_uri: str#
```

```python
to_openai_format(detail: Literal['auto', 'low', 'high'] = 'auto') → Dict[str, Any][source]#
```

```python
class RoutedAgent(description: str)[source]#
```

【中文翻译】Bases: BaseAgent
A base class for agents that route messages to handlers based on the type of the message
and optional matching functions.
To create a routed agent, subclass this class and add message handlers as methods decorated with
either event() or rpc() decorator.
Example:
from dataclasses import dataclass
from autogen_core import MessageContext
from autogen_core import RoutedAgent, event, rpc


@dataclass
class Message:
    pass


@dataclass
class MessageWithContent:
    content: str


@dataclass
class Response:
    pass


class MyAgent(RoutedAgent):
    def __init__(self):
        super().__init__("MyAgent")

    @event
    async def handle_event_message(self, message: Message, ctx: MessageContext) -> None:
        assert ctx.topic_id is not None
        await self.publish_message(MessageWithContent("event handled"), ctx.topic_id)

    @rpc(match=lambda message, ctx: message.content == "special")  # type: ignore
    async def handle_special_rpc_message(self, message: MessageWithContent, ctx: MessageContext) -> Response:
        return Response()




async on_message_impl(message: Any, ctx: MessageContext) → Any | None[source]#
Handle a message by routing it to the appropriate message handler.
Do not override this method in subclasses. Instead, add message handlers as methods decorated with
either the event() or rpc() decorator.



async on_unhandled_message(message: Any, ctx: MessageContext) → None[source]#
Called when a message is received that does not have a matching message handler.
The default implementation logs an info message.

**示例**:
```python
from dataclasses import dataclass
from autogen_core import MessageContext
from autogen_core import RoutedAgent, event, rpc


@dataclass
class Message:
    pass


@dataclass
class MessageWithContent:
    content: str


@dataclass
class Response:
    pass


class MyAgent(RoutedAgent):
    def __init__(self):
        super().__init__("MyAgent")

    @event
    async def handle_event_message(self, message: Message, ctx: MessageContext) -> None:
        assert ctx.topic_id is not None
        await self.publish_message(MessageWithContent("event handled"), ctx.topic_id)

    @rpc(match=lambda message, ctx: message.content == "special")  # type: ignore
    async def handle_special_rpc_message(self, message: MessageWithContent, ctx: MessageContext) -> Response:
        return Response()

```

```python
async on_message_impl(message: Any, ctx: MessageContext) → Any | None[source]#
```

【中文翻译】Handle a message by routing it to the appropriate message handler.
Do not override this method in subclasses. Instead, add message handlers as methods decorated with
either the event() or rpc() decorator.

```python
async on_unhandled_message(message: Any, ctx: MessageContext) → None[source]#
```

【中文翻译】Called when a message is received that does not have a matching message handler.
The default implementation logs an info message.

```python
class ClosureAgent(description: str, closure: Callable[[ClosureContext, T, MessageContext], Awaitable[Any]], *, unknown_type_policy: Literal['error', 'warn', 'ignore'] = 'warn')[source]#
```

【中文翻译】Bases: BaseAgent, ClosureContext


property metadata: AgentMetadata#
Metadata of the agent.



property id: AgentId#
ID of the agent.



property runtime: AgentRuntime#



async on_message_impl(message: Any, ctx: MessageContext) → Any[source]#



async save_state() → Mapping[str, Any][source]#
Closure agents do not have state. So this method always returns an empty dictionary.



async load_state(state: Mapping[str, Any]) → None[source]#
Closure agents do not have state. So this method does nothing.



async classmethod register_closure(runtime: AgentRuntime, type: str, closure: Callable[[ClosureContext, T, MessageContext], Awaitable[Any]], *, unknown_type_policy: Literal['error', 'warn', 'ignore'] = 'warn', skip_direct_message_subscription: bool = False, description: str = '', subscriptions: Callable[[], list[Subscription] | Awaitable[list[Subscription]]] | None = None) → AgentType[source]#
The closure agent allows you to define an agent using a closure, or function without needing to define a class. It allows values to be extracted out of the runtime.
The closure can define the type of message which is expected, or Any can be used to accept any type of message.
Example:
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId


@dataclass
class MyMessage:
    content: str


async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(
        runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
    )

    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

    result = await queue.get()
    print(result)


asyncio.run(main())



Parameters:

runtime (AgentRuntime) – Runtime to register the agent to
type (str) – Agent type of registered agent
closure (Callable[[ClosureContext, T, MessageContext], Awaitable[Any]]) – Closure to handle messages
unknown_type_policy (Literal["error", "warn", "ignore"], optional) – What to do if a type is encountered that does not match the closure type. Defaults to “warn”.
skip_direct_message_subscription (bool, optional) – Do not add direct message subscription for this agent. Defaults to False.
description (str, optional) – Description of what agent does. Defaults to “”.
subscriptions (Callable[[], list[Subscription]  |  Awaitable[list[Subscription]]] | None, optional) – List of subscriptions for this closure agent. Defaults to None.


Returns:
AgentType – Type of the agent that was registered

**示例**:
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId


@dataclass
class MyMessage:
    content: str


async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(
        runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
    )

    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

    result = await queue.get()
    print(result)


asyncio.run(main())

```

```python
property metadata: AgentMetadata#
```

【中文翻译】Metadata of the agent.

```python
property id: AgentId#
```

【中文翻译】ID of the agent.

```python
property runtime: AgentRuntime#
```

```python
async on_message_impl(message: Any, ctx: MessageContext) → Any[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Closure agents do not have state. So this method always returns an empty dictionary.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Closure agents do not have state. So this method does nothing.

```python
async classmethod register_closure(runtime: AgentRuntime, type: str, closure: Callable[[ClosureContext, T, MessageContext], Awaitable[Any]], *, unknown_type_policy: Literal['error', 'warn', 'ignore'] = 'warn', skip_direct_message_subscription: bool = False, description: str = '', subscriptions: Callable[[], list[Subscription] | Awaitable[list[Subscription]]] | None = None) → AgentType[source]#
```

【中文翻译】The closure agent allows you to define an agent using a closure, or function without needing to define a class. It allows values to be extracted out of the runtime.
The closure can define the type of message which is expected, or Any can be used to accept any type of message.
Example:
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId


@dataclass
class MyMessage:
    content: str


async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(
        runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
    )

    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

    result = await queue.get()
    print(result)


asyncio.run(main())



Parameters:

runtime (AgentRuntime) – Runtime to register the agent to
type (str) – Agent type of registered agent
closure (Callable[[ClosureContext, T, MessageContext], Awaitable[Any]]) – Closure to handle messages
unknown_type_policy (Literal["error", "warn", "ignore"], optional) – What to do if a type is encountered that does not match the closure type. Defaults to “warn”.
skip_direct_message_subscription (bool, optional) – Do not add direct message subscription for this agent. Defaults to False.
description (str, optional) – Description of what agent does. Defaults to “”.
subscriptions (Callable[[], list[Subscription]  |  Awaitable[list[Subscription]]] | None, optional) – List of subscriptions for this closure agent. Defaults to None.


Returns:
AgentType – Type of the agent that was registered

**示例**:
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId


@dataclass
class MyMessage:
    content: str


async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(
        runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
    )

    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

    result = await queue.get()
    print(result)


asyncio.run(main())

```

```python
class ClosureContext(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


property id: AgentId#



async send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#



async publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) → None[source]#

```python
property id: AgentId#
```

```python
async send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

```python
async publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) → None[source]#
```

```python
message_handler(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None) → Callable[[Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]]], MessageHandler[AgentT, ReceivesT, ProducesT]] | MessageHandler[AgentT, ReceivesT, ProducesT][source]#
```

【中文翻译】Decorator for generic message handlers.
Add this decorator to methods in a RoutedAgent class that are intended to handle both event and RPC messages.
These methods must have a specific signature that needs to be followed for it to be valid:

The method must be an async method.
The method must be decorated with the @message_handler decorator.

The method must have exactly 3 arguments:
self
message: The message to be handled, this must be type-hinted with the message type that it is intended to handle.
ctx: A autogen_core.MessageContext object.




The method must be type hinted with what message types it can return as a response, or it can return None if it does not return anything.

Handlers can handle more than one message type by accepting a Union of the message types. It can also return more than one message type by returning a Union of the message types.

Parameters:

func – The function to be decorated.
strict – If True, the handler will raise an exception if the message type or return type is not in the target types. If False, it will log a warning instead.
match – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

```python
event(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, None]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None) → Callable[[Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, None]]], MessageHandler[AgentT, ReceivesT, None]] | MessageHandler[AgentT, ReceivesT, None][source]#
```

【中文翻译】Decorator for event message handlers.
Add this decorator to methods in a RoutedAgent class that are intended to handle event messages.
These methods must have a specific signature that needs to be followed for it to be valid:

The method must be an async method.
The method must be decorated with the @message_handler decorator.

The method must have exactly 3 arguments:
self
message: The event message to be handled, this must be type-hinted with the message type that it is intended to handle.
ctx: A autogen_core.MessageContext object.




The method must return None.

Handlers can handle more than one message type by accepting a Union of the message types.

Parameters:

func – The function to be decorated.
strict – If True, the handler will raise an exception if the message type is not in the target types. If False, it will log a warning instead.
match – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

```python
rpc(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None) → Callable[[Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]]], MessageHandler[AgentT, ReceivesT, ProducesT]] | MessageHandler[AgentT, ReceivesT, ProducesT][source]#
```

【中文翻译】Decorator for RPC message handlers.
Add this decorator to methods in a RoutedAgent class that are intended to handle RPC messages.
These methods must have a specific signature that needs to be followed for it to be valid:

The method must be an async method.
The method must be decorated with the @message_handler decorator.

The method must have exactly 3 arguments:
self
message: The message to be handled, this must be type-hinted with the message type that it is intended to handle.
ctx: A autogen_core.MessageContext object.




The method must be type hinted with what message types it can return as a response, or it can return None if it does not return anything.

Handlers can handle more than one message type by accepting a Union of the message types. It can also return more than one message type by returning a Union of the message types.

Parameters:

func – The function to be decorated.
strict – If True, the handler will raise an exception if the message type or return type is not in the target types. If False, it will log a warning instead.
match – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

```python
class FunctionCall(id: 'str', arguments: 'str', name: 'str')[source]#
```

【中文翻译】Bases: object


id: str#



arguments: str#



name: str#

```python
id: str#
```

```python
arguments: str#
```

```python
name: str#
```

```python
class TypeSubscription(topic_type: str, agent_type: str | AgentType, id: str | None = None)[source]#
```

【中文翻译】Bases: Subscription
This subscription matches on topics based on the type and maps to agents using the source of the topic as the agent key.
This subscription causes each source to have its own agent instance.
Example
from autogen_core import TypeSubscription

subscription = TypeSubscription(topic_type="t1", agent_type="a1")


In this case:

A topic_id with type t1 and source s1 will be handled by an agent of type a1 with key s1
A topic_id with type t1 and source s2 will be handled by an agent of type a1 with key s2.


Parameters:

topic_type (str) – Topic type to match against
agent_type (str) – Agent type to handle this subscription





property id: str#
Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.





property topic_type: str#



property agent_type: str#



is_match(topic_id: TopicId) → bool[source]#
Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.





map_to_agent(topic_id: TopicId) → AgentId[source]#
Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

**示例**:
```python
from autogen_core import TypeSubscription

subscription = TypeSubscription(topic_type="t1", agent_type="a1")

```

```python
property id: str#
```

【中文翻译】Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.

```python
property topic_type: str#
```

```python
property agent_type: str#
```

```python
is_match(topic_id: TopicId) → bool[source]#
```

【中文翻译】Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.

```python
map_to_agent(topic_id: TopicId) → AgentId[source]#
```

【中文翻译】Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

```python
class DefaultSubscription(topic_type: str = 'default', agent_type: str | AgentType | None = None)[source]#
```

【中文翻译】Bases: TypeSubscription
The default subscription is designed to be a sensible default for applications that only need global scope for agents.
This topic by default uses the “default” topic type and attempts to detect the agent type to use based on the instantiation context.

Parameters:

topic_type (str, optional) – The topic type to subscribe to. Defaults to “default”.
agent_type (str, optional) – The agent type to use for the subscription. Defaults to None, in which case it will attempt to detect the agent type based on the instantiation context.

```python
class DefaultTopicId(type: str = 'default', source: str | None = None)[source]#
```

【中文翻译】Bases: TopicId
DefaultTopicId provides a sensible default for the topic_id and source fields of a TopicId.
If created in the context of a message handler, the source will be set to the agent_id of the message handler, otherwise it will be set to “default”.

Parameters:

type (str, optional) – Topic type to publish message to. Defaults to “default”.
source (str | None, optional) – Topic source to publish message to. If None, the source will be set to the agent_id of the message handler if in the context of a message handler, otherwise it will be set to “default”. Defaults to None.

```python
default_subscription(cls: Type[BaseAgentType] | None = None) → Callable[[Type[BaseAgentType]], Type[BaseAgentType]] | Type[BaseAgentType][source]#
```

```python
type_subscription(topic_type: str) → Callable[[Type[BaseAgentType]], Type[BaseAgentType]][source]#
```

```python
class TypePrefixSubscription(topic_type_prefix: str, agent_type: str | AgentType, id: str | None = None)[source]#
```

【中文翻译】Bases: Subscription
This subscription matches on topics based on a prefix of the type and maps to agents using the source of the topic as the agent key.
This subscription causes each source to have its own agent instance.
Example
from autogen_core import TypePrefixSubscription

subscription = TypePrefixSubscription(topic_type_prefix="t1", agent_type="a1")


In this case:

A topic_id with type t1 and source s1 will be handled by an agent of type a1 with key s1
A topic_id with type t1 and source s2 will be handled by an agent of type a1 with key s2.
A topic_id with type t1SUFFIX and source s2 will be handled by an agent of type a1 with key s2.


Parameters:

topic_type_prefix (str) – Topic type prefix to match against
agent_type (str) – Agent type to handle this subscription





property id: str#
Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.





property topic_type_prefix: str#



property agent_type: str#



is_match(topic_id: TopicId) → bool[source]#
Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.





map_to_agent(topic_id: TopicId) → AgentId[source]#
Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

**示例**:
```python
from autogen_core import TypePrefixSubscription

subscription = TypePrefixSubscription(topic_type_prefix="t1", agent_type="a1")

```

```python
property id: str#
```

【中文翻译】Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.

```python
property topic_type_prefix: str#
```

```python
property agent_type: str#
```

```python
is_match(topic_id: TopicId) → bool[source]#
```

【中文翻译】Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.

```python
map_to_agent(topic_id: TopicId) → AgentId[source]#
```

【中文翻译】Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

```python
JSON_DATA_CONTENT_TYPE = 'application/json'#
```

【中文翻译】The content type for JSON data.

```python
PROTOBUF_DATA_CONTENT_TYPE = 'application/x-protobuf'#
```

【中文翻译】The content type for Protobuf data.

```python
class SingleThreadedAgentRuntime(*, intervention_handlers: List[InterventionHandler] | None = None, tracer_provider: TracerProvider | None = None, ignore_unhandled_exceptions: bool = True)[source]#
```

【中文翻译】Bases: AgentRuntime
A single-threaded agent runtime that processes all messages using a single asyncio queue.
Messages are delivered in the order they are received, and the runtime processes
each message in a separate asyncio task concurrently.

Note
This runtime is suitable for development and standalone applications.
It is not suitable for high-throughput or high-concurrency scenarios.


Parameters:

intervention_handlers (List[InterventionHandler], optional) – A list of intervention
handlers that can intercept messages before they are sent or published. Defaults to None.
tracer_provider (TracerProvider, optional) – The tracer provider to use for tracing. Defaults to None.
ignore_unhandled_exceptions (bool, optional) – Whether to ignore unhandled exceptions in that occur in agent event handlers. Any background exceptions will be raised on the next call to process_next or from an awaited stop, stop_when_idle or stop_when. Note, this does not apply to RPC handlers. Defaults to True.



Examples
A simple example of creating a runtime, registering an agent, sending a message and stopping the runtime:
import asyncio
from dataclasses import dataclass

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


async def main() -> None:
    # Create a runtime and register the agent
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

    # Start the runtime, send a message and stop the runtime
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()


asyncio.run(main())


An example of creating a runtime, registering an agent, publishing a message and stopping the runtime:
import asyncio
from dataclasses import dataclass

from autogen_core import (
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    default_subscription,
    message_handler,
)


@dataclass
class MyMessage:
    content: str


# The agent is subscribed to the default topic.
@default_subscription
class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


async def main() -> None:
    # Create a runtime and register the agent
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

    # Start the runtime.
    runtime.start()
    # Publish a message to the default topic that the agent is subscribed to.
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    # Wait for the message to be processed and then stop the runtime.
    await runtime.stop_when_idle()


asyncio.run(main())




property unprocessed_messages_count: int#



async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.





async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.





async save_state() → Mapping[str, Any][source]#
Save the state of all instantiated agents.
This method calls the save_state() method on each agent and returns a dictionary
mapping agent IDs to their state.

Note
This method does not currently save the subscription state. We will add this in the future.


Returns:
A dictionary mapping agent IDs to their state.





async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of all instantiated agents.
This method calls the load_state() method on each agent with the state
provided in the dictionary. The keys of the dictionary are the agent IDs, and the values are the state
dictionaries returned by the save_state() method.

Note
This method does not currently load the subscription state. We will add this in the future.




async process_next() → None[source]#
Process the next message in the queue.
If there is an unhandled exception in the background task, it will be raised here. process_next cannot be called again after an unhandled exception is raised.



start() → None[source]#
Start the runtime message processing loop. This runs in a background task.
Example:
import asyncio
from autogen_core import SingleThreadedAgentRuntime


async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()

    # ... do other things ...

    await runtime.stop()


asyncio.run(main())





async close() → None[source]#
Calls stop() if applicable and the Agent.close() method on all instantiated agents



async stop() → None[source]#
Immediately stop the runtime message processing loop. The currently processing message will be completed, but all others following it will be discarded.



async stop_when_idle() → None[source]#
Stop the runtime message processing loop when there is
no outstanding message being processed or queued. This is the most common way to stop the runtime.



async stop_when(condition: Callable[[], bool]) → None[source]#
Stop the runtime message processing loop when the condition is met.

Caution
This method is not recommended to be used, and is here for legacy
reasons. It will spawn a busy loop to continually check the
condition. It is much more efficient to call stop_when_idle or
stop instead. If you need to stop the runtime based on a
condition, consider using a background task and asyncio.Event to
signal when the condition is met and the background task should call
stop.




async agent_metadata(agent: AgentId) → AgentMetadata[source]#
Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.





async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.





async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.






async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.






async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.






async add_subscription(subscription: Subscription) → None[source]#
Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add





async remove_subscription(id: str) → None[source]#
Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist





async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId[source]#



add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

**示例**:
```python
import asyncio
from dataclasses import dataclass

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


async def main() -> None:
    # Create a runtime and register the agent
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

    # Start the runtime, send a message and stop the runtime
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()


asyncio.run(main())

```

**示例**:
```python
import asyncio
from dataclasses import dataclass

from autogen_core import (
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    default_subscription,
    message_handler,
)


@dataclass
class MyMessage:
    content: str


# The agent is subscribed to the default topic.
@default_subscription
class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


async def main() -> None:
    # Create a runtime and register the agent
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

    # Start the runtime.
    runtime.start()
    # Publish a message to the default topic that the agent is subscribed to.
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    # Wait for the message to be processed and then stop the runtime.
    await runtime.stop_when_idle()


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime


async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()

    # ... do other things ...

    await runtime.stop()


asyncio.run(main())

```

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
property unprocessed_messages_count: int#
```

```python
async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

【中文翻译】Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.

```python
async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
```

【中文翻译】Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of all instantiated agents.
This method calls the save_state() method on each agent and returns a dictionary
mapping agent IDs to their state.

Note
This method does not currently save the subscription state. We will add this in the future.


Returns:
A dictionary mapping agent IDs to their state.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of all instantiated agents.
This method calls the load_state() method on each agent with the state
provided in the dictionary. The keys of the dictionary are the agent IDs, and the values are the state
dictionaries returned by the save_state() method.

Note
This method does not currently load the subscription state. We will add this in the future.

```python
async process_next() → None[source]#
```

【中文翻译】Process the next message in the queue.
If there is an unhandled exception in the background task, it will be raised here. process_next cannot be called again after an unhandled exception is raised.

```python
start() → None[source]#
```

【中文翻译】Start the runtime message processing loop. This runs in a background task.
Example:
import asyncio
from autogen_core import SingleThreadedAgentRuntime


async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()

    # ... do other things ...

    await runtime.stop()


asyncio.run(main())

**示例**:
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime


async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()

    # ... do other things ...

    await runtime.stop()


asyncio.run(main())

```

```python
async close() → None[source]#
```

【中文翻译】Calls stop() if applicable and the Agent.close() method on all instantiated agents

```python
async stop() → None[source]#
```

【中文翻译】Immediately stop the runtime message processing loop. The currently processing message will be completed, but all others following it will be discarded.

```python
async stop_when_idle() → None[source]#
```

【中文翻译】Stop the runtime message processing loop when there is
no outstanding message being processed or queued. This is the most common way to stop the runtime.

```python
async stop_when(condition: Callable[[], bool]) → None[source]#
```

【中文翻译】Stop the runtime message processing loop when the condition is met.

Caution
This method is not recommended to be used, and is here for legacy
reasons. It will spawn a busy loop to continually check the
condition. It is much more efficient to call stop_when_idle or
stop instead. If you need to stop the runtime based on a
condition, consider using a background task and asyncio.Event to
signal when the condition is met and the background task should call
stop.

```python
async agent_metadata(agent: AgentId) → AgentMetadata[source]#
```

【中文翻译】Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.

```python
async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
```

【中文翻译】Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.

```python
async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.

```python
async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
```

【中文翻译】Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
```

【中文翻译】Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.

```python
async add_subscription(subscription: Subscription) → None[source]#
```

【中文翻译】Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add

```python
async remove_subscription(id: str) → None[source]#
```

【中文翻译】Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist

```python
async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId[source]#
```

```python
add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
```

【中文翻译】Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

```python
ROOT_LOGGER_NAME = 'autogen_core'#
```

【中文翻译】The name of the root logger.

```python
EVENT_LOGGER_NAME = 'autogen_core.events'#
```

【中文翻译】The name of the logger used for structured events.

```python
TRACE_LOGGER_NAME = 'autogen_core.trace'#
```

【中文翻译】Logger name used for developer intended trace logging. The content and format of this log should not be depended upon.

```python
class Component[source]#
```

【中文翻译】Bases: ComponentFromConfig[ConfigT], ComponentSchemaType[ConfigT], Generic[ConfigT]
To create a component class, inherit from this class for the concrete class and ComponentBase on the interface. Then implement two class variables:

component_config_schema - A Pydantic model class which represents the configuration of the component. This is also the type parameter of Component.
component_type - What is the logical type of the component.

Example:
from __future__ import annotations

from pydantic import BaseModel
from autogen_core import Component


class Config(BaseModel):
    value: str


class MyComponent(Component[Config]):
    component_type = "custom"
    component_config_schema = Config

    def __init__(self, value: str):
        self.value = value

    def _to_config(self) -> Config:
        return Config(value=self.value)

    @classmethod
    def _from_config(cls, config: Config) -> MyComponent:
        return cls(value=config.value)

**示例**:
```python
from __future__ import annotations

from pydantic import BaseModel
from autogen_core import Component


class Config(BaseModel):
    value: str


class MyComponent(Component[Config]):
    component_type = "custom"
    component_config_schema = Config

    def __init__(self, value: str):
        self.value = value

    def _to_config(self) -> Config:
        return Config(value=self.value)

    @classmethod
    def _from_config(cls, config: Config) -> MyComponent:
        return cls(value=config.value)

```

```python
class ComponentBase[source]#
```

【中文翻译】Bases: ComponentToConfig[ConfigT], ComponentLoader, Generic[ConfigT]

```python
class ComponentFromConfig[source]#
```

【中文翻译】Bases: Generic[FromConfigT]


classmethod _from_config(config: FromConfigT) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





classmethod _from_config_past_version(config: Dict[str, Any], version: int) → Self[source]#
Create a new instance of the component from a previous version of the configuration object.
This is only called when the version of the configuration object is less than the current version, since in this case the schema is not known.

Parameters:

config (Dict[str, Any]) – The configuration object.
version (int) – The version of the configuration object.


Returns:
Self – The new instance of the component.

```python
classmethod _from_config(config: FromConfigT) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
classmethod _from_config_past_version(config: Dict[str, Any], version: int) → Self[source]#
```

【中文翻译】Create a new instance of the component from a previous version of the configuration object.
This is only called when the version of the configuration object is less than the current version, since in this case the schema is not known.

Parameters:

config (Dict[str, Any]) – The configuration object.
version (int) – The version of the configuration object.


Returns:
Self – The new instance of the component.

```python
class ComponentLoader[source]#
```

【中文翻译】Bases: object


classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → Self[source]#

classmethod load_component(model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType]) → ExpectedType
Load a component from a model. Intended to be used with the return type of autogen_core.ComponentConfig.dump_component().
Example
from autogen_core import ComponentModel
from autogen_core.models import ChatCompletionClient

component: ComponentModel = ...  # type: ignore

model_client = ChatCompletionClient.load_component(component)



Parameters:

model (ComponentModel) – The model to load the component from.
model – _description_
expected (Type[ExpectedType] | None, optional) – Explicit type only if used directly on ComponentLoader. Defaults to None.


Returns:
Self – The loaded component.

Raises:

ValueError – If the provider string is invalid.
TypeError – Provider is not a subclass of ComponentConfigImpl, or the expected type does not match.


Returns:
Self | ExpectedType – The loaded component.

**示例**:
```python
from autogen_core import ComponentModel
from autogen_core.models import ChatCompletionClient

component: ComponentModel = ...  # type: ignore

model_client = ChatCompletionClient.load_component(component)

```

```python
classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → Self[source]#
```

【中文翻译】Load a component from a model. Intended to be used with the return type of autogen_core.ComponentConfig.dump_component().
Example
from autogen_core import ComponentModel
from autogen_core.models import ChatCompletionClient

component: ComponentModel = ...  # type: ignore

model_client = ChatCompletionClient.load_component(component)



Parameters:

model (ComponentModel) – The model to load the component from.
model – _description_
expected (Type[ExpectedType] | None, optional) – Explicit type only if used directly on ComponentLoader. Defaults to None.


Returns:
Self – The loaded component.

Raises:

ValueError – If the provider string is invalid.
TypeError – Provider is not a subclass of ComponentConfigImpl, or the expected type does not match.


Returns:
Self | ExpectedType – The loaded component.

**示例**:
```python
from autogen_core import ComponentModel
from autogen_core.models import ChatCompletionClient

component: ComponentModel = ...  # type: ignore

model_client = ChatCompletionClient.load_component(component)

```

```python
pydantic model ComponentModel[source]#
```

【中文翻译】Bases: BaseModel
Model class for a component. Contains all information required to instantiate a component.

Show JSON schema{
   "title": "ComponentModel",
   "description": "Model class for a component. Contains all information required to instantiate a component.",
   "type": "object",
   "properties": {
      "provider": {
         "title": "Provider",
         "type": "string"
      },
      "component_type": {
         "anyOf": [
            {
               "enum": [
                  "model",
                  "agent",
                  "tool",
                  "termination",
                  "token_provider",
                  "workbench"
               ],
               "type": "string"
            },
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Component Type"
      },
      "version": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Version"
      },
      "component_version": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Component Version"
      },
      "description": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Description"
      },
      "label": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Label"
      },
      "config": {
         "title": "Config",
         "type": "object"
      }
   },
   "required": [
      "provider",
      "config"
   ]
}



Fields:

component_type (Literal['model', 'agent', 'tool', 'termination', 'token_provider', 'workbench'] | str | None)
component_version (int | None)
config (dict[str, Any])
description (str | None)
label (str | None)
provider (str)
version (int | None)





field provider: str [Required]#
Describes how the component can be instantiated.



field component_type: ComponentType | None = None#
Logical type of the component. If missing, the component assumes the default type of the provider.



field version: int | None = None#
Version of the component specification. If missing, the component assumes whatever is the current version of the library used to load it. This is obviously dangerous and should be used for user authored ephmeral config. For all other configs version should be specified.



field component_version: int | None = None#
Version of the component. If missing, the component assumes the default version of the provider.



field description: str | None = None#
Description of the component.



field label: str | None = None#
Human readable label for the component. If missing the component assumes the class name of the provider.



field config: dict[str, Any] [Required]#
The schema validated config field is passed to a given class’s implmentation of autogen_core.ComponentConfigImpl._from_config() to create a new instance of the component class.

**示例**:
```python
{
   "title": "ComponentModel",
   "description": "Model class for a component. Contains all information required to instantiate a component.",
   "type": "object",
   "properties": {
      "provider": {
         "title": "Provider",
         "type": "string"
      },
      "component_type": {
         "anyOf": [
            {
               "enum": [
                  "model",
                  "agent",
                  "tool",
                  "termination",
                  "token_provider",
                  "workbench"
               ],
               "type": "string"
            },
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Component Type"
      },
      "version": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Version"
      },
      "component_version": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Component Version"
      },
      "description": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Description"
      },
      "label": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Label"
      },
      "config": {
         "title": "Config",
         "type": "object"
      }
   },
   "required": [
      "provider",
      "config"
   ]
}

```

```python
field provider: str [Required]#
```

【中文翻译】Describes how the component can be instantiated.

```python
field component_type: ComponentType | None = None#
```

【中文翻译】Logical type of the component. If missing, the component assumes the default type of the provider.

```python
field version: int | None = None#
```

【中文翻译】Version of the component specification. If missing, the component assumes whatever is the current version of the library used to load it. This is obviously dangerous and should be used for user authored ephmeral config. For all other configs version should be specified.

```python
field component_version: int | None = None#
```

【中文翻译】Version of the component. If missing, the component assumes the default version of the provider.

```python
field description: str | None = None#
```

【中文翻译】Description of the component.

```python
field label: str | None = None#
```

【中文翻译】Human readable label for the component. If missing the component assumes the class name of the provider.

```python
field config: dict[str, Any] [Required]#
```

【中文翻译】The schema validated config field is passed to a given class’s implmentation of autogen_core.ComponentConfigImpl._from_config() to create a new instance of the component class.

```python
class ComponentSchemaType[source]#
```

【中文翻译】Bases: Generic[ConfigT]


component_config_schema: Type[ConfigT]#
The Pydantic model class which represents the configuration of the component.



required_class_vars = ['component_config_schema', 'component_type']#

```python
component_config_schema: Type[ConfigT]#
```

【中文翻译】The Pydantic model class which represents the configuration of the component.

```python
required_class_vars = ['component_config_schema', 'component_type']#
```

```python
class ComponentToConfig[source]#
```

【中文翻译】Bases: Generic[ToConfigT]
The two methods a class must implement to be a component.

Parameters:
Protocol (ConfigT) – Type which derives from pydantic.BaseModel.




component_type: ClassVar[Literal['model', 'agent', 'tool', 'termination', 'token_provider', 'workbench'] | str]#
The logical type of the component.



component_version: ClassVar[int] = 1#
The version of the component, if schema incompatibilities are introduced this should be updated.



component_provider_override: ClassVar[str | None] = None#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_description: ClassVar[str | None] = None#
A description of the component. If not provided, the docstring of the class will be used.



component_label: ClassVar[str | None] = None#
A human readable label for the component. If not provided, the component class name will be used.



_to_config() → ToConfigT[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





dump_component() → ComponentModel[source]#
Dump the component to a model that can be loaded back in.

Raises:
TypeError – If the component is a local class.

Returns:
ComponentModel – The model representing the component.

```python
component_type: ClassVar[Literal['model', 'agent', 'tool', 'termination', 'token_provider', 'workbench'] | str]#
```

【中文翻译】The logical type of the component.

```python
component_version: ClassVar[int] = 1#
```

【中文翻译】The version of the component, if schema incompatibilities are introduced this should be updated.

```python
component_provider_override: ClassVar[str | None] = None#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_description: ClassVar[str | None] = None#
```

【中文翻译】A description of the component. If not provided, the docstring of the class will be used.

```python
component_label: ClassVar[str | None] = None#
```

【中文翻译】A human readable label for the component. If not provided, the component class name will be used.

```python
_to_config() → ToConfigT[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
dump_component() → ComponentModel[source]#
```

【中文翻译】Dump the component to a model that can be loaded back in.

Raises:
TypeError – If the component is a local class.

Returns:
ComponentModel – The model representing the component.

```python
is_component_class(cls: type) → TypeGuard[Type[_ConcreteComponent[BaseModel]]][source]#
```

```python
is_component_instance(cls: Any) → TypeGuard[_ConcreteComponent[BaseModel]][source]#
```

```python
final class DropMessage[source]#
```

【中文翻译】Bases: object
Marker type for signalling that a message should be dropped by an intervention handler. The type itself should be returned from the handler.

```python
class InterventionHandler(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol
An intervention handler is a class that can be used to modify, log or drop messages that are being processed by the autogen_core.base.AgentRuntime.
The handler is called when the message is submitted to the runtime.
Currently the only runtime which supports this is the autogen_core.base.SingleThreadedAgentRuntime.
Note: Returning None from any of the intervention handler methods will result in a warning being issued and treated as “no change”. If you intend to drop a message, you should return DropMessage explicitly.
Example:
from autogen_core import DefaultInterventionHandler, MessageContext, AgentId, SingleThreadedAgentRuntime
from dataclasses import dataclass
from typing import Any


@dataclass
class MyMessage:
    content: str


class MyInterventionHandler(DefaultInterventionHandler):
    async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> MyMessage:
        if isinstance(message, MyMessage):
            message.content = message.content.upper()
        return message


runtime = SingleThreadedAgentRuntime(intervention_handlers=[MyInterventionHandler()])




async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) → Any | type[DropMessage][source]#
Called when a message is submitted to the AgentRuntime using autogen_core.base.AgentRuntime.send_message().



async on_publish(message: Any, *, message_context: MessageContext) → Any | type[DropMessage][source]#
Called when a message is published to the AgentRuntime using autogen_core.base.AgentRuntime.publish_message().



async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) → Any | type[DropMessage][source]#
Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

**示例**:
```python
from autogen_core import DefaultInterventionHandler, MessageContext, AgentId, SingleThreadedAgentRuntime
from dataclasses import dataclass
from typing import Any


@dataclass
class MyMessage:
    content: str


class MyInterventionHandler(DefaultInterventionHandler):
    async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> MyMessage:
        if isinstance(message, MyMessage):
            message.content = message.content.upper()
        return message


runtime = SingleThreadedAgentRuntime(intervention_handlers=[MyInterventionHandler()])

```

```python
async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a message is submitted to the AgentRuntime using autogen_core.base.AgentRuntime.send_message().

```python
async on_publish(message: Any, *, message_context: MessageContext) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a message is published to the AgentRuntime using autogen_core.base.AgentRuntime.publish_message().

```python
async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

```python
class DefaultInterventionHandler(*args, **kwargs)[source]#
```

【中文翻译】Bases: InterventionHandler
Simple class that provides a default implementation for all intervention
handler methods, that simply returns the message unchanged. Allows for easy
subclassing to override only the desired methods.


async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) → Any | type[DropMessage][source]#
Called when a message is submitted to the AgentRuntime using autogen_core.base.AgentRuntime.send_message().



async on_publish(message: Any, *, message_context: MessageContext) → Any | type[DropMessage][source]#
Called when a message is published to the AgentRuntime using autogen_core.base.AgentRuntime.publish_message().



async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) → Any | type[DropMessage][source]#
Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

```python
async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a message is submitted to the AgentRuntime using autogen_core.base.AgentRuntime.send_message().

```python
async on_publish(message: Any, *, message_context: MessageContext) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a message is published to the AgentRuntime using autogen_core.base.AgentRuntime.publish_message().

```python
async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

```python
ComponentType#
```

【中文翻译】alias of Literal[‘model’, ‘agent’, ‘tool’, ‘termination’, ‘token_provider’, ‘workbench’] | str

【中文翻译】previous

【中文翻译】autogen_agentchat.state

【中文翻译】next

【中文翻译】autogen_core.code_executor

### autogen_core {autogen_core}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html)

```python
class Agent(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


property metadata: AgentMetadata#
Metadata of the agent.



property id: AgentId#
ID of the agent.



async on_message(message: Any, ctx: MessageContext) → Any[source]#
Message handler for the agent. This should only be called by the runtime, not by other agents.

Parameters:

message (Any) – Received message. Type is one of the types in subscriptions.
ctx (MessageContext) – Context of the message.


Returns:
Any – Response to the message. Can be None.

Raises:

CancelledError – If the message was cancelled.
CantHandleException – If the agent cannot handle the message.






async save_state() → Mapping[str, Any][source]#
Save the state of the agent. The result must be JSON serializable.



async load_state(state: Mapping[str, Any]) → None[source]#
Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.





async close() → None[source]#
Called when the runtime is closed

```python
property metadata: AgentMetadata#
```

【中文翻译】Metadata of the agent.

```python
property id: AgentId#
```

【中文翻译】ID of the agent.

```python
async on_message(message: Any, ctx: MessageContext) → Any[source]#
```

【中文翻译】Message handler for the agent. This should only be called by the runtime, not by other agents.

Parameters:

message (Any) – Received message. Type is one of the types in subscriptions.
ctx (MessageContext) – Context of the message.


Returns:
Any – Response to the message. Can be None.

Raises:

CancelledError – If the message was cancelled.
CantHandleException – If the agent cannot handle the message.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the agent. The result must be JSON serializable.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.

```python
async close() → None[source]#
```

【中文翻译】Called when the runtime is closed

```python
class AgentId(type: str | AgentType, key: str)[source]#
```

【中文翻译】Bases: object
Agent ID uniquely identifies an agent instance within an agent runtime - including distributed runtime. It is the ‘address’ of the agent instance for receiving messages.
See here for more information: Agent Identity and Lifecycle


classmethod from_str(agent_id: str) → Self[source]#
Convert a string of the format type/key into an AgentId



property type: str#
An identifier that associates an agent with a specific factory function.
Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (_).



property key: str#
Agent instance identifier.
Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (_).

```python
classmethod from_str(agent_id: str) → Self[source]#
```

【中文翻译】Convert a string of the format type/key into an AgentId

```python
property type: str#
```

【中文翻译】An identifier that associates an agent with a specific factory function.
Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (_).

```python
property key: str#
```

【中文翻译】Agent instance identifier.
Strings may only be composed of alphanumeric letters (a-z) and (0-9), or underscores (_).

```python
class AgentProxy(agent: AgentId, runtime: AgentRuntime)[source]#
```

【中文翻译】Bases: object
A helper class that allows you to use an AgentId in place of its associated Agent


property id: AgentId#
Target agent for this proxy



property metadata: Awaitable[AgentMetadata]#
Metadata of the agent.



async send_message(message: Any, *, sender: AgentId, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#



async save_state() → Mapping[str, Any][source]#
Save the state of the agent. The result must be JSON serializable.



async load_state(state: Mapping[str, Any]) → None[source]#
Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.

```python
property id: AgentId#
```

【中文翻译】Target agent for this proxy

```python
property metadata: Awaitable[AgentMetadata]#
```

【中文翻译】Metadata of the agent.

```python
async send_message(message: Any, *, sender: AgentId, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the agent. The result must be JSON serializable.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.

```python
class AgentMetadata[source]#
```

【中文翻译】Bases: TypedDict


type: str#



key: str#



description: str#

```python
type: str#
```

```python
key: str#
```

```python
description: str#
```

```python
class AgentRuntime(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.





async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.





async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.






async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.






async get(id: AgentId, /, *, lazy: bool = True) → AgentId[source]#

async get(type: AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId



async save_state() → Mapping[str, Any][source]#
Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to load_state().
The structure of the state is implementation defined and can be any JSON serializable object.

Returns:
Mapping[str, Any] – The saved state.





async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by save_state().

Parameters:
state (Mapping[str, Any]) – The saved state.





async agent_metadata(agent: AgentId) → AgentMetadata[source]#
Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.





async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.





async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.






async add_subscription(subscription: Subscription) → None[source]#
Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add





async remove_subscription(id: str) → None[source]#
Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist





add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

【中文翻译】Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.

```python
async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
```

【中文翻译】Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.

```python
async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
```

【中文翻译】Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
```

【中文翻译】Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.

```python
async get(id: AgentId, /, *, lazy: bool = True) → AgentId[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to load_state().
The structure of the state is implementation defined and can be any JSON serializable object.

Returns:
Mapping[str, Any] – The saved state.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by save_state().

Parameters:
state (Mapping[str, Any]) – The saved state.

```python
async agent_metadata(agent: AgentId) → AgentMetadata[source]#
```

【中文翻译】Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.

```python
async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
```

【中文翻译】Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.

```python
async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.

```python
async add_subscription(subscription: Subscription) → None[source]#
```

【中文翻译】Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add

```python
async remove_subscription(id: str) → None[source]#
```

【中文翻译】Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist

```python
add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
```

【中文翻译】Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

```python
class BaseAgent(description: str)[source]#
```

【中文翻译】Bases: ABC, Agent


property metadata: AgentMetadata#
Metadata of the agent.



property type: str#



property id: AgentId#
ID of the agent.



property runtime: AgentRuntime#



final async on_message(message: Any, ctx: MessageContext) → Any[source]#
Message handler for the agent. This should only be called by the runtime, not by other agents.

Parameters:

message (Any) – Received message. Type is one of the types in subscriptions.
ctx (MessageContext) – Context of the message.


Returns:
Any – Response to the message. Can be None.

Raises:

CancelledError – If the message was cancelled.
CantHandleException – If the agent cannot handle the message.






abstract async on_message_impl(message: Any, ctx: MessageContext) → Any[source]#



async send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
See autogen_core.AgentRuntime.send_message() for more information.



async publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) → None[source]#



async save_state() → Mapping[str, Any][source]#
Save the state of the agent. The result must be JSON serializable.



async load_state(state: Mapping[str, Any]) → None[source]#
Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.





async close() → None[source]#
Called when the runtime is closed



async classmethod register(runtime: AgentRuntime, type: str, factory: Callable[[], Self | Awaitable[Self]], *, skip_class_subscriptions: bool = False, skip_direct_message_subscription: bool = False) → AgentType[source]#
Register a virtual subclass of an ABC.
Returns the subclass, to allow usage as a class decorator.

```python
property metadata: AgentMetadata#
```

【中文翻译】Metadata of the agent.

```python
property type: str#
```

```python
property id: AgentId#
```

【中文翻译】ID of the agent.

```python
property runtime: AgentRuntime#
```

```python
final async on_message(message: Any, ctx: MessageContext) → Any[source]#
```

【中文翻译】Message handler for the agent. This should only be called by the runtime, not by other agents.

Parameters:

message (Any) – Received message. Type is one of the types in subscriptions.
ctx (MessageContext) – Context of the message.


Returns:
Any – Response to the message. Can be None.

Raises:

CancelledError – If the message was cancelled.
CantHandleException – If the agent cannot handle the message.

```python
abstract async on_message_impl(message: Any, ctx: MessageContext) → Any[source]#
```

```python
async send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

【中文翻译】See autogen_core.AgentRuntime.send_message() for more information.

```python
async publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) → None[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the agent. The result must be JSON serializable.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load in the state of the agent obtained from save_state.

Parameters:
state (Mapping[str, Any]) – State of the agent. Must be JSON serializable.

```python
async close() → None[source]#
```

【中文翻译】Called when the runtime is closed

```python
async classmethod register(runtime: AgentRuntime, type: str, factory: Callable[[], Self | Awaitable[Self]], *, skip_class_subscriptions: bool = False, skip_direct_message_subscription: bool = False) → AgentType[source]#
```

【中文翻译】Register a virtual subclass of an ABC.
Returns the subclass, to allow usage as a class decorator.

```python
class CacheStore[source]#
```

【中文翻译】Bases: ABC, Generic[T], ComponentBase[BaseModel]
This protocol defines the basic interface for store/cache operations.
Sub-classes should handle the lifecycle of underlying storage.


component_type: ClassVar[ComponentType] = 'cache_store'#
The logical type of the component.



abstract get(key: str, default: T | None = None) → T | None[source]#
Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.





abstract set(key: str, value: T) → None[source]#
Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
component_type: ClassVar[ComponentType] = 'cache_store'#
```

【中文翻译】The logical type of the component.

```python
abstract get(key: str, default: T | None = None) → T | None[source]#
```

【中文翻译】Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.

```python
abstract set(key: str, value: T) → None[source]#
```

【中文翻译】Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
class InMemoryStore[source]#
```

【中文翻译】Bases: CacheStore[T], Component[InMemoryStoreConfig]


component_provider_override: ClassVar[str | None] = 'autogen_core.InMemoryStore'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_config_schema#
alias of InMemoryStoreConfig



get(key: str, default: T | None = None) → T | None[source]#
Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.





set(key: str, value: T) → None[source]#
Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.






_to_config() → InMemoryStoreConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





classmethod _from_config(config: InMemoryStoreConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.InMemoryStore'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_config_schema#
```

【中文翻译】alias of InMemoryStoreConfig

```python
get(key: str, default: T | None = None) → T | None[source]#
```

【中文翻译】Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.

```python
set(key: str, value: T) → None[source]#
```

【中文翻译】Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
_to_config() → InMemoryStoreConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
classmethod _from_config(config: InMemoryStoreConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
class CancellationToken[source]#
```

【中文翻译】Bases: object
A token used to cancel pending async calls


cancel() → None[source]#
Cancel pending async calls linked to this cancellation token.



is_cancelled() → bool[source]#
Check if the CancellationToken has been used



add_callback(callback: Callable[[], None]) → None[source]#
Attach a callback that will be called when cancel is invoked



link_future(future: Future[Any]) → Future[Any][source]#
Link a pending async call to a token to allow its cancellation

```python
cancel() → None[source]#
```

【中文翻译】Cancel pending async calls linked to this cancellation token.

```python
is_cancelled() → bool[source]#
```

【中文翻译】Check if the CancellationToken has been used

```python
add_callback(callback: Callable[[], None]) → None[source]#
```

【中文翻译】Attach a callback that will be called when cancel is invoked

```python
link_future(future: Future[Any]) → Future[Any][source]#
```

【中文翻译】Link a pending async call to a token to allow its cancellation

```python
class AgentInstantiationContext[source]#
```

【中文翻译】Bases: object
A static class that provides context for agent instantiation.
This static class can be used to access the current runtime and agent ID
during agent instantiation – inside the factory function or the agent’s
class constructor.
Example
Get the current runtime and agent ID inside the factory function and
the agent’s constructor:
import asyncio
from dataclasses import dataclass

from autogen_core import (
    AgentId,
    AgentInstantiationContext,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    message_handler,
)


@dataclass
class TestMessage:
    content: str


class TestAgent(RoutedAgent):
    def __init__(self, description: str):
        super().__init__(description)
        # Get the current runtime -- we don't use it here, but it's available.
        _ = AgentInstantiationContext.current_runtime()
        # Get the current agent ID.
        agent_id = AgentInstantiationContext.current_agent_id()
        print(f"Current AgentID from constructor: {agent_id}")

    @message_handler
    async def handle_test_message(self, message: TestMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


def test_agent_factory() -> TestAgent:
    # Get the current runtime -- we don't use it here, but it's available.
    _ = AgentInstantiationContext.current_runtime()
    # Get the current agent ID.
    agent_id = AgentInstantiationContext.current_agent_id()
    print(f"Current AgentID from factory: {agent_id}")
    return TestAgent(description="Test agent")


async def main() -> None:
    # Create a SingleThreadedAgentRuntime instance.
    runtime = SingleThreadedAgentRuntime()

    # Start the runtime.
    runtime.start()

    # Register the agent type with a factory function.
    await runtime.register_factory("test_agent", test_agent_factory)

    # Send a message to the agent. The runtime will instantiate the agent and call the message handler.
    await runtime.send_message(TestMessage(content="Hello, world!"), AgentId("test_agent", "default"))

    # Stop the runtime.
    await runtime.stop()


asyncio.run(main())




classmethod current_runtime() → AgentRuntime[source]#



classmethod current_agent_id() → AgentId[source]#

**示例**:
```python
import asyncio
from dataclasses import dataclass

from autogen_core import (
    AgentId,
    AgentInstantiationContext,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    message_handler,
)


@dataclass
class TestMessage:
    content: str


class TestAgent(RoutedAgent):
    def __init__(self, description: str):
        super().__init__(description)
        # Get the current runtime -- we don't use it here, but it's available.
        _ = AgentInstantiationContext.current_runtime()
        # Get the current agent ID.
        agent_id = AgentInstantiationContext.current_agent_id()
        print(f"Current AgentID from constructor: {agent_id}")

    @message_handler
    async def handle_test_message(self, message: TestMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


def test_agent_factory() -> TestAgent:
    # Get the current runtime -- we don't use it here, but it's available.
    _ = AgentInstantiationContext.current_runtime()
    # Get the current agent ID.
    agent_id = AgentInstantiationContext.current_agent_id()
    print(f"Current AgentID from factory: {agent_id}")
    return TestAgent(description="Test agent")


async def main() -> None:
    # Create a SingleThreadedAgentRuntime instance.
    runtime = SingleThreadedAgentRuntime()

    # Start the runtime.
    runtime.start()

    # Register the agent type with a factory function.
    await runtime.register_factory("test_agent", test_agent_factory)

    # Send a message to the agent. The runtime will instantiate the agent and call the message handler.
    await runtime.send_message(TestMessage(content="Hello, world!"), AgentId("test_agent", "default"))

    # Stop the runtime.
    await runtime.stop()


asyncio.run(main())

```

```python
classmethod current_runtime() → AgentRuntime[source]#
```

```python
classmethod current_agent_id() → AgentId[source]#
```

```python
class TopicId(type: str, source: str)[source]#
```

【中文翻译】Bases: object
TopicId defines the scope of a broadcast message. In essence, agent runtime implements a publish-subscribe model through its broadcast API: when publishing a message, the topic must be specified.
See here for more information: Topic


type: str#
Type of the event that this topic_id contains. Adhere’s to the cloud event spec.
Must match the pattern: ^[w-.:=]+Z
Learn more here: cloudevents/spec



source: str#
Identifies the context in which an event happened. Adhere’s to the cloud event spec.
Learn more here: cloudevents/spec



classmethod from_str(topic_id: str) → Self[source]#
Convert a string of the format type/source into a TopicId

```python
type: str#
```

【中文翻译】Type of the event that this topic_id contains. Adhere’s to the cloud event spec.
Must match the pattern: ^[w-.:=]+Z
Learn more here: cloudevents/spec

```python
source: str#
```

【中文翻译】Identifies the context in which an event happened. Adhere’s to the cloud event spec.
Learn more here: cloudevents/spec

```python
classmethod from_str(topic_id: str) → Self[source]#
```

【中文翻译】Convert a string of the format type/source into a TopicId

```python
class Subscription(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol
Subscriptions define the topics that an agent is interested in.


property id: str#
Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.





is_match(topic_id: TopicId) → bool[source]#
Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.





map_to_agent(topic_id: TopicId) → AgentId[source]#
Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

```python
property id: str#
```

【中文翻译】Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.

```python
is_match(topic_id: TopicId) → bool[source]#
```

【中文翻译】Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.

```python
map_to_agent(topic_id: TopicId) → AgentId[source]#
```

【中文翻译】Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

```python
class MessageContext(sender: AgentId | None, topic_id: TopicId | None, is_rpc: bool, cancellation_token: CancellationToken, message_id: str)[source]#
```

【中文翻译】Bases: object


sender: AgentId | None#



topic_id: TopicId | None#



is_rpc: bool#



cancellation_token: CancellationToken#



message_id: str#

```python
sender: AgentId | None#
```

```python
topic_id: TopicId | None#
```

```python
is_rpc: bool#
```

```python
cancellation_token: CancellationToken#
```

```python
message_id: str#
```

```python
class AgentType(type: str)[source]#
```

【中文翻译】Bases: object


type: str#
String representation of this agent type.

```python
type: str#
```

【中文翻译】String representation of this agent type.

```python
class SubscriptionInstantiationContext[source]#
```

【中文翻译】Bases: object


classmethod agent_type() → AgentType[source]#

```python
classmethod agent_type() → AgentType[source]#
```

```python
class MessageHandlerContext[source]#
```

【中文翻译】Bases: object


classmethod agent_id() → AgentId[source]#

```python
classmethod agent_id() → AgentId[source]#
```

```python
class MessageSerializer(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol[T]


property data_content_type: str#



property type_name: str#



deserialize(payload: bytes) → T[source]#



serialize(message: T) → bytes[source]#

```python
property data_content_type: str#
```

```python
property type_name: str#
```

```python
deserialize(payload: bytes) → T[source]#
```

```python
serialize(message: T) → bytes[source]#
```

```python
class UnknownPayload(type_name: str, data_content_type: str, payload: bytes)[source]#
```

【中文翻译】Bases: object


type_name: str#



data_content_type: str#



payload: bytes#

```python
type_name: str#
```

```python
data_content_type: str#
```

```python
payload: bytes#
```

```python
class Image(image: Image)[source]#
```

【中文翻译】Bases: object
Represents an image.
Example
Loading an image from a URL:
from autogen_core import Image
from PIL import Image as PILImage
import aiohttp
import asyncio


async def from_url(url: str) -> Image:
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            content = await response.read()
            return Image.from_pil(PILImage.open(content))


image = asyncio.run(from_url("https://example.com/image"))




classmethod from_pil(pil_image: Image) → Image[source]#



classmethod from_uri(uri: str) → Image[source]#



classmethod from_base64(base64_str: str) → Image[source]#



to_base64() → str[source]#



classmethod from_file(file_path: Path) → Image[source]#



property data_uri: str#



to_openai_format(detail: Literal['auto', 'low', 'high'] = 'auto') → Dict[str, Any][source]#

**示例**:
```python
from autogen_core import Image
from PIL import Image as PILImage
import aiohttp
import asyncio


async def from_url(url: str) -> Image:
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            content = await response.read()
            return Image.from_pil(PILImage.open(content))


image = asyncio.run(from_url("https://example.com/image"))

```

```python
classmethod from_pil(pil_image: Image) → Image[source]#
```

```python
classmethod from_uri(uri: str) → Image[source]#
```

```python
classmethod from_base64(base64_str: str) → Image[source]#
```

```python
to_base64() → str[source]#
```

```python
classmethod from_file(file_path: Path) → Image[source]#
```

```python
property data_uri: str#
```

```python
to_openai_format(detail: Literal['auto', 'low', 'high'] = 'auto') → Dict[str, Any][source]#
```

```python
class RoutedAgent(description: str)[source]#
```

【中文翻译】Bases: BaseAgent
A base class for agents that route messages to handlers based on the type of the message
and optional matching functions.
To create a routed agent, subclass this class and add message handlers as methods decorated with
either event() or rpc() decorator.
Example:
from dataclasses import dataclass
from autogen_core import MessageContext
from autogen_core import RoutedAgent, event, rpc


@dataclass
class Message:
    pass


@dataclass
class MessageWithContent:
    content: str


@dataclass
class Response:
    pass


class MyAgent(RoutedAgent):
    def __init__(self):
        super().__init__("MyAgent")

    @event
    async def handle_event_message(self, message: Message, ctx: MessageContext) -> None:
        assert ctx.topic_id is not None
        await self.publish_message(MessageWithContent("event handled"), ctx.topic_id)

    @rpc(match=lambda message, ctx: message.content == "special")  # type: ignore
    async def handle_special_rpc_message(self, message: MessageWithContent, ctx: MessageContext) -> Response:
        return Response()




async on_message_impl(message: Any, ctx: MessageContext) → Any | None[source]#
Handle a message by routing it to the appropriate message handler.
Do not override this method in subclasses. Instead, add message handlers as methods decorated with
either the event() or rpc() decorator.



async on_unhandled_message(message: Any, ctx: MessageContext) → None[source]#
Called when a message is received that does not have a matching message handler.
The default implementation logs an info message.

**示例**:
```python
from dataclasses import dataclass
from autogen_core import MessageContext
from autogen_core import RoutedAgent, event, rpc


@dataclass
class Message:
    pass


@dataclass
class MessageWithContent:
    content: str


@dataclass
class Response:
    pass


class MyAgent(RoutedAgent):
    def __init__(self):
        super().__init__("MyAgent")

    @event
    async def handle_event_message(self, message: Message, ctx: MessageContext) -> None:
        assert ctx.topic_id is not None
        await self.publish_message(MessageWithContent("event handled"), ctx.topic_id)

    @rpc(match=lambda message, ctx: message.content == "special")  # type: ignore
    async def handle_special_rpc_message(self, message: MessageWithContent, ctx: MessageContext) -> Response:
        return Response()

```

```python
async on_message_impl(message: Any, ctx: MessageContext) → Any | None[source]#
```

【中文翻译】Handle a message by routing it to the appropriate message handler.
Do not override this method in subclasses. Instead, add message handlers as methods decorated with
either the event() or rpc() decorator.

```python
async on_unhandled_message(message: Any, ctx: MessageContext) → None[source]#
```

【中文翻译】Called when a message is received that does not have a matching message handler.
The default implementation logs an info message.

```python
class ClosureAgent(description: str, closure: Callable[[ClosureContext, T, MessageContext], Awaitable[Any]], *, unknown_type_policy: Literal['error', 'warn', 'ignore'] = 'warn')[source]#
```

【中文翻译】Bases: BaseAgent, ClosureContext


property metadata: AgentMetadata#
Metadata of the agent.



property id: AgentId#
ID of the agent.



property runtime: AgentRuntime#



async on_message_impl(message: Any, ctx: MessageContext) → Any[source]#



async save_state() → Mapping[str, Any][source]#
Closure agents do not have state. So this method always returns an empty dictionary.



async load_state(state: Mapping[str, Any]) → None[source]#
Closure agents do not have state. So this method does nothing.



async classmethod register_closure(runtime: AgentRuntime, type: str, closure: Callable[[ClosureContext, T, MessageContext], Awaitable[Any]], *, unknown_type_policy: Literal['error', 'warn', 'ignore'] = 'warn', skip_direct_message_subscription: bool = False, description: str = '', subscriptions: Callable[[], list[Subscription] | Awaitable[list[Subscription]]] | None = None) → AgentType[source]#
The closure agent allows you to define an agent using a closure, or function without needing to define a class. It allows values to be extracted out of the runtime.
The closure can define the type of message which is expected, or Any can be used to accept any type of message.
Example:
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId


@dataclass
class MyMessage:
    content: str


async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(
        runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
    )

    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

    result = await queue.get()
    print(result)


asyncio.run(main())



Parameters:

runtime (AgentRuntime) – Runtime to register the agent to
type (str) – Agent type of registered agent
closure (Callable[[ClosureContext, T, MessageContext], Awaitable[Any]]) – Closure to handle messages
unknown_type_policy (Literal["error", "warn", "ignore"], optional) – What to do if a type is encountered that does not match the closure type. Defaults to “warn”.
skip_direct_message_subscription (bool, optional) – Do not add direct message subscription for this agent. Defaults to False.
description (str, optional) – Description of what agent does. Defaults to “”.
subscriptions (Callable[[], list[Subscription]  |  Awaitable[list[Subscription]]] | None, optional) – List of subscriptions for this closure agent. Defaults to None.


Returns:
AgentType – Type of the agent that was registered

**示例**:
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId


@dataclass
class MyMessage:
    content: str


async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(
        runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
    )

    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

    result = await queue.get()
    print(result)


asyncio.run(main())

```

```python
property metadata: AgentMetadata#
```

【中文翻译】Metadata of the agent.

```python
property id: AgentId#
```

【中文翻译】ID of the agent.

```python
property runtime: AgentRuntime#
```

```python
async on_message_impl(message: Any, ctx: MessageContext) → Any[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Closure agents do not have state. So this method always returns an empty dictionary.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Closure agents do not have state. So this method does nothing.

```python
async classmethod register_closure(runtime: AgentRuntime, type: str, closure: Callable[[ClosureContext, T, MessageContext], Awaitable[Any]], *, unknown_type_policy: Literal['error', 'warn', 'ignore'] = 'warn', skip_direct_message_subscription: bool = False, description: str = '', subscriptions: Callable[[], list[Subscription] | Awaitable[list[Subscription]]] | None = None) → AgentType[source]#
```

【中文翻译】The closure agent allows you to define an agent using a closure, or function without needing to define a class. It allows values to be extracted out of the runtime.
The closure can define the type of message which is expected, or Any can be used to accept any type of message.
Example:
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId


@dataclass
class MyMessage:
    content: str


async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(
        runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
    )

    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

    result = await queue.get()
    print(result)


asyncio.run(main())



Parameters:

runtime (AgentRuntime) – Runtime to register the agent to
type (str) – Agent type of registered agent
closure (Callable[[ClosureContext, T, MessageContext], Awaitable[Any]]) – Closure to handle messages
unknown_type_policy (Literal["error", "warn", "ignore"], optional) – What to do if a type is encountered that does not match the closure type. Defaults to “warn”.
skip_direct_message_subscription (bool, optional) – Do not add direct message subscription for this agent. Defaults to False.
description (str, optional) – Description of what agent does. Defaults to “”.
subscriptions (Callable[[], list[Subscription]  |  Awaitable[list[Subscription]]] | None, optional) – List of subscriptions for this closure agent. Defaults to None.


Returns:
AgentType – Type of the agent that was registered

**示例**:
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime, MessageContext, ClosureAgent, ClosureContext
from dataclasses import dataclass

from autogen_core._default_subscription import DefaultSubscription
from autogen_core._default_topic import DefaultTopicId


@dataclass
class MyMessage:
    content: str


async def main():
    queue = asyncio.Queue[MyMessage]()

    async def output_result(_ctx: ClosureContext, message: MyMessage, ctx: MessageContext) -> None:
        await queue.put(message)

    runtime = SingleThreadedAgentRuntime()
    await ClosureAgent.register_closure(
        runtime, "output_result", output_result, subscriptions=lambda: [DefaultSubscription()]
    )

    runtime.start()
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    await runtime.stop_when_idle()

    result = await queue.get()
    print(result)


asyncio.run(main())

```

```python
class ClosureContext(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


property id: AgentId#



async send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#



async publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) → None[source]#

```python
property id: AgentId#
```

```python
async send_message(message: Any, recipient: AgentId, *, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

```python
async publish_message(message: Any, topic_id: TopicId, *, cancellation_token: CancellationToken | None = None) → None[source]#
```

```python
message_handler(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None) → Callable[[Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]]], MessageHandler[AgentT, ReceivesT, ProducesT]] | MessageHandler[AgentT, ReceivesT, ProducesT][source]#
```

【中文翻译】Decorator for generic message handlers.
Add this decorator to methods in a RoutedAgent class that are intended to handle both event and RPC messages.
These methods must have a specific signature that needs to be followed for it to be valid:

The method must be an async method.
The method must be decorated with the @message_handler decorator.

The method must have exactly 3 arguments:
self
message: The message to be handled, this must be type-hinted with the message type that it is intended to handle.
ctx: A autogen_core.MessageContext object.




The method must be type hinted with what message types it can return as a response, or it can return None if it does not return anything.

Handlers can handle more than one message type by accepting a Union of the message types. It can also return more than one message type by returning a Union of the message types.

Parameters:

func – The function to be decorated.
strict – If True, the handler will raise an exception if the message type or return type is not in the target types. If False, it will log a warning instead.
match – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

```python
event(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, None]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None) → Callable[[Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, None]]], MessageHandler[AgentT, ReceivesT, None]] | MessageHandler[AgentT, ReceivesT, None][source]#
```

【中文翻译】Decorator for event message handlers.
Add this decorator to methods in a RoutedAgent class that are intended to handle event messages.
These methods must have a specific signature that needs to be followed for it to be valid:

The method must be an async method.
The method must be decorated with the @message_handler decorator.

The method must have exactly 3 arguments:
self
message: The event message to be handled, this must be type-hinted with the message type that it is intended to handle.
ctx: A autogen_core.MessageContext object.




The method must return None.

Handlers can handle more than one message type by accepting a Union of the message types.

Parameters:

func – The function to be decorated.
strict – If True, the handler will raise an exception if the message type is not in the target types. If False, it will log a warning instead.
match – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

```python
rpc(func: None | Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]] = None, *, strict: bool = True, match: None | Callable[[ReceivesT, MessageContext], bool] = None) → Callable[[Callable[[AgentT, ReceivesT, MessageContext], Coroutine[Any, Any, ProducesT]]], MessageHandler[AgentT, ReceivesT, ProducesT]] | MessageHandler[AgentT, ReceivesT, ProducesT][source]#
```

【中文翻译】Decorator for RPC message handlers.
Add this decorator to methods in a RoutedAgent class that are intended to handle RPC messages.
These methods must have a specific signature that needs to be followed for it to be valid:

The method must be an async method.
The method must be decorated with the @message_handler decorator.

The method must have exactly 3 arguments:
self
message: The message to be handled, this must be type-hinted with the message type that it is intended to handle.
ctx: A autogen_core.MessageContext object.




The method must be type hinted with what message types it can return as a response, or it can return None if it does not return anything.

Handlers can handle more than one message type by accepting a Union of the message types. It can also return more than one message type by returning a Union of the message types.

Parameters:

func – The function to be decorated.
strict – If True, the handler will raise an exception if the message type or return type is not in the target types. If False, it will log a warning instead.
match – A function that takes the message and the context as arguments and returns a boolean. This is used for secondary routing after the message type. For handlers addressing the same message type, the match function is applied in alphabetical order of the handlers and the first matching handler will be called while the rest are skipped. If None, the first handler in alphabetical order matching the same message type will be called.

```python
class FunctionCall(id: 'str', arguments: 'str', name: 'str')[source]#
```

【中文翻译】Bases: object


id: str#



arguments: str#



name: str#

```python
id: str#
```

```python
arguments: str#
```

```python
name: str#
```

```python
class TypeSubscription(topic_type: str, agent_type: str | AgentType, id: str | None = None)[source]#
```

【中文翻译】Bases: Subscription
This subscription matches on topics based on the type and maps to agents using the source of the topic as the agent key.
This subscription causes each source to have its own agent instance.
Example
from autogen_core import TypeSubscription

subscription = TypeSubscription(topic_type="t1", agent_type="a1")


In this case:

A topic_id with type t1 and source s1 will be handled by an agent of type a1 with key s1
A topic_id with type t1 and source s2 will be handled by an agent of type a1 with key s2.


Parameters:

topic_type (str) – Topic type to match against
agent_type (str) – Agent type to handle this subscription





property id: str#
Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.





property topic_type: str#



property agent_type: str#



is_match(topic_id: TopicId) → bool[source]#
Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.





map_to_agent(topic_id: TopicId) → AgentId[source]#
Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

**示例**:
```python
from autogen_core import TypeSubscription

subscription = TypeSubscription(topic_type="t1", agent_type="a1")

```

```python
property id: str#
```

【中文翻译】Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.

```python
property topic_type: str#
```

```python
property agent_type: str#
```

```python
is_match(topic_id: TopicId) → bool[source]#
```

【中文翻译】Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.

```python
map_to_agent(topic_id: TopicId) → AgentId[source]#
```

【中文翻译】Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

```python
class DefaultSubscription(topic_type: str = 'default', agent_type: str | AgentType | None = None)[source]#
```

【中文翻译】Bases: TypeSubscription
The default subscription is designed to be a sensible default for applications that only need global scope for agents.
This topic by default uses the “default” topic type and attempts to detect the agent type to use based on the instantiation context.

Parameters:

topic_type (str, optional) – The topic type to subscribe to. Defaults to “default”.
agent_type (str, optional) – The agent type to use for the subscription. Defaults to None, in which case it will attempt to detect the agent type based on the instantiation context.

```python
class DefaultTopicId(type: str = 'default', source: str | None = None)[source]#
```

【中文翻译】Bases: TopicId
DefaultTopicId provides a sensible default for the topic_id and source fields of a TopicId.
If created in the context of a message handler, the source will be set to the agent_id of the message handler, otherwise it will be set to “default”.

Parameters:

type (str, optional) – Topic type to publish message to. Defaults to “default”.
source (str | None, optional) – Topic source to publish message to. If None, the source will be set to the agent_id of the message handler if in the context of a message handler, otherwise it will be set to “default”. Defaults to None.

```python
default_subscription(cls: Type[BaseAgentType] | None = None) → Callable[[Type[BaseAgentType]], Type[BaseAgentType]] | Type[BaseAgentType][source]#
```

```python
type_subscription(topic_type: str) → Callable[[Type[BaseAgentType]], Type[BaseAgentType]][source]#
```

```python
class TypePrefixSubscription(topic_type_prefix: str, agent_type: str | AgentType, id: str | None = None)[source]#
```

【中文翻译】Bases: Subscription
This subscription matches on topics based on a prefix of the type and maps to agents using the source of the topic as the agent key.
This subscription causes each source to have its own agent instance.
Example
from autogen_core import TypePrefixSubscription

subscription = TypePrefixSubscription(topic_type_prefix="t1", agent_type="a1")


In this case:

A topic_id with type t1 and source s1 will be handled by an agent of type a1 with key s1
A topic_id with type t1 and source s2 will be handled by an agent of type a1 with key s2.
A topic_id with type t1SUFFIX and source s2 will be handled by an agent of type a1 with key s2.


Parameters:

topic_type_prefix (str) – Topic type prefix to match against
agent_type (str) – Agent type to handle this subscription





property id: str#
Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.





property topic_type_prefix: str#



property agent_type: str#



is_match(topic_id: TopicId) → bool[source]#
Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.





map_to_agent(topic_id: TopicId) → AgentId[source]#
Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

**示例**:
```python
from autogen_core import TypePrefixSubscription

subscription = TypePrefixSubscription(topic_type_prefix="t1", agent_type="a1")

```

```python
property id: str#
```

【中文翻译】Get the ID of the subscription.
Implementations should return a unique ID for the subscription. Usually this is a UUID.

Returns:
str – ID of the subscription.

```python
property topic_type_prefix: str#
```

```python
property agent_type: str#
```

```python
is_match(topic_id: TopicId) → bool[source]#
```

【中文翻译】Check if a given topic_id matches the subscription.

Parameters:
topic_id (TopicId) – TopicId to check.

Returns:
bool – True if the topic_id matches the subscription, False otherwise.

```python
map_to_agent(topic_id: TopicId) → AgentId[source]#
```

【中文翻译】Map a topic_id to an agent. Should only be called if is_match returns True for the given topic_id.

Parameters:
topic_id (TopicId) – TopicId to map.

Returns:
AgentId – ID of the agent that should handle the topic_id.

Raises:
CantHandleException – If the subscription cannot handle the topic_id.

```python
JSON_DATA_CONTENT_TYPE = 'application/json'#
```

【中文翻译】The content type for JSON data.

```python
PROTOBUF_DATA_CONTENT_TYPE = 'application/x-protobuf'#
```

【中文翻译】The content type for Protobuf data.

```python
class SingleThreadedAgentRuntime(*, intervention_handlers: List[InterventionHandler] | None = None, tracer_provider: TracerProvider | None = None, ignore_unhandled_exceptions: bool = True)[source]#
```

【中文翻译】Bases: AgentRuntime
A single-threaded agent runtime that processes all messages using a single asyncio queue.
Messages are delivered in the order they are received, and the runtime processes
each message in a separate asyncio task concurrently.

Note
This runtime is suitable for development and standalone applications.
It is not suitable for high-throughput or high-concurrency scenarios.


Parameters:

intervention_handlers (List[InterventionHandler], optional) – A list of intervention
handlers that can intercept messages before they are sent or published. Defaults to None.
tracer_provider (TracerProvider, optional) – The tracer provider to use for tracing. Defaults to None.
ignore_unhandled_exceptions (bool, optional) – Whether to ignore unhandled exceptions in that occur in agent event handlers. Any background exceptions will be raised on the next call to process_next or from an awaited stop, stop_when_idle or stop_when. Note, this does not apply to RPC handlers. Defaults to True.



Examples
A simple example of creating a runtime, registering an agent, sending a message and stopping the runtime:
import asyncio
from dataclasses import dataclass

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


async def main() -> None:
    # Create a runtime and register the agent
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

    # Start the runtime, send a message and stop the runtime
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()


asyncio.run(main())


An example of creating a runtime, registering an agent, publishing a message and stopping the runtime:
import asyncio
from dataclasses import dataclass

from autogen_core import (
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    default_subscription,
    message_handler,
)


@dataclass
class MyMessage:
    content: str


# The agent is subscribed to the default topic.
@default_subscription
class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


async def main() -> None:
    # Create a runtime and register the agent
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

    # Start the runtime.
    runtime.start()
    # Publish a message to the default topic that the agent is subscribed to.
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    # Wait for the message to be processed and then stop the runtime.
    await runtime.stop_when_idle()


asyncio.run(main())




property unprocessed_messages_count: int#



async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.





async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.





async save_state() → Mapping[str, Any][source]#
Save the state of all instantiated agents.
This method calls the save_state() method on each agent and returns a dictionary
mapping agent IDs to their state.

Note
This method does not currently save the subscription state. We will add this in the future.


Returns:
A dictionary mapping agent IDs to their state.





async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of all instantiated agents.
This method calls the load_state() method on each agent with the state
provided in the dictionary. The keys of the dictionary are the agent IDs, and the values are the state
dictionaries returned by the save_state() method.

Note
This method does not currently load the subscription state. We will add this in the future.




async process_next() → None[source]#
Process the next message in the queue.
If there is an unhandled exception in the background task, it will be raised here. process_next cannot be called again after an unhandled exception is raised.



start() → None[source]#
Start the runtime message processing loop. This runs in a background task.
Example:
import asyncio
from autogen_core import SingleThreadedAgentRuntime


async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()

    # ... do other things ...

    await runtime.stop()


asyncio.run(main())





async close() → None[source]#
Calls stop() if applicable and the Agent.close() method on all instantiated agents



async stop() → None[source]#
Immediately stop the runtime message processing loop. The currently processing message will be completed, but all others following it will be discarded.



async stop_when_idle() → None[source]#
Stop the runtime message processing loop when there is
no outstanding message being processed or queued. This is the most common way to stop the runtime.



async stop_when(condition: Callable[[], bool]) → None[source]#
Stop the runtime message processing loop when the condition is met.

Caution
This method is not recommended to be used, and is here for legacy
reasons. It will spawn a busy loop to continually check the
condition. It is much more efficient to call stop_when_idle or
stop instead. If you need to stop the runtime based on a
condition, consider using a background task and asyncio.Event to
signal when the condition is met and the background task should call
stop.




async agent_metadata(agent: AgentId) → AgentMetadata[source]#
Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.





async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.





async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.






async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.






async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.






async add_subscription(subscription: Subscription) → None[source]#
Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add





async remove_subscription(id: str) → None[source]#
Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist





async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId[source]#



add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

**示例**:
```python
import asyncio
from dataclasses import dataclass

from autogen_core import AgentId, MessageContext, RoutedAgent, SingleThreadedAgentRuntime, message_handler


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


async def main() -> None:
    # Create a runtime and register the agent
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

    # Start the runtime, send a message and stop the runtime
    runtime.start()
    await runtime.send_message(MyMessage("Hello, world!"), recipient=AgentId("my_agent", "default"))
    await runtime.stop()


asyncio.run(main())

```

**示例**:
```python
import asyncio
from dataclasses import dataclass

from autogen_core import (
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    default_subscription,
    message_handler,
)


@dataclass
class MyMessage:
    content: str


# The agent is subscribed to the default topic.
@default_subscription
class MyAgent(RoutedAgent):
    @message_handler
    async def handle_my_message(self, message: MyMessage, ctx: MessageContext) -> None:
        print(f"Received message: {message.content}")


async def main() -> None:
    # Create a runtime and register the agent
    runtime = SingleThreadedAgentRuntime()
    await MyAgent.register(runtime, "my_agent", lambda: MyAgent("My agent"))

    # Start the runtime.
    runtime.start()
    # Publish a message to the default topic that the agent is subscribed to.
    await runtime.publish_message(MyMessage("Hello, world!"), DefaultTopicId())
    # Wait for the message to be processed and then stop the runtime.
    await runtime.stop_when_idle()


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime


async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()

    # ... do other things ...

    await runtime.stop()


asyncio.run(main())

```

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
property unprocessed_messages_count: int#
```

```python
async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

【中文翻译】Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.

```python
async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
```

【中文翻译】Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of all instantiated agents.
This method calls the save_state() method on each agent and returns a dictionary
mapping agent IDs to their state.

Note
This method does not currently save the subscription state. We will add this in the future.


Returns:
A dictionary mapping agent IDs to their state.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of all instantiated agents.
This method calls the load_state() method on each agent with the state
provided in the dictionary. The keys of the dictionary are the agent IDs, and the values are the state
dictionaries returned by the save_state() method.

Note
This method does not currently load the subscription state. We will add this in the future.

```python
async process_next() → None[source]#
```

【中文翻译】Process the next message in the queue.
If there is an unhandled exception in the background task, it will be raised here. process_next cannot be called again after an unhandled exception is raised.

```python
start() → None[source]#
```

【中文翻译】Start the runtime message processing loop. This runs in a background task.
Example:
import asyncio
from autogen_core import SingleThreadedAgentRuntime


async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()

    # ... do other things ...

    await runtime.stop()


asyncio.run(main())

**示例**:
```python
import asyncio
from autogen_core import SingleThreadedAgentRuntime


async def main() -> None:
    runtime = SingleThreadedAgentRuntime()
    runtime.start()

    # ... do other things ...

    await runtime.stop()


asyncio.run(main())

```

```python
async close() → None[source]#
```

【中文翻译】Calls stop() if applicable and the Agent.close() method on all instantiated agents

```python
async stop() → None[source]#
```

【中文翻译】Immediately stop the runtime message processing loop. The currently processing message will be completed, but all others following it will be discarded.

```python
async stop_when_idle() → None[source]#
```

【中文翻译】Stop the runtime message processing loop when there is
no outstanding message being processed or queued. This is the most common way to stop the runtime.

```python
async stop_when(condition: Callable[[], bool]) → None[source]#
```

【中文翻译】Stop the runtime message processing loop when the condition is met.

Caution
This method is not recommended to be used, and is here for legacy
reasons. It will spawn a busy loop to continually check the
condition. It is much more efficient to call stop_when_idle or
stop instead. If you need to stop the runtime based on a
condition, consider using a background task and asyncio.Event to
signal when the condition is met and the background task should call
stop.

```python
async agent_metadata(agent: AgentId) → AgentMetadata[source]#
```

【中文翻译】Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.

```python
async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
```

【中文翻译】Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.

```python
async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.

```python
async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
```

【中文翻译】Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
```

【中文翻译】Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.

```python
async add_subscription(subscription: Subscription) → None[source]#
```

【中文翻译】Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add

```python
async remove_subscription(id: str) → None[source]#
```

【中文翻译】Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist

```python
async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId[source]#
```

```python
add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
```

【中文翻译】Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

```python
ROOT_LOGGER_NAME = 'autogen_core'#
```

【中文翻译】The name of the root logger.

```python
EVENT_LOGGER_NAME = 'autogen_core.events'#
```

【中文翻译】The name of the logger used for structured events.

```python
TRACE_LOGGER_NAME = 'autogen_core.trace'#
```

【中文翻译】Logger name used for developer intended trace logging. The content and format of this log should not be depended upon.

```python
class Component[source]#
```

【中文翻译】Bases: ComponentFromConfig[ConfigT], ComponentSchemaType[ConfigT], Generic[ConfigT]
To create a component class, inherit from this class for the concrete class and ComponentBase on the interface. Then implement two class variables:

component_config_schema - A Pydantic model class which represents the configuration of the component. This is also the type parameter of Component.
component_type - What is the logical type of the component.

Example:
from __future__ import annotations

from pydantic import BaseModel
from autogen_core import Component


class Config(BaseModel):
    value: str


class MyComponent(Component[Config]):
    component_type = "custom"
    component_config_schema = Config

    def __init__(self, value: str):
        self.value = value

    def _to_config(self) -> Config:
        return Config(value=self.value)

    @classmethod
    def _from_config(cls, config: Config) -> MyComponent:
        return cls(value=config.value)

**示例**:
```python
from __future__ import annotations

from pydantic import BaseModel
from autogen_core import Component


class Config(BaseModel):
    value: str


class MyComponent(Component[Config]):
    component_type = "custom"
    component_config_schema = Config

    def __init__(self, value: str):
        self.value = value

    def _to_config(self) -> Config:
        return Config(value=self.value)

    @classmethod
    def _from_config(cls, config: Config) -> MyComponent:
        return cls(value=config.value)

```

```python
class ComponentBase[source]#
```

【中文翻译】Bases: ComponentToConfig[ConfigT], ComponentLoader, Generic[ConfigT]

```python
class ComponentFromConfig[source]#
```

【中文翻译】Bases: Generic[FromConfigT]


classmethod _from_config(config: FromConfigT) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





classmethod _from_config_past_version(config: Dict[str, Any], version: int) → Self[source]#
Create a new instance of the component from a previous version of the configuration object.
This is only called when the version of the configuration object is less than the current version, since in this case the schema is not known.

Parameters:

config (Dict[str, Any]) – The configuration object.
version (int) – The version of the configuration object.


Returns:
Self – The new instance of the component.

```python
classmethod _from_config(config: FromConfigT) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
classmethod _from_config_past_version(config: Dict[str, Any], version: int) → Self[source]#
```

【中文翻译】Create a new instance of the component from a previous version of the configuration object.
This is only called when the version of the configuration object is less than the current version, since in this case the schema is not known.

Parameters:

config (Dict[str, Any]) – The configuration object.
version (int) – The version of the configuration object.


Returns:
Self – The new instance of the component.

```python
class ComponentLoader[source]#
```

【中文翻译】Bases: object


classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → Self[source]#

classmethod load_component(model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType]) → ExpectedType
Load a component from a model. Intended to be used with the return type of autogen_core.ComponentConfig.dump_component().
Example
from autogen_core import ComponentModel
from autogen_core.models import ChatCompletionClient

component: ComponentModel = ...  # type: ignore

model_client = ChatCompletionClient.load_component(component)



Parameters:

model (ComponentModel) – The model to load the component from.
model – _description_
expected (Type[ExpectedType] | None, optional) – Explicit type only if used directly on ComponentLoader. Defaults to None.


Returns:
Self – The loaded component.

Raises:

ValueError – If the provider string is invalid.
TypeError – Provider is not a subclass of ComponentConfigImpl, or the expected type does not match.


Returns:
Self | ExpectedType – The loaded component.

**示例**:
```python
from autogen_core import ComponentModel
from autogen_core.models import ChatCompletionClient

component: ComponentModel = ...  # type: ignore

model_client = ChatCompletionClient.load_component(component)

```

```python
classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → Self[source]#
```

【中文翻译】Load a component from a model. Intended to be used with the return type of autogen_core.ComponentConfig.dump_component().
Example
from autogen_core import ComponentModel
from autogen_core.models import ChatCompletionClient

component: ComponentModel = ...  # type: ignore

model_client = ChatCompletionClient.load_component(component)



Parameters:

model (ComponentModel) – The model to load the component from.
model – _description_
expected (Type[ExpectedType] | None, optional) – Explicit type only if used directly on ComponentLoader. Defaults to None.


Returns:
Self – The loaded component.

Raises:

ValueError – If the provider string is invalid.
TypeError – Provider is not a subclass of ComponentConfigImpl, or the expected type does not match.


Returns:
Self | ExpectedType – The loaded component.

**示例**:
```python
from autogen_core import ComponentModel
from autogen_core.models import ChatCompletionClient

component: ComponentModel = ...  # type: ignore

model_client = ChatCompletionClient.load_component(component)

```

```python
pydantic model ComponentModel[source]#
```

【中文翻译】Bases: BaseModel
Model class for a component. Contains all information required to instantiate a component.

Show JSON schema{
   "title": "ComponentModel",
   "description": "Model class for a component. Contains all information required to instantiate a component.",
   "type": "object",
   "properties": {
      "provider": {
         "title": "Provider",
         "type": "string"
      },
      "component_type": {
         "anyOf": [
            {
               "enum": [
                  "model",
                  "agent",
                  "tool",
                  "termination",
                  "token_provider",
                  "workbench"
               ],
               "type": "string"
            },
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Component Type"
      },
      "version": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Version"
      },
      "component_version": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Component Version"
      },
      "description": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Description"
      },
      "label": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Label"
      },
      "config": {
         "title": "Config",
         "type": "object"
      }
   },
   "required": [
      "provider",
      "config"
   ]
}



Fields:

component_type (Literal['model', 'agent', 'tool', 'termination', 'token_provider', 'workbench'] | str | None)
component_version (int | None)
config (dict[str, Any])
description (str | None)
label (str | None)
provider (str)
version (int | None)





field provider: str [Required]#
Describes how the component can be instantiated.



field component_type: ComponentType | None = None#
Logical type of the component. If missing, the component assumes the default type of the provider.



field version: int | None = None#
Version of the component specification. If missing, the component assumes whatever is the current version of the library used to load it. This is obviously dangerous and should be used for user authored ephmeral config. For all other configs version should be specified.



field component_version: int | None = None#
Version of the component. If missing, the component assumes the default version of the provider.



field description: str | None = None#
Description of the component.



field label: str | None = None#
Human readable label for the component. If missing the component assumes the class name of the provider.



field config: dict[str, Any] [Required]#
The schema validated config field is passed to a given class’s implmentation of autogen_core.ComponentConfigImpl._from_config() to create a new instance of the component class.

**示例**:
```python
{
   "title": "ComponentModel",
   "description": "Model class for a component. Contains all information required to instantiate a component.",
   "type": "object",
   "properties": {
      "provider": {
         "title": "Provider",
         "type": "string"
      },
      "component_type": {
         "anyOf": [
            {
               "enum": [
                  "model",
                  "agent",
                  "tool",
                  "termination",
                  "token_provider",
                  "workbench"
               ],
               "type": "string"
            },
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Component Type"
      },
      "version": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Version"
      },
      "component_version": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Component Version"
      },
      "description": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Description"
      },
      "label": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Label"
      },
      "config": {
         "title": "Config",
         "type": "object"
      }
   },
   "required": [
      "provider",
      "config"
   ]
}

```

```python
field provider: str [Required]#
```

【中文翻译】Describes how the component can be instantiated.

```python
field component_type: ComponentType | None = None#
```

【中文翻译】Logical type of the component. If missing, the component assumes the default type of the provider.

```python
field version: int | None = None#
```

【中文翻译】Version of the component specification. If missing, the component assumes whatever is the current version of the library used to load it. This is obviously dangerous and should be used for user authored ephmeral config. For all other configs version should be specified.

```python
field component_version: int | None = None#
```

【中文翻译】Version of the component. If missing, the component assumes the default version of the provider.

```python
field description: str | None = None#
```

【中文翻译】Description of the component.

```python
field label: str | None = None#
```

【中文翻译】Human readable label for the component. If missing the component assumes the class name of the provider.

```python
field config: dict[str, Any] [Required]#
```

【中文翻译】The schema validated config field is passed to a given class’s implmentation of autogen_core.ComponentConfigImpl._from_config() to create a new instance of the component class.

```python
class ComponentSchemaType[source]#
```

【中文翻译】Bases: Generic[ConfigT]


component_config_schema: Type[ConfigT]#
The Pydantic model class which represents the configuration of the component.



required_class_vars = ['component_config_schema', 'component_type']#

```python
component_config_schema: Type[ConfigT]#
```

【中文翻译】The Pydantic model class which represents the configuration of the component.

```python
required_class_vars = ['component_config_schema', 'component_type']#
```

```python
class ComponentToConfig[source]#
```

【中文翻译】Bases: Generic[ToConfigT]
The two methods a class must implement to be a component.

Parameters:
Protocol (ConfigT) – Type which derives from pydantic.BaseModel.




component_type: ClassVar[Literal['model', 'agent', 'tool', 'termination', 'token_provider', 'workbench'] | str]#
The logical type of the component.



component_version: ClassVar[int] = 1#
The version of the component, if schema incompatibilities are introduced this should be updated.



component_provider_override: ClassVar[str | None] = None#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_description: ClassVar[str | None] = None#
A description of the component. If not provided, the docstring of the class will be used.



component_label: ClassVar[str | None] = None#
A human readable label for the component. If not provided, the component class name will be used.



_to_config() → ToConfigT[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





dump_component() → ComponentModel[source]#
Dump the component to a model that can be loaded back in.

Raises:
TypeError – If the component is a local class.

Returns:
ComponentModel – The model representing the component.

```python
component_type: ClassVar[Literal['model', 'agent', 'tool', 'termination', 'token_provider', 'workbench'] | str]#
```

【中文翻译】The logical type of the component.

```python
component_version: ClassVar[int] = 1#
```

【中文翻译】The version of the component, if schema incompatibilities are introduced this should be updated.

```python
component_provider_override: ClassVar[str | None] = None#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_description: ClassVar[str | None] = None#
```

【中文翻译】A description of the component. If not provided, the docstring of the class will be used.

```python
component_label: ClassVar[str | None] = None#
```

【中文翻译】A human readable label for the component. If not provided, the component class name will be used.

```python
_to_config() → ToConfigT[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
dump_component() → ComponentModel[source]#
```

【中文翻译】Dump the component to a model that can be loaded back in.

Raises:
TypeError – If the component is a local class.

Returns:
ComponentModel – The model representing the component.

```python
is_component_class(cls: type) → TypeGuard[Type[_ConcreteComponent[BaseModel]]][source]#
```

```python
is_component_instance(cls: Any) → TypeGuard[_ConcreteComponent[BaseModel]][source]#
```

```python
final class DropMessage[source]#
```

【中文翻译】Bases: object
Marker type for signalling that a message should be dropped by an intervention handler. The type itself should be returned from the handler.

```python
class InterventionHandler(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol
An intervention handler is a class that can be used to modify, log or drop messages that are being processed by the autogen_core.base.AgentRuntime.
The handler is called when the message is submitted to the runtime.
Currently the only runtime which supports this is the autogen_core.base.SingleThreadedAgentRuntime.
Note: Returning None from any of the intervention handler methods will result in a warning being issued and treated as “no change”. If you intend to drop a message, you should return DropMessage explicitly.
Example:
from autogen_core import DefaultInterventionHandler, MessageContext, AgentId, SingleThreadedAgentRuntime
from dataclasses import dataclass
from typing import Any


@dataclass
class MyMessage:
    content: str


class MyInterventionHandler(DefaultInterventionHandler):
    async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> MyMessage:
        if isinstance(message, MyMessage):
            message.content = message.content.upper()
        return message


runtime = SingleThreadedAgentRuntime(intervention_handlers=[MyInterventionHandler()])




async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) → Any | type[DropMessage][source]#
Called when a message is submitted to the AgentRuntime using autogen_core.base.AgentRuntime.send_message().



async on_publish(message: Any, *, message_context: MessageContext) → Any | type[DropMessage][source]#
Called when a message is published to the AgentRuntime using autogen_core.base.AgentRuntime.publish_message().



async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) → Any | type[DropMessage][source]#
Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

**示例**:
```python
from autogen_core import DefaultInterventionHandler, MessageContext, AgentId, SingleThreadedAgentRuntime
from dataclasses import dataclass
from typing import Any


@dataclass
class MyMessage:
    content: str


class MyInterventionHandler(DefaultInterventionHandler):
    async def on_send(self, message: Any, *, message_context: MessageContext, recipient: AgentId) -> MyMessage:
        if isinstance(message, MyMessage):
            message.content = message.content.upper()
        return message


runtime = SingleThreadedAgentRuntime(intervention_handlers=[MyInterventionHandler()])

```

```python
async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a message is submitted to the AgentRuntime using autogen_core.base.AgentRuntime.send_message().

```python
async on_publish(message: Any, *, message_context: MessageContext) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a message is published to the AgentRuntime using autogen_core.base.AgentRuntime.publish_message().

```python
async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

```python
class DefaultInterventionHandler(*args, **kwargs)[source]#
```

【中文翻译】Bases: InterventionHandler
Simple class that provides a default implementation for all intervention
handler methods, that simply returns the message unchanged. Allows for easy
subclassing to override only the desired methods.


async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) → Any | type[DropMessage][source]#
Called when a message is submitted to the AgentRuntime using autogen_core.base.AgentRuntime.send_message().



async on_publish(message: Any, *, message_context: MessageContext) → Any | type[DropMessage][source]#
Called when a message is published to the AgentRuntime using autogen_core.base.AgentRuntime.publish_message().



async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) → Any | type[DropMessage][source]#
Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

```python
async on_send(message: Any, *, message_context: MessageContext, recipient: AgentId) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a message is submitted to the AgentRuntime using autogen_core.base.AgentRuntime.send_message().

```python
async on_publish(message: Any, *, message_context: MessageContext) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a message is published to the AgentRuntime using autogen_core.base.AgentRuntime.publish_message().

```python
async on_response(message: Any, *, sender: AgentId, recipient: AgentId | None) → Any | type[DropMessage][source]#
```

【中文翻译】Called when a response is received by the AgentRuntime from an Agent’s message handler returning a value.

```python
ComponentType#
```

【中文翻译】alias of Literal[‘model’, ‘agent’, ‘tool’, ‘termination’, ‘token_provider’, ‘workbench’] | str

【中文翻译】previous

【中文翻译】autogen_agentchat.state

【中文翻译】next

【中文翻译】autogen_core.code_executor

### autogen_core.code_executor {autogen_corecode_executor}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html)

```python
class Alias(name: 'str', alias: 'str')[source]#
```

【中文翻译】Bases: object


alias: str#



name: str#

```python
alias: str#
```

```python
name: str#
```

```python
class CodeBlock(code: str, language: str)[source]#
```

【中文翻译】Bases: object
A code block extracted fromm an agent message.


code: str#



language: str#

```python
code: str#
```

```python
language: str#
```

```python
class CodeExecutor[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
Executes code blocks and returns the result.
This is an abstract base class for code executors. It defines the interface
for executing code blocks and returning the result. A concrete implementation
of this class should be provided to execute code blocks in a specific
environment. For example, DockerCommandLineCodeExecutor executes
code blocks in a command line environment in a Docker container.
It is recommended for subclass to be used as a context manager to ensure
that resources are cleaned up properly. To do this, implement the
start() and
stop() methods
that will be called when entering and exiting the context manager.


component_type: ClassVar[ComponentType] = 'code_executor'#
The logical type of the component.



abstract async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
Execute code blocks and return the result.
This method should be implemented by the code executor.

Parameters:
code_blocks (List[CodeBlock]) – The code blocks to execute.

Returns:
CodeResult – The result of the code execution.

Raises:

ValueError – Errors in user inputs
TimeoutError – Code execution timeouts
CancelledError – CancellationToken evoked during execution






abstract async restart() → None[source]#
Restart the code executor.
This method should be implemented by the code executor.
This method is called when the agent is reset.



abstract async start() → None[source]#
Start the code executor.



abstract async stop() → None[source]#
Stop the code executor and release any resources.

```python
component_type: ClassVar[ComponentType] = 'code_executor'#
```

【中文翻译】The logical type of the component.

```python
abstract async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
```

【中文翻译】Execute code blocks and return the result.
This method should be implemented by the code executor.

Parameters:
code_blocks (List[CodeBlock]) – The code blocks to execute.

Returns:
CodeResult – The result of the code execution.

Raises:

ValueError – Errors in user inputs
TimeoutError – Code execution timeouts
CancelledError – CancellationToken evoked during execution

```python
abstract async restart() → None[source]#
```

【中文翻译】Restart the code executor.
This method should be implemented by the code executor.
This method is called when the agent is reset.

```python
abstract async start() → None[source]#
```

【中文翻译】Start the code executor.

```python
abstract async stop() → None[source]#
```

【中文翻译】Stop the code executor and release any resources.

```python
class CodeResult(exit_code: int, output: str)[source]#
```

【中文翻译】Bases: object
Result of a code execution.


exit_code: int#



output: str#

```python
exit_code: int#
```

```python
output: str#
```

```python
class FunctionWithRequirements(func: 'Callable[P, T]', python_packages: 'Sequence[str]' = <factory>, global_imports: 'Sequence[Import]' = <factory>)[source]#
```

【中文翻译】Bases: Generic[T, P]


classmethod from_callable(func: Callable[[P], T], python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → FunctionWithRequirements[T, P][source]#



static from_str(func: str, python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → FunctionWithRequirementsStr[source]#



func: Callable[[P], T]#



global_imports: Sequence[str | ImportFromModule | Alias]#



python_packages: Sequence[str]#

```python
classmethod from_callable(func: Callable[[P], T], python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → FunctionWithRequirements[T, P][source]#
```

```python
static from_str(func: str, python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → FunctionWithRequirementsStr[source]#
```

```python
func: Callable[[P], T]#
```

```python
global_imports: Sequence[str | ImportFromModule | Alias]#
```

```python
python_packages: Sequence[str]#
```

```python
class FunctionWithRequirementsStr(func: 'str', python_packages: 'Sequence[str]' = [], global_imports: 'Sequence[Import]' = [])[source]#
```

【中文翻译】Bases: object


compiled_func: Callable[[...], Any]#



func: str#



global_imports: Sequence[str | ImportFromModule | Alias]#



python_packages: Sequence[str]#

```python
compiled_func: Callable[[...], Any]#
```

```python
func: str#
```

```python
global_imports: Sequence[str | ImportFromModule | Alias]#
```

```python
python_packages: Sequence[str]#
```

```python
class ImportFromModule(module: 'str', imports: 'Union[Tuple[Union[str, Alias], ...], List[Union[str, Alias]]]')[source]#
```

【中文翻译】Bases: object


imports: Tuple[str | Alias, ...]#



module: str#

```python
imports: Tuple[str | Alias, ...]#
```

```python
module: str#
```

```python
with_requirements(python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → Callable[[Callable[[P], T]], FunctionWithRequirements[T, P]][source]#
```

【中文翻译】Decorate a function with package and import requirements for code execution environments.
This decorator makes a function available for reference in dynamically executed code blocks
by wrapping it in a FunctionWithRequirements object that tracks its dependencies. When the
decorated function is passed to a code executor, it can be imported by name in the executed
code, with all dependencies automatically handled.

Parameters:

python_packages (Sequence[str], optional) – Python packages required by the function.
Can include version specifications (e.g., [“pandas>=1.0.0”]). Defaults to [].
global_imports (Sequence[Import], optional) – Import statements required by the function.
Can be strings (“numpy”), ImportFromModule objects, or Alias objects. Defaults to [].


Returns:
Callable[[Callable[P, T]], FunctionWithRequirements[T, P]] – A decorator that wraps
the target function, preserving its functionality while registering its dependencies.


Example
import tempfile
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import with_requirements, CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
import pandas

@with_requirements(python_packages=["pandas"], global_imports=["pandas"])
def load_data() -> pandas.DataFrame:
    """Load some sample data.

    Returns:
        pandas.DataFrame: A DataFrame with sample data
    """
    data = {
        "name": ["John", "Anna", "Peter", "Linda"],
        "location": ["New York", "Paris", "Berlin", "London"],
        "age": [24, 13, 53, 33],
    }
    return pandas.DataFrame(data)

async def run_example():
    # The decorated function can be used in executed code
    with tempfile.TemporaryDirectory() as temp_dir:
        executor = LocalCommandLineCodeExecutor(work_dir=temp_dir, functions=[load_data])
        code = f"""from {executor.functions_module} import load_data

        # Use the imported function
        data = load_data()
        print(data['name'][0])"""

        result = await executor.execute_code_blocks(
            code_blocks=[CodeBlock(language="python", code=code)],
            cancellation_token=CancellationToken(),
        )
        print(result.output)  # Output: John

# Run the async example
asyncio.run(run_example())

**示例**:
```python
import tempfile
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import with_requirements, CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
import pandas

@with_requirements(python_packages=["pandas"], global_imports=["pandas"])
def load_data() -> pandas.DataFrame:
    """Load some sample data.

    Returns:
        pandas.DataFrame: A DataFrame with sample data
    """
    data = {
        "name": ["John", "Anna", "Peter", "Linda"],
        "location": ["New York", "Paris", "Berlin", "London"],
        "age": [24, 13, 53, 33],
    }
    return pandas.DataFrame(data)

async def run_example():
    # The decorated function can be used in executed code
    with tempfile.TemporaryDirectory() as temp_dir:
        executor = LocalCommandLineCodeExecutor(work_dir=temp_dir, functions=[load_data])
        code = f"""from {executor.functions_module} import load_data

        # Use the imported function
        data = load_data()
        print(data['name'][0])"""

        result = await executor.execute_code_blocks(
            code_blocks=[CodeBlock(language="python", code=code)],
            cancellation_token=CancellationToken(),
        )
        print(result.output)  # Output: John

# Run the async example
asyncio.run(run_example())

```

【中文翻译】previous

【中文翻译】autogen_core

【中文翻译】next

【中文翻译】autogen_core.models

### autogen_core.code_executor {autogen_corecode_executor}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html)

```python
class Alias(name: 'str', alias: 'str')[source]#
```

【中文翻译】Bases: object


alias: str#



name: str#

```python
alias: str#
```

```python
name: str#
```

```python
class CodeBlock(code: str, language: str)[source]#
```

【中文翻译】Bases: object
A code block extracted fromm an agent message.


code: str#



language: str#

```python
code: str#
```

```python
language: str#
```

```python
class CodeExecutor[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
Executes code blocks and returns the result.
This is an abstract base class for code executors. It defines the interface
for executing code blocks and returning the result. A concrete implementation
of this class should be provided to execute code blocks in a specific
environment. For example, DockerCommandLineCodeExecutor executes
code blocks in a command line environment in a Docker container.
It is recommended for subclass to be used as a context manager to ensure
that resources are cleaned up properly. To do this, implement the
start() and
stop() methods
that will be called when entering and exiting the context manager.


component_type: ClassVar[ComponentType] = 'code_executor'#
The logical type of the component.



abstract async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
Execute code blocks and return the result.
This method should be implemented by the code executor.

Parameters:
code_blocks (List[CodeBlock]) – The code blocks to execute.

Returns:
CodeResult – The result of the code execution.

Raises:

ValueError – Errors in user inputs
TimeoutError – Code execution timeouts
CancelledError – CancellationToken evoked during execution






abstract async restart() → None[source]#
Restart the code executor.
This method should be implemented by the code executor.
This method is called when the agent is reset.



abstract async start() → None[source]#
Start the code executor.



abstract async stop() → None[source]#
Stop the code executor and release any resources.

```python
component_type: ClassVar[ComponentType] = 'code_executor'#
```

【中文翻译】The logical type of the component.

```python
abstract async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
```

【中文翻译】Execute code blocks and return the result.
This method should be implemented by the code executor.

Parameters:
code_blocks (List[CodeBlock]) – The code blocks to execute.

Returns:
CodeResult – The result of the code execution.

Raises:

ValueError – Errors in user inputs
TimeoutError – Code execution timeouts
CancelledError – CancellationToken evoked during execution

```python
abstract async restart() → None[source]#
```

【中文翻译】Restart the code executor.
This method should be implemented by the code executor.
This method is called when the agent is reset.

```python
abstract async start() → None[source]#
```

【中文翻译】Start the code executor.

```python
abstract async stop() → None[source]#
```

【中文翻译】Stop the code executor and release any resources.

```python
class CodeResult(exit_code: int, output: str)[source]#
```

【中文翻译】Bases: object
Result of a code execution.


exit_code: int#



output: str#

```python
exit_code: int#
```

```python
output: str#
```

```python
class FunctionWithRequirements(func: 'Callable[P, T]', python_packages: 'Sequence[str]' = <factory>, global_imports: 'Sequence[Import]' = <factory>)[source]#
```

【中文翻译】Bases: Generic[T, P]


classmethod from_callable(func: Callable[[P], T], python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → FunctionWithRequirements[T, P][source]#



static from_str(func: str, python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → FunctionWithRequirementsStr[source]#



func: Callable[[P], T]#



global_imports: Sequence[str | ImportFromModule | Alias]#



python_packages: Sequence[str]#

```python
classmethod from_callable(func: Callable[[P], T], python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → FunctionWithRequirements[T, P][source]#
```

```python
static from_str(func: str, python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → FunctionWithRequirementsStr[source]#
```

```python
func: Callable[[P], T]#
```

```python
global_imports: Sequence[str | ImportFromModule | Alias]#
```

```python
python_packages: Sequence[str]#
```

```python
class FunctionWithRequirementsStr(func: 'str', python_packages: 'Sequence[str]' = [], global_imports: 'Sequence[Import]' = [])[source]#
```

【中文翻译】Bases: object


compiled_func: Callable[[...], Any]#



func: str#



global_imports: Sequence[str | ImportFromModule | Alias]#



python_packages: Sequence[str]#

```python
compiled_func: Callable[[...], Any]#
```

```python
func: str#
```

```python
global_imports: Sequence[str | ImportFromModule | Alias]#
```

```python
python_packages: Sequence[str]#
```

```python
class ImportFromModule(module: 'str', imports: 'Union[Tuple[Union[str, Alias], ...], List[Union[str, Alias]]]')[source]#
```

【中文翻译】Bases: object


imports: Tuple[str | Alias, ...]#



module: str#

```python
imports: Tuple[str | Alias, ...]#
```

```python
module: str#
```

```python
with_requirements(python_packages: Sequence[str] = [], global_imports: Sequence[str | ImportFromModule | Alias] = []) → Callable[[Callable[[P], T]], FunctionWithRequirements[T, P]][source]#
```

【中文翻译】Decorate a function with package and import requirements for code execution environments.
This decorator makes a function available for reference in dynamically executed code blocks
by wrapping it in a FunctionWithRequirements object that tracks its dependencies. When the
decorated function is passed to a code executor, it can be imported by name in the executed
code, with all dependencies automatically handled.

Parameters:

python_packages (Sequence[str], optional) – Python packages required by the function.
Can include version specifications (e.g., [“pandas>=1.0.0”]). Defaults to [].
global_imports (Sequence[Import], optional) – Import statements required by the function.
Can be strings (“numpy”), ImportFromModule objects, or Alias objects. Defaults to [].


Returns:
Callable[[Callable[P, T]], FunctionWithRequirements[T, P]] – A decorator that wraps
the target function, preserving its functionality while registering its dependencies.


Example
import tempfile
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import with_requirements, CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
import pandas

@with_requirements(python_packages=["pandas"], global_imports=["pandas"])
def load_data() -> pandas.DataFrame:
    """Load some sample data.

    Returns:
        pandas.DataFrame: A DataFrame with sample data
    """
    data = {
        "name": ["John", "Anna", "Peter", "Linda"],
        "location": ["New York", "Paris", "Berlin", "London"],
        "age": [24, 13, 53, 33],
    }
    return pandas.DataFrame(data)

async def run_example():
    # The decorated function can be used in executed code
    with tempfile.TemporaryDirectory() as temp_dir:
        executor = LocalCommandLineCodeExecutor(work_dir=temp_dir, functions=[load_data])
        code = f"""from {executor.functions_module} import load_data

        # Use the imported function
        data = load_data()
        print(data['name'][0])"""

        result = await executor.execute_code_blocks(
            code_blocks=[CodeBlock(language="python", code=code)],
            cancellation_token=CancellationToken(),
        )
        print(result.output)  # Output: John

# Run the async example
asyncio.run(run_example())

**示例**:
```python
import tempfile
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import with_requirements, CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
import pandas

@with_requirements(python_packages=["pandas"], global_imports=["pandas"])
def load_data() -> pandas.DataFrame:
    """Load some sample data.

    Returns:
        pandas.DataFrame: A DataFrame with sample data
    """
    data = {
        "name": ["John", "Anna", "Peter", "Linda"],
        "location": ["New York", "Paris", "Berlin", "London"],
        "age": [24, 13, 53, 33],
    }
    return pandas.DataFrame(data)

async def run_example():
    # The decorated function can be used in executed code
    with tempfile.TemporaryDirectory() as temp_dir:
        executor = LocalCommandLineCodeExecutor(work_dir=temp_dir, functions=[load_data])
        code = f"""from {executor.functions_module} import load_data

        # Use the imported function
        data = load_data()
        print(data['name'][0])"""

        result = await executor.execute_code_blocks(
            code_blocks=[CodeBlock(language="python", code=code)],
            cancellation_token=CancellationToken(),
        )
        print(result.output)  # Output: John

# Run the async example
asyncio.run(run_example())

```

【中文翻译】previous

【中文翻译】autogen_core

【中文翻译】next

【中文翻译】autogen_core.models

### autogen_core.exceptions {autogen_coreexceptions}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html)

```python
exception CantHandleException[source]#
```

【中文翻译】Bases: Exception
Raised when a handler can’t handle the exception.

```python
exception MessageDroppedException[source]#
```

【中文翻译】Bases: Exception
Raised when a message is dropped.

```python
exception NotAccessibleError[source]#
```

【中文翻译】Bases: Exception
Tried to access a value that is not accessible. For example if it is remote cannot be accessed locally.

```python
exception UndeliverableException[source]#
```

【中文翻译】Bases: Exception
Raised when a message can’t be delivered.

【中文翻译】previous

【中文翻译】autogen_core.memory

【中文翻译】next

【中文翻译】autogen_core.logging

### autogen_core.exceptions {autogen_coreexceptions}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.exceptions.html)

```python
exception CantHandleException[source]#
```

【中文翻译】Bases: Exception
Raised when a handler can’t handle the exception.

```python
exception MessageDroppedException[source]#
```

【中文翻译】Bases: Exception
Raised when a message is dropped.

```python
exception NotAccessibleError[source]#
```

【中文翻译】Bases: Exception
Tried to access a value that is not accessible. For example if it is remote cannot be accessed locally.

```python
exception UndeliverableException[source]#
```

【中文翻译】Bases: Exception
Raised when a message can’t be delivered.

【中文翻译】previous

【中文翻译】autogen_core.memory

【中文翻译】next

【中文翻译】autogen_core.logging

### autogen_core.logging {autogen_corelogging}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html)

```python
class AgentConstructionExceptionEvent(*, agent_id: AgentId, exception: BaseException, **kwargs: Any)[source]#
```

【中文翻译】Bases: object

```python
class DeliveryStage(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)[source]#
```

【中文翻译】Bases: Enum


DELIVER = 2#



SEND = 1#

```python
DELIVER = 2#
```

```python
SEND = 1#
```

```python
class LLMCallEvent(*, messages: List[Dict[str, Any]], response: Dict[str, Any], prompt_tokens: int, completion_tokens: int, **kwargs: Any)[source]#
```

【中文翻译】Bases: object


property completion_tokens: int#



property prompt_tokens: int#

```python
property completion_tokens: int#
```

```python
property prompt_tokens: int#
```

```python
class LLMStreamEndEvent(*, response: Dict[str, Any], prompt_tokens: int, completion_tokens: int, **kwargs: Any)[source]#
```

【中文翻译】Bases: object


property completion_tokens: int#



property prompt_tokens: int#

```python
property completion_tokens: int#
```

```python
property prompt_tokens: int#
```

```python
class LLMStreamStartEvent(*, messages: List[Dict[str, Any]], **kwargs: Any)[source]#
```

【中文翻译】Bases: object
To be used by model clients to log the start of a stream.

Parameters:
messages (List[Dict[str, Any]]) – The messages used in the call. Must be json serializable.


Example
import logging
from autogen_core import EVENT_LOGGER_NAME
from autogen_core.logging import LLMStreamStartEvent

messages = [{"role": "user", "content": "Hello, world!"}]
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.info(LLMStreamStartEvent(messages=messages))

**示例**:
```python
import logging
from autogen_core import EVENT_LOGGER_NAME
from autogen_core.logging import LLMStreamStartEvent

messages = [{"role": "user", "content": "Hello, world!"}]
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.info(LLMStreamStartEvent(messages=messages))

```

```python
class MessageDroppedEvent(*, payload: str, sender: AgentId | None, receiver: AgentId | TopicId | None, kind: MessageKind, **kwargs: Any)[source]#
```

【中文翻译】Bases: object

```python
class MessageEvent(*, payload: str, sender: AgentId | None, receiver: AgentId | TopicId | None, kind: MessageKind, delivery_stage: DeliveryStage, **kwargs: Any)[source]#
```

【中文翻译】Bases: object

```python
class MessageHandlerExceptionEvent(*, payload: str, handling_agent: AgentId, exception: BaseException, **kwargs: Any)[source]#
```

【中文翻译】Bases: object

```python
class MessageKind(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)[source]#
```

【中文翻译】Bases: Enum


DIRECT = 1#



PUBLISH = 2#



RESPOND = 3#

```python
DIRECT = 1#
```

```python
PUBLISH = 2#
```

```python
RESPOND = 3#
```

```python
class ToolCallEvent(*, tool_name: str, arguments: Dict[str, Any], result: str)[source]#
```

【中文翻译】Bases: object

【中文翻译】previous

【中文翻译】autogen_core.exceptions

【中文翻译】next

【中文翻译】autogen_ext.agents.azure

### autogen_core.logging {autogen_corelogging}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.logging.html)

```python
class AgentConstructionExceptionEvent(*, agent_id: AgentId, exception: BaseException, **kwargs: Any)[source]#
```

【中文翻译】Bases: object

```python
class DeliveryStage(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)[source]#
```

【中文翻译】Bases: Enum


DELIVER = 2#



SEND = 1#

```python
DELIVER = 2#
```

```python
SEND = 1#
```

```python
class LLMCallEvent(*, messages: List[Dict[str, Any]], response: Dict[str, Any], prompt_tokens: int, completion_tokens: int, **kwargs: Any)[source]#
```

【中文翻译】Bases: object


property completion_tokens: int#



property prompt_tokens: int#

```python
property completion_tokens: int#
```

```python
property prompt_tokens: int#
```

```python
class LLMStreamEndEvent(*, response: Dict[str, Any], prompt_tokens: int, completion_tokens: int, **kwargs: Any)[source]#
```

【中文翻译】Bases: object


property completion_tokens: int#



property prompt_tokens: int#

```python
property completion_tokens: int#
```

```python
property prompt_tokens: int#
```

```python
class LLMStreamStartEvent(*, messages: List[Dict[str, Any]], **kwargs: Any)[source]#
```

【中文翻译】Bases: object
To be used by model clients to log the start of a stream.

Parameters:
messages (List[Dict[str, Any]]) – The messages used in the call. Must be json serializable.


Example
import logging
from autogen_core import EVENT_LOGGER_NAME
from autogen_core.logging import LLMStreamStartEvent

messages = [{"role": "user", "content": "Hello, world!"}]
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.info(LLMStreamStartEvent(messages=messages))

**示例**:
```python
import logging
from autogen_core import EVENT_LOGGER_NAME
from autogen_core.logging import LLMStreamStartEvent

messages = [{"role": "user", "content": "Hello, world!"}]
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.info(LLMStreamStartEvent(messages=messages))

```

```python
class MessageDroppedEvent(*, payload: str, sender: AgentId | None, receiver: AgentId | TopicId | None, kind: MessageKind, **kwargs: Any)[source]#
```

【中文翻译】Bases: object

```python
class MessageEvent(*, payload: str, sender: AgentId | None, receiver: AgentId | TopicId | None, kind: MessageKind, delivery_stage: DeliveryStage, **kwargs: Any)[source]#
```

【中文翻译】Bases: object

```python
class MessageHandlerExceptionEvent(*, payload: str, handling_agent: AgentId, exception: BaseException, **kwargs: Any)[source]#
```

【中文翻译】Bases: object

```python
class MessageKind(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)[source]#
```

【中文翻译】Bases: Enum


DIRECT = 1#



PUBLISH = 2#



RESPOND = 3#

```python
DIRECT = 1#
```

```python
PUBLISH = 2#
```

```python
RESPOND = 3#
```

```python
class ToolCallEvent(*, tool_name: str, arguments: Dict[str, Any], result: str)[source]#
```

【中文翻译】Bases: object

【中文翻译】previous

【中文翻译】autogen_core.exceptions

【中文翻译】next

【中文翻译】autogen_ext.agents.azure

### autogen_core.memory {autogen_corememory}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html)

```python
class ListMemory(name: str | None = None, memory_contents: List[MemoryContent] | None = None)[source]#
```

【中文翻译】Bases: Memory, Component[ListMemoryConfig]
Simple chronological list-based memory implementation.
This memory implementation stores contents in a list and retrieves them in
chronological order. It has an update_context method that updates model contexts
by appending all stored memories.
The memory content can be directly accessed and modified through the content property,
allowing external applications to manage memory contents directly.
Example
import asyncio
from autogen_core.memory import ListMemory, MemoryContent
from autogen_core.model_context import BufferedChatCompletionContext


async def main() -> None:
    # Initialize memory
    memory = ListMemory(name="chat_history")

    # Add memory content
    content = MemoryContent(content="User prefers formal language", mime_type="text/plain")
    await memory.add(content)

    # Directly modify memory contents
    memory.content = [MemoryContent(content="New preference", mime_type="text/plain")]

    # Create a model context
    model_context = BufferedChatCompletionContext(buffer_size=10)

    # Update a model context with memory
    await memory.update_context(model_context)

    # See the updated model context
    print(await model_context.get_messages())


asyncio.run(main())



Parameters:
name – Optional identifier for this memory instance




classmethod _from_config(config: ListMemoryConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → ListMemoryConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
Add new content to memory.

Parameters:

content – Memory content to store
cancellation_token – Optional token to cancel operation






async clear() → None[source]#
Clear all memory content.



async close() → None[source]#
Cleanup resources if needed.



component_config_schema#
alias of ListMemoryConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.memory.ListMemory'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'memory'#
The logical type of the component.



property content: List[MemoryContent]#
Get the current memory contents.

Returns:
List[MemoryContent] – List of stored memory contents





property name: str#
Get the memory instance identifier.

Returns:
str – Memory instance name





async query(query: str | MemoryContent = '', cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
Return all memories without any filtering.

Parameters:

query – Ignored in this implementation
cancellation_token – Optional token to cancel operation
**kwargs – Additional parameters (ignored)


Returns:
MemoryQueryResult containing all stored memories





async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
Update the model context by appending memory content.
This method mutates the provided model_context by adding all memories as a
SystemMessage.

Parameters:
model_context – The context to update. Will be mutated if memories exist.

Returns:
UpdateContextResult containing the memories that were added to the context

**示例**:
```python
import asyncio
from autogen_core.memory import ListMemory, MemoryContent
from autogen_core.model_context import BufferedChatCompletionContext


async def main() -> None:
    # Initialize memory
    memory = ListMemory(name="chat_history")

    # Add memory content
    content = MemoryContent(content="User prefers formal language", mime_type="text/plain")
    await memory.add(content)

    # Directly modify memory contents
    memory.content = [MemoryContent(content="New preference", mime_type="text/plain")]

    # Create a model context
    model_context = BufferedChatCompletionContext(buffer_size=10)

    # Update a model context with memory
    await memory.update_context(model_context)

    # See the updated model context
    print(await model_context.get_messages())


asyncio.run(main())

```

```python
classmethod _from_config(config: ListMemoryConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → ListMemoryConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Add new content to memory.

Parameters:

content – Memory content to store
cancellation_token – Optional token to cancel operation

```python
async clear() → None[source]#
```

【中文翻译】Clear all memory content.

```python
async close() → None[source]#
```

【中文翻译】Cleanup resources if needed.

```python
component_config_schema#
```

【中文翻译】alias of ListMemoryConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.memory.ListMemory'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'memory'#
```

【中文翻译】The logical type of the component.

```python
property content: List[MemoryContent]#
```

【中文翻译】Get the current memory contents.

Returns:
List[MemoryContent] – List of stored memory contents

```python
property name: str#
```

【中文翻译】Get the memory instance identifier.

Returns:
str – Memory instance name

```python
async query(query: str | MemoryContent = '', cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
```

【中文翻译】Return all memories without any filtering.

Parameters:

query – Ignored in this implementation
cancellation_token – Optional token to cancel operation
**kwargs – Additional parameters (ignored)


Returns:
MemoryQueryResult containing all stored memories

```python
async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
```

【中文翻译】Update the model context by appending memory content.
This method mutates the provided model_context by adding all memories as a
SystemMessage.

Parameters:
model_context – The context to update. Will be mutated if memories exist.

Returns:
UpdateContextResult containing the memories that were added to the context

```python
class Memory[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
Protocol defining the interface for memory implementations.
A memory is the storage for data that can be used to enrich or modify the model context.
A memory implementation can use any storage mechanism, such as a list, a database, or a file system.
It can also use any retrieval mechanism, such as vector search or text search.
It is up to the implementation to decide how to store and retrieve data.
It is also a memory implementation’s responsibility to update the model context
with relevant memory content based on the current model context and querying the memory store.
See ListMemory for an example implementation.


abstract async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
Add a new content to memory.

Parameters:

content – The memory content to add
cancellation_token – Optional token to cancel operation






abstract async clear() → None[source]#
Clear all entries from memory.



abstract async close() → None[source]#
Clean up any resources used by the memory implementation.



component_type: ClassVar[ComponentType] = 'memory'#
The logical type of the component.



abstract async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
Query the memory store and return relevant entries.

Parameters:

query – Query content item
cancellation_token – Optional token to cancel operation
**kwargs – Additional implementation-specific parameters


Returns:
MemoryQueryResult containing memory entries with relevance scores





abstract async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
Update the provided model context using relevant memory content.

Parameters:
model_context – The context to update.

Returns:
UpdateContextResult containing relevant memories

```python
abstract async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Add a new content to memory.

Parameters:

content – The memory content to add
cancellation_token – Optional token to cancel operation

```python
abstract async clear() → None[source]#
```

【中文翻译】Clear all entries from memory.

```python
abstract async close() → None[source]#
```

【中文翻译】Clean up any resources used by the memory implementation.

```python
component_type: ClassVar[ComponentType] = 'memory'#
```

【中文翻译】The logical type of the component.

```python
abstract async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
```

【中文翻译】Query the memory store and return relevant entries.

Parameters:

query – Query content item
cancellation_token – Optional token to cancel operation
**kwargs – Additional implementation-specific parameters


Returns:
MemoryQueryResult containing memory entries with relevance scores

```python
abstract async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
```

【中文翻译】Update the provided model context using relevant memory content.

Parameters:
model_context – The context to update.

Returns:
UpdateContextResult containing relevant memories

```python
pydantic model MemoryContent[source]#
```

【中文翻译】Bases: BaseModel
A memory content item.

Show JSON schema{
   "title": "MemoryContent",
   "description": "A memory content item.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "format": "binary",
               "type": "string"
            },
            {
               "type": "object"
            },
            {}
         ],
         "title": "Content"
      },
      "mime_type": {
         "anyOf": [
            {
               "$ref": "#/$defs/MemoryMimeType"
            },
            {
               "type": "string"
            }
         ],
         "title": "Mime Type"
      },
      "metadata": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      }
   },
   "$defs": {
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      }
   },
   "required": [
      "content",
      "mime_type"
   ]
}



Fields:

content (str | bytes | Dict[str, Any] | autogen_core._image.Image)
metadata (Dict[str, Any] | None)
mime_type (autogen_core.memory._base_memory.MemoryMimeType | str)





field content: str | bytes | Dict[str, Any] | Image [Required]#
The content of the memory item. It can be a string, bytes, dict, or Image.



field metadata: Dict[str, Any] | None = None#
Metadata associated with the memory item.



field mime_type: MemoryMimeType | str [Required]#
The MIME type of the memory content.



serialize_mime_type(mime_type: MemoryMimeType | str) → str[source]#
Serialize the MIME type to a string.

**示例**:
```python
{
   "title": "MemoryContent",
   "description": "A memory content item.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "format": "binary",
               "type": "string"
            },
            {
               "type": "object"
            },
            {}
         ],
         "title": "Content"
      },
      "mime_type": {
         "anyOf": [
            {
               "$ref": "#/$defs/MemoryMimeType"
            },
            {
               "type": "string"
            }
         ],
         "title": "Mime Type"
      },
      "metadata": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      }
   },
   "$defs": {
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      }
   },
   "required": [
      "content",
      "mime_type"
   ]
}

```

```python
field content: str | bytes | Dict[str, Any] | Image [Required]#
```

【中文翻译】The content of the memory item. It can be a string, bytes, dict, or Image.

```python
field metadata: Dict[str, Any] | None = None#
```

【中文翻译】Metadata associated with the memory item.

```python
field mime_type: MemoryMimeType | str [Required]#
```

【中文翻译】The MIME type of the memory content.

```python
serialize_mime_type(mime_type: MemoryMimeType | str) → str[source]#
```

【中文翻译】Serialize the MIME type to a string.

```python
class MemoryMimeType(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)[source]#
```

【中文翻译】Bases: Enum
Supported MIME types for memory content.


BINARY = 'application/octet-stream'#



IMAGE = 'image/*'#



JSON = 'application/json'#



MARKDOWN = 'text/markdown'#



TEXT = 'text/plain'#

```python
BINARY = 'application/octet-stream'#
```

```python
IMAGE = 'image/*'#
```

```python
JSON = 'application/json'#
```

```python
MARKDOWN = 'text/markdown'#
```

```python
TEXT = 'text/plain'#
```

```python
pydantic model MemoryQueryResult[source]#
```

【中文翻译】Bases: BaseModel
Result of a memory query() operation.

Show JSON schema{
   "title": "MemoryQueryResult",
   "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
   "type": "object",
   "properties": {
      "results": {
         "items": {
            "$ref": "#/$defs/MemoryContent"
         },
         "title": "Results",
         "type": "array"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      }
   },
   "required": [
      "results"
   ]
}



Fields:

results (List[autogen_core.memory._base_memory.MemoryContent])





field results: List[MemoryContent] [Required]#

**示例**:
```python
{
   "title": "MemoryQueryResult",
   "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
   "type": "object",
   "properties": {
      "results": {
         "items": {
            "$ref": "#/$defs/MemoryContent"
         },
         "title": "Results",
         "type": "array"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      }
   },
   "required": [
      "results"
   ]
}

```

```python
field results: List[MemoryContent] [Required]#
```

```python
pydantic model UpdateContextResult[source]#
```

【中文翻译】Bases: BaseModel
Result of a memory update_context() operation.

Show JSON schema{
   "title": "UpdateContextResult",
   "description": "Result of a memory :meth:`~autogen_core.memory.Memory.update_context` operation.",
   "type": "object",
   "properties": {
      "memories": {
         "$ref": "#/$defs/MemoryQueryResult"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      },
      "MemoryQueryResult": {
         "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
         "properties": {
            "results": {
               "items": {
                  "$ref": "#/$defs/MemoryContent"
               },
               "title": "Results",
               "type": "array"
            }
         },
         "required": [
            "results"
         ],
         "title": "MemoryQueryResult",
         "type": "object"
      }
   },
   "required": [
      "memories"
   ]
}



Fields:

memories (autogen_core.memory._base_memory.MemoryQueryResult)





field memories: MemoryQueryResult [Required]#

**示例**:
```python
{
   "title": "UpdateContextResult",
   "description": "Result of a memory :meth:`~autogen_core.memory.Memory.update_context` operation.",
   "type": "object",
   "properties": {
      "memories": {
         "$ref": "#/$defs/MemoryQueryResult"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      },
      "MemoryQueryResult": {
         "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
         "properties": {
            "results": {
               "items": {
                  "$ref": "#/$defs/MemoryContent"
               },
               "title": "Results",
               "type": "array"
            }
         },
         "required": [
            "results"
         ],
         "title": "MemoryQueryResult",
         "type": "object"
      }
   },
   "required": [
      "memories"
   ]
}

```

```python
field memories: MemoryQueryResult [Required]#
```

【中文翻译】previous

【中文翻译】autogen_core.tool_agent

【中文翻译】next

【中文翻译】autogen_core.exceptions

### autogen_core.memory {autogen_corememory}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.memory.html)

```python
class ListMemory(name: str | None = None, memory_contents: List[MemoryContent] | None = None)[source]#
```

【中文翻译】Bases: Memory, Component[ListMemoryConfig]
Simple chronological list-based memory implementation.
This memory implementation stores contents in a list and retrieves them in
chronological order. It has an update_context method that updates model contexts
by appending all stored memories.
The memory content can be directly accessed and modified through the content property,
allowing external applications to manage memory contents directly.
Example
import asyncio
from autogen_core.memory import ListMemory, MemoryContent
from autogen_core.model_context import BufferedChatCompletionContext


async def main() -> None:
    # Initialize memory
    memory = ListMemory(name="chat_history")

    # Add memory content
    content = MemoryContent(content="User prefers formal language", mime_type="text/plain")
    await memory.add(content)

    # Directly modify memory contents
    memory.content = [MemoryContent(content="New preference", mime_type="text/plain")]

    # Create a model context
    model_context = BufferedChatCompletionContext(buffer_size=10)

    # Update a model context with memory
    await memory.update_context(model_context)

    # See the updated model context
    print(await model_context.get_messages())


asyncio.run(main())



Parameters:
name – Optional identifier for this memory instance




classmethod _from_config(config: ListMemoryConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → ListMemoryConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
Add new content to memory.

Parameters:

content – Memory content to store
cancellation_token – Optional token to cancel operation






async clear() → None[source]#
Clear all memory content.



async close() → None[source]#
Cleanup resources if needed.



component_config_schema#
alias of ListMemoryConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.memory.ListMemory'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'memory'#
The logical type of the component.



property content: List[MemoryContent]#
Get the current memory contents.

Returns:
List[MemoryContent] – List of stored memory contents





property name: str#
Get the memory instance identifier.

Returns:
str – Memory instance name





async query(query: str | MemoryContent = '', cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
Return all memories without any filtering.

Parameters:

query – Ignored in this implementation
cancellation_token – Optional token to cancel operation
**kwargs – Additional parameters (ignored)


Returns:
MemoryQueryResult containing all stored memories





async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
Update the model context by appending memory content.
This method mutates the provided model_context by adding all memories as a
SystemMessage.

Parameters:
model_context – The context to update. Will be mutated if memories exist.

Returns:
UpdateContextResult containing the memories that were added to the context

**示例**:
```python
import asyncio
from autogen_core.memory import ListMemory, MemoryContent
from autogen_core.model_context import BufferedChatCompletionContext


async def main() -> None:
    # Initialize memory
    memory = ListMemory(name="chat_history")

    # Add memory content
    content = MemoryContent(content="User prefers formal language", mime_type="text/plain")
    await memory.add(content)

    # Directly modify memory contents
    memory.content = [MemoryContent(content="New preference", mime_type="text/plain")]

    # Create a model context
    model_context = BufferedChatCompletionContext(buffer_size=10)

    # Update a model context with memory
    await memory.update_context(model_context)

    # See the updated model context
    print(await model_context.get_messages())


asyncio.run(main())

```

```python
classmethod _from_config(config: ListMemoryConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → ListMemoryConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Add new content to memory.

Parameters:

content – Memory content to store
cancellation_token – Optional token to cancel operation

```python
async clear() → None[source]#
```

【中文翻译】Clear all memory content.

```python
async close() → None[source]#
```

【中文翻译】Cleanup resources if needed.

```python
component_config_schema#
```

【中文翻译】alias of ListMemoryConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.memory.ListMemory'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'memory'#
```

【中文翻译】The logical type of the component.

```python
property content: List[MemoryContent]#
```

【中文翻译】Get the current memory contents.

Returns:
List[MemoryContent] – List of stored memory contents

```python
property name: str#
```

【中文翻译】Get the memory instance identifier.

Returns:
str – Memory instance name

```python
async query(query: str | MemoryContent = '', cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
```

【中文翻译】Return all memories without any filtering.

Parameters:

query – Ignored in this implementation
cancellation_token – Optional token to cancel operation
**kwargs – Additional parameters (ignored)


Returns:
MemoryQueryResult containing all stored memories

```python
async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
```

【中文翻译】Update the model context by appending memory content.
This method mutates the provided model_context by adding all memories as a
SystemMessage.

Parameters:
model_context – The context to update. Will be mutated if memories exist.

Returns:
UpdateContextResult containing the memories that were added to the context

```python
class Memory[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
Protocol defining the interface for memory implementations.
A memory is the storage for data that can be used to enrich or modify the model context.
A memory implementation can use any storage mechanism, such as a list, a database, or a file system.
It can also use any retrieval mechanism, such as vector search or text search.
It is up to the implementation to decide how to store and retrieve data.
It is also a memory implementation’s responsibility to update the model context
with relevant memory content based on the current model context and querying the memory store.
See ListMemory for an example implementation.


abstract async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
Add a new content to memory.

Parameters:

content – The memory content to add
cancellation_token – Optional token to cancel operation






abstract async clear() → None[source]#
Clear all entries from memory.



abstract async close() → None[source]#
Clean up any resources used by the memory implementation.



component_type: ClassVar[ComponentType] = 'memory'#
The logical type of the component.



abstract async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
Query the memory store and return relevant entries.

Parameters:

query – Query content item
cancellation_token – Optional token to cancel operation
**kwargs – Additional implementation-specific parameters


Returns:
MemoryQueryResult containing memory entries with relevance scores





abstract async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
Update the provided model context using relevant memory content.

Parameters:
model_context – The context to update.

Returns:
UpdateContextResult containing relevant memories

```python
abstract async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Add a new content to memory.

Parameters:

content – The memory content to add
cancellation_token – Optional token to cancel operation

```python
abstract async clear() → None[source]#
```

【中文翻译】Clear all entries from memory.

```python
abstract async close() → None[source]#
```

【中文翻译】Clean up any resources used by the memory implementation.

```python
component_type: ClassVar[ComponentType] = 'memory'#
```

【中文翻译】The logical type of the component.

```python
abstract async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
```

【中文翻译】Query the memory store and return relevant entries.

Parameters:

query – Query content item
cancellation_token – Optional token to cancel operation
**kwargs – Additional implementation-specific parameters


Returns:
MemoryQueryResult containing memory entries with relevance scores

```python
abstract async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
```

【中文翻译】Update the provided model context using relevant memory content.

Parameters:
model_context – The context to update.

Returns:
UpdateContextResult containing relevant memories

```python
pydantic model MemoryContent[source]#
```

【中文翻译】Bases: BaseModel
A memory content item.

Show JSON schema{
   "title": "MemoryContent",
   "description": "A memory content item.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "format": "binary",
               "type": "string"
            },
            {
               "type": "object"
            },
            {}
         ],
         "title": "Content"
      },
      "mime_type": {
         "anyOf": [
            {
               "$ref": "#/$defs/MemoryMimeType"
            },
            {
               "type": "string"
            }
         ],
         "title": "Mime Type"
      },
      "metadata": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      }
   },
   "$defs": {
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      }
   },
   "required": [
      "content",
      "mime_type"
   ]
}



Fields:

content (str | bytes | Dict[str, Any] | autogen_core._image.Image)
metadata (Dict[str, Any] | None)
mime_type (autogen_core.memory._base_memory.MemoryMimeType | str)





field content: str | bytes | Dict[str, Any] | Image [Required]#
The content of the memory item. It can be a string, bytes, dict, or Image.



field metadata: Dict[str, Any] | None = None#
Metadata associated with the memory item.



field mime_type: MemoryMimeType | str [Required]#
The MIME type of the memory content.



serialize_mime_type(mime_type: MemoryMimeType | str) → str[source]#
Serialize the MIME type to a string.

**示例**:
```python
{
   "title": "MemoryContent",
   "description": "A memory content item.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "format": "binary",
               "type": "string"
            },
            {
               "type": "object"
            },
            {}
         ],
         "title": "Content"
      },
      "mime_type": {
         "anyOf": [
            {
               "$ref": "#/$defs/MemoryMimeType"
            },
            {
               "type": "string"
            }
         ],
         "title": "Mime Type"
      },
      "metadata": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      }
   },
   "$defs": {
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      }
   },
   "required": [
      "content",
      "mime_type"
   ]
}

```

```python
field content: str | bytes | Dict[str, Any] | Image [Required]#
```

【中文翻译】The content of the memory item. It can be a string, bytes, dict, or Image.

```python
field metadata: Dict[str, Any] | None = None#
```

【中文翻译】Metadata associated with the memory item.

```python
field mime_type: MemoryMimeType | str [Required]#
```

【中文翻译】The MIME type of the memory content.

```python
serialize_mime_type(mime_type: MemoryMimeType | str) → str[source]#
```

【中文翻译】Serialize the MIME type to a string.

```python
class MemoryMimeType(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)[source]#
```

【中文翻译】Bases: Enum
Supported MIME types for memory content.


BINARY = 'application/octet-stream'#



IMAGE = 'image/*'#



JSON = 'application/json'#



MARKDOWN = 'text/markdown'#



TEXT = 'text/plain'#

```python
BINARY = 'application/octet-stream'#
```

```python
IMAGE = 'image/*'#
```

```python
JSON = 'application/json'#
```

```python
MARKDOWN = 'text/markdown'#
```

```python
TEXT = 'text/plain'#
```

```python
pydantic model MemoryQueryResult[source]#
```

【中文翻译】Bases: BaseModel
Result of a memory query() operation.

Show JSON schema{
   "title": "MemoryQueryResult",
   "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
   "type": "object",
   "properties": {
      "results": {
         "items": {
            "$ref": "#/$defs/MemoryContent"
         },
         "title": "Results",
         "type": "array"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      }
   },
   "required": [
      "results"
   ]
}



Fields:

results (List[autogen_core.memory._base_memory.MemoryContent])





field results: List[MemoryContent] [Required]#

**示例**:
```python
{
   "title": "MemoryQueryResult",
   "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
   "type": "object",
   "properties": {
      "results": {
         "items": {
            "$ref": "#/$defs/MemoryContent"
         },
         "title": "Results",
         "type": "array"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      }
   },
   "required": [
      "results"
   ]
}

```

```python
field results: List[MemoryContent] [Required]#
```

```python
pydantic model UpdateContextResult[source]#
```

【中文翻译】Bases: BaseModel
Result of a memory update_context() operation.

Show JSON schema{
   "title": "UpdateContextResult",
   "description": "Result of a memory :meth:`~autogen_core.memory.Memory.update_context` operation.",
   "type": "object",
   "properties": {
      "memories": {
         "$ref": "#/$defs/MemoryQueryResult"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      },
      "MemoryQueryResult": {
         "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
         "properties": {
            "results": {
               "items": {
                  "$ref": "#/$defs/MemoryContent"
               },
               "title": "Results",
               "type": "array"
            }
         },
         "required": [
            "results"
         ],
         "title": "MemoryQueryResult",
         "type": "object"
      }
   },
   "required": [
      "memories"
   ]
}



Fields:

memories (autogen_core.memory._base_memory.MemoryQueryResult)





field memories: MemoryQueryResult [Required]#

**示例**:
```python
{
   "title": "UpdateContextResult",
   "description": "Result of a memory :meth:`~autogen_core.memory.Memory.update_context` operation.",
   "type": "object",
   "properties": {
      "memories": {
         "$ref": "#/$defs/MemoryQueryResult"
      }
   },
   "$defs": {
      "MemoryContent": {
         "description": "A memory content item.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "format": "binary",
                     "type": "string"
                  },
                  {
                     "type": "object"
                  },
                  {}
               ],
               "title": "Content"
            },
            "mime_type": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/MemoryMimeType"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Mime Type"
            },
            "metadata": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Metadata"
            }
         },
         "required": [
            "content",
            "mime_type"
         ],
         "title": "MemoryContent",
         "type": "object"
      },
      "MemoryMimeType": {
         "description": "Supported MIME types for memory content.",
         "enum": [
            "text/plain",
            "application/json",
            "text/markdown",
            "image/*",
            "application/octet-stream"
         ],
         "title": "MemoryMimeType",
         "type": "string"
      },
      "MemoryQueryResult": {
         "description": "Result of a memory :meth:`~autogen_core.memory.Memory.query` operation.",
         "properties": {
            "results": {
               "items": {
                  "$ref": "#/$defs/MemoryContent"
               },
               "title": "Results",
               "type": "array"
            }
         },
         "required": [
            "results"
         ],
         "title": "MemoryQueryResult",
         "type": "object"
      }
   },
   "required": [
      "memories"
   ]
}

```

```python
field memories: MemoryQueryResult [Required]#
```

【中文翻译】previous

【中文翻译】autogen_core.tool_agent

【中文翻译】next

【中文翻译】autogen_core.exceptions

### autogen_core.model_context {autogen_coremodel_context}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html)

```python
class BufferedChatCompletionContext(buffer_size: int, initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionContext, Component[BufferedChatCompletionContextConfig]
A buffered chat completion context that keeps a view of the last n messages,
where n is the buffer size. The buffer size is set at initialization.

Parameters:

buffer_size (int) – The size of the buffer.
initial_messages (List[LLMMessage] | None) – The initial messages.





classmethod _from_config(config: BufferedChatCompletionContextConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → BufferedChatCompletionContextConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of BufferedChatCompletionContextConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.BufferedChatCompletionContext'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
Get at most buffer_size recent messages.

```python
classmethod _from_config(config: BufferedChatCompletionContextConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → BufferedChatCompletionContextConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of BufferedChatCompletionContextConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.BufferedChatCompletionContext'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Get at most buffer_size recent messages.

```python
class ChatCompletionContext(initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
An abstract base class for defining the interface of a chat completion context.
A chat completion context lets agents store and retrieve LLM messages.
It can be implemented with different recall strategies.

Parameters:
initial_messages (List[LLMMessage] | None) – The initial messages.


Example
To create a custom model context that filters out the thought field from AssistantMessage.
This is useful for reasoning models like DeepSeek R1, which produces
very long thought that is not needed for subsequent completions.
from typing import List

from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, LLMMessage


class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out: List[LLMMessage] = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out




async add_message(message: Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]) → None[source]#
Add a message to the context.



async clear() → None[source]#
Clear the context.



component_type: ClassVar[ComponentType] = 'chat_completion_context'#
The logical type of the component.



abstract async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#



async load_state(state: Mapping[str, Any]) → None[source]#



async save_state() → Mapping[str, Any][source]#

**示例**:
```python
from typing import List

from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, LLMMessage


class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out: List[LLMMessage] = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out

```

```python
async add_message(message: Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]) → None[source]#
```

【中文翻译】Add a message to the context.

```python
async clear() → None[source]#
```

【中文翻译】Clear the context.

```python
component_type: ClassVar[ComponentType] = 'chat_completion_context'#
```

【中文翻译】The logical type of the component.

```python
abstract async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

```python
pydantic model ChatCompletionContextState[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "ChatCompletionContextState",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "discriminator": {
               "mapping": {
                  "AssistantMessage": "#/$defs/AssistantMessage",
                  "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                  "SystemMessage": "#/$defs/SystemMessage",
                  "UserMessage": "#/$defs/UserMessage"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/SystemMessage"
               },
               {
                  "$ref": "#/$defs/UserMessage"
               },
               {
                  "$ref": "#/$defs/AssistantMessage"
               },
               {
                  "$ref": "#/$defs/FunctionExecutionResultMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      }
   },
   "$defs": {
      "AssistantMessage": {
         "description": "Assistant message are sampled from the language model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "$ref": "#/$defs/FunctionCall"
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "thought": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Thought"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "AssistantMessage",
               "default": "AssistantMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "AssistantMessage",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "FunctionExecutionResultMessage": {
         "description": "Function execution result message contains the output of multiple function calls.",
         "properties": {
            "content": {
               "items": {
                  "$ref": "#/$defs/FunctionExecutionResult"
               },
               "title": "Content",
               "type": "array"
            },
            "type": {
               "const": "FunctionExecutionResultMessage",
               "default": "FunctionExecutionResultMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "FunctionExecutionResultMessage",
         "type": "object"
      },
      "SystemMessage": {
         "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "type": {
               "const": "SystemMessage",
               "default": "SystemMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "SystemMessage",
         "type": "object"
      },
      "UserMessage": {
         "description": "User message contains input from end users, or a catch-all for data provided to the model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "anyOf": [
                           {
                              "type": "string"
                           },
                           {}
                        ]
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "UserMessage",
               "default": "UserMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "UserMessage",
         "type": "object"
      }
   }
}



Fields:

messages (List[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage])





field messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Optional]#

**示例**:
```python
{
   "title": "ChatCompletionContextState",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "discriminator": {
               "mapping": {
                  "AssistantMessage": "#/$defs/AssistantMessage",
                  "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                  "SystemMessage": "#/$defs/SystemMessage",
                  "UserMessage": "#/$defs/UserMessage"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/SystemMessage"
               },
               {
                  "$ref": "#/$defs/UserMessage"
               },
               {
                  "$ref": "#/$defs/AssistantMessage"
               },
               {
                  "$ref": "#/$defs/FunctionExecutionResultMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      }
   },
   "$defs": {
      "AssistantMessage": {
         "description": "Assistant message are sampled from the language model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "$ref": "#/$defs/FunctionCall"
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "thought": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Thought"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "AssistantMessage",
               "default": "AssistantMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "AssistantMessage",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "FunctionExecutionResultMessage": {
         "description": "Function execution result message contains the output of multiple function calls.",
         "properties": {
            "content": {
               "items": {
                  "$ref": "#/$defs/FunctionExecutionResult"
               },
               "title": "Content",
               "type": "array"
            },
            "type": {
               "const": "FunctionExecutionResultMessage",
               "default": "FunctionExecutionResultMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "FunctionExecutionResultMessage",
         "type": "object"
      },
      "SystemMessage": {
         "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "type": {
               "const": "SystemMessage",
               "default": "SystemMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "SystemMessage",
         "type": "object"
      },
      "UserMessage": {
         "description": "User message contains input from end users, or a catch-all for data provided to the model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "anyOf": [
                           {
                              "type": "string"
                           },
                           {}
                        ]
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "UserMessage",
               "default": "UserMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "UserMessage",
         "type": "object"
      }
   }
}

```

```python
field messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Optional]#
```

```python
class HeadAndTailChatCompletionContext(head_size: int, tail_size: int, initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionContext, Component[HeadAndTailChatCompletionContextConfig]
A chat completion context that keeps a view of the first n and last m messages,
where n is the head size and m is the tail size. The head and tail sizes
are set at initialization.

Parameters:

head_size (int) – The size of the head.
tail_size (int) – The size of the tail.
initial_messages (List[LLMMessage] | None) – The initial messages.





classmethod _from_config(config: HeadAndTailChatCompletionContextConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → HeadAndTailChatCompletionContextConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of HeadAndTailChatCompletionContextConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.HeadAndTailChatCompletionContext'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
Get at most head_size recent messages and tail_size oldest messages.

```python
classmethod _from_config(config: HeadAndTailChatCompletionContextConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → HeadAndTailChatCompletionContextConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of HeadAndTailChatCompletionContextConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.HeadAndTailChatCompletionContext'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Get at most head_size recent messages and tail_size oldest messages.

```python
class TokenLimitedChatCompletionContext(model_client: ChatCompletionClient, *, token_limit: int | None = None, tool_schema: List[ToolSchema] | None = None, initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionContext, Component[TokenLimitedChatCompletionContextConfig]
(Experimental) A token based chat completion context maintains a view of the context up to a token limit.

Note
Added in v0.4.10. This is an experimental component and may change in the future.


Parameters:

model_client (ChatCompletionClient) – The model client to use for token counting.
The model client must implement the count_tokens()
and remaining_tokens() methods.
token_limit (int | None) – The maximum number of tokens to keep in the context
using the count_tokens() method.
If None, the context will be limited by the model client using the
remaining_tokens() method.
tools (List[ToolSchema] | None) – A list of tool schema to use in the context.
initial_messages (List[LLMMessage] | None) – A list of initial messages to include in the context.





classmethod _from_config(config: TokenLimitedChatCompletionContextConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TokenLimitedChatCompletionContextConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TokenLimitedChatCompletionContextConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.TokenLimitedChatCompletionContext'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
Get at most token_limit tokens in recent messages. If the token limit is not
provided, then return as many messages as the remaining token allowed by the model client.

```python
classmethod _from_config(config: TokenLimitedChatCompletionContextConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TokenLimitedChatCompletionContextConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TokenLimitedChatCompletionContextConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.TokenLimitedChatCompletionContext'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Get at most token_limit tokens in recent messages. If the token limit is not
provided, then return as many messages as the remaining token allowed by the model client.

```python
class UnboundedChatCompletionContext(initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionContext, Component[UnboundedChatCompletionContextConfig]
An unbounded chat completion context that keeps a view of the all the messages.


classmethod _from_config(config: UnboundedChatCompletionContextConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → UnboundedChatCompletionContextConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of UnboundedChatCompletionContextConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.UnboundedChatCompletionContext'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
Get at most buffer_size recent messages.

```python
classmethod _from_config(config: UnboundedChatCompletionContextConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → UnboundedChatCompletionContextConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of UnboundedChatCompletionContextConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.UnboundedChatCompletionContext'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Get at most buffer_size recent messages.

【中文翻译】previous

【中文翻译】autogen_core.models

【中文翻译】next

【中文翻译】autogen_core.tools

### autogen_core.model_context {autogen_coremodel_context}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.model_context.html)

```python
class BufferedChatCompletionContext(buffer_size: int, initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionContext, Component[BufferedChatCompletionContextConfig]
A buffered chat completion context that keeps a view of the last n messages,
where n is the buffer size. The buffer size is set at initialization.

Parameters:

buffer_size (int) – The size of the buffer.
initial_messages (List[LLMMessage] | None) – The initial messages.





classmethod _from_config(config: BufferedChatCompletionContextConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → BufferedChatCompletionContextConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of BufferedChatCompletionContextConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.BufferedChatCompletionContext'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
Get at most buffer_size recent messages.

```python
classmethod _from_config(config: BufferedChatCompletionContextConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → BufferedChatCompletionContextConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of BufferedChatCompletionContextConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.BufferedChatCompletionContext'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Get at most buffer_size recent messages.

```python
class ChatCompletionContext(initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
An abstract base class for defining the interface of a chat completion context.
A chat completion context lets agents store and retrieve LLM messages.
It can be implemented with different recall strategies.

Parameters:
initial_messages (List[LLMMessage] | None) – The initial messages.


Example
To create a custom model context that filters out the thought field from AssistantMessage.
This is useful for reasoning models like DeepSeek R1, which produces
very long thought that is not needed for subsequent completions.
from typing import List

from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, LLMMessage


class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out: List[LLMMessage] = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out




async add_message(message: Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]) → None[source]#
Add a message to the context.



async clear() → None[source]#
Clear the context.



component_type: ClassVar[ComponentType] = 'chat_completion_context'#
The logical type of the component.



abstract async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#



async load_state(state: Mapping[str, Any]) → None[source]#



async save_state() → Mapping[str, Any][source]#

**示例**:
```python
from typing import List

from autogen_core.model_context import UnboundedChatCompletionContext
from autogen_core.models import AssistantMessage, LLMMessage


class ReasoningModelContext(UnboundedChatCompletionContext):
    """A model context for reasoning models."""

    async def get_messages(self) -> List[LLMMessage]:
        messages = await super().get_messages()
        # Filter out thought field from AssistantMessage.
        messages_out: List[LLMMessage] = []
        for message in messages:
            if isinstance(message, AssistantMessage):
                message.thought = None
            messages_out.append(message)
        return messages_out

```

```python
async add_message(message: Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]) → None[source]#
```

【中文翻译】Add a message to the context.

```python
async clear() → None[source]#
```

【中文翻译】Clear the context.

```python
component_type: ClassVar[ComponentType] = 'chat_completion_context'#
```

【中文翻译】The logical type of the component.

```python
abstract async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

```python
async save_state() → Mapping[str, Any][source]#
```

```python
pydantic model ChatCompletionContextState[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "ChatCompletionContextState",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "discriminator": {
               "mapping": {
                  "AssistantMessage": "#/$defs/AssistantMessage",
                  "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                  "SystemMessage": "#/$defs/SystemMessage",
                  "UserMessage": "#/$defs/UserMessage"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/SystemMessage"
               },
               {
                  "$ref": "#/$defs/UserMessage"
               },
               {
                  "$ref": "#/$defs/AssistantMessage"
               },
               {
                  "$ref": "#/$defs/FunctionExecutionResultMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      }
   },
   "$defs": {
      "AssistantMessage": {
         "description": "Assistant message are sampled from the language model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "$ref": "#/$defs/FunctionCall"
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "thought": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Thought"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "AssistantMessage",
               "default": "AssistantMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "AssistantMessage",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "FunctionExecutionResultMessage": {
         "description": "Function execution result message contains the output of multiple function calls.",
         "properties": {
            "content": {
               "items": {
                  "$ref": "#/$defs/FunctionExecutionResult"
               },
               "title": "Content",
               "type": "array"
            },
            "type": {
               "const": "FunctionExecutionResultMessage",
               "default": "FunctionExecutionResultMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "FunctionExecutionResultMessage",
         "type": "object"
      },
      "SystemMessage": {
         "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "type": {
               "const": "SystemMessage",
               "default": "SystemMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "SystemMessage",
         "type": "object"
      },
      "UserMessage": {
         "description": "User message contains input from end users, or a catch-all for data provided to the model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "anyOf": [
                           {
                              "type": "string"
                           },
                           {}
                        ]
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "UserMessage",
               "default": "UserMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "UserMessage",
         "type": "object"
      }
   }
}



Fields:

messages (List[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage])





field messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Optional]#

**示例**:
```python
{
   "title": "ChatCompletionContextState",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "discriminator": {
               "mapping": {
                  "AssistantMessage": "#/$defs/AssistantMessage",
                  "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage",
                  "SystemMessage": "#/$defs/SystemMessage",
                  "UserMessage": "#/$defs/UserMessage"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/SystemMessage"
               },
               {
                  "$ref": "#/$defs/UserMessage"
               },
               {
                  "$ref": "#/$defs/AssistantMessage"
               },
               {
                  "$ref": "#/$defs/FunctionExecutionResultMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      }
   },
   "$defs": {
      "AssistantMessage": {
         "description": "Assistant message are sampled from the language model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "$ref": "#/$defs/FunctionCall"
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "thought": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Thought"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "AssistantMessage",
               "default": "AssistantMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "AssistantMessage",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      },
      "FunctionExecutionResultMessage": {
         "description": "Function execution result message contains the output of multiple function calls.",
         "properties": {
            "content": {
               "items": {
                  "$ref": "#/$defs/FunctionExecutionResult"
               },
               "title": "Content",
               "type": "array"
            },
            "type": {
               "const": "FunctionExecutionResultMessage",
               "default": "FunctionExecutionResultMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "FunctionExecutionResultMessage",
         "type": "object"
      },
      "SystemMessage": {
         "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "type": {
               "const": "SystemMessage",
               "default": "SystemMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "SystemMessage",
         "type": "object"
      },
      "UserMessage": {
         "description": "User message contains input from end users, or a catch-all for data provided to the model.",
         "properties": {
            "content": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "items": {
                        "anyOf": [
                           {
                              "type": "string"
                           },
                           {}
                        ]
                     },
                     "type": "array"
                  }
               ],
               "title": "Content"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "type": {
               "const": "UserMessage",
               "default": "UserMessage",
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "content",
            "source"
         ],
         "title": "UserMessage",
         "type": "object"
      }
   }
}

```

```python
field messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Optional]#
```

```python
class HeadAndTailChatCompletionContext(head_size: int, tail_size: int, initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionContext, Component[HeadAndTailChatCompletionContextConfig]
A chat completion context that keeps a view of the first n and last m messages,
where n is the head size and m is the tail size. The head and tail sizes
are set at initialization.

Parameters:

head_size (int) – The size of the head.
tail_size (int) – The size of the tail.
initial_messages (List[LLMMessage] | None) – The initial messages.





classmethod _from_config(config: HeadAndTailChatCompletionContextConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → HeadAndTailChatCompletionContextConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of HeadAndTailChatCompletionContextConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.HeadAndTailChatCompletionContext'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
Get at most head_size recent messages and tail_size oldest messages.

```python
classmethod _from_config(config: HeadAndTailChatCompletionContextConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → HeadAndTailChatCompletionContextConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of HeadAndTailChatCompletionContextConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.HeadAndTailChatCompletionContext'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Get at most head_size recent messages and tail_size oldest messages.

```python
class TokenLimitedChatCompletionContext(model_client: ChatCompletionClient, *, token_limit: int | None = None, tool_schema: List[ToolSchema] | None = None, initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionContext, Component[TokenLimitedChatCompletionContextConfig]
(Experimental) A token based chat completion context maintains a view of the context up to a token limit.

Note
Added in v0.4.10. This is an experimental component and may change in the future.


Parameters:

model_client (ChatCompletionClient) – The model client to use for token counting.
The model client must implement the count_tokens()
and remaining_tokens() methods.
token_limit (int | None) – The maximum number of tokens to keep in the context
using the count_tokens() method.
If None, the context will be limited by the model client using the
remaining_tokens() method.
tools (List[ToolSchema] | None) – A list of tool schema to use in the context.
initial_messages (List[LLMMessage] | None) – A list of initial messages to include in the context.





classmethod _from_config(config: TokenLimitedChatCompletionContextConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → TokenLimitedChatCompletionContextConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of TokenLimitedChatCompletionContextConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.TokenLimitedChatCompletionContext'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
Get at most token_limit tokens in recent messages. If the token limit is not
provided, then return as many messages as the remaining token allowed by the model client.

```python
classmethod _from_config(config: TokenLimitedChatCompletionContextConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → TokenLimitedChatCompletionContextConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of TokenLimitedChatCompletionContextConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.TokenLimitedChatCompletionContext'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Get at most token_limit tokens in recent messages. If the token limit is not
provided, then return as many messages as the remaining token allowed by the model client.

```python
class UnboundedChatCompletionContext(initial_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionContext, Component[UnboundedChatCompletionContextConfig]
An unbounded chat completion context that keeps a view of the all the messages.


classmethod _from_config(config: UnboundedChatCompletionContextConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → UnboundedChatCompletionContextConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of UnboundedChatCompletionContextConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.UnboundedChatCompletionContext'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
Get at most buffer_size recent messages.

```python
classmethod _from_config(config: UnboundedChatCompletionContextConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → UnboundedChatCompletionContextConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of UnboundedChatCompletionContextConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.model_context.UnboundedChatCompletionContext'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async get_messages() → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Get at most buffer_size recent messages.

【中文翻译】previous

【中文翻译】autogen_core.models

【中文翻译】next

【中文翻译】autogen_core.tools

### autogen_core.models {autogen_coremodels}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html)

```python
pydantic model AssistantMessage[source]#
```

【中文翻译】Bases: BaseModel
Assistant message are sampled from the language model.

Show JSON schema{
   "title": "AssistantMessage",
   "description": "Assistant message are sampled from the language model.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "$ref": "#/$defs/FunctionCall"
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "thought": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Thought"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "type": {
         "const": "AssistantMessage",
         "default": "AssistantMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      }
   },
   "required": [
      "content",
      "source"
   ]
}



Fields:

content (str | List[autogen_core._types.FunctionCall])
source (str)
thought (str | None)
type (Literal['AssistantMessage'])





field content: str | List[FunctionCall] [Required]#
The content of the message.



field source: str [Required]#
The name of the agent that sent this message.



field thought: str | None = None#
The reasoning text for the completion if available. Used for reasoning model and additional text content besides function calls.



field type: Literal['AssistantMessage'] = 'AssistantMessage'#

**示例**:
```python
{
   "title": "AssistantMessage",
   "description": "Assistant message are sampled from the language model.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "$ref": "#/$defs/FunctionCall"
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "thought": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Thought"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "type": {
         "const": "AssistantMessage",
         "default": "AssistantMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      }
   },
   "required": [
      "content",
      "source"
   ]
}

```

```python
field content: str | List[FunctionCall] [Required]#
```

【中文翻译】The content of the message.

```python
field source: str [Required]#
```

【中文翻译】The name of the agent that sent this message.

```python
field thought: str | None = None#
```

【中文翻译】The reasoning text for the completion if available. Used for reasoning model and additional text content besides function calls.

```python
field type: Literal['AssistantMessage'] = 'AssistantMessage'#
```

```python
class ChatCompletionClient[source]#
```

【中文翻译】Bases: ComponentBase[BaseModel], ABC


abstract actual_usage() → RequestUsage[source]#



abstract property capabilities: ModelCapabilities#



abstract async close() → None[source]#



abstract count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



abstract async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





abstract create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.





abstract property model_info: ModelInfo#



abstract remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



abstract total_usage() → RequestUsage[source]#

```python
abstract actual_usage() → RequestUsage[source]#
```

```python
abstract property capabilities: ModelCapabilities#
```

```python
abstract async close() → None[source]#
```

```python
abstract count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
abstract async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
abstract create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.

```python
abstract property model_info: ModelInfo#
```

```python
abstract remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
abstract total_usage() → RequestUsage[source]#
```

```python
pydantic model ChatCompletionTokenLogprob[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "ChatCompletionTokenLogprob",
   "type": "object",
   "properties": {
      "token": {
         "title": "Token",
         "type": "string"
      },
      "logprob": {
         "title": "Logprob",
         "type": "number"
      },
      "top_logprobs": {
         "anyOf": [
            {
               "items": {
                  "$ref": "#/$defs/TopLogprob"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top Logprobs"
      },
      "bytes": {
         "anyOf": [
            {
               "items": {
                  "type": "integer"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Bytes"
      }
   },
   "$defs": {
      "TopLogprob": {
         "properties": {
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "logprob"
         ],
         "title": "TopLogprob",
         "type": "object"
      }
   },
   "required": [
      "token",
      "logprob"
   ]
}



Fields:

bytes (List[int] | None)
logprob (float)
token (str)
top_logprobs (List[autogen_core.models._types.TopLogprob] | None)





field bytes: List[int] | None = None#



field logprob: float [Required]#



field token: str [Required]#



field top_logprobs: List[TopLogprob] | None = None#

**示例**:
```python
{
   "title": "ChatCompletionTokenLogprob",
   "type": "object",
   "properties": {
      "token": {
         "title": "Token",
         "type": "string"
      },
      "logprob": {
         "title": "Logprob",
         "type": "number"
      },
      "top_logprobs": {
         "anyOf": [
            {
               "items": {
                  "$ref": "#/$defs/TopLogprob"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top Logprobs"
      },
      "bytes": {
         "anyOf": [
            {
               "items": {
                  "type": "integer"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Bytes"
      }
   },
   "$defs": {
      "TopLogprob": {
         "properties": {
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "logprob"
         ],
         "title": "TopLogprob",
         "type": "object"
      }
   },
   "required": [
      "token",
      "logprob"
   ]
}

```

```python
field bytes: List[int] | None = None#
```

```python
field logprob: float [Required]#
```

```python
field token: str [Required]#
```

```python
field top_logprobs: List[TopLogprob] | None = None#
```

```python
pydantic model CreateResult[source]#
```

【中文翻译】Bases: BaseModel
Create result contains the output of a model completion.

Show JSON schema{
   "title": "CreateResult",
   "description": "Create result contains the output of a model completion.",
   "type": "object",
   "properties": {
      "finish_reason": {
         "enum": [
            "stop",
            "length",
            "function_calls",
            "content_filter",
            "unknown"
         ],
         "title": "Finish Reason",
         "type": "string"
      },
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "$ref": "#/$defs/FunctionCall"
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "usage": {
         "$ref": "#/$defs/RequestUsage"
      },
      "cached": {
         "title": "Cached",
         "type": "boolean"
      },
      "logprobs": {
         "anyOf": [
            {
               "items": {
                  "$ref": "#/$defs/ChatCompletionTokenLogprob"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logprobs"
      },
      "thought": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Thought"
      }
   },
   "$defs": {
      "ChatCompletionTokenLogprob": {
         "properties": {
            "token": {
               "title": "Token",
               "type": "string"
            },
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "top_logprobs": {
               "anyOf": [
                  {
                     "items": {
                        "$ref": "#/$defs/TopLogprob"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top Logprobs"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "token",
            "logprob"
         ],
         "title": "ChatCompletionTokenLogprob",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      },
      "TopLogprob": {
         "properties": {
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "logprob"
         ],
         "title": "TopLogprob",
         "type": "object"
      }
   },
   "required": [
      "finish_reason",
      "content",
      "usage",
      "cached"
   ]
}



Fields:

cached (bool)
content (str | List[autogen_core._types.FunctionCall])
finish_reason (Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown'])
logprobs (List[autogen_core.models._types.ChatCompletionTokenLogprob] | None)
thought (str | None)
usage (autogen_core.models._types.RequestUsage)





field cached: bool [Required]#
Whether the completion was generated from a cached response.



field content: str | List[FunctionCall] [Required]#
The output of the model completion.



field finish_reason: Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown'] [Required]#
The reason the model finished generating the completion.



field logprobs: List[ChatCompletionTokenLogprob] | None = None#
The logprobs of the tokens in the completion.



field thought: str | None = None#
The reasoning text for the completion if available. Used for reasoning models
and additional text content besides function calls.



field usage: RequestUsage [Required]#
The usage of tokens in the prompt and completion.

**示例**:
```python
{
   "title": "CreateResult",
   "description": "Create result contains the output of a model completion.",
   "type": "object",
   "properties": {
      "finish_reason": {
         "enum": [
            "stop",
            "length",
            "function_calls",
            "content_filter",
            "unknown"
         ],
         "title": "Finish Reason",
         "type": "string"
      },
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "$ref": "#/$defs/FunctionCall"
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "usage": {
         "$ref": "#/$defs/RequestUsage"
      },
      "cached": {
         "title": "Cached",
         "type": "boolean"
      },
      "logprobs": {
         "anyOf": [
            {
               "items": {
                  "$ref": "#/$defs/ChatCompletionTokenLogprob"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logprobs"
      },
      "thought": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Thought"
      }
   },
   "$defs": {
      "ChatCompletionTokenLogprob": {
         "properties": {
            "token": {
               "title": "Token",
               "type": "string"
            },
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "top_logprobs": {
               "anyOf": [
                  {
                     "items": {
                        "$ref": "#/$defs/TopLogprob"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top Logprobs"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "token",
            "logprob"
         ],
         "title": "ChatCompletionTokenLogprob",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      },
      "TopLogprob": {
         "properties": {
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "logprob"
         ],
         "title": "TopLogprob",
         "type": "object"
      }
   },
   "required": [
      "finish_reason",
      "content",
      "usage",
      "cached"
   ]
}

```

```python
field cached: bool [Required]#
```

【中文翻译】Whether the completion was generated from a cached response.

```python
field content: str | List[FunctionCall] [Required]#
```

【中文翻译】The output of the model completion.

```python
field finish_reason: Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown'] [Required]#
```

【中文翻译】The reason the model finished generating the completion.

```python
field logprobs: List[ChatCompletionTokenLogprob] | None = None#
```

【中文翻译】The logprobs of the tokens in the completion.

```python
field thought: str | None = None#
```

【中文翻译】The reasoning text for the completion if available. Used for reasoning models
and additional text content besides function calls.

```python
field usage: RequestUsage [Required]#
```

【中文翻译】The usage of tokens in the prompt and completion.

```python
pydantic model FunctionExecutionResult[source]#
```

【中文翻译】Bases: BaseModel
Function execution result contains the output of a function call.

Show JSON schema{
   "title": "FunctionExecutionResult",
   "description": "Function execution result contains the output of a function call.",
   "type": "object",
   "properties": {
      "content": {
         "title": "Content",
         "type": "string"
      },
      "name": {
         "title": "Name",
         "type": "string"
      },
      "call_id": {
         "title": "Call Id",
         "type": "string"
      },
      "is_error": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Is Error"
      }
   },
   "required": [
      "content",
      "name",
      "call_id"
   ]
}



Fields:

call_id (str)
content (str)
is_error (bool | None)
name (str)





field call_id: str [Required]#
The ID of the function call. Note this ID may be empty for some models.



field content: str [Required]#
The output of the function call.



field is_error: bool | None = None#
Whether the function call resulted in an error.



field name: str [Required]#
(New in v0.4.8) The name of the function that was called.

**示例**:
```python
{
   "title": "FunctionExecutionResult",
   "description": "Function execution result contains the output of a function call.",
   "type": "object",
   "properties": {
      "content": {
         "title": "Content",
         "type": "string"
      },
      "name": {
         "title": "Name",
         "type": "string"
      },
      "call_id": {
         "title": "Call Id",
         "type": "string"
      },
      "is_error": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Is Error"
      }
   },
   "required": [
      "content",
      "name",
      "call_id"
   ]
}

```

```python
field call_id: str [Required]#
```

【中文翻译】The ID of the function call. Note this ID may be empty for some models.

```python
field content: str [Required]#
```

【中文翻译】The output of the function call.

```python
field is_error: bool | None = None#
```

【中文翻译】Whether the function call resulted in an error.

```python
field name: str [Required]#
```

【中文翻译】(New in v0.4.8) The name of the function that was called.

```python
pydantic model FunctionExecutionResultMessage[source]#
```

【中文翻译】Bases: BaseModel
Function execution result message contains the output of multiple function calls.

Show JSON schema{
   "title": "FunctionExecutionResultMessage",
   "description": "Function execution result message contains the output of multiple function calls.",
   "type": "object",
   "properties": {
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionExecutionResult"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "FunctionExecutionResultMessage",
         "default": "FunctionExecutionResultMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      }
   },
   "required": [
      "content"
   ]
}



Fields:

content (List[autogen_core.models._types.FunctionExecutionResult])
type (Literal['FunctionExecutionResultMessage'])





field content: List[FunctionExecutionResult] [Required]#



field type: Literal['FunctionExecutionResultMessage'] = 'FunctionExecutionResultMessage'#

**示例**:
```python
{
   "title": "FunctionExecutionResultMessage",
   "description": "Function execution result message contains the output of multiple function calls.",
   "type": "object",
   "properties": {
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionExecutionResult"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "FunctionExecutionResultMessage",
         "default": "FunctionExecutionResultMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      }
   },
   "required": [
      "content"
   ]
}

```

```python
field content: List[FunctionExecutionResult] [Required]#
```

```python
field type: Literal['FunctionExecutionResultMessage'] = 'FunctionExecutionResultMessage'#
```

```python
class ModelCapabilities(**kwargs)[source]#
```

【中文翻译】Bases: TypedDict


function_calling: Required[bool]#



json_output: Required[bool]#



vision: Required[bool]#

```python
function_calling: Required[bool]#
```

```python
json_output: Required[bool]#
```

```python
vision: Required[bool]#
```

```python
class ModelFamily(*args: Any, **kwargs: Any)[source]#
```

【中文翻译】Bases: object
A model family is a group of models that share similar characteristics from a capabilities perspective. This is different to discrete supported features such as vision, function calling, and JSON output.
This namespace class holds constants for the model families that AutoGen understands. Other families definitely exist and can be represented by a string, however, AutoGen will treat them as unknown.


ANY#
alias of Literal[‘gpt-41’, ‘gpt-45’, ‘gpt-4o’, ‘o1’, ‘o3’, ‘o4’, ‘gpt-4’, ‘gpt-35’, ‘r1’, ‘gemini-1.5-flash’, ‘gemini-1.5-pro’, ‘gemini-2.0-flash’, ‘gemini-2.5-pro’, ‘claude-3-haiku’, ‘claude-3-sonnet’, ‘claude-3-opus’, ‘claude-3-5-haiku’, ‘claude-3-5-sonnet’, ‘claude-3-7-sonnet’, ‘unknown’]



CLAUDE_3_5_HAIKU = 'claude-3-5-haiku'#



CLAUDE_3_5_SONNET = 'claude-3-5-sonnet'#



CLAUDE_3_7_SONNET = 'claude-3-7-sonnet'#



CLAUDE_3_HAIKU = 'claude-3-haiku'#



CLAUDE_3_OPUS = 'claude-3-opus'#



CLAUDE_3_SONNET = 'claude-3-sonnet'#



GEMINI_1_5_FLASH = 'gemini-1.5-flash'#



GEMINI_1_5_PRO = 'gemini-1.5-pro'#



GEMINI_2_0_FLASH = 'gemini-2.0-flash'#



GEMINI_2_5_PRO = 'gemini-2.5-pro'#



GPT_35 = 'gpt-35'#



GPT_4 = 'gpt-4'#



GPT_41 = 'gpt-41'#



GPT_45 = 'gpt-45'#



GPT_4O = 'gpt-4o'#



O1 = 'o1'#



O3 = 'o3'#



O4 = 'o4'#



R1 = 'r1'#



UNKNOWN = 'unknown'#



static is_claude(family: str) → bool[source]#



static is_gemini(family: str) → bool[source]#



static is_openai(family: str) → bool[source]#

```python
ANY#
```

【中文翻译】alias of Literal[‘gpt-41’, ‘gpt-45’, ‘gpt-4o’, ‘o1’, ‘o3’, ‘o4’, ‘gpt-4’, ‘gpt-35’, ‘r1’, ‘gemini-1.5-flash’, ‘gemini-1.5-pro’, ‘gemini-2.0-flash’, ‘gemini-2.5-pro’, ‘claude-3-haiku’, ‘claude-3-sonnet’, ‘claude-3-opus’, ‘claude-3-5-haiku’, ‘claude-3-5-sonnet’, ‘claude-3-7-sonnet’, ‘unknown’]

```python
CLAUDE_3_5_HAIKU = 'claude-3-5-haiku'#
```

```python
CLAUDE_3_5_SONNET = 'claude-3-5-sonnet'#
```

```python
CLAUDE_3_7_SONNET = 'claude-3-7-sonnet'#
```

```python
CLAUDE_3_HAIKU = 'claude-3-haiku'#
```

```python
CLAUDE_3_OPUS = 'claude-3-opus'#
```

```python
CLAUDE_3_SONNET = 'claude-3-sonnet'#
```

```python
GEMINI_1_5_FLASH = 'gemini-1.5-flash'#
```

```python
GEMINI_1_5_PRO = 'gemini-1.5-pro'#
```

```python
GEMINI_2_0_FLASH = 'gemini-2.0-flash'#
```

```python
GEMINI_2_5_PRO = 'gemini-2.5-pro'#
```

```python
GPT_35 = 'gpt-35'#
```

```python
GPT_4 = 'gpt-4'#
```

```python
GPT_41 = 'gpt-41'#
```

```python
GPT_45 = 'gpt-45'#
```

```python
GPT_4O = 'gpt-4o'#
```

```python
O1 = 'o1'#
```

```python
O3 = 'o3'#
```

```python
O4 = 'o4'#
```

```python
R1 = 'r1'#
```

```python
UNKNOWN = 'unknown'#
```

```python
static is_claude(family: str) → bool[source]#
```

```python
static is_gemini(family: str) → bool[source]#
```

```python
static is_openai(family: str) → bool[source]#
```

```python
class ModelInfo[source]#
```

【中文翻译】Bases: TypedDict
ModelInfo is a dictionary that contains information about a model’s properties.
It is expected to be used in the model_info property of a model client.
We are expecting this to grow over time as we add more features.


family: Required[Literal['gpt-41', 'gpt-45', 'gpt-4o', 'o1', 'o3', 'o4', 'gpt-4', 'gpt-35', 'r1', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-2.5-pro', 'claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus', 'claude-3-5-haiku', 'claude-3-5-sonnet', 'claude-3-7-sonnet', 'unknown'] | str]#
Model family should be one of the constants from ModelFamily or a string representing an unknown model family.



function_calling: Required[bool]#
True if the model supports function calling, otherwise False.



json_output: Required[bool]#
this is different to structured json.

Type:
True if the model supports json output, otherwise False. Note





multiple_system_messages: bool | None#
True if the model supports multiple, non-consecutive system messages, otherwise False.



structured_output: Required[bool]#
True if the model supports structured output, otherwise False. This is different to json_output.



vision: Required[bool]#
True if the model supports vision, aka image input, otherwise False.

```python
family: Required[Literal['gpt-41', 'gpt-45', 'gpt-4o', 'o1', 'o3', 'o4', 'gpt-4', 'gpt-35', 'r1', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-2.5-pro', 'claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus', 'claude-3-5-haiku', 'claude-3-5-sonnet', 'claude-3-7-sonnet', 'unknown'] | str]#
```

【中文翻译】Model family should be one of the constants from ModelFamily or a string representing an unknown model family.

```python
function_calling: Required[bool]#
```

【中文翻译】True if the model supports function calling, otherwise False.

```python
json_output: Required[bool]#
```

【中文翻译】this is different to structured json.

Type:
True if the model supports json output, otherwise False. Note

```python
multiple_system_messages: bool | None#
```

【中文翻译】True if the model supports multiple, non-consecutive system messages, otherwise False.

```python
structured_output: Required[bool]#
```

【中文翻译】True if the model supports structured output, otherwise False. This is different to json_output.

```python
vision: Required[bool]#
```

【中文翻译】True if the model supports vision, aka image input, otherwise False.

```python
class RequestUsage(prompt_tokens: int, completion_tokens: int)[source]#
```

【中文翻译】Bases: object


completion_tokens: int#



prompt_tokens: int#

```python
completion_tokens: int#
```

```python
prompt_tokens: int#
```

```python
pydantic model SystemMessage[source]#
```

【中文翻译】Bases: BaseModel
System message contains instructions for the model coming from the developer.

Note
Open AI is moving away from using ‘system’ role in favor of ‘developer’ role.
See Model Spec for more details.
However, the ‘system’ role is still allowed in their API and will be automatically converted to ‘developer’ role
on the server side.
So, you can use SystemMessage for developer messages.


Show JSON schema{
   "title": "SystemMessage",
   "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
   "type": "object",
   "properties": {
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "SystemMessage",
         "default": "SystemMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "required": [
      "content"
   ]
}



Fields:

content (str)
type (Literal['SystemMessage'])





field content: str [Required]#
The content of the message.



field type: Literal['SystemMessage'] = 'SystemMessage'#

**示例**:
```python
{
   "title": "SystemMessage",
   "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
   "type": "object",
   "properties": {
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "SystemMessage",
         "default": "SystemMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "required": [
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】The content of the message.

```python
field type: Literal['SystemMessage'] = 'SystemMessage'#
```

```python
class TopLogprob(logprob: float, bytes: List[int] | None = None)[source]#
```

【中文翻译】Bases: object


bytes: List[int] | None = None#



logprob: float#

```python
bytes: List[int] | None = None#
```

```python
logprob: float#
```

```python
pydantic model UserMessage[source]#
```

【中文翻译】Bases: BaseModel
User message contains input from end users, or a catch-all for data provided to the model.

Show JSON schema{
   "title": "UserMessage",
   "description": "User message contains input from end users, or a catch-all for data provided to the model.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "anyOf": [
                     {
                        "type": "string"
                     },
                     {}
                  ]
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "type": {
         "const": "UserMessage",
         "default": "UserMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "required": [
      "content",
      "source"
   ]
}



Fields:

content (str | List[str | autogen_core._image.Image])
source (str)
type (Literal['UserMessage'])





field content: str | List[str | Image] [Required]#
The content of the message.



field source: str [Required]#
The name of the agent that sent this message.



field type: Literal['UserMessage'] = 'UserMessage'#

**示例**:
```python
{
   "title": "UserMessage",
   "description": "User message contains input from end users, or a catch-all for data provided to the model.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "anyOf": [
                     {
                        "type": "string"
                     },
                     {}
                  ]
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "type": {
         "const": "UserMessage",
         "default": "UserMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "required": [
      "content",
      "source"
   ]
}

```

```python
field content: str | List[str | Image] [Required]#
```

【中文翻译】The content of the message.

```python
field source: str [Required]#
```

【中文翻译】The name of the agent that sent this message.

```python
field type: Literal['UserMessage'] = 'UserMessage'#
```

```python
validate_model_info(model_info: ModelInfo) → None[source]#
```

【中文翻译】Validates the model info dictionary.

Raises:
ValueError – If the model info dictionary is missing required fields.

【中文翻译】previous

【中文翻译】autogen_core.code_executor

【中文翻译】next

【中文翻译】autogen_core.model_context

### autogen_core.models {autogen_coremodels}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html)

```python
pydantic model AssistantMessage[source]#
```

【中文翻译】Bases: BaseModel
Assistant message are sampled from the language model.

Show JSON schema{
   "title": "AssistantMessage",
   "description": "Assistant message are sampled from the language model.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "$ref": "#/$defs/FunctionCall"
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "thought": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Thought"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "type": {
         "const": "AssistantMessage",
         "default": "AssistantMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      }
   },
   "required": [
      "content",
      "source"
   ]
}



Fields:

content (str | List[autogen_core._types.FunctionCall])
source (str)
thought (str | None)
type (Literal['AssistantMessage'])





field content: str | List[FunctionCall] [Required]#
The content of the message.



field source: str [Required]#
The name of the agent that sent this message.



field thought: str | None = None#
The reasoning text for the completion if available. Used for reasoning model and additional text content besides function calls.



field type: Literal['AssistantMessage'] = 'AssistantMessage'#

**示例**:
```python
{
   "title": "AssistantMessage",
   "description": "Assistant message are sampled from the language model.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "$ref": "#/$defs/FunctionCall"
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "thought": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Thought"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "type": {
         "const": "AssistantMessage",
         "default": "AssistantMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      }
   },
   "required": [
      "content",
      "source"
   ]
}

```

```python
field content: str | List[FunctionCall] [Required]#
```

【中文翻译】The content of the message.

```python
field source: str [Required]#
```

【中文翻译】The name of the agent that sent this message.

```python
field thought: str | None = None#
```

【中文翻译】The reasoning text for the completion if available. Used for reasoning model and additional text content besides function calls.

```python
field type: Literal['AssistantMessage'] = 'AssistantMessage'#
```

```python
class ChatCompletionClient[source]#
```

【中文翻译】Bases: ComponentBase[BaseModel], ABC


abstract actual_usage() → RequestUsage[source]#



abstract property capabilities: ModelCapabilities#



abstract async close() → None[source]#



abstract count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



abstract async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





abstract create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.





abstract property model_info: ModelInfo#



abstract remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



abstract total_usage() → RequestUsage[source]#

```python
abstract actual_usage() → RequestUsage[source]#
```

```python
abstract property capabilities: ModelCapabilities#
```

```python
abstract async close() → None[source]#
```

```python
abstract count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
abstract async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
abstract create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.

```python
abstract property model_info: ModelInfo#
```

```python
abstract remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
abstract total_usage() → RequestUsage[source]#
```

```python
pydantic model ChatCompletionTokenLogprob[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "ChatCompletionTokenLogprob",
   "type": "object",
   "properties": {
      "token": {
         "title": "Token",
         "type": "string"
      },
      "logprob": {
         "title": "Logprob",
         "type": "number"
      },
      "top_logprobs": {
         "anyOf": [
            {
               "items": {
                  "$ref": "#/$defs/TopLogprob"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top Logprobs"
      },
      "bytes": {
         "anyOf": [
            {
               "items": {
                  "type": "integer"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Bytes"
      }
   },
   "$defs": {
      "TopLogprob": {
         "properties": {
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "logprob"
         ],
         "title": "TopLogprob",
         "type": "object"
      }
   },
   "required": [
      "token",
      "logprob"
   ]
}



Fields:

bytes (List[int] | None)
logprob (float)
token (str)
top_logprobs (List[autogen_core.models._types.TopLogprob] | None)





field bytes: List[int] | None = None#



field logprob: float [Required]#



field token: str [Required]#



field top_logprobs: List[TopLogprob] | None = None#

**示例**:
```python
{
   "title": "ChatCompletionTokenLogprob",
   "type": "object",
   "properties": {
      "token": {
         "title": "Token",
         "type": "string"
      },
      "logprob": {
         "title": "Logprob",
         "type": "number"
      },
      "top_logprobs": {
         "anyOf": [
            {
               "items": {
                  "$ref": "#/$defs/TopLogprob"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top Logprobs"
      },
      "bytes": {
         "anyOf": [
            {
               "items": {
                  "type": "integer"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Bytes"
      }
   },
   "$defs": {
      "TopLogprob": {
         "properties": {
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "logprob"
         ],
         "title": "TopLogprob",
         "type": "object"
      }
   },
   "required": [
      "token",
      "logprob"
   ]
}

```

```python
field bytes: List[int] | None = None#
```

```python
field logprob: float [Required]#
```

```python
field token: str [Required]#
```

```python
field top_logprobs: List[TopLogprob] | None = None#
```

```python
pydantic model CreateResult[source]#
```

【中文翻译】Bases: BaseModel
Create result contains the output of a model completion.

Show JSON schema{
   "title": "CreateResult",
   "description": "Create result contains the output of a model completion.",
   "type": "object",
   "properties": {
      "finish_reason": {
         "enum": [
            "stop",
            "length",
            "function_calls",
            "content_filter",
            "unknown"
         ],
         "title": "Finish Reason",
         "type": "string"
      },
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "$ref": "#/$defs/FunctionCall"
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "usage": {
         "$ref": "#/$defs/RequestUsage"
      },
      "cached": {
         "title": "Cached",
         "type": "boolean"
      },
      "logprobs": {
         "anyOf": [
            {
               "items": {
                  "$ref": "#/$defs/ChatCompletionTokenLogprob"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logprobs"
      },
      "thought": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Thought"
      }
   },
   "$defs": {
      "ChatCompletionTokenLogprob": {
         "properties": {
            "token": {
               "title": "Token",
               "type": "string"
            },
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "top_logprobs": {
               "anyOf": [
                  {
                     "items": {
                        "$ref": "#/$defs/TopLogprob"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top Logprobs"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "token",
            "logprob"
         ],
         "title": "ChatCompletionTokenLogprob",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      },
      "TopLogprob": {
         "properties": {
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "logprob"
         ],
         "title": "TopLogprob",
         "type": "object"
      }
   },
   "required": [
      "finish_reason",
      "content",
      "usage",
      "cached"
   ]
}



Fields:

cached (bool)
content (str | List[autogen_core._types.FunctionCall])
finish_reason (Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown'])
logprobs (List[autogen_core.models._types.ChatCompletionTokenLogprob] | None)
thought (str | None)
usage (autogen_core.models._types.RequestUsage)





field cached: bool [Required]#
Whether the completion was generated from a cached response.



field content: str | List[FunctionCall] [Required]#
The output of the model completion.



field finish_reason: Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown'] [Required]#
The reason the model finished generating the completion.



field logprobs: List[ChatCompletionTokenLogprob] | None = None#
The logprobs of the tokens in the completion.



field thought: str | None = None#
The reasoning text for the completion if available. Used for reasoning models
and additional text content besides function calls.



field usage: RequestUsage [Required]#
The usage of tokens in the prompt and completion.

**示例**:
```python
{
   "title": "CreateResult",
   "description": "Create result contains the output of a model completion.",
   "type": "object",
   "properties": {
      "finish_reason": {
         "enum": [
            "stop",
            "length",
            "function_calls",
            "content_filter",
            "unknown"
         ],
         "title": "Finish Reason",
         "type": "string"
      },
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "$ref": "#/$defs/FunctionCall"
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "usage": {
         "$ref": "#/$defs/RequestUsage"
      },
      "cached": {
         "title": "Cached",
         "type": "boolean"
      },
      "logprobs": {
         "anyOf": [
            {
               "items": {
                  "$ref": "#/$defs/ChatCompletionTokenLogprob"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logprobs"
      },
      "thought": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Thought"
      }
   },
   "$defs": {
      "ChatCompletionTokenLogprob": {
         "properties": {
            "token": {
               "title": "Token",
               "type": "string"
            },
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "top_logprobs": {
               "anyOf": [
                  {
                     "items": {
                        "$ref": "#/$defs/TopLogprob"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top Logprobs"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "token",
            "logprob"
         ],
         "title": "ChatCompletionTokenLogprob",
         "type": "object"
      },
      "FunctionCall": {
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "arguments": {
               "title": "Arguments",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            }
         },
         "required": [
            "id",
            "arguments",
            "name"
         ],
         "title": "FunctionCall",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      },
      "TopLogprob": {
         "properties": {
            "logprob": {
               "title": "Logprob",
               "type": "number"
            },
            "bytes": {
               "anyOf": [
                  {
                     "items": {
                        "type": "integer"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Bytes"
            }
         },
         "required": [
            "logprob"
         ],
         "title": "TopLogprob",
         "type": "object"
      }
   },
   "required": [
      "finish_reason",
      "content",
      "usage",
      "cached"
   ]
}

```

```python
field cached: bool [Required]#
```

【中文翻译】Whether the completion was generated from a cached response.

```python
field content: str | List[FunctionCall] [Required]#
```

【中文翻译】The output of the model completion.

```python
field finish_reason: Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown'] [Required]#
```

【中文翻译】The reason the model finished generating the completion.

```python
field logprobs: List[ChatCompletionTokenLogprob] | None = None#
```

【中文翻译】The logprobs of the tokens in the completion.

```python
field thought: str | None = None#
```

【中文翻译】The reasoning text for the completion if available. Used for reasoning models
and additional text content besides function calls.

```python
field usage: RequestUsage [Required]#
```

【中文翻译】The usage of tokens in the prompt and completion.

```python
pydantic model FunctionExecutionResult[source]#
```

【中文翻译】Bases: BaseModel
Function execution result contains the output of a function call.

Show JSON schema{
   "title": "FunctionExecutionResult",
   "description": "Function execution result contains the output of a function call.",
   "type": "object",
   "properties": {
      "content": {
         "title": "Content",
         "type": "string"
      },
      "name": {
         "title": "Name",
         "type": "string"
      },
      "call_id": {
         "title": "Call Id",
         "type": "string"
      },
      "is_error": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Is Error"
      }
   },
   "required": [
      "content",
      "name",
      "call_id"
   ]
}



Fields:

call_id (str)
content (str)
is_error (bool | None)
name (str)





field call_id: str [Required]#
The ID of the function call. Note this ID may be empty for some models.



field content: str [Required]#
The output of the function call.



field is_error: bool | None = None#
Whether the function call resulted in an error.



field name: str [Required]#
(New in v0.4.8) The name of the function that was called.

**示例**:
```python
{
   "title": "FunctionExecutionResult",
   "description": "Function execution result contains the output of a function call.",
   "type": "object",
   "properties": {
      "content": {
         "title": "Content",
         "type": "string"
      },
      "name": {
         "title": "Name",
         "type": "string"
      },
      "call_id": {
         "title": "Call Id",
         "type": "string"
      },
      "is_error": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Is Error"
      }
   },
   "required": [
      "content",
      "name",
      "call_id"
   ]
}

```

```python
field call_id: str [Required]#
```

【中文翻译】The ID of the function call. Note this ID may be empty for some models.

```python
field content: str [Required]#
```

【中文翻译】The output of the function call.

```python
field is_error: bool | None = None#
```

【中文翻译】Whether the function call resulted in an error.

```python
field name: str [Required]#
```

【中文翻译】(New in v0.4.8) The name of the function that was called.

```python
pydantic model FunctionExecutionResultMessage[source]#
```

【中文翻译】Bases: BaseModel
Function execution result message contains the output of multiple function calls.

Show JSON schema{
   "title": "FunctionExecutionResultMessage",
   "description": "Function execution result message contains the output of multiple function calls.",
   "type": "object",
   "properties": {
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionExecutionResult"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "FunctionExecutionResultMessage",
         "default": "FunctionExecutionResultMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      }
   },
   "required": [
      "content"
   ]
}



Fields:

content (List[autogen_core.models._types.FunctionExecutionResult])
type (Literal['FunctionExecutionResultMessage'])





field content: List[FunctionExecutionResult] [Required]#



field type: Literal['FunctionExecutionResultMessage'] = 'FunctionExecutionResultMessage'#

**示例**:
```python
{
   "title": "FunctionExecutionResultMessage",
   "description": "Function execution result message contains the output of multiple function calls.",
   "type": "object",
   "properties": {
      "content": {
         "items": {
            "$ref": "#/$defs/FunctionExecutionResult"
         },
         "title": "Content",
         "type": "array"
      },
      "type": {
         "const": "FunctionExecutionResultMessage",
         "default": "FunctionExecutionResultMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "$defs": {
      "FunctionExecutionResult": {
         "description": "Function execution result contains the output of a function call.",
         "properties": {
            "content": {
               "title": "Content",
               "type": "string"
            },
            "name": {
               "title": "Name",
               "type": "string"
            },
            "call_id": {
               "title": "Call Id",
               "type": "string"
            },
            "is_error": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Is Error"
            }
         },
         "required": [
            "content",
            "name",
            "call_id"
         ],
         "title": "FunctionExecutionResult",
         "type": "object"
      }
   },
   "required": [
      "content"
   ]
}

```

```python
field content: List[FunctionExecutionResult] [Required]#
```

```python
field type: Literal['FunctionExecutionResultMessage'] = 'FunctionExecutionResultMessage'#
```

```python
class ModelCapabilities(**kwargs)[source]#
```

【中文翻译】Bases: TypedDict


function_calling: Required[bool]#



json_output: Required[bool]#



vision: Required[bool]#

```python
function_calling: Required[bool]#
```

```python
json_output: Required[bool]#
```

```python
vision: Required[bool]#
```

```python
class ModelFamily(*args: Any, **kwargs: Any)[source]#
```

【中文翻译】Bases: object
A model family is a group of models that share similar characteristics from a capabilities perspective. This is different to discrete supported features such as vision, function calling, and JSON output.
This namespace class holds constants for the model families that AutoGen understands. Other families definitely exist and can be represented by a string, however, AutoGen will treat them as unknown.


ANY#
alias of Literal[‘gpt-41’, ‘gpt-45’, ‘gpt-4o’, ‘o1’, ‘o3’, ‘o4’, ‘gpt-4’, ‘gpt-35’, ‘r1’, ‘gemini-1.5-flash’, ‘gemini-1.5-pro’, ‘gemini-2.0-flash’, ‘gemini-2.5-pro’, ‘claude-3-haiku’, ‘claude-3-sonnet’, ‘claude-3-opus’, ‘claude-3-5-haiku’, ‘claude-3-5-sonnet’, ‘claude-3-7-sonnet’, ‘unknown’]



CLAUDE_3_5_HAIKU = 'claude-3-5-haiku'#



CLAUDE_3_5_SONNET = 'claude-3-5-sonnet'#



CLAUDE_3_7_SONNET = 'claude-3-7-sonnet'#



CLAUDE_3_HAIKU = 'claude-3-haiku'#



CLAUDE_3_OPUS = 'claude-3-opus'#



CLAUDE_3_SONNET = 'claude-3-sonnet'#



GEMINI_1_5_FLASH = 'gemini-1.5-flash'#



GEMINI_1_5_PRO = 'gemini-1.5-pro'#



GEMINI_2_0_FLASH = 'gemini-2.0-flash'#



GEMINI_2_5_PRO = 'gemini-2.5-pro'#



GPT_35 = 'gpt-35'#



GPT_4 = 'gpt-4'#



GPT_41 = 'gpt-41'#



GPT_45 = 'gpt-45'#



GPT_4O = 'gpt-4o'#



O1 = 'o1'#



O3 = 'o3'#



O4 = 'o4'#



R1 = 'r1'#



UNKNOWN = 'unknown'#



static is_claude(family: str) → bool[source]#



static is_gemini(family: str) → bool[source]#



static is_openai(family: str) → bool[source]#

```python
ANY#
```

【中文翻译】alias of Literal[‘gpt-41’, ‘gpt-45’, ‘gpt-4o’, ‘o1’, ‘o3’, ‘o4’, ‘gpt-4’, ‘gpt-35’, ‘r1’, ‘gemini-1.5-flash’, ‘gemini-1.5-pro’, ‘gemini-2.0-flash’, ‘gemini-2.5-pro’, ‘claude-3-haiku’, ‘claude-3-sonnet’, ‘claude-3-opus’, ‘claude-3-5-haiku’, ‘claude-3-5-sonnet’, ‘claude-3-7-sonnet’, ‘unknown’]

```python
CLAUDE_3_5_HAIKU = 'claude-3-5-haiku'#
```

```python
CLAUDE_3_5_SONNET = 'claude-3-5-sonnet'#
```

```python
CLAUDE_3_7_SONNET = 'claude-3-7-sonnet'#
```

```python
CLAUDE_3_HAIKU = 'claude-3-haiku'#
```

```python
CLAUDE_3_OPUS = 'claude-3-opus'#
```

```python
CLAUDE_3_SONNET = 'claude-3-sonnet'#
```

```python
GEMINI_1_5_FLASH = 'gemini-1.5-flash'#
```

```python
GEMINI_1_5_PRO = 'gemini-1.5-pro'#
```

```python
GEMINI_2_0_FLASH = 'gemini-2.0-flash'#
```

```python
GEMINI_2_5_PRO = 'gemini-2.5-pro'#
```

```python
GPT_35 = 'gpt-35'#
```

```python
GPT_4 = 'gpt-4'#
```

```python
GPT_41 = 'gpt-41'#
```

```python
GPT_45 = 'gpt-45'#
```

```python
GPT_4O = 'gpt-4o'#
```

```python
O1 = 'o1'#
```

```python
O3 = 'o3'#
```

```python
O4 = 'o4'#
```

```python
R1 = 'r1'#
```

```python
UNKNOWN = 'unknown'#
```

```python
static is_claude(family: str) → bool[source]#
```

```python
static is_gemini(family: str) → bool[source]#
```

```python
static is_openai(family: str) → bool[source]#
```

```python
class ModelInfo[source]#
```

【中文翻译】Bases: TypedDict
ModelInfo is a dictionary that contains information about a model’s properties.
It is expected to be used in the model_info property of a model client.
We are expecting this to grow over time as we add more features.


family: Required[Literal['gpt-41', 'gpt-45', 'gpt-4o', 'o1', 'o3', 'o4', 'gpt-4', 'gpt-35', 'r1', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-2.5-pro', 'claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus', 'claude-3-5-haiku', 'claude-3-5-sonnet', 'claude-3-7-sonnet', 'unknown'] | str]#
Model family should be one of the constants from ModelFamily or a string representing an unknown model family.



function_calling: Required[bool]#
True if the model supports function calling, otherwise False.



json_output: Required[bool]#
this is different to structured json.

Type:
True if the model supports json output, otherwise False. Note





multiple_system_messages: bool | None#
True if the model supports multiple, non-consecutive system messages, otherwise False.



structured_output: Required[bool]#
True if the model supports structured output, otherwise False. This is different to json_output.



vision: Required[bool]#
True if the model supports vision, aka image input, otherwise False.

```python
family: Required[Literal['gpt-41', 'gpt-45', 'gpt-4o', 'o1', 'o3', 'o4', 'gpt-4', 'gpt-35', 'r1', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-2.5-pro', 'claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus', 'claude-3-5-haiku', 'claude-3-5-sonnet', 'claude-3-7-sonnet', 'unknown'] | str]#
```

【中文翻译】Model family should be one of the constants from ModelFamily or a string representing an unknown model family.

```python
function_calling: Required[bool]#
```

【中文翻译】True if the model supports function calling, otherwise False.

```python
json_output: Required[bool]#
```

【中文翻译】this is different to structured json.

Type:
True if the model supports json output, otherwise False. Note

```python
multiple_system_messages: bool | None#
```

【中文翻译】True if the model supports multiple, non-consecutive system messages, otherwise False.

```python
structured_output: Required[bool]#
```

【中文翻译】True if the model supports structured output, otherwise False. This is different to json_output.

```python
vision: Required[bool]#
```

【中文翻译】True if the model supports vision, aka image input, otherwise False.

```python
class RequestUsage(prompt_tokens: int, completion_tokens: int)[source]#
```

【中文翻译】Bases: object


completion_tokens: int#



prompt_tokens: int#

```python
completion_tokens: int#
```

```python
prompt_tokens: int#
```

```python
pydantic model SystemMessage[source]#
```

【中文翻译】Bases: BaseModel
System message contains instructions for the model coming from the developer.

Note
Open AI is moving away from using ‘system’ role in favor of ‘developer’ role.
See Model Spec for more details.
However, the ‘system’ role is still allowed in their API and will be automatically converted to ‘developer’ role
on the server side.
So, you can use SystemMessage for developer messages.


Show JSON schema{
   "title": "SystemMessage",
   "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
   "type": "object",
   "properties": {
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "SystemMessage",
         "default": "SystemMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "required": [
      "content"
   ]
}



Fields:

content (str)
type (Literal['SystemMessage'])





field content: str [Required]#
The content of the message.



field type: Literal['SystemMessage'] = 'SystemMessage'#

**示例**:
```python
{
   "title": "SystemMessage",
   "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n    Open AI is moving away from using 'system' role in favor of 'developer' role.\n    See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n    However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n    on the server side.\n    So, you can use `SystemMessage` for developer messages.",
   "type": "object",
   "properties": {
      "content": {
         "title": "Content",
         "type": "string"
      },
      "type": {
         "const": "SystemMessage",
         "default": "SystemMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "required": [
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】The content of the message.

```python
field type: Literal['SystemMessage'] = 'SystemMessage'#
```

```python
class TopLogprob(logprob: float, bytes: List[int] | None = None)[source]#
```

【中文翻译】Bases: object


bytes: List[int] | None = None#



logprob: float#

```python
bytes: List[int] | None = None#
```

```python
logprob: float#
```

```python
pydantic model UserMessage[source]#
```

【中文翻译】Bases: BaseModel
User message contains input from end users, or a catch-all for data provided to the model.

Show JSON schema{
   "title": "UserMessage",
   "description": "User message contains input from end users, or a catch-all for data provided to the model.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "anyOf": [
                     {
                        "type": "string"
                     },
                     {}
                  ]
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "type": {
         "const": "UserMessage",
         "default": "UserMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "required": [
      "content",
      "source"
   ]
}



Fields:

content (str | List[str | autogen_core._image.Image])
source (str)
type (Literal['UserMessage'])





field content: str | List[str | Image] [Required]#
The content of the message.



field source: str [Required]#
The name of the agent that sent this message.



field type: Literal['UserMessage'] = 'UserMessage'#

**示例**:
```python
{
   "title": "UserMessage",
   "description": "User message contains input from end users, or a catch-all for data provided to the model.",
   "type": "object",
   "properties": {
      "content": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "anyOf": [
                     {
                        "type": "string"
                     },
                     {}
                  ]
               },
               "type": "array"
            }
         ],
         "title": "Content"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "type": {
         "const": "UserMessage",
         "default": "UserMessage",
         "title": "Type",
         "type": "string"
      }
   },
   "required": [
      "content",
      "source"
   ]
}

```

```python
field content: str | List[str | Image] [Required]#
```

【中文翻译】The content of the message.

```python
field source: str [Required]#
```

【中文翻译】The name of the agent that sent this message.

```python
field type: Literal['UserMessage'] = 'UserMessage'#
```

```python
validate_model_info(model_info: ModelInfo) → None[source]#
```

【中文翻译】Validates the model info dictionary.

Raises:
ValueError – If the model info dictionary is missing required fields.

【中文翻译】previous

【中文翻译】autogen_core.code_executor

【中文翻译】next

【中文翻译】autogen_core.model_context

### autogen_core.tool_agent {autogen_coretool_agent}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html)

```python
exception InvalidToolArgumentsException(call_id: str, content: str, name: str)[source]#
```

【中文翻译】Bases: ToolException

```python
class ToolAgent(description: str, tools: List[Tool])[source]#
```

【中文翻译】Bases: RoutedAgent
A tool agent accepts direct messages of the type FunctionCall,
executes the requested tool with the provided arguments, and returns the
result as FunctionExecutionResult messages.

Parameters:

description (str) – The description of the agent.
tools (List[Tool]) – The list of tools that the agent can execute.





async handle_function_call(message: FunctionCall, ctx: MessageContext) → FunctionExecutionResult[source]#
Handles a FunctionCall message by executing the requested tool with the provided arguments.

Parameters:

message (FunctionCall) – The function call message.
cancellation_token (CancellationToken) – The cancellation token.


Returns:
FunctionExecutionResult – The result of the function execution.

Raises:

ToolNotFoundException – If the tool is not found.
InvalidToolArgumentsException – If the tool arguments are invalid.
ToolExecutionException – If the tool execution fails.






property tools: List[Tool]#

```python
async handle_function_call(message: FunctionCall, ctx: MessageContext) → FunctionExecutionResult[source]#
```

【中文翻译】Handles a FunctionCall message by executing the requested tool with the provided arguments.

Parameters:

message (FunctionCall) – The function call message.
cancellation_token (CancellationToken) – The cancellation token.


Returns:
FunctionExecutionResult – The result of the function execution.

Raises:

ToolNotFoundException – If the tool is not found.
InvalidToolArgumentsException – If the tool arguments are invalid.
ToolExecutionException – If the tool execution fails.

```python
property tools: List[Tool]#
```

```python
exception ToolException(call_id: str, content: str, name: str)[source]#
```

【中文翻译】Bases: BaseException


call_id: str#



content: str#



name: str#

```python
call_id: str#
```

```python
content: str#
```

```python
name: str#
```

```python
exception ToolExecutionException(call_id: str, content: str, name: str)[source]#
```

【中文翻译】Bases: ToolException

```python
exception ToolNotFoundException(call_id: str, content: str, name: str)[source]#
```

【中文翻译】Bases: ToolException

```python
async tool_agent_caller_loop(caller: BaseAgent | AgentRuntime, tool_agent_id: AgentId, model_client: ChatCompletionClient, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], tool_schema: List[ToolSchema] | List[Tool], cancellation_token: CancellationToken | None = None, caller_source: str = 'assistant') → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Start a caller loop for a tool agent. This function sends messages to the tool agent
and the model client in an alternating fashion until the model client stops generating tool calls.

Parameters:

tool_agent_id (AgentId) – The Agent ID of the tool agent.
input_messages (List[LLMMessage]) – The list of input messages.
model_client (ChatCompletionClient) – The model client to use for the model API.
tool_schema (List[Tool | ToolSchema]) – The list of tools that the model can use.


Returns:
List[LLMMessage] – The list of output messages created in the caller loop.

【中文翻译】previous

【中文翻译】autogen_core.tools

【中文翻译】next

【中文翻译】autogen_core.memory

### autogen_core.tool_agent {autogen_coretool_agent}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool_agent.html)

```python
exception InvalidToolArgumentsException(call_id: str, content: str, name: str)[source]#
```

【中文翻译】Bases: ToolException

```python
class ToolAgent(description: str, tools: List[Tool])[source]#
```

【中文翻译】Bases: RoutedAgent
A tool agent accepts direct messages of the type FunctionCall,
executes the requested tool with the provided arguments, and returns the
result as FunctionExecutionResult messages.

Parameters:

description (str) – The description of the agent.
tools (List[Tool]) – The list of tools that the agent can execute.





async handle_function_call(message: FunctionCall, ctx: MessageContext) → FunctionExecutionResult[source]#
Handles a FunctionCall message by executing the requested tool with the provided arguments.

Parameters:

message (FunctionCall) – The function call message.
cancellation_token (CancellationToken) – The cancellation token.


Returns:
FunctionExecutionResult – The result of the function execution.

Raises:

ToolNotFoundException – If the tool is not found.
InvalidToolArgumentsException – If the tool arguments are invalid.
ToolExecutionException – If the tool execution fails.






property tools: List[Tool]#

```python
async handle_function_call(message: FunctionCall, ctx: MessageContext) → FunctionExecutionResult[source]#
```

【中文翻译】Handles a FunctionCall message by executing the requested tool with the provided arguments.

Parameters:

message (FunctionCall) – The function call message.
cancellation_token (CancellationToken) – The cancellation token.


Returns:
FunctionExecutionResult – The result of the function execution.

Raises:

ToolNotFoundException – If the tool is not found.
InvalidToolArgumentsException – If the tool arguments are invalid.
ToolExecutionException – If the tool execution fails.

```python
property tools: List[Tool]#
```

```python
exception ToolException(call_id: str, content: str, name: str)[source]#
```

【中文翻译】Bases: BaseException


call_id: str#



content: str#



name: str#

```python
call_id: str#
```

```python
content: str#
```

```python
name: str#
```

```python
exception ToolExecutionException(call_id: str, content: str, name: str)[source]#
```

【中文翻译】Bases: ToolException

```python
exception ToolNotFoundException(call_id: str, content: str, name: str)[source]#
```

【中文翻译】Bases: ToolException

```python
async tool_agent_caller_loop(caller: BaseAgent | AgentRuntime, tool_agent_id: AgentId, model_client: ChatCompletionClient, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], tool_schema: List[ToolSchema] | List[Tool], cancellation_token: CancellationToken | None = None, caller_source: str = 'assistant') → List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]][source]#
```

【中文翻译】Start a caller loop for a tool agent. This function sends messages to the tool agent
and the model client in an alternating fashion until the model client stops generating tool calls.

Parameters:

tool_agent_id (AgentId) – The Agent ID of the tool agent.
input_messages (List[LLMMessage]) – The list of input messages.
model_client (ChatCompletionClient) – The model client to use for the model API.
tool_schema (List[Tool | ToolSchema]) – The list of tools that the model can use.


Returns:
List[LLMMessage] – The list of output messages created in the caller loop.

【中文翻译】previous

【中文翻译】autogen_core.tools

【中文翻译】next

【中文翻译】autogen_core.memory

### autogen_core.tools {autogen_coretools}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html)

```python
class BaseTool(args_type: Type[ArgsT], return_type: Type[ReturnT], name: str, description: str, strict: bool = False)[source]#
```

【中文翻译】Bases: ABC, Tool, Generic[ArgsT, ReturnT], ComponentBase[BaseModel]


args_type() → Type[BaseModel][source]#



component_type: ClassVar[ComponentType] = 'tool'#
The logical type of the component.



property description: str#



async load_state_json(state: Mapping[str, Any]) → None[source]#



property name: str#



return_type() → Type[Any][source]#



return_value_as_string(value: Any) → str[source]#



abstract async run(args: ArgsT, cancellation_token: CancellationToken) → ReturnT[source]#



async run_json(args: Mapping[str, Any], cancellation_token: CancellationToken) → Any[source]#



async save_state_json() → Mapping[str, Any][source]#



property schema: ToolSchema#



state_type() → Type[BaseModel] | None[source]#

```python
args_type() → Type[BaseModel][source]#
```

```python
component_type: ClassVar[ComponentType] = 'tool'#
```

【中文翻译】The logical type of the component.

```python
property description: str#
```

```python
async load_state_json(state: Mapping[str, Any]) → None[source]#
```

```python
property name: str#
```

```python
return_type() → Type[Any][source]#
```

```python
return_value_as_string(value: Any) → str[source]#
```

```python
abstract async run(args: ArgsT, cancellation_token: CancellationToken) → ReturnT[source]#
```

```python
async run_json(args: Mapping[str, Any], cancellation_token: CancellationToken) → Any[source]#
```

```python
async save_state_json() → Mapping[str, Any][source]#
```

```python
property schema: ToolSchema#
```

```python
state_type() → Type[BaseModel] | None[source]#
```

```python
class BaseToolWithState(args_type: Type[ArgsT], return_type: Type[ReturnT], state_type: Type[StateT], name: str, description: str)[source]#
```

【中文翻译】Bases: BaseTool[ArgsT, ReturnT], ABC, Generic[ArgsT, ReturnT, StateT], ComponentBase[BaseModel]


component_type: ClassVar[ComponentType] = 'tool'#
The logical type of the component.



abstract load_state(state: StateT) → None[source]#



async load_state_json(state: Mapping[str, Any]) → None[source]#



abstract save_state() → StateT[source]#



async save_state_json() → Mapping[str, Any][source]#

```python
component_type: ClassVar[ComponentType] = 'tool'#
```

【中文翻译】The logical type of the component.

```python
abstract load_state(state: StateT) → None[source]#
```

```python
async load_state_json(state: Mapping[str, Any]) → None[source]#
```

```python
abstract save_state() → StateT[source]#
```

```python
async save_state_json() → Mapping[str, Any][source]#
```

```python
class FunctionTool(func: Callable[[...], Any], description: str, name: str | None = None, global_imports: Sequence[str | ImportFromModule | Alias] = [], strict: bool = False)[source]#
```

【中文翻译】Bases: BaseTool[BaseModel, BaseModel], Component[FunctionToolConfig]
Create custom tools by wrapping standard Python functions.
FunctionTool offers an interface for executing Python functions either asynchronously or synchronously.
Each function must include type annotations for all parameters and its return type. These annotations
enable FunctionTool to generate a schema necessary for input validation, serialization, and for informing
the LLM about expected parameters. When the LLM prepares a function call, it leverages this schema to
generate arguments that align with the function’s specifications.

Note
It is the user’s responsibility to verify that the tool’s output type matches the expected type.


Parameters:

func (Callable[..., ReturnT | Awaitable[ReturnT]]) – The function to wrap and expose as a tool.
description (str) – A description to inform the model of the function’s purpose, specifying what
it does and the context in which it should be called.
name (str, optional) – An optional custom name for the tool. Defaults to
the function’s original name if not provided.
strict (bool, optional) – If set to True, the tool schema will only contain arguments that are explicitly
defined in the function signature, and no default values will be allowed. Defaults to False.
This is required to be set to True when used with models in structured output mode.



Example
import random
from autogen_core import CancellationToken
from autogen_core.tools import FunctionTool
from typing_extensions import Annotated
import asyncio


async def get_stock_price(ticker: str, date: Annotated[str, "Date in YYYY/MM/DD"]) -> float:
    # Simulates a stock price retrieval by returning a random float within a specified range.
    return random.uniform(10, 200)


async def example():
    # Initialize a FunctionTool instance for retrieving stock prices.
    stock_price_tool = FunctionTool(get_stock_price, description="Fetch the stock price for a given ticker.")

    # Execute the tool with cancellation support.
    cancellation_token = CancellationToken()
    result = await stock_price_tool.run_json({"ticker": "AAPL", "date": "2021/01/01"}, cancellation_token)

    # Output the result as a formatted string.
    print(stock_price_tool.return_value_as_string(result))


asyncio.run(example())




classmethod _from_config(config: FunctionToolConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → FunctionToolConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of FunctionToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.tools.FunctionTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#

**示例**:
```python
import random
from autogen_core import CancellationToken
from autogen_core.tools import FunctionTool
from typing_extensions import Annotated
import asyncio


async def get_stock_price(ticker: str, date: Annotated[str, "Date in YYYY/MM/DD"]) -> float:
    # Simulates a stock price retrieval by returning a random float within a specified range.
    return random.uniform(10, 200)


async def example():
    # Initialize a FunctionTool instance for retrieving stock prices.
    stock_price_tool = FunctionTool(get_stock_price, description="Fetch the stock price for a given ticker.")

    # Execute the tool with cancellation support.
    cancellation_token = CancellationToken()
    result = await stock_price_tool.run_json({"ticker": "AAPL", "date": "2021/01/01"}, cancellation_token)

    # Output the result as a formatted string.
    print(stock_price_tool.return_value_as_string(result))


asyncio.run(example())

```

```python
classmethod _from_config(config: FunctionToolConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → FunctionToolConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of FunctionToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.tools.FunctionTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#
```

```python
pydantic model ImageResultContent[source]#
```

【中文翻译】Bases: BaseModel
Image result content of a tool execution.

Show JSON schema{
   "title": "ImageResultContent",
   "description": "Image result content of a tool execution.",
   "type": "object",
   "properties": {
      "type": {
         "const": "ImageResultContent",
         "default": "ImageResultContent",
         "title": "Type",
         "type": "string"
      },
      "content": {
         "title": "Content"
      }
   },
   "required": [
      "content"
   ]
}



Fields:

content (autogen_core._image.Image)
type (Literal['ImageResultContent'])





field content: Image [Required]#
The image content of the result.



field type: Literal['ImageResultContent'] = 'ImageResultContent'#

**示例**:
```python
{
   "title": "ImageResultContent",
   "description": "Image result content of a tool execution.",
   "type": "object",
   "properties": {
      "type": {
         "const": "ImageResultContent",
         "default": "ImageResultContent",
         "title": "Type",
         "type": "string"
      },
      "content": {
         "title": "Content"
      }
   },
   "required": [
      "content"
   ]
}

```

```python
field content: Image [Required]#
```

【中文翻译】The image content of the result.

```python
field type: Literal['ImageResultContent'] = 'ImageResultContent'#
```

```python
class ParametersSchema[source]#
```

【中文翻译】Bases: TypedDict


additionalProperties: NotRequired[bool]#



properties: Dict[str, Any]#



required: NotRequired[Sequence[str]]#



type: str#

```python
additionalProperties: NotRequired[bool]#
```

```python
properties: Dict[str, Any]#
```

```python
required: NotRequired[Sequence[str]]#
```

```python
type: str#
```

```python
class StaticWorkbench(tools: List[BaseTool[Any, Any]])[source]#
```

【中文翻译】Bases: Workbench, Component[StaticWorkbenchConfig]
A workbench that provides a static set of tools that do not change after
each tool execution.

Parameters:
tools (List[BaseTool[Any, Any]]) – A list of tools to be included in the workbench.
The tools should be subclasses of BaseTool.




classmethod _from_config(config: StaticWorkbenchConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → StaticWorkbenchConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.





component_config_schema#
alias of StaticWorkbenchConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.tools.StaticWorkbench'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async list_tools() → List[ToolSchema][source]#
List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.



async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.





async reset() → None[source]#
Reset the workbench to its initialized, started state.



async save_state() → Mapping[str, Any][source]#
Save the state of the workbench.
This method should be called to persist the state of the workbench.



async start() → None[source]#
Start the workbench and initialize any resources.
This method should be called before using the workbench.



async stop() → None[source]#
Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

```python
classmethod _from_config(config: StaticWorkbenchConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → StaticWorkbenchConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
```

【中文翻译】Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.

```python
component_config_schema#
```

【中文翻译】alias of StaticWorkbenchConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.tools.StaticWorkbench'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async list_tools() → List[ToolSchema][source]#
```

【中文翻译】List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.

```python
async reset() → None[source]#
```

【中文翻译】Reset the workbench to its initialized, started state.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the workbench.
This method should be called to persist the state of the workbench.

```python
async start() → None[source]#
```

【中文翻译】Start the workbench and initialize any resources.
This method should be called before using the workbench.

```python
async stop() → None[source]#
```

【中文翻译】Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

```python
pydantic model TextResultContent[source]#
```

【中文翻译】Bases: BaseModel
Text result content of a tool execution.

Show JSON schema{
   "title": "TextResultContent",
   "description": "Text result content of a tool execution.",
   "type": "object",
   "properties": {
      "type": {
         "const": "TextResultContent",
         "default": "TextResultContent",
         "title": "Type",
         "type": "string"
      },
      "content": {
         "title": "Content",
         "type": "string"
      }
   },
   "required": [
      "content"
   ]
}



Fields:

content (str)
type (Literal['TextResultContent'])





field content: str [Required]#
The text content of the result.



field type: Literal['TextResultContent'] = 'TextResultContent'#

**示例**:
```python
{
   "title": "TextResultContent",
   "description": "Text result content of a tool execution.",
   "type": "object",
   "properties": {
      "type": {
         "const": "TextResultContent",
         "default": "TextResultContent",
         "title": "Type",
         "type": "string"
      },
      "content": {
         "title": "Content",
         "type": "string"
      }
   },
   "required": [
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】The text content of the result.

```python
field type: Literal['TextResultContent'] = 'TextResultContent'#
```

```python
class Tool(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


args_type() → Type[BaseModel][source]#



property description: str#



async load_state_json(state: Mapping[str, Any]) → None[source]#



property name: str#



return_type() → Type[Any][source]#



return_value_as_string(value: Any) → str[source]#



async run_json(args: Mapping[str, Any], cancellation_token: CancellationToken) → Any[source]#



async save_state_json() → Mapping[str, Any][source]#



property schema: ToolSchema#



state_type() → Type[BaseModel] | None[source]#

```python
args_type() → Type[BaseModel][source]#
```

```python
property description: str#
```

```python
async load_state_json(state: Mapping[str, Any]) → None[source]#
```

```python
property name: str#
```

```python
return_type() → Type[Any][source]#
```

```python
return_value_as_string(value: Any) → str[source]#
```

```python
async run_json(args: Mapping[str, Any], cancellation_token: CancellationToken) → Any[source]#
```

```python
async save_state_json() → Mapping[str, Any][source]#
```

```python
property schema: ToolSchema#
```

```python
state_type() → Type[BaseModel] | None[source]#
```

```python
pydantic model ToolResult[source]#
```

【中文翻译】Bases: BaseModel
A result of a tool execution by a workbench.

Show JSON schema{
   "title": "ToolResult",
   "description": "A result of a tool execution by a workbench.",
   "type": "object",
   "properties": {
      "type": {
         "const": "ToolResult",
         "default": "ToolResult",
         "title": "Type",
         "type": "string"
      },
      "name": {
         "title": "Name",
         "type": "string"
      },
      "result": {
         "items": {
            "discriminator": {
               "mapping": {
                  "ImageResultContent": "#/$defs/ImageResultContent",
                  "TextResultContent": "#/$defs/TextResultContent"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/TextResultContent"
               },
               {
                  "$ref": "#/$defs/ImageResultContent"
               }
            ]
         },
         "title": "Result",
         "type": "array"
      },
      "is_error": {
         "default": false,
         "title": "Is Error",
         "type": "boolean"
      }
   },
   "$defs": {
      "ImageResultContent": {
         "description": "Image result content of a tool execution.",
         "properties": {
            "type": {
               "const": "ImageResultContent",
               "default": "ImageResultContent",
               "title": "Type",
               "type": "string"
            },
            "content": {
               "title": "Content"
            }
         },
         "required": [
            "content"
         ],
         "title": "ImageResultContent",
         "type": "object"
      },
      "TextResultContent": {
         "description": "Text result content of a tool execution.",
         "properties": {
            "type": {
               "const": "TextResultContent",
               "default": "TextResultContent",
               "title": "Type",
               "type": "string"
            },
            "content": {
               "title": "Content",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "TextResultContent",
         "type": "object"
      }
   },
   "required": [
      "name",
      "result"
   ]
}



Fields:

is_error (bool)
name (str)
result (List[autogen_core.tools._workbench.TextResultContent | autogen_core.tools._workbench.ImageResultContent])
type (Literal['ToolResult'])





field is_error: bool = False#
Whether the tool execution resulted in an error.



field name: str [Required]#
The name of the tool that was executed.



field result: List[Annotated[TextResultContent | ImageResultContent, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Required]#
The result of the tool execution.



field type: Literal['ToolResult'] = 'ToolResult'#



to_text(replace_image: str | None = None) → str[source]#
Convert the result to a text string.

Parameters:
replace_image (str | None) – The string to replace the image content with.
If None, the image content will be included in the text as base64 string.

Returns:
str – The text representation of the result.

**示例**:
```python
{
   "title": "ToolResult",
   "description": "A result of a tool execution by a workbench.",
   "type": "object",
   "properties": {
      "type": {
         "const": "ToolResult",
         "default": "ToolResult",
         "title": "Type",
         "type": "string"
      },
      "name": {
         "title": "Name",
         "type": "string"
      },
      "result": {
         "items": {
            "discriminator": {
               "mapping": {
                  "ImageResultContent": "#/$defs/ImageResultContent",
                  "TextResultContent": "#/$defs/TextResultContent"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/TextResultContent"
               },
               {
                  "$ref": "#/$defs/ImageResultContent"
               }
            ]
         },
         "title": "Result",
         "type": "array"
      },
      "is_error": {
         "default": false,
         "title": "Is Error",
         "type": "boolean"
      }
   },
   "$defs": {
      "ImageResultContent": {
         "description": "Image result content of a tool execution.",
         "properties": {
            "type": {
               "const": "ImageResultContent",
               "default": "ImageResultContent",
               "title": "Type",
               "type": "string"
            },
            "content": {
               "title": "Content"
            }
         },
         "required": [
            "content"
         ],
         "title": "ImageResultContent",
         "type": "object"
      },
      "TextResultContent": {
         "description": "Text result content of a tool execution.",
         "properties": {
            "type": {
               "const": "TextResultContent",
               "default": "TextResultContent",
               "title": "Type",
               "type": "string"
            },
            "content": {
               "title": "Content",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "TextResultContent",
         "type": "object"
      }
   },
   "required": [
      "name",
      "result"
   ]
}

```

```python
field is_error: bool = False#
```

【中文翻译】Whether the tool execution resulted in an error.

```python
field name: str [Required]#
```

【中文翻译】The name of the tool that was executed.

```python
field result: List[Annotated[TextResultContent | ImageResultContent, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Required]#
```

【中文翻译】The result of the tool execution.

```python
field type: Literal['ToolResult'] = 'ToolResult'#
```

```python
to_text(replace_image: str | None = None) → str[source]#
```

【中文翻译】Convert the result to a text string.

Parameters:
replace_image (str | None) – The string to replace the image content with.
If None, the image content will be included in the text as base64 string.

Returns:
str – The text representation of the result.

```python
class ToolSchema[source]#
```

【中文翻译】Bases: TypedDict


description: NotRequired[str]#



name: str#



parameters: NotRequired[ParametersSchema]#



strict: NotRequired[bool]#

```python
description: NotRequired[str]#
```

```python
name: str#
```

```python
parameters: NotRequired[ParametersSchema]#
```

```python
strict: NotRequired[bool]#
```

```python
class Workbench[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
A workbench is a component that provides a set of tools that may share
resources and state.
A workbench is responsible for managing the lifecycle of the tools and
providing a single interface to call them. The tools provided by the workbench
may be dynamic and their availabilities may change after each tool execution.
A workbench can be started by calling the start() method
and stopped by calling the stop() method.
It can also be used as an asynchronous context manager, which will automatically
start and stop the workbench when entering and exiting the context.


abstract async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.





component_type: ClassVar[ComponentType] = 'workbench'#
The logical type of the component.



abstract async list_tools() → List[ToolSchema][source]#
List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.



abstract async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.





abstract async reset() → None[source]#
Reset the workbench to its initialized, started state.



abstract async save_state() → Mapping[str, Any][source]#
Save the state of the workbench.
This method should be called to persist the state of the workbench.



abstract async start() → None[source]#
Start the workbench and initialize any resources.
This method should be called before using the workbench.



abstract async stop() → None[source]#
Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

```python
abstract async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
```

【中文翻译】Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.

```python
component_type: ClassVar[ComponentType] = 'workbench'#
```

【中文翻译】The logical type of the component.

```python
abstract async list_tools() → List[ToolSchema][source]#
```

【中文翻译】List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.

```python
abstract async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.

```python
abstract async reset() → None[source]#
```

【中文翻译】Reset the workbench to its initialized, started state.

```python
abstract async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the workbench.
This method should be called to persist the state of the workbench.

```python
abstract async start() → None[source]#
```

【中文翻译】Start the workbench and initialize any resources.
This method should be called before using the workbench.

```python
abstract async stop() → None[source]#
```

【中文翻译】Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

【中文翻译】previous

【中文翻译】autogen_core.model_context

【中文翻译】next

【中文翻译】autogen_core.tool_agent

### autogen_core.tools {autogen_coretools}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html)

```python
class BaseTool(args_type: Type[ArgsT], return_type: Type[ReturnT], name: str, description: str, strict: bool = False)[source]#
```

【中文翻译】Bases: ABC, Tool, Generic[ArgsT, ReturnT], ComponentBase[BaseModel]


args_type() → Type[BaseModel][source]#



component_type: ClassVar[ComponentType] = 'tool'#
The logical type of the component.



property description: str#



async load_state_json(state: Mapping[str, Any]) → None[source]#



property name: str#



return_type() → Type[Any][source]#



return_value_as_string(value: Any) → str[source]#



abstract async run(args: ArgsT, cancellation_token: CancellationToken) → ReturnT[source]#



async run_json(args: Mapping[str, Any], cancellation_token: CancellationToken) → Any[source]#



async save_state_json() → Mapping[str, Any][source]#



property schema: ToolSchema#



state_type() → Type[BaseModel] | None[source]#

```python
args_type() → Type[BaseModel][source]#
```

```python
component_type: ClassVar[ComponentType] = 'tool'#
```

【中文翻译】The logical type of the component.

```python
property description: str#
```

```python
async load_state_json(state: Mapping[str, Any]) → None[source]#
```

```python
property name: str#
```

```python
return_type() → Type[Any][source]#
```

```python
return_value_as_string(value: Any) → str[source]#
```

```python
abstract async run(args: ArgsT, cancellation_token: CancellationToken) → ReturnT[source]#
```

```python
async run_json(args: Mapping[str, Any], cancellation_token: CancellationToken) → Any[source]#
```

```python
async save_state_json() → Mapping[str, Any][source]#
```

```python
property schema: ToolSchema#
```

```python
state_type() → Type[BaseModel] | None[source]#
```

```python
class BaseToolWithState(args_type: Type[ArgsT], return_type: Type[ReturnT], state_type: Type[StateT], name: str, description: str)[source]#
```

【中文翻译】Bases: BaseTool[ArgsT, ReturnT], ABC, Generic[ArgsT, ReturnT, StateT], ComponentBase[BaseModel]


component_type: ClassVar[ComponentType] = 'tool'#
The logical type of the component.



abstract load_state(state: StateT) → None[source]#



async load_state_json(state: Mapping[str, Any]) → None[source]#



abstract save_state() → StateT[source]#



async save_state_json() → Mapping[str, Any][source]#

```python
component_type: ClassVar[ComponentType] = 'tool'#
```

【中文翻译】The logical type of the component.

```python
abstract load_state(state: StateT) → None[source]#
```

```python
async load_state_json(state: Mapping[str, Any]) → None[source]#
```

```python
abstract save_state() → StateT[source]#
```

```python
async save_state_json() → Mapping[str, Any][source]#
```

```python
class FunctionTool(func: Callable[[...], Any], description: str, name: str | None = None, global_imports: Sequence[str | ImportFromModule | Alias] = [], strict: bool = False)[source]#
```

【中文翻译】Bases: BaseTool[BaseModel, BaseModel], Component[FunctionToolConfig]
Create custom tools by wrapping standard Python functions.
FunctionTool offers an interface for executing Python functions either asynchronously or synchronously.
Each function must include type annotations for all parameters and its return type. These annotations
enable FunctionTool to generate a schema necessary for input validation, serialization, and for informing
the LLM about expected parameters. When the LLM prepares a function call, it leverages this schema to
generate arguments that align with the function’s specifications.

Note
It is the user’s responsibility to verify that the tool’s output type matches the expected type.


Parameters:

func (Callable[..., ReturnT | Awaitable[ReturnT]]) – The function to wrap and expose as a tool.
description (str) – A description to inform the model of the function’s purpose, specifying what
it does and the context in which it should be called.
name (str, optional) – An optional custom name for the tool. Defaults to
the function’s original name if not provided.
strict (bool, optional) – If set to True, the tool schema will only contain arguments that are explicitly
defined in the function signature, and no default values will be allowed. Defaults to False.
This is required to be set to True when used with models in structured output mode.



Example
import random
from autogen_core import CancellationToken
from autogen_core.tools import FunctionTool
from typing_extensions import Annotated
import asyncio


async def get_stock_price(ticker: str, date: Annotated[str, "Date in YYYY/MM/DD"]) -> float:
    # Simulates a stock price retrieval by returning a random float within a specified range.
    return random.uniform(10, 200)


async def example():
    # Initialize a FunctionTool instance for retrieving stock prices.
    stock_price_tool = FunctionTool(get_stock_price, description="Fetch the stock price for a given ticker.")

    # Execute the tool with cancellation support.
    cancellation_token = CancellationToken()
    result = await stock_price_tool.run_json({"ticker": "AAPL", "date": "2021/01/01"}, cancellation_token)

    # Output the result as a formatted string.
    print(stock_price_tool.return_value_as_string(result))


asyncio.run(example())




classmethod _from_config(config: FunctionToolConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → FunctionToolConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of FunctionToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.tools.FunctionTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#

**示例**:
```python
import random
from autogen_core import CancellationToken
from autogen_core.tools import FunctionTool
from typing_extensions import Annotated
import asyncio


async def get_stock_price(ticker: str, date: Annotated[str, "Date in YYYY/MM/DD"]) -> float:
    # Simulates a stock price retrieval by returning a random float within a specified range.
    return random.uniform(10, 200)


async def example():
    # Initialize a FunctionTool instance for retrieving stock prices.
    stock_price_tool = FunctionTool(get_stock_price, description="Fetch the stock price for a given ticker.")

    # Execute the tool with cancellation support.
    cancellation_token = CancellationToken()
    result = await stock_price_tool.run_json({"ticker": "AAPL", "date": "2021/01/01"}, cancellation_token)

    # Output the result as a formatted string.
    print(stock_price_tool.return_value_as_string(result))


asyncio.run(example())

```

```python
classmethod _from_config(config: FunctionToolConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → FunctionToolConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of FunctionToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.tools.FunctionTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#
```

```python
pydantic model ImageResultContent[source]#
```

【中文翻译】Bases: BaseModel
Image result content of a tool execution.

Show JSON schema{
   "title": "ImageResultContent",
   "description": "Image result content of a tool execution.",
   "type": "object",
   "properties": {
      "type": {
         "const": "ImageResultContent",
         "default": "ImageResultContent",
         "title": "Type",
         "type": "string"
      },
      "content": {
         "title": "Content"
      }
   },
   "required": [
      "content"
   ]
}



Fields:

content (autogen_core._image.Image)
type (Literal['ImageResultContent'])





field content: Image [Required]#
The image content of the result.



field type: Literal['ImageResultContent'] = 'ImageResultContent'#

**示例**:
```python
{
   "title": "ImageResultContent",
   "description": "Image result content of a tool execution.",
   "type": "object",
   "properties": {
      "type": {
         "const": "ImageResultContent",
         "default": "ImageResultContent",
         "title": "Type",
         "type": "string"
      },
      "content": {
         "title": "Content"
      }
   },
   "required": [
      "content"
   ]
}

```

```python
field content: Image [Required]#
```

【中文翻译】The image content of the result.

```python
field type: Literal['ImageResultContent'] = 'ImageResultContent'#
```

```python
class ParametersSchema[source]#
```

【中文翻译】Bases: TypedDict


additionalProperties: NotRequired[bool]#



properties: Dict[str, Any]#



required: NotRequired[Sequence[str]]#



type: str#

```python
additionalProperties: NotRequired[bool]#
```

```python
properties: Dict[str, Any]#
```

```python
required: NotRequired[Sequence[str]]#
```

```python
type: str#
```

```python
class StaticWorkbench(tools: List[BaseTool[Any, Any]])[source]#
```

【中文翻译】Bases: Workbench, Component[StaticWorkbenchConfig]
A workbench that provides a static set of tools that do not change after
each tool execution.

Parameters:
tools (List[BaseTool[Any, Any]]) – A list of tools to be included in the workbench.
The tools should be subclasses of BaseTool.




classmethod _from_config(config: StaticWorkbenchConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → StaticWorkbenchConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.





component_config_schema#
alias of StaticWorkbenchConfig



component_provider_override: ClassVar[str | None] = 'autogen_core.tools.StaticWorkbench'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async list_tools() → List[ToolSchema][source]#
List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.



async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.





async reset() → None[source]#
Reset the workbench to its initialized, started state.



async save_state() → Mapping[str, Any][source]#
Save the state of the workbench.
This method should be called to persist the state of the workbench.



async start() → None[source]#
Start the workbench and initialize any resources.
This method should be called before using the workbench.



async stop() → None[source]#
Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

```python
classmethod _from_config(config: StaticWorkbenchConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → StaticWorkbenchConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
```

【中文翻译】Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.

```python
component_config_schema#
```

【中文翻译】alias of StaticWorkbenchConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_core.tools.StaticWorkbench'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async list_tools() → List[ToolSchema][source]#
```

【中文翻译】List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.

```python
async reset() → None[source]#
```

【中文翻译】Reset the workbench to its initialized, started state.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the workbench.
This method should be called to persist the state of the workbench.

```python
async start() → None[source]#
```

【中文翻译】Start the workbench and initialize any resources.
This method should be called before using the workbench.

```python
async stop() → None[source]#
```

【中文翻译】Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

```python
pydantic model TextResultContent[source]#
```

【中文翻译】Bases: BaseModel
Text result content of a tool execution.

Show JSON schema{
   "title": "TextResultContent",
   "description": "Text result content of a tool execution.",
   "type": "object",
   "properties": {
      "type": {
         "const": "TextResultContent",
         "default": "TextResultContent",
         "title": "Type",
         "type": "string"
      },
      "content": {
         "title": "Content",
         "type": "string"
      }
   },
   "required": [
      "content"
   ]
}



Fields:

content (str)
type (Literal['TextResultContent'])





field content: str [Required]#
The text content of the result.



field type: Literal['TextResultContent'] = 'TextResultContent'#

**示例**:
```python
{
   "title": "TextResultContent",
   "description": "Text result content of a tool execution.",
   "type": "object",
   "properties": {
      "type": {
         "const": "TextResultContent",
         "default": "TextResultContent",
         "title": "Type",
         "type": "string"
      },
      "content": {
         "title": "Content",
         "type": "string"
      }
   },
   "required": [
      "content"
   ]
}

```

```python
field content: str [Required]#
```

【中文翻译】The text content of the result.

```python
field type: Literal['TextResultContent'] = 'TextResultContent'#
```

```python
class Tool(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


args_type() → Type[BaseModel][source]#



property description: str#



async load_state_json(state: Mapping[str, Any]) → None[source]#



property name: str#



return_type() → Type[Any][source]#



return_value_as_string(value: Any) → str[source]#



async run_json(args: Mapping[str, Any], cancellation_token: CancellationToken) → Any[source]#



async save_state_json() → Mapping[str, Any][source]#



property schema: ToolSchema#



state_type() → Type[BaseModel] | None[source]#

```python
args_type() → Type[BaseModel][source]#
```

```python
property description: str#
```

```python
async load_state_json(state: Mapping[str, Any]) → None[source]#
```

```python
property name: str#
```

```python
return_type() → Type[Any][source]#
```

```python
return_value_as_string(value: Any) → str[source]#
```

```python
async run_json(args: Mapping[str, Any], cancellation_token: CancellationToken) → Any[source]#
```

```python
async save_state_json() → Mapping[str, Any][source]#
```

```python
property schema: ToolSchema#
```

```python
state_type() → Type[BaseModel] | None[source]#
```

```python
pydantic model ToolResult[source]#
```

【中文翻译】Bases: BaseModel
A result of a tool execution by a workbench.

Show JSON schema{
   "title": "ToolResult",
   "description": "A result of a tool execution by a workbench.",
   "type": "object",
   "properties": {
      "type": {
         "const": "ToolResult",
         "default": "ToolResult",
         "title": "Type",
         "type": "string"
      },
      "name": {
         "title": "Name",
         "type": "string"
      },
      "result": {
         "items": {
            "discriminator": {
               "mapping": {
                  "ImageResultContent": "#/$defs/ImageResultContent",
                  "TextResultContent": "#/$defs/TextResultContent"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/TextResultContent"
               },
               {
                  "$ref": "#/$defs/ImageResultContent"
               }
            ]
         },
         "title": "Result",
         "type": "array"
      },
      "is_error": {
         "default": false,
         "title": "Is Error",
         "type": "boolean"
      }
   },
   "$defs": {
      "ImageResultContent": {
         "description": "Image result content of a tool execution.",
         "properties": {
            "type": {
               "const": "ImageResultContent",
               "default": "ImageResultContent",
               "title": "Type",
               "type": "string"
            },
            "content": {
               "title": "Content"
            }
         },
         "required": [
            "content"
         ],
         "title": "ImageResultContent",
         "type": "object"
      },
      "TextResultContent": {
         "description": "Text result content of a tool execution.",
         "properties": {
            "type": {
               "const": "TextResultContent",
               "default": "TextResultContent",
               "title": "Type",
               "type": "string"
            },
            "content": {
               "title": "Content",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "TextResultContent",
         "type": "object"
      }
   },
   "required": [
      "name",
      "result"
   ]
}



Fields:

is_error (bool)
name (str)
result (List[autogen_core.tools._workbench.TextResultContent | autogen_core.tools._workbench.ImageResultContent])
type (Literal['ToolResult'])





field is_error: bool = False#
Whether the tool execution resulted in an error.



field name: str [Required]#
The name of the tool that was executed.



field result: List[Annotated[TextResultContent | ImageResultContent, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Required]#
The result of the tool execution.



field type: Literal['ToolResult'] = 'ToolResult'#



to_text(replace_image: str | None = None) → str[source]#
Convert the result to a text string.

Parameters:
replace_image (str | None) – The string to replace the image content with.
If None, the image content will be included in the text as base64 string.

Returns:
str – The text representation of the result.

**示例**:
```python
{
   "title": "ToolResult",
   "description": "A result of a tool execution by a workbench.",
   "type": "object",
   "properties": {
      "type": {
         "const": "ToolResult",
         "default": "ToolResult",
         "title": "Type",
         "type": "string"
      },
      "name": {
         "title": "Name",
         "type": "string"
      },
      "result": {
         "items": {
            "discriminator": {
               "mapping": {
                  "ImageResultContent": "#/$defs/ImageResultContent",
                  "TextResultContent": "#/$defs/TextResultContent"
               },
               "propertyName": "type"
            },
            "oneOf": [
               {
                  "$ref": "#/$defs/TextResultContent"
               },
               {
                  "$ref": "#/$defs/ImageResultContent"
               }
            ]
         },
         "title": "Result",
         "type": "array"
      },
      "is_error": {
         "default": false,
         "title": "Is Error",
         "type": "boolean"
      }
   },
   "$defs": {
      "ImageResultContent": {
         "description": "Image result content of a tool execution.",
         "properties": {
            "type": {
               "const": "ImageResultContent",
               "default": "ImageResultContent",
               "title": "Type",
               "type": "string"
            },
            "content": {
               "title": "Content"
            }
         },
         "required": [
            "content"
         ],
         "title": "ImageResultContent",
         "type": "object"
      },
      "TextResultContent": {
         "description": "Text result content of a tool execution.",
         "properties": {
            "type": {
               "const": "TextResultContent",
               "default": "TextResultContent",
               "title": "Type",
               "type": "string"
            },
            "content": {
               "title": "Content",
               "type": "string"
            }
         },
         "required": [
            "content"
         ],
         "title": "TextResultContent",
         "type": "object"
      }
   },
   "required": [
      "name",
      "result"
   ]
}

```

```python
field is_error: bool = False#
```

【中文翻译】Whether the tool execution resulted in an error.

```python
field name: str [Required]#
```

【中文翻译】The name of the tool that was executed.

```python
field result: List[Annotated[TextResultContent | ImageResultContent, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Required]#
```

【中文翻译】The result of the tool execution.

```python
field type: Literal['ToolResult'] = 'ToolResult'#
```

```python
to_text(replace_image: str | None = None) → str[source]#
```

【中文翻译】Convert the result to a text string.

Parameters:
replace_image (str | None) – The string to replace the image content with.
If None, the image content will be included in the text as base64 string.

Returns:
str – The text representation of the result.

```python
class ToolSchema[source]#
```

【中文翻译】Bases: TypedDict


description: NotRequired[str]#



name: str#



parameters: NotRequired[ParametersSchema]#



strict: NotRequired[bool]#

```python
description: NotRequired[str]#
```

```python
name: str#
```

```python
parameters: NotRequired[ParametersSchema]#
```

```python
strict: NotRequired[bool]#
```

```python
class Workbench[source]#
```

【中文翻译】Bases: ABC, ComponentBase[BaseModel]
A workbench is a component that provides a set of tools that may share
resources and state.
A workbench is responsible for managing the lifecycle of the tools and
providing a single interface to call them. The tools provided by the workbench
may be dynamic and their availabilities may change after each tool execution.
A workbench can be started by calling the start() method
and stopped by calling the stop() method.
It can also be used as an asynchronous context manager, which will automatically
start and stop the workbench when entering and exiting the context.


abstract async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.





component_type: ClassVar[ComponentType] = 'workbench'#
The logical type of the component.



abstract async list_tools() → List[ToolSchema][source]#
List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.



abstract async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.





abstract async reset() → None[source]#
Reset the workbench to its initialized, started state.



abstract async save_state() → Mapping[str, Any][source]#
Save the state of the workbench.
This method should be called to persist the state of the workbench.



abstract async start() → None[source]#
Start the workbench and initialize any resources.
This method should be called before using the workbench.



abstract async stop() → None[source]#
Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

```python
abstract async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
```

【中文翻译】Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.

```python
component_type: ClassVar[ComponentType] = 'workbench'#
```

【中文翻译】The logical type of the component.

```python
abstract async list_tools() → List[ToolSchema][source]#
```

【中文翻译】List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.

```python
abstract async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.

```python
abstract async reset() → None[source]#
```

【中文翻译】Reset the workbench to its initialized, started state.

```python
abstract async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the workbench.
This method should be called to persist the state of the workbench.

```python
abstract async start() → None[source]#
```

【中文翻译】Start the workbench and initialize any resources.
This method should be called before using the workbench.

```python
abstract async stop() → None[source]#
```

【中文翻译】Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

【中文翻译】previous

【中文翻译】autogen_core.model_context

【中文翻译】next

【中文翻译】autogen_core.tool_agent

### autogen_ext.agents.azure {autogen_extagentsazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html)

```python
class AzureAIAgent(name: str, description: str, project_client: AIProjectClient, deployment_name: str, instructions: str, tools: Iterable[Literal['file_search', 'code_interpreter', 'bing_grounding', 'azure_ai_search', 'azure_function', 'sharepoint_grounding'] | BingGroundingToolDefinition | CodeInterpreterToolDefinition | SharepointToolDefinition | AzureAISearchToolDefinition | FileSearchToolDefinition | AzureFunctionToolDefinition | Tool | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, agent_id: str | None = None, thread_id: str | None = None, metadata: Dict[str, str] | None = None, response_format: _types.AgentsApiResponseFormatOption | None = None, temperature: float | None = None, tool_resources: models.ToolResources | None = None, top_p: float | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent
Azure AI Assistant agent for AutoGen.
Installation:
pip install "autogen-ext[azure]"  # For Azure AI Foundry Agent Service


This agent leverages the Azure AI Assistant API to create AI assistants with capabilities like:

Code interpretation and execution
Grounding with Bing search
File handling and search
Custom function calling
Multi-turn conversations

The agent integrates with AutoGen’s messaging system, providing a seamless way to use Azure AI
capabilities within the AutoGen framework. It supports tools like code interpreter,
file search, and various grounding mechanisms.

Agent name must be a valid Python identifier:
It must start with a letter (A-Z, a-z) or an underscore (_).
It can only contain letters, digits (0-9), or underscores.
It cannot be a Python keyword.
It cannot contain spaces or special characters.
It cannot start with a digit.



Check here on how to create a new secured agent with user-managed identity:
https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/virtual-networks
Examples
Use the AzureAIAgent to create an agent grounded with Bing:
import asyncio
import os

from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential
import azure.ai.projects.models as models
import dotenv


async def bing_example():
    credential = DefaultAzureCredential()

    async with AIProjectClient.from_connection_string(  # type: ignore
        credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
    ) as project_client:
        conn = await project_client.connections.get(connection_name=os.getenv("BING_CONNECTION_NAME", ""))

        bing_tool = models.BingGroundingTool(conn.id)
        agent_with_bing_grounding = AzureAIAgent(
            name="bing_agent",
            description="An AI assistant with Bing grounding",
            project_client=project_client,
            deployment_name="gpt-4o",
            instructions="You are a helpful assistant.",
            tools=bing_tool.definitions,
            metadata={"source": "AzureAIAgent"},
        )

        # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them.
        # For example: "Please provide citations for the answers"

        result = await agent_with_bing_grounding.on_messages(
            messages=[
                TextMessage(
                    content="What is Microsoft's annual leave policy? Provide citations for your answers.",
                    source="user",
                )
            ],
            cancellation_token=CancellationToken(),
            message_limit=5,
        )
        print(result)


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(bing_example())


Use the AzureAIAgent to create an agent with file search capability:
import asyncio
import os
import tempfile
import urllib.request

import dotenv
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential


async def file_search_example():
    # Download README.md from GitHub
    readme_url = "https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md"
    temp_file = None

    try:
        # Create a temporary file to store the downloaded README
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".md")
        urllib.request.urlretrieve(readme_url, temp_file.name)
        print(f"Downloaded README.md to {temp_file.name}")

        credential = DefaultAzureCredential()
        async with AIProjectClient.from_connection_string(  # type: ignore
            credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
        ) as project_client:
            agent_with_file_search = AzureAIAgent(
                name="file_search_agent",
                description="An AI assistant with file search capabilities",
                project_client=project_client,
                deployment_name="gpt-4o",
                instructions="You are a helpful assistant.",
                tools=["file_search"],
                metadata={"source": "AzureAIAgent"},
            )

            ct: CancellationToken = CancellationToken()
            # Use the downloaded README file for file search
            await agent_with_file_search.on_upload_for_file_search(
                file_paths=[temp_file.name],
                vector_store_name="file_upload_index",
                vector_store_metadata={"source": "AzureAIAgent"},
                cancellation_token=ct,
            )
            result = await agent_with_file_search.on_messages(
                messages=[
                    TextMessage(content="Hello, what is AutoGen and what capabilities does it have?", source="user")
                ],
                cancellation_token=ct,
                message_limit=5,
            )
            print(result)
    finally:
        # Clean up the temporary file
        if temp_file and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
            print(f"Removed temporary file {temp_file.name}")


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(file_search_example())


Use the AzureAIAgent to create an agent with code interpreter capability:
import asyncio
import os

import dotenv
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential


async def code_interpreter_example():
    credential = DefaultAzureCredential()
    async with AIProjectClient.from_connection_string(  # type: ignore
        credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
    ) as project_client:
        agent_with_code_interpreter = AzureAIAgent(
            name="code_interpreter_agent",
            description="An AI assistant with code interpreter capabilities",
            project_client=project_client,
            deployment_name="gpt-4o",
            instructions="You are a helpful assistant.",
            tools=["code_interpreter"],
            metadata={"source": "AzureAIAgent"},
        )

        await agent_with_code_interpreter.on_upload_for_code_interpreter(
            file_paths="/workspaces/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/data/nifty_500_quarterly_results.csv",
            cancellation_token=CancellationToken(),
        )

        result = await agent_with_code_interpreter.on_messages(
            messages=[
                TextMessage(
                    content="Aggregate the number of stocks per industry and give me a markdown table as a result?",
                    source="user",
                )
            ],
            cancellation_token=CancellationToken(),
        )

        print(result)


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(code_interpreter_example())




property agent_id: str#



property deployment_name: str#



property description: str#
The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.



async handle_text_message(content: str, cancellation_token: CancellationToken | None = None) → None[source]#
Handle a text message by adding it to the conversation thread.

Parameters:

content (str) – The text content of the message
cancellation_token (CancellationToken) – Token for cancellation handling


Returns:
None





property instructions: str#



async load_state(state: Mapping[str, Any]) → None[source]#
Load a previously saved state into this agent.
This method deserializes and restores a previously saved agent state,
setting up the agent to continue a previous conversation or session.

Parameters:
state (Mapping[str, Any]) – The previously saved state dictionary





async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1) → Response[source]#
Process incoming messages and return a response from the Azure AI agent.
This method is the primary entry point for interaction with the agent.
It delegates to on_messages_stream and returns the final response.

Parameters:

messages (Sequence[ChatMessage]) – The messages to process
cancellation_token (CancellationToken) – Token for cancellation handling
message_limit (int, optional) – Maximum number of messages to retrieve from the thread


Returns:
Response – The agent’s response, including the chat message and any inner events

Raises:
AssertionError – If the stream doesn’t return a final result





async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1, sleep_interval: float = 0.5) → AsyncGenerator[Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Response, None][source]#
Process incoming messages and yield streaming responses from the Azure AI agent.
This method handles the complete interaction flow with the Azure AI agent:
1. Processing input messages
2. Creating and monitoring a run
3. Handling tool calls and their results
4. Retrieving and returning the agent’s final response
The method yields events during processing (like tool calls) and finally yields
the complete Response with the agent’s message.

Parameters:

messages (Sequence[ChatMessage]) – The messages to process
cancellation_token (CancellationToken) – Token for cancellation handling
message_limit (int, optional) – Maximum number of messages to retrieve from the thread
sleep_interval (float, optional) – Time to sleep between polling for run status


Yields:
AgentEvent | ChatMessage | Response – Events during processing and the final response

Raises:
ValueError – If the run fails or no message is received from the assistant





async on_reset(cancellation_token: CancellationToken) → None[source]#
Reset the agent’s conversation by creating a new thread.
This method allows for resetting a conversation without losing the agent
definition or capabilities. It creates a new thread for fresh conversations.
Note: Currently the Azure AI Agent API has no support for deleting messages,
so a new thread is created instead.

Parameters:
cancellation_token (CancellationToken) – Token for cancellation handling





async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken | None = None, sleep_interval: float = 0.5) → None[source]#
Upload files to be used with the code interpreter tool.
This method uploads files for the agent’s code interpreter tool and
updates the thread’s tool resources to include these files.

Parameters:

file_paths (str | Iterable[str]) – Path(s) to file(s) to upload
cancellation_token (Optional[CancellationToken]) – Token for cancellation handling
sleep_interval (float) – Time to sleep between polling for file status


Raises:
ValueError – If file upload fails or the agent doesn’t have code interpreter capability





async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken, vector_store_name: str | None = None, data_sources: List[VectorStoreDataSource] | None = None, expires_after: VectorStoreExpirationPolicy | None = None, chunking_strategy: VectorStoreChunkingStrategyRequest | None = None, vector_store_metadata: Dict[str, str] | None = None, vector_store_polling_sleep_interval: float = 1) → None[source]#
Upload files to be used with the file search tool.
This method handles uploading files for the file search capability, creating a vector
store if necessary, and updating the agent’s configuration to use the vector store.

Parameters:

file_paths (str | Iterable[str]) – Path(s) to file(s) to upload
cancellation_token (CancellationToken) – Token for cancellation handling
vector_store_name (Optional[str]) – Name to assign to the vector store if creating a new one
data_sources (Optional[List[models.VectorStoreDataSource]]) – Additional data sources for the vector store
expires_after (Optional[models.VectorStoreExpirationPolicy]) – Expiration policy for vector store content
chunking_strategy (Optional[models.VectorStoreChunkingStrategyRequest]) – Strategy for chunking file content
vector_store_metadata (Optional[Dict[str, str]]) – Additional metadata for the vector store
vector_store_polling_sleep_interval (float) – Time to sleep between polling for vector store status


Raises:
ValueError – If file search is not enabled for this agent or file upload fails





property produced_message_types: Sequence[type[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]#
The types of messages that the assistant agent produces.



async save_state() → Mapping[str, Any][source]#
Save the current state of the agent for future restoration.
This method serializes the agent’s state including IDs for the agent, thread,
messages, and associated resources like vector stores and uploaded files.

Returns:
Mapping[str, Any] – A dictionary containing the serialized state data





property thread_id: str#



property tools: List[ToolDefinition]#
Get the list of tools available to the agent.

Returns:
List[models.ToolDefinition] – The list of tool definitions.

**示例**:
```python
pip install "autogen-ext[azure]"  # For Azure AI Foundry Agent Service

```

**示例**:
```python
import asyncio
import os

from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential
import azure.ai.projects.models as models
import dotenv


async def bing_example():
    credential = DefaultAzureCredential()

    async with AIProjectClient.from_connection_string(  # type: ignore
        credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
    ) as project_client:
        conn = await project_client.connections.get(connection_name=os.getenv("BING_CONNECTION_NAME", ""))

        bing_tool = models.BingGroundingTool(conn.id)
        agent_with_bing_grounding = AzureAIAgent(
            name="bing_agent",
            description="An AI assistant with Bing grounding",
            project_client=project_client,
            deployment_name="gpt-4o",
            instructions="You are a helpful assistant.",
            tools=bing_tool.definitions,
            metadata={"source": "AzureAIAgent"},
        )

        # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them.
        # For example: "Please provide citations for the answers"

        result = await agent_with_bing_grounding.on_messages(
            messages=[
                TextMessage(
                    content="What is Microsoft's annual leave policy? Provide citations for your answers.",
                    source="user",
                )
            ],
            cancellation_token=CancellationToken(),
            message_limit=5,
        )
        print(result)


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(bing_example())

```

**示例**:
```python
import asyncio
import os
import tempfile
import urllib.request

import dotenv
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential


async def file_search_example():
    # Download README.md from GitHub
    readme_url = "https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md"
    temp_file = None

    try:
        # Create a temporary file to store the downloaded README
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".md")
        urllib.request.urlretrieve(readme_url, temp_file.name)
        print(f"Downloaded README.md to {temp_file.name}")

        credential = DefaultAzureCredential()
        async with AIProjectClient.from_connection_string(  # type: ignore
            credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
        ) as project_client:
            agent_with_file_search = AzureAIAgent(
                name="file_search_agent",
                description="An AI assistant with file search capabilities",
                project_client=project_client,
                deployment_name="gpt-4o",
                instructions="You are a helpful assistant.",
                tools=["file_search"],
                metadata={"source": "AzureAIAgent"},
            )

            ct: CancellationToken = CancellationToken()
            # Use the downloaded README file for file search
            await agent_with_file_search.on_upload_for_file_search(
                file_paths=[temp_file.name],
                vector_store_name="file_upload_index",
                vector_store_metadata={"source": "AzureAIAgent"},
                cancellation_token=ct,
            )
            result = await agent_with_file_search.on_messages(
                messages=[
                    TextMessage(content="Hello, what is AutoGen and what capabilities does it have?", source="user")
                ],
                cancellation_token=ct,
                message_limit=5,
            )
            print(result)
    finally:
        # Clean up the temporary file
        if temp_file and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
            print(f"Removed temporary file {temp_file.name}")


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(file_search_example())

```

**示例**:
```python
import asyncio
import os

import dotenv
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential


async def code_interpreter_example():
    credential = DefaultAzureCredential()
    async with AIProjectClient.from_connection_string(  # type: ignore
        credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
    ) as project_client:
        agent_with_code_interpreter = AzureAIAgent(
            name="code_interpreter_agent",
            description="An AI assistant with code interpreter capabilities",
            project_client=project_client,
            deployment_name="gpt-4o",
            instructions="You are a helpful assistant.",
            tools=["code_interpreter"],
            metadata={"source": "AzureAIAgent"},
        )

        await agent_with_code_interpreter.on_upload_for_code_interpreter(
            file_paths="/workspaces/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/data/nifty_500_quarterly_results.csv",
            cancellation_token=CancellationToken(),
        )

        result = await agent_with_code_interpreter.on_messages(
            messages=[
                TextMessage(
                    content="Aggregate the number of stocks per industry and give me a markdown table as a result?",
                    source="user",
                )
            ],
            cancellation_token=CancellationToken(),
        )

        print(result)


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(code_interpreter_example())

```

```python
property agent_id: str#
```

```python
property deployment_name: str#
```

```python
property description: str#
```

【中文翻译】The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.

```python
async handle_text_message(content: str, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Handle a text message by adding it to the conversation thread.

Parameters:

content (str) – The text content of the message
cancellation_token (CancellationToken) – Token for cancellation handling


Returns:
None

```python
property instructions: str#
```

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load a previously saved state into this agent.
This method deserializes and restores a previously saved agent state,
setting up the agent to continue a previous conversation or session.

Parameters:
state (Mapping[str, Any]) – The previously saved state dictionary

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1) → Response[source]#
```

【中文翻译】Process incoming messages and return a response from the Azure AI agent.
This method is the primary entry point for interaction with the agent.
It delegates to on_messages_stream and returns the final response.

Parameters:

messages (Sequence[ChatMessage]) – The messages to process
cancellation_token (CancellationToken) – Token for cancellation handling
message_limit (int, optional) – Maximum number of messages to retrieve from the thread


Returns:
Response – The agent’s response, including the chat message and any inner events

Raises:
AssertionError – If the stream doesn’t return a final result

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1, sleep_interval: float = 0.5) → AsyncGenerator[Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Response, None][source]#
```

【中文翻译】Process incoming messages and yield streaming responses from the Azure AI agent.
This method handles the complete interaction flow with the Azure AI agent:
1. Processing input messages
2. Creating and monitoring a run
3. Handling tool calls and their results
4. Retrieving and returning the agent’s final response
The method yields events during processing (like tool calls) and finally yields
the complete Response with the agent’s message.

Parameters:

messages (Sequence[ChatMessage]) – The messages to process
cancellation_token (CancellationToken) – Token for cancellation handling
message_limit (int, optional) – Maximum number of messages to retrieve from the thread
sleep_interval (float, optional) – Time to sleep between polling for run status


Yields:
AgentEvent | ChatMessage | Response – Events during processing and the final response

Raises:
ValueError – If the run fails or no message is received from the assistant

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Reset the agent’s conversation by creating a new thread.
This method allows for resetting a conversation without losing the agent
definition or capabilities. It creates a new thread for fresh conversations.
Note: Currently the Azure AI Agent API has no support for deleting messages,
so a new thread is created instead.

Parameters:
cancellation_token (CancellationToken) – Token for cancellation handling

```python
async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken | None = None, sleep_interval: float = 0.5) → None[source]#
```

【中文翻译】Upload files to be used with the code interpreter tool.
This method uploads files for the agent’s code interpreter tool and
updates the thread’s tool resources to include these files.

Parameters:

file_paths (str | Iterable[str]) – Path(s) to file(s) to upload
cancellation_token (Optional[CancellationToken]) – Token for cancellation handling
sleep_interval (float) – Time to sleep between polling for file status


Raises:
ValueError – If file upload fails or the agent doesn’t have code interpreter capability

```python
async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken, vector_store_name: str | None = None, data_sources: List[VectorStoreDataSource] | None = None, expires_after: VectorStoreExpirationPolicy | None = None, chunking_strategy: VectorStoreChunkingStrategyRequest | None = None, vector_store_metadata: Dict[str, str] | None = None, vector_store_polling_sleep_interval: float = 1) → None[source]#
```

【中文翻译】Upload files to be used with the file search tool.
This method handles uploading files for the file search capability, creating a vector
store if necessary, and updating the agent’s configuration to use the vector store.

Parameters:

file_paths (str | Iterable[str]) – Path(s) to file(s) to upload
cancellation_token (CancellationToken) – Token for cancellation handling
vector_store_name (Optional[str]) – Name to assign to the vector store if creating a new one
data_sources (Optional[List[models.VectorStoreDataSource]]) – Additional data sources for the vector store
expires_after (Optional[models.VectorStoreExpirationPolicy]) – Expiration policy for vector store content
chunking_strategy (Optional[models.VectorStoreChunkingStrategyRequest]) – Strategy for chunking file content
vector_store_metadata (Optional[Dict[str, str]]) – Additional metadata for the vector store
vector_store_polling_sleep_interval (float) – Time to sleep between polling for vector store status


Raises:
ValueError – If file search is not enabled for this agent or file upload fails

```python
property produced_message_types: Sequence[type[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]#
```

【中文翻译】The types of messages that the assistant agent produces.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the current state of the agent for future restoration.
This method serializes the agent’s state including IDs for the agent, thread,
messages, and associated resources like vector stores and uploaded files.

Returns:
Mapping[str, Any] – A dictionary containing the serialized state data

```python
property thread_id: str#
```

```python
property tools: List[ToolDefinition]#
```

【中文翻译】Get the list of tools available to the agent.

Returns:
List[models.ToolDefinition] – The list of tool definitions.

【中文翻译】previous

【中文翻译】autogen_core.logging

【中文翻译】next

【中文翻译】autogen_ext.agents.magentic_one

### autogen_ext.agents.azure {autogen_extagentsazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html)

```python
class AzureAIAgent(name: str, description: str, project_client: AIProjectClient, deployment_name: str, instructions: str, tools: Iterable[Literal['file_search', 'code_interpreter', 'bing_grounding', 'azure_ai_search', 'azure_function', 'sharepoint_grounding'] | BingGroundingToolDefinition | CodeInterpreterToolDefinition | SharepointToolDefinition | AzureAISearchToolDefinition | FileSearchToolDefinition | AzureFunctionToolDefinition | Tool | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, agent_id: str | None = None, thread_id: str | None = None, metadata: Dict[str, str] | None = None, response_format: _types.AgentsApiResponseFormatOption | None = None, temperature: float | None = None, tool_resources: models.ToolResources | None = None, top_p: float | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent
Azure AI Assistant agent for AutoGen.
Installation:
pip install "autogen-ext[azure]"  # For Azure AI Foundry Agent Service


This agent leverages the Azure AI Assistant API to create AI assistants with capabilities like:

Code interpretation and execution
Grounding with Bing search
File handling and search
Custom function calling
Multi-turn conversations

The agent integrates with AutoGen’s messaging system, providing a seamless way to use Azure AI
capabilities within the AutoGen framework. It supports tools like code interpreter,
file search, and various grounding mechanisms.

Agent name must be a valid Python identifier:
It must start with a letter (A-Z, a-z) or an underscore (_).
It can only contain letters, digits (0-9), or underscores.
It cannot be a Python keyword.
It cannot contain spaces or special characters.
It cannot start with a digit.



Check here on how to create a new secured agent with user-managed identity:
https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/virtual-networks
Examples
Use the AzureAIAgent to create an agent grounded with Bing:
import asyncio
import os

from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential
import azure.ai.projects.models as models
import dotenv


async def bing_example():
    credential = DefaultAzureCredential()

    async with AIProjectClient.from_connection_string(  # type: ignore
        credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
    ) as project_client:
        conn = await project_client.connections.get(connection_name=os.getenv("BING_CONNECTION_NAME", ""))

        bing_tool = models.BingGroundingTool(conn.id)
        agent_with_bing_grounding = AzureAIAgent(
            name="bing_agent",
            description="An AI assistant with Bing grounding",
            project_client=project_client,
            deployment_name="gpt-4o",
            instructions="You are a helpful assistant.",
            tools=bing_tool.definitions,
            metadata={"source": "AzureAIAgent"},
        )

        # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them.
        # For example: "Please provide citations for the answers"

        result = await agent_with_bing_grounding.on_messages(
            messages=[
                TextMessage(
                    content="What is Microsoft's annual leave policy? Provide citations for your answers.",
                    source="user",
                )
            ],
            cancellation_token=CancellationToken(),
            message_limit=5,
        )
        print(result)


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(bing_example())


Use the AzureAIAgent to create an agent with file search capability:
import asyncio
import os
import tempfile
import urllib.request

import dotenv
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential


async def file_search_example():
    # Download README.md from GitHub
    readme_url = "https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md"
    temp_file = None

    try:
        # Create a temporary file to store the downloaded README
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".md")
        urllib.request.urlretrieve(readme_url, temp_file.name)
        print(f"Downloaded README.md to {temp_file.name}")

        credential = DefaultAzureCredential()
        async with AIProjectClient.from_connection_string(  # type: ignore
            credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
        ) as project_client:
            agent_with_file_search = AzureAIAgent(
                name="file_search_agent",
                description="An AI assistant with file search capabilities",
                project_client=project_client,
                deployment_name="gpt-4o",
                instructions="You are a helpful assistant.",
                tools=["file_search"],
                metadata={"source": "AzureAIAgent"},
            )

            ct: CancellationToken = CancellationToken()
            # Use the downloaded README file for file search
            await agent_with_file_search.on_upload_for_file_search(
                file_paths=[temp_file.name],
                vector_store_name="file_upload_index",
                vector_store_metadata={"source": "AzureAIAgent"},
                cancellation_token=ct,
            )
            result = await agent_with_file_search.on_messages(
                messages=[
                    TextMessage(content="Hello, what is AutoGen and what capabilities does it have?", source="user")
                ],
                cancellation_token=ct,
                message_limit=5,
            )
            print(result)
    finally:
        # Clean up the temporary file
        if temp_file and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
            print(f"Removed temporary file {temp_file.name}")


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(file_search_example())


Use the AzureAIAgent to create an agent with code interpreter capability:
import asyncio
import os

import dotenv
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential


async def code_interpreter_example():
    credential = DefaultAzureCredential()
    async with AIProjectClient.from_connection_string(  # type: ignore
        credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
    ) as project_client:
        agent_with_code_interpreter = AzureAIAgent(
            name="code_interpreter_agent",
            description="An AI assistant with code interpreter capabilities",
            project_client=project_client,
            deployment_name="gpt-4o",
            instructions="You are a helpful assistant.",
            tools=["code_interpreter"],
            metadata={"source": "AzureAIAgent"},
        )

        await agent_with_code_interpreter.on_upload_for_code_interpreter(
            file_paths="/workspaces/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/data/nifty_500_quarterly_results.csv",
            cancellation_token=CancellationToken(),
        )

        result = await agent_with_code_interpreter.on_messages(
            messages=[
                TextMessage(
                    content="Aggregate the number of stocks per industry and give me a markdown table as a result?",
                    source="user",
                )
            ],
            cancellation_token=CancellationToken(),
        )

        print(result)


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(code_interpreter_example())




property agent_id: str#



property deployment_name: str#



property description: str#
The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.



async handle_text_message(content: str, cancellation_token: CancellationToken | None = None) → None[source]#
Handle a text message by adding it to the conversation thread.

Parameters:

content (str) – The text content of the message
cancellation_token (CancellationToken) – Token for cancellation handling


Returns:
None





property instructions: str#



async load_state(state: Mapping[str, Any]) → None[source]#
Load a previously saved state into this agent.
This method deserializes and restores a previously saved agent state,
setting up the agent to continue a previous conversation or session.

Parameters:
state (Mapping[str, Any]) – The previously saved state dictionary





async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1) → Response[source]#
Process incoming messages and return a response from the Azure AI agent.
This method is the primary entry point for interaction with the agent.
It delegates to on_messages_stream and returns the final response.

Parameters:

messages (Sequence[ChatMessage]) – The messages to process
cancellation_token (CancellationToken) – Token for cancellation handling
message_limit (int, optional) – Maximum number of messages to retrieve from the thread


Returns:
Response – The agent’s response, including the chat message and any inner events

Raises:
AssertionError – If the stream doesn’t return a final result





async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1, sleep_interval: float = 0.5) → AsyncGenerator[Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Response, None][source]#
Process incoming messages and yield streaming responses from the Azure AI agent.
This method handles the complete interaction flow with the Azure AI agent:
1. Processing input messages
2. Creating and monitoring a run
3. Handling tool calls and their results
4. Retrieving and returning the agent’s final response
The method yields events during processing (like tool calls) and finally yields
the complete Response with the agent’s message.

Parameters:

messages (Sequence[ChatMessage]) – The messages to process
cancellation_token (CancellationToken) – Token for cancellation handling
message_limit (int, optional) – Maximum number of messages to retrieve from the thread
sleep_interval (float, optional) – Time to sleep between polling for run status


Yields:
AgentEvent | ChatMessage | Response – Events during processing and the final response

Raises:
ValueError – If the run fails or no message is received from the assistant





async on_reset(cancellation_token: CancellationToken) → None[source]#
Reset the agent’s conversation by creating a new thread.
This method allows for resetting a conversation without losing the agent
definition or capabilities. It creates a new thread for fresh conversations.
Note: Currently the Azure AI Agent API has no support for deleting messages,
so a new thread is created instead.

Parameters:
cancellation_token (CancellationToken) – Token for cancellation handling





async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken | None = None, sleep_interval: float = 0.5) → None[source]#
Upload files to be used with the code interpreter tool.
This method uploads files for the agent’s code interpreter tool and
updates the thread’s tool resources to include these files.

Parameters:

file_paths (str | Iterable[str]) – Path(s) to file(s) to upload
cancellation_token (Optional[CancellationToken]) – Token for cancellation handling
sleep_interval (float) – Time to sleep between polling for file status


Raises:
ValueError – If file upload fails or the agent doesn’t have code interpreter capability





async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken, vector_store_name: str | None = None, data_sources: List[VectorStoreDataSource] | None = None, expires_after: VectorStoreExpirationPolicy | None = None, chunking_strategy: VectorStoreChunkingStrategyRequest | None = None, vector_store_metadata: Dict[str, str] | None = None, vector_store_polling_sleep_interval: float = 1) → None[source]#
Upload files to be used with the file search tool.
This method handles uploading files for the file search capability, creating a vector
store if necessary, and updating the agent’s configuration to use the vector store.

Parameters:

file_paths (str | Iterable[str]) – Path(s) to file(s) to upload
cancellation_token (CancellationToken) – Token for cancellation handling
vector_store_name (Optional[str]) – Name to assign to the vector store if creating a new one
data_sources (Optional[List[models.VectorStoreDataSource]]) – Additional data sources for the vector store
expires_after (Optional[models.VectorStoreExpirationPolicy]) – Expiration policy for vector store content
chunking_strategy (Optional[models.VectorStoreChunkingStrategyRequest]) – Strategy for chunking file content
vector_store_metadata (Optional[Dict[str, str]]) – Additional metadata for the vector store
vector_store_polling_sleep_interval (float) – Time to sleep between polling for vector store status


Raises:
ValueError – If file search is not enabled for this agent or file upload fails





property produced_message_types: Sequence[type[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]#
The types of messages that the assistant agent produces.



async save_state() → Mapping[str, Any][source]#
Save the current state of the agent for future restoration.
This method serializes the agent’s state including IDs for the agent, thread,
messages, and associated resources like vector stores and uploaded files.

Returns:
Mapping[str, Any] – A dictionary containing the serialized state data





property thread_id: str#



property tools: List[ToolDefinition]#
Get the list of tools available to the agent.

Returns:
List[models.ToolDefinition] – The list of tool definitions.

**示例**:
```python
pip install "autogen-ext[azure]"  # For Azure AI Foundry Agent Service

```

**示例**:
```python
import asyncio
import os

from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential
import azure.ai.projects.models as models
import dotenv


async def bing_example():
    credential = DefaultAzureCredential()

    async with AIProjectClient.from_connection_string(  # type: ignore
        credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
    ) as project_client:
        conn = await project_client.connections.get(connection_name=os.getenv("BING_CONNECTION_NAME", ""))

        bing_tool = models.BingGroundingTool(conn.id)
        agent_with_bing_grounding = AzureAIAgent(
            name="bing_agent",
            description="An AI assistant with Bing grounding",
            project_client=project_client,
            deployment_name="gpt-4o",
            instructions="You are a helpful assistant.",
            tools=bing_tool.definitions,
            metadata={"source": "AzureAIAgent"},
        )

        # For the bing grounding tool to return the citations, the message must contain an instruction for the model to do return them.
        # For example: "Please provide citations for the answers"

        result = await agent_with_bing_grounding.on_messages(
            messages=[
                TextMessage(
                    content="What is Microsoft's annual leave policy? Provide citations for your answers.",
                    source="user",
                )
            ],
            cancellation_token=CancellationToken(),
            message_limit=5,
        )
        print(result)


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(bing_example())

```

**示例**:
```python
import asyncio
import os
import tempfile
import urllib.request

import dotenv
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential


async def file_search_example():
    # Download README.md from GitHub
    readme_url = "https://raw.githubusercontent.com/microsoft/autogen/refs/heads/main/README.md"
    temp_file = None

    try:
        # Create a temporary file to store the downloaded README
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".md")
        urllib.request.urlretrieve(readme_url, temp_file.name)
        print(f"Downloaded README.md to {temp_file.name}")

        credential = DefaultAzureCredential()
        async with AIProjectClient.from_connection_string(  # type: ignore
            credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
        ) as project_client:
            agent_with_file_search = AzureAIAgent(
                name="file_search_agent",
                description="An AI assistant with file search capabilities",
                project_client=project_client,
                deployment_name="gpt-4o",
                instructions="You are a helpful assistant.",
                tools=["file_search"],
                metadata={"source": "AzureAIAgent"},
            )

            ct: CancellationToken = CancellationToken()
            # Use the downloaded README file for file search
            await agent_with_file_search.on_upload_for_file_search(
                file_paths=[temp_file.name],
                vector_store_name="file_upload_index",
                vector_store_metadata={"source": "AzureAIAgent"},
                cancellation_token=ct,
            )
            result = await agent_with_file_search.on_messages(
                messages=[
                    TextMessage(content="Hello, what is AutoGen and what capabilities does it have?", source="user")
                ],
                cancellation_token=ct,
                message_limit=5,
            )
            print(result)
    finally:
        # Clean up the temporary file
        if temp_file and os.path.exists(temp_file.name):
            os.unlink(temp_file.name)
            print(f"Removed temporary file {temp_file.name}")


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(file_search_example())

```

**示例**:
```python
import asyncio
import os

import dotenv
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.agents.azure._azure_ai_agent import AzureAIAgent
from azure.ai.projects.aio import AIProjectClient
from azure.identity.aio import DefaultAzureCredential


async def code_interpreter_example():
    credential = DefaultAzureCredential()
    async with AIProjectClient.from_connection_string(  # type: ignore
        credential=credential, conn_str=os.getenv("AI_PROJECT_CONNECTION_STRING", "")
    ) as project_client:
        agent_with_code_interpreter = AzureAIAgent(
            name="code_interpreter_agent",
            description="An AI assistant with code interpreter capabilities",
            project_client=project_client,
            deployment_name="gpt-4o",
            instructions="You are a helpful assistant.",
            tools=["code_interpreter"],
            metadata={"source": "AzureAIAgent"},
        )

        await agent_with_code_interpreter.on_upload_for_code_interpreter(
            file_paths="/workspaces/autogen/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/data/nifty_500_quarterly_results.csv",
            cancellation_token=CancellationToken(),
        )

        result = await agent_with_code_interpreter.on_messages(
            messages=[
                TextMessage(
                    content="Aggregate the number of stocks per industry and give me a markdown table as a result?",
                    source="user",
                )
            ],
            cancellation_token=CancellationToken(),
        )

        print(result)


if __name__ == "__main__":
    dotenv.load_dotenv()
    asyncio.run(code_interpreter_example())

```

```python
property agent_id: str#
```

```python
property deployment_name: str#
```

```python
property description: str#
```

【中文翻译】The description of the agent. This is used by team to
make decisions about which agents to use. The description should
describe the agent’s capabilities and how to interact with it.

```python
async handle_text_message(content: str, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Handle a text message by adding it to the conversation thread.

Parameters:

content (str) – The text content of the message
cancellation_token (CancellationToken) – Token for cancellation handling


Returns:
None

```python
property instructions: str#
```

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load a previously saved state into this agent.
This method deserializes and restores a previously saved agent state,
setting up the agent to continue a previous conversation or session.

Parameters:
state (Mapping[str, Any]) – The previously saved state dictionary

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1) → Response[source]#
```

【中文翻译】Process incoming messages and return a response from the Azure AI agent.
This method is the primary entry point for interaction with the agent.
It delegates to on_messages_stream and returns the final response.

Parameters:

messages (Sequence[ChatMessage]) – The messages to process
cancellation_token (CancellationToken) – Token for cancellation handling
message_limit (int, optional) – Maximum number of messages to retrieve from the thread


Returns:
Response – The agent’s response, including the chat message and any inner events

Raises:
AssertionError – If the stream doesn’t return a final result

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken | None = None, message_limit: int = 1, sleep_interval: float = 0.5) → AsyncGenerator[Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')] | Response, None][source]#
```

【中文翻译】Process incoming messages and yield streaming responses from the Azure AI agent.
This method handles the complete interaction flow with the Azure AI agent:
1. Processing input messages
2. Creating and monitoring a run
3. Handling tool calls and their results
4. Retrieving and returning the agent’s final response
The method yields events during processing (like tool calls) and finally yields
the complete Response with the agent’s message.

Parameters:

messages (Sequence[ChatMessage]) – The messages to process
cancellation_token (CancellationToken) – Token for cancellation handling
message_limit (int, optional) – Maximum number of messages to retrieve from the thread
sleep_interval (float, optional) – Time to sleep between polling for run status


Yields:
AgentEvent | ChatMessage | Response – Events during processing and the final response

Raises:
ValueError – If the run fails or no message is received from the assistant

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Reset the agent’s conversation by creating a new thread.
This method allows for resetting a conversation without losing the agent
definition or capabilities. It creates a new thread for fresh conversations.
Note: Currently the Azure AI Agent API has no support for deleting messages,
so a new thread is created instead.

Parameters:
cancellation_token (CancellationToken) – Token for cancellation handling

```python
async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken | None = None, sleep_interval: float = 0.5) → None[source]#
```

【中文翻译】Upload files to be used with the code interpreter tool.
This method uploads files for the agent’s code interpreter tool and
updates the thread’s tool resources to include these files.

Parameters:

file_paths (str | Iterable[str]) – Path(s) to file(s) to upload
cancellation_token (Optional[CancellationToken]) – Token for cancellation handling
sleep_interval (float) – Time to sleep between polling for file status


Raises:
ValueError – If file upload fails or the agent doesn’t have code interpreter capability

```python
async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken, vector_store_name: str | None = None, data_sources: List[VectorStoreDataSource] | None = None, expires_after: VectorStoreExpirationPolicy | None = None, chunking_strategy: VectorStoreChunkingStrategyRequest | None = None, vector_store_metadata: Dict[str, str] | None = None, vector_store_polling_sleep_interval: float = 1) → None[source]#
```

【中文翻译】Upload files to be used with the file search tool.
This method handles uploading files for the file search capability, creating a vector
store if necessary, and updating the agent’s configuration to use the vector store.

Parameters:

file_paths (str | Iterable[str]) – Path(s) to file(s) to upload
cancellation_token (CancellationToken) – Token for cancellation handling
vector_store_name (Optional[str]) – Name to assign to the vector store if creating a new one
data_sources (Optional[List[models.VectorStoreDataSource]]) – Additional data sources for the vector store
expires_after (Optional[models.VectorStoreExpirationPolicy]) – Expiration policy for vector store content
chunking_strategy (Optional[models.VectorStoreChunkingStrategyRequest]) – Strategy for chunking file content
vector_store_metadata (Optional[Dict[str, str]]) – Additional metadata for the vector store
vector_store_polling_sleep_interval (float) – Time to sleep between polling for vector store status


Raises:
ValueError – If file search is not enabled for this agent or file upload fails

```python
property produced_message_types: Sequence[type[Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]]#
```

【中文翻译】The types of messages that the assistant agent produces.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the current state of the agent for future restoration.
This method serializes the agent’s state including IDs for the agent, thread,
messages, and associated resources like vector stores and uploaded files.

Returns:
Mapping[str, Any] – A dictionary containing the serialized state data

```python
property thread_id: str#
```

```python
property tools: List[ToolDefinition]#
```

【中文翻译】Get the list of tools available to the agent.

Returns:
List[models.ToolDefinition] – The list of tool definitions.

【中文翻译】previous

【中文翻译】autogen_core.logging

【中文翻译】next

【中文翻译】autogen_ext.agents.magentic_one

### autogen_ext.agents.file_surfer {autogen_extagentsfile_surfer}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html)

```python
class FileSurfer(name: str, model_client: ChatCompletionClient, description: str = DEFAULT_DESCRIPTION, base_path: str = os.getcwd())[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[FileSurferConfig]
An agent, used by MagenticOne, that acts as a local file previewer. FileSurfer can open and read a variety of common file types, and can navigate the local file hierarchy.
Installation:
pip install "autogen-ext[file-surfer]"



Parameters:

name (str) – The agent’s name
model_client (ChatCompletionClient) – The model to use (must be tool-use enabled)
description (str) – The agent’s description used by the team. Defaults to DEFAULT_DESCRIPTION
base_path (str) – The base path to use for the file browser. Defaults to the current working directory.





DEFAULT_DESCRIPTION = 'An agent that can handle local files.'#



DEFAULT_SYSTEM_MESSAGES = [SystemMessage(content='\n        You are a helpful AI Assistant.\n        When given a user query, use available functions to help the user with their request.', type='SystemMessage')]#



classmethod _from_config(config: FileSurferConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → FileSurferConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of FileSurferConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.file_surfer.FileSurfer'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

**示例**:
```python
pip install "autogen-ext[file-surfer]"

```

```python
DEFAULT_DESCRIPTION = 'An agent that can handle local files.'#
```

```python
DEFAULT_SYSTEM_MESSAGES = [SystemMessage(content='\n        You are a helpful AI Assistant.\n        When given a user query, use available functions to help the user with their request.', type='SystemMessage')]#
```

```python
classmethod _from_config(config: FileSurferConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → FileSurferConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of FileSurferConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.file_surfer.FileSurfer'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

【中文翻译】previous

【中文翻译】autogen_ext.agents.web_surfer

【中文翻译】next

【中文翻译】autogen_ext.agents.video_surfer

### autogen_ext.agents.file_surfer {autogen_extagentsfile_surfer}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html)

```python
class FileSurfer(name: str, model_client: ChatCompletionClient, description: str = DEFAULT_DESCRIPTION, base_path: str = os.getcwd())[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[FileSurferConfig]
An agent, used by MagenticOne, that acts as a local file previewer. FileSurfer can open and read a variety of common file types, and can navigate the local file hierarchy.
Installation:
pip install "autogen-ext[file-surfer]"



Parameters:

name (str) – The agent’s name
model_client (ChatCompletionClient) – The model to use (must be tool-use enabled)
description (str) – The agent’s description used by the team. Defaults to DEFAULT_DESCRIPTION
base_path (str) – The base path to use for the file browser. Defaults to the current working directory.





DEFAULT_DESCRIPTION = 'An agent that can handle local files.'#



DEFAULT_SYSTEM_MESSAGES = [SystemMessage(content='\n        You are a helpful AI Assistant.\n        When given a user query, use available functions to help the user with their request.', type='SystemMessage')]#



classmethod _from_config(config: FileSurferConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → FileSurferConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of FileSurferConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.file_surfer.FileSurfer'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

**示例**:
```python
pip install "autogen-ext[file-surfer]"

```

```python
DEFAULT_DESCRIPTION = 'An agent that can handle local files.'#
```

```python
DEFAULT_SYSTEM_MESSAGES = [SystemMessage(content='\n        You are a helpful AI Assistant.\n        When given a user query, use available functions to help the user with their request.', type='SystemMessage')]#
```

```python
classmethod _from_config(config: FileSurferConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → FileSurferConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of FileSurferConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.file_surfer.FileSurfer'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

【中文翻译】previous

【中文翻译】autogen_ext.agents.web_surfer

【中文翻译】next

【中文翻译】autogen_ext.agents.video_surfer

### autogen_ext.agents.magentic_one {autogen_extagentsmagentic_one}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html)

```python
class MagenticOneCoderAgent(name: str, model_client: ChatCompletionClient, **kwargs: Any)[source]#
```

【中文翻译】Bases: AssistantAgent
An agent, used by MagenticOne that provides coding assistance using an LLM model client.
The prompts and description are sealed, to replicate the original MagenticOne configuration. See AssistantAgent if you wish to modify these values.


component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.magentic_one.MagenticOneCoderAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.magentic_one.MagenticOneCoderAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

【中文翻译】previous

【中文翻译】autogen_ext.agents.azure

【中文翻译】next

【中文翻译】autogen_ext.agents.openai

### autogen_ext.agents.magentic_one {autogen_extagentsmagentic_one}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html)

```python
class MagenticOneCoderAgent(name: str, model_client: ChatCompletionClient, **kwargs: Any)[source]#
```

【中文翻译】Bases: AssistantAgent
An agent, used by MagenticOne that provides coding assistance using an LLM model client.
The prompts and description are sealed, to replicate the original MagenticOne configuration. See AssistantAgent if you wish to modify these values.


component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.magentic_one.MagenticOneCoderAgent'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.magentic_one.MagenticOneCoderAgent'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

【中文翻译】previous

【中文翻译】autogen_ext.agents.azure

【中文翻译】next

【中文翻译】autogen_ext.agents.openai

### autogen_ext.agents.openai {autogen_extagentsopenai}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html)

```python
class OpenAIAssistantAgent(name: str, description: str, client: AsyncOpenAI | AsyncAzureOpenAI, model: str, instructions: str, tools: Iterable[Literal['code_interpreter', 'file_search'] | Tool | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, assistant_id: str | None = None, thread_id: str | None = None, metadata: Dict[str, str] | None = None, response_format: Literal['auto'] | ResponseFormatText | ResponseFormatJSONObject | ResponseFormatJSONSchema | None = None, temperature: float | None = None, tool_resources: ToolResources | None = None, top_p: float | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent
An agent implementation that uses the Assistant API to generate responses.
Installation:
pip install "autogen-ext[openai]"
# pip install "autogen-ext[openai,azure]"  # For Azure OpenAI Assistant


This agent leverages the Assistant API to create AI assistants with capabilities like:

Code interpretation and execution
File handling and search
Custom function calling
Multi-turn conversations

The agent maintains a thread of conversation and can use various tools including

Code interpreter: For executing code and working with files
File search: For searching through uploaded documents
Custom functions: For extending capabilities with user-defined tools

Key Features:

Supports multiple file formats including code, documents, images
Can handle up to 128 tools per assistant
Maintains conversation context in threads
Supports file uploads for code interpreter and search
Vector store integration for efficient file search
Automatic file parsing and embedding

You can use an existing thread or assistant by providing the thread_id or assistant_id parameters.
Examples
Use the assistant to analyze data in a CSV file:
from openai import AsyncOpenAI
from autogen_core import CancellationToken
import asyncio
from autogen_ext.agents.openai import OpenAIAssistantAgent
from autogen_agentchat.messages import TextMessage


async def example():
    cancellation_token = CancellationToken()

    # Create an OpenAI client
    client = AsyncOpenAI(api_key="your-api-key", base_url="your-base-url")

    # Create an assistant with code interpreter
    assistant = OpenAIAssistantAgent(
        name="Python Helper",
        description="Helps with Python programming",
        client=client,
        model="gpt-4",
        instructions="You are a helpful Python programming assistant.",
        tools=["code_interpreter"],
    )

    # Upload files for the assistant to use
    await assistant.on_upload_for_code_interpreter("data.csv", cancellation_token)

    # Get response from the assistant
    response = await assistant.on_messages(
        [TextMessage(source="user", content="Analyze the data in data.csv")], cancellation_token
    )

    print(response)

    # Clean up resources
    await assistant.delete_uploaded_files(cancellation_token)
    await assistant.delete_assistant(cancellation_token)


asyncio.run(example())


Use Azure OpenAI Assistant with AAD authentication:
from openai import AsyncAzureOpenAI
import asyncio
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from autogen_core import CancellationToken
from autogen_ext.agents.openai import OpenAIAssistantAgent
from autogen_agentchat.messages import TextMessage


async def example():
    cancellation_token = CancellationToken()

    # Create an Azure OpenAI client
    token_provider = get_bearer_token_provider(DefaultAzureCredential())
    client = AsyncAzureOpenAI(
        azure_deployment="YOUR_AZURE_DEPLOYMENT",
        api_version="YOUR_API_VERSION",
        azure_endpoint="YOUR_AZURE_ENDPOINT",
        azure_ad_token_provider=token_provider,
    )

    # Create an assistant with code interpreter
    assistant = OpenAIAssistantAgent(
        name="Python Helper",
        description="Helps with Python programming",
        client=client,
        model="gpt-4o",
        instructions="You are a helpful Python programming assistant.",
        tools=["code_interpreter"],
    )

    # Get response from the assistant
    response = await assistant.on_messages([TextMessage(source="user", content="Hello.")], cancellation_token)

    print(response)

    # Clean up resources
    await assistant.delete_assistant(cancellation_token)


asyncio.run(example())



Parameters:

name (str) – Name of the assistant
description (str) – Description of the assistant’s purpose
client (AsyncOpenAI | AsyncAzureOpenAI) – OpenAI client or Azure OpenAI client instance
model (str) – Model to use (e.g. “gpt-4”)
instructions (str) – System instructions for the assistant
tools (Optional[Iterable[Union[Literal["code_interpreter", "file_search"], Tool | Callable[..., Any] | Callable[..., Awaitable[Any]]]]]) – Tools the assistant can use
assistant_id (Optional[str]) – ID of existing assistant to use
thread_id (Optional[str]) – ID of existing thread to use
metadata (Optional[Dict[str, str]]) – Additional metadata for the assistant.
response_format (Optional[AssistantResponseFormatOptionParam]) – Response format settings
temperature (Optional[float]) – Temperature for response generation
tool_resources (Optional[ToolResources]) – Additional tool configuration
top_p (Optional[float]) – Top p sampling parameter





async delete_assistant(cancellation_token: CancellationToken) → None[source]#
Delete the assistant if it was created by this instance.



async delete_uploaded_files(cancellation_token: CancellationToken) → None[source]#
Delete all files that were uploaded by this agent instance.



async delete_vector_store(cancellation_token: CancellationToken) → None[source]#
Delete the vector store if it was created by this instance.



async handle_incoming_message(message: BaseChatMessage, cancellation_token: CancellationToken) → None[source]#
Handle regular text messages by adding them to the thread.



async load_state(state: Mapping[str, Any]) → None[source]#
Restore agent from saved state. Default implementation for stateless agents.



property messages: AsyncMessages#



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handle incoming messages and return a response.



async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handle incoming messages and return a response.



async on_reset(cancellation_token: CancellationToken) → None[source]#
Handle reset command by deleting new messages and runs since initialization.



async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken) → None[source]#
Handle file uploads for the code interpreter.



async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken) → None[source]#
Handle file uploads for file search.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the assistant agent produces.



property runs: AsyncRuns#



async save_state() → Mapping[str, Any][source]#
Export state. Default implementation for stateless agents.



property threads: AsyncThreads#

**示例**:
```python
pip install "autogen-ext[openai]"
# pip install "autogen-ext[openai,azure]"  # For Azure OpenAI Assistant

```

**示例**:
```python
from openai import AsyncOpenAI
from autogen_core import CancellationToken
import asyncio
from autogen_ext.agents.openai import OpenAIAssistantAgent
from autogen_agentchat.messages import TextMessage


async def example():
    cancellation_token = CancellationToken()

    # Create an OpenAI client
    client = AsyncOpenAI(api_key="your-api-key", base_url="your-base-url")

    # Create an assistant with code interpreter
    assistant = OpenAIAssistantAgent(
        name="Python Helper",
        description="Helps with Python programming",
        client=client,
        model="gpt-4",
        instructions="You are a helpful Python programming assistant.",
        tools=["code_interpreter"],
    )

    # Upload files for the assistant to use
    await assistant.on_upload_for_code_interpreter("data.csv", cancellation_token)

    # Get response from the assistant
    response = await assistant.on_messages(
        [TextMessage(source="user", content="Analyze the data in data.csv")], cancellation_token
    )

    print(response)

    # Clean up resources
    await assistant.delete_uploaded_files(cancellation_token)
    await assistant.delete_assistant(cancellation_token)


asyncio.run(example())

```

**示例**:
```python
from openai import AsyncAzureOpenAI
import asyncio
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from autogen_core import CancellationToken
from autogen_ext.agents.openai import OpenAIAssistantAgent
from autogen_agentchat.messages import TextMessage


async def example():
    cancellation_token = CancellationToken()

    # Create an Azure OpenAI client
    token_provider = get_bearer_token_provider(DefaultAzureCredential())
    client = AsyncAzureOpenAI(
        azure_deployment="YOUR_AZURE_DEPLOYMENT",
        api_version="YOUR_API_VERSION",
        azure_endpoint="YOUR_AZURE_ENDPOINT",
        azure_ad_token_provider=token_provider,
    )

    # Create an assistant with code interpreter
    assistant = OpenAIAssistantAgent(
        name="Python Helper",
        description="Helps with Python programming",
        client=client,
        model="gpt-4o",
        instructions="You are a helpful Python programming assistant.",
        tools=["code_interpreter"],
    )

    # Get response from the assistant
    response = await assistant.on_messages([TextMessage(source="user", content="Hello.")], cancellation_token)

    print(response)

    # Clean up resources
    await assistant.delete_assistant(cancellation_token)


asyncio.run(example())

```

```python
async delete_assistant(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Delete the assistant if it was created by this instance.

```python
async delete_uploaded_files(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Delete all files that were uploaded by this agent instance.

```python
async delete_vector_store(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Delete the vector store if it was created by this instance.

```python
async handle_incoming_message(message: BaseChatMessage, cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Handle regular text messages by adding them to the thread.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Restore agent from saved state. Default implementation for stateless agents.

```python
property messages: AsyncMessages#
```

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handle incoming messages and return a response.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handle incoming messages and return a response.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Handle reset command by deleting new messages and runs since initialization.

```python
async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Handle file uploads for the code interpreter.

```python
async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Handle file uploads for file search.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the assistant agent produces.

```python
property runs: AsyncRuns#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Export state. Default implementation for stateless agents.

```python
property threads: AsyncThreads#
```

【中文翻译】previous

【中文翻译】autogen_ext.agents.magentic_one

【中文翻译】next

【中文翻译】autogen_ext.agents.web_surfer

### autogen_ext.agents.openai {autogen_extagentsopenai}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html)

```python
class OpenAIAssistantAgent(name: str, description: str, client: AsyncOpenAI | AsyncAzureOpenAI, model: str, instructions: str, tools: Iterable[Literal['code_interpreter', 'file_search'] | Tool | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, assistant_id: str | None = None, thread_id: str | None = None, metadata: Dict[str, str] | None = None, response_format: Literal['auto'] | ResponseFormatText | ResponseFormatJSONObject | ResponseFormatJSONSchema | None = None, temperature: float | None = None, tool_resources: ToolResources | None = None, top_p: float | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent
An agent implementation that uses the Assistant API to generate responses.
Installation:
pip install "autogen-ext[openai]"
# pip install "autogen-ext[openai,azure]"  # For Azure OpenAI Assistant


This agent leverages the Assistant API to create AI assistants with capabilities like:

Code interpretation and execution
File handling and search
Custom function calling
Multi-turn conversations

The agent maintains a thread of conversation and can use various tools including

Code interpreter: For executing code and working with files
File search: For searching through uploaded documents
Custom functions: For extending capabilities with user-defined tools

Key Features:

Supports multiple file formats including code, documents, images
Can handle up to 128 tools per assistant
Maintains conversation context in threads
Supports file uploads for code interpreter and search
Vector store integration for efficient file search
Automatic file parsing and embedding

You can use an existing thread or assistant by providing the thread_id or assistant_id parameters.
Examples
Use the assistant to analyze data in a CSV file:
from openai import AsyncOpenAI
from autogen_core import CancellationToken
import asyncio
from autogen_ext.agents.openai import OpenAIAssistantAgent
from autogen_agentchat.messages import TextMessage


async def example():
    cancellation_token = CancellationToken()

    # Create an OpenAI client
    client = AsyncOpenAI(api_key="your-api-key", base_url="your-base-url")

    # Create an assistant with code interpreter
    assistant = OpenAIAssistantAgent(
        name="Python Helper",
        description="Helps with Python programming",
        client=client,
        model="gpt-4",
        instructions="You are a helpful Python programming assistant.",
        tools=["code_interpreter"],
    )

    # Upload files for the assistant to use
    await assistant.on_upload_for_code_interpreter("data.csv", cancellation_token)

    # Get response from the assistant
    response = await assistant.on_messages(
        [TextMessage(source="user", content="Analyze the data in data.csv")], cancellation_token
    )

    print(response)

    # Clean up resources
    await assistant.delete_uploaded_files(cancellation_token)
    await assistant.delete_assistant(cancellation_token)


asyncio.run(example())


Use Azure OpenAI Assistant with AAD authentication:
from openai import AsyncAzureOpenAI
import asyncio
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from autogen_core import CancellationToken
from autogen_ext.agents.openai import OpenAIAssistantAgent
from autogen_agentchat.messages import TextMessage


async def example():
    cancellation_token = CancellationToken()

    # Create an Azure OpenAI client
    token_provider = get_bearer_token_provider(DefaultAzureCredential())
    client = AsyncAzureOpenAI(
        azure_deployment="YOUR_AZURE_DEPLOYMENT",
        api_version="YOUR_API_VERSION",
        azure_endpoint="YOUR_AZURE_ENDPOINT",
        azure_ad_token_provider=token_provider,
    )

    # Create an assistant with code interpreter
    assistant = OpenAIAssistantAgent(
        name="Python Helper",
        description="Helps with Python programming",
        client=client,
        model="gpt-4o",
        instructions="You are a helpful Python programming assistant.",
        tools=["code_interpreter"],
    )

    # Get response from the assistant
    response = await assistant.on_messages([TextMessage(source="user", content="Hello.")], cancellation_token)

    print(response)

    # Clean up resources
    await assistant.delete_assistant(cancellation_token)


asyncio.run(example())



Parameters:

name (str) – Name of the assistant
description (str) – Description of the assistant’s purpose
client (AsyncOpenAI | AsyncAzureOpenAI) – OpenAI client or Azure OpenAI client instance
model (str) – Model to use (e.g. “gpt-4”)
instructions (str) – System instructions for the assistant
tools (Optional[Iterable[Union[Literal["code_interpreter", "file_search"], Tool | Callable[..., Any] | Callable[..., Awaitable[Any]]]]]) – Tools the assistant can use
assistant_id (Optional[str]) – ID of existing assistant to use
thread_id (Optional[str]) – ID of existing thread to use
metadata (Optional[Dict[str, str]]) – Additional metadata for the assistant.
response_format (Optional[AssistantResponseFormatOptionParam]) – Response format settings
temperature (Optional[float]) – Temperature for response generation
tool_resources (Optional[ToolResources]) – Additional tool configuration
top_p (Optional[float]) – Top p sampling parameter





async delete_assistant(cancellation_token: CancellationToken) → None[source]#
Delete the assistant if it was created by this instance.



async delete_uploaded_files(cancellation_token: CancellationToken) → None[source]#
Delete all files that were uploaded by this agent instance.



async delete_vector_store(cancellation_token: CancellationToken) → None[source]#
Delete the vector store if it was created by this instance.



async handle_incoming_message(message: BaseChatMessage, cancellation_token: CancellationToken) → None[source]#
Handle regular text messages by adding them to the thread.



async load_state(state: Mapping[str, Any]) → None[source]#
Restore agent from saved state. Default implementation for stateless agents.



property messages: AsyncMessages#



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handle incoming messages and return a response.



async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handle incoming messages and return a response.



async on_reset(cancellation_token: CancellationToken) → None[source]#
Handle reset command by deleting new messages and runs since initialization.



async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken) → None[source]#
Handle file uploads for the code interpreter.



async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken) → None[source]#
Handle file uploads for file search.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the assistant agent produces.



property runs: AsyncRuns#



async save_state() → Mapping[str, Any][source]#
Export state. Default implementation for stateless agents.



property threads: AsyncThreads#

**示例**:
```python
pip install "autogen-ext[openai]"
# pip install "autogen-ext[openai,azure]"  # For Azure OpenAI Assistant

```

**示例**:
```python
from openai import AsyncOpenAI
from autogen_core import CancellationToken
import asyncio
from autogen_ext.agents.openai import OpenAIAssistantAgent
from autogen_agentchat.messages import TextMessage


async def example():
    cancellation_token = CancellationToken()

    # Create an OpenAI client
    client = AsyncOpenAI(api_key="your-api-key", base_url="your-base-url")

    # Create an assistant with code interpreter
    assistant = OpenAIAssistantAgent(
        name="Python Helper",
        description="Helps with Python programming",
        client=client,
        model="gpt-4",
        instructions="You are a helpful Python programming assistant.",
        tools=["code_interpreter"],
    )

    # Upload files for the assistant to use
    await assistant.on_upload_for_code_interpreter("data.csv", cancellation_token)

    # Get response from the assistant
    response = await assistant.on_messages(
        [TextMessage(source="user", content="Analyze the data in data.csv")], cancellation_token
    )

    print(response)

    # Clean up resources
    await assistant.delete_uploaded_files(cancellation_token)
    await assistant.delete_assistant(cancellation_token)


asyncio.run(example())

```

**示例**:
```python
from openai import AsyncAzureOpenAI
import asyncio
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from autogen_core import CancellationToken
from autogen_ext.agents.openai import OpenAIAssistantAgent
from autogen_agentchat.messages import TextMessage


async def example():
    cancellation_token = CancellationToken()

    # Create an Azure OpenAI client
    token_provider = get_bearer_token_provider(DefaultAzureCredential())
    client = AsyncAzureOpenAI(
        azure_deployment="YOUR_AZURE_DEPLOYMENT",
        api_version="YOUR_API_VERSION",
        azure_endpoint="YOUR_AZURE_ENDPOINT",
        azure_ad_token_provider=token_provider,
    )

    # Create an assistant with code interpreter
    assistant = OpenAIAssistantAgent(
        name="Python Helper",
        description="Helps with Python programming",
        client=client,
        model="gpt-4o",
        instructions="You are a helpful Python programming assistant.",
        tools=["code_interpreter"],
    )

    # Get response from the assistant
    response = await assistant.on_messages([TextMessage(source="user", content="Hello.")], cancellation_token)

    print(response)

    # Clean up resources
    await assistant.delete_assistant(cancellation_token)


asyncio.run(example())

```

```python
async delete_assistant(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Delete the assistant if it was created by this instance.

```python
async delete_uploaded_files(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Delete all files that were uploaded by this agent instance.

```python
async delete_vector_store(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Delete the vector store if it was created by this instance.

```python
async handle_incoming_message(message: BaseChatMessage, cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Handle regular text messages by adding them to the thread.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Restore agent from saved state. Default implementation for stateless agents.

```python
property messages: AsyncMessages#
```

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handle incoming messages and return a response.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handle incoming messages and return a response.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Handle reset command by deleting new messages and runs since initialization.

```python
async on_upload_for_code_interpreter(file_paths: str | Iterable[str], cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Handle file uploads for the code interpreter.

```python
async on_upload_for_file_search(file_paths: str | Iterable[str], cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Handle file uploads for file search.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the assistant agent produces.

```python
property runs: AsyncRuns#
```

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Export state. Default implementation for stateless agents.

```python
property threads: AsyncThreads#
```

【中文翻译】previous

【中文翻译】autogen_ext.agents.magentic_one

【中文翻译】next

【中文翻译】autogen_ext.agents.web_surfer

### autogen_ext.agents.video_surfer {autogen_extagentsvideo_surfer}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html)

```python
class VideoSurfer(name: str, model_client: ChatCompletionClient, *, tools: List[BaseTool[BaseModel, BaseModel] | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, description: str | None = None, system_message: str | None = None)[source]#
```

【中文翻译】Bases: AssistantAgent
VideoSurfer is a specialized agent designed to answer questions about a local video file.
Installation:
pip install "autogen-ext[video-surfer]"


This agent utilizes various tools to extract information from the video, such as its length, screenshots at specific timestamps, and audio transcriptions. It processes these elements to provide detailed answers to user queries.
Available tools:

extract_audio()
get_video_length()
transcribe_audio_with_timestamps()
get_screenshot_at()
save_screenshot()
transcribe_video_screenshot()


Parameters:

name (str) – The name of the agent.
model_client (ChatCompletionClient) – The model client used for generating responses.
tools (List[BaseTool[BaseModel, BaseModel]  | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – A list of tools or functions the agent can use. If not provided, defaults to all video tools from the action space.
description (str, optional) – A brief description of the agent. Defaults to “An agent that can answer questions about a local video.”.
system_message (str | None, optional) – The system message guiding the agent’s behavior. Defaults to a predefined message.



Example usage:
The following example demonstrates how to create an video surfing agent with
a model client and generate a response to a simple query about a local video
called video.mp4.

import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.video_surfer import VideoSurfer

async def main() -> None:
    """
    Main function to run the video agent.
    """
    # Define an agent
    video_agent = VideoSurfer(
        name="VideoSurfer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")
        )

    # Define termination condition
    termination = TextMentionTermination("TERMINATE")

    # Define a team
    agent_team = RoundRobinGroupChat([video_agent], termination_condition=termination)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="How does Adam define complex tasks in video.mp4? What concrete example of complex does his use? Can you save this example to disk as well?")
    await Console(stream)

asyncio.run(main())



The following example demonstrates how to create and use a VideoSurfer and UserProxyAgent with MagenticOneGroupChat.

import asyncio

from autogen_agentchat.ui import Console
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.agents import UserProxyAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.video_surfer import VideoSurfer

async def main() -> None:
    """
    Main function to run the video agent.
    """

    model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")

    # Define an agent
    video_agent = VideoSurfer(
        name="VideoSurfer",
        model_client=model_client
        )

    web_surfer_agent = UserProxyAgent(
        name="User"
    )

    # Define a team
    agent_team = MagenticOneGroupChat([web_surfer_agent, video_agent], model_client=model_client,)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="Find a latest video about magentic one on youtube and extract quotes from it that make sense.")
    await Console(stream)

asyncio.run(main())





DEFAULT_DESCRIPTION = 'An agent that can answer questions about a local video.'#



DEFAULT_SYSTEM_MESSAGE = '\n    You are a helpful agent that is an expert at answering questions from a video.\n    When asked to answer a question about a video, you should:\n    1. Check if that video is available locally.\n    2. Use the transcription to find which part of the video the question is referring to.\n    3. Optionally use screenshots from those timestamps\n    4. Provide a detailed answer to the question.\n    Reply with TERMINATE when the task has been completed.\n    '#



async vs_transribe_video_screenshot(video_path: str, timestamp: float) → str[source]#
Transcribes the video screenshot at a specific timestamp.

Parameters:

video_path (str) – Path to the video file.
timestamp (float) – Timestamp to take the screenshot.


Returns:
str – Transcription of the video screenshot.

**示例**:
```python
pip install "autogen-ext[video-surfer]"

```

**示例**:
```python
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.video_surfer import VideoSurfer

async def main() -> None:
    """
    Main function to run the video agent.
    """
    # Define an agent
    video_agent = VideoSurfer(
        name="VideoSurfer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")
        )

    # Define termination condition
    termination = TextMentionTermination("TERMINATE")

    # Define a team
    agent_team = RoundRobinGroupChat([video_agent], termination_condition=termination)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="How does Adam define complex tasks in video.mp4? What concrete example of complex does his use? Can you save this example to disk as well?")
    await Console(stream)

asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.ui import Console
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.agents import UserProxyAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.video_surfer import VideoSurfer

async def main() -> None:
    """
    Main function to run the video agent.
    """

    model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")

    # Define an agent
    video_agent = VideoSurfer(
        name="VideoSurfer",
        model_client=model_client
        )

    web_surfer_agent = UserProxyAgent(
        name="User"
    )

    # Define a team
    agent_team = MagenticOneGroupChat([web_surfer_agent, video_agent], model_client=model_client,)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="Find a latest video about magentic one on youtube and extract quotes from it that make sense.")
    await Console(stream)

asyncio.run(main())

```

```python
DEFAULT_DESCRIPTION = 'An agent that can answer questions about a local video.'#
```

```python
DEFAULT_SYSTEM_MESSAGE = '\n    You are a helpful agent that is an expert at answering questions from a video.\n    When asked to answer a question about a video, you should:\n    1. Check if that video is available locally.\n    2. Use the transcription to find which part of the video the question is referring to.\n    3. Optionally use screenshots from those timestamps\n    4. Provide a detailed answer to the question.\n    Reply with TERMINATE when the task has been completed.\n    '#
```

```python
async vs_transribe_video_screenshot(video_path: str, timestamp: float) → str[source]#
```

【中文翻译】Transcribes the video screenshot at a specific timestamp.

Parameters:

video_path (str) – Path to the video file.
timestamp (float) – Timestamp to take the screenshot.


Returns:
str – Transcription of the video screenshot.

【中文翻译】previous

【中文翻译】autogen_ext.agents.file_surfer

【中文翻译】next

【中文翻译】autogen_ext.agents.video_surfer.tools

### autogen_ext.agents.video_surfer {autogen_extagentsvideo_surfer}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html)

```python
class VideoSurfer(name: str, model_client: ChatCompletionClient, *, tools: List[BaseTool[BaseModel, BaseModel] | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, description: str | None = None, system_message: str | None = None)[source]#
```

【中文翻译】Bases: AssistantAgent
VideoSurfer is a specialized agent designed to answer questions about a local video file.
Installation:
pip install "autogen-ext[video-surfer]"


This agent utilizes various tools to extract information from the video, such as its length, screenshots at specific timestamps, and audio transcriptions. It processes these elements to provide detailed answers to user queries.
Available tools:

extract_audio()
get_video_length()
transcribe_audio_with_timestamps()
get_screenshot_at()
save_screenshot()
transcribe_video_screenshot()


Parameters:

name (str) – The name of the agent.
model_client (ChatCompletionClient) – The model client used for generating responses.
tools (List[BaseTool[BaseModel, BaseModel]  | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – A list of tools or functions the agent can use. If not provided, defaults to all video tools from the action space.
description (str, optional) – A brief description of the agent. Defaults to “An agent that can answer questions about a local video.”.
system_message (str | None, optional) – The system message guiding the agent’s behavior. Defaults to a predefined message.



Example usage:
The following example demonstrates how to create an video surfing agent with
a model client and generate a response to a simple query about a local video
called video.mp4.

import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.video_surfer import VideoSurfer

async def main() -> None:
    """
    Main function to run the video agent.
    """
    # Define an agent
    video_agent = VideoSurfer(
        name="VideoSurfer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")
        )

    # Define termination condition
    termination = TextMentionTermination("TERMINATE")

    # Define a team
    agent_team = RoundRobinGroupChat([video_agent], termination_condition=termination)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="How does Adam define complex tasks in video.mp4? What concrete example of complex does his use? Can you save this example to disk as well?")
    await Console(stream)

asyncio.run(main())



The following example demonstrates how to create and use a VideoSurfer and UserProxyAgent with MagenticOneGroupChat.

import asyncio

from autogen_agentchat.ui import Console
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.agents import UserProxyAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.video_surfer import VideoSurfer

async def main() -> None:
    """
    Main function to run the video agent.
    """

    model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")

    # Define an agent
    video_agent = VideoSurfer(
        name="VideoSurfer",
        model_client=model_client
        )

    web_surfer_agent = UserProxyAgent(
        name="User"
    )

    # Define a team
    agent_team = MagenticOneGroupChat([web_surfer_agent, video_agent], model_client=model_client,)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="Find a latest video about magentic one on youtube and extract quotes from it that make sense.")
    await Console(stream)

asyncio.run(main())





DEFAULT_DESCRIPTION = 'An agent that can answer questions about a local video.'#



DEFAULT_SYSTEM_MESSAGE = '\n    You are a helpful agent that is an expert at answering questions from a video.\n    When asked to answer a question about a video, you should:\n    1. Check if that video is available locally.\n    2. Use the transcription to find which part of the video the question is referring to.\n    3. Optionally use screenshots from those timestamps\n    4. Provide a detailed answer to the question.\n    Reply with TERMINATE when the task has been completed.\n    '#



async vs_transribe_video_screenshot(video_path: str, timestamp: float) → str[source]#
Transcribes the video screenshot at a specific timestamp.

Parameters:

video_path (str) – Path to the video file.
timestamp (float) – Timestamp to take the screenshot.


Returns:
str – Transcription of the video screenshot.

**示例**:
```python
pip install "autogen-ext[video-surfer]"

```

**示例**:
```python
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.video_surfer import VideoSurfer

async def main() -> None:
    """
    Main function to run the video agent.
    """
    # Define an agent
    video_agent = VideoSurfer(
        name="VideoSurfer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")
        )

    # Define termination condition
    termination = TextMentionTermination("TERMINATE")

    # Define a team
    agent_team = RoundRobinGroupChat([video_agent], termination_condition=termination)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="How does Adam define complex tasks in video.mp4? What concrete example of complex does his use? Can you save this example to disk as well?")
    await Console(stream)

asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.ui import Console
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.agents import UserProxyAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.video_surfer import VideoSurfer

async def main() -> None:
    """
    Main function to run the video agent.
    """

    model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")

    # Define an agent
    video_agent = VideoSurfer(
        name="VideoSurfer",
        model_client=model_client
        )

    web_surfer_agent = UserProxyAgent(
        name="User"
    )

    # Define a team
    agent_team = MagenticOneGroupChat([web_surfer_agent, video_agent], model_client=model_client,)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="Find a latest video about magentic one on youtube and extract quotes from it that make sense.")
    await Console(stream)

asyncio.run(main())

```

```python
DEFAULT_DESCRIPTION = 'An agent that can answer questions about a local video.'#
```

```python
DEFAULT_SYSTEM_MESSAGE = '\n    You are a helpful agent that is an expert at answering questions from a video.\n    When asked to answer a question about a video, you should:\n    1. Check if that video is available locally.\n    2. Use the transcription to find which part of the video the question is referring to.\n    3. Optionally use screenshots from those timestamps\n    4. Provide a detailed answer to the question.\n    Reply with TERMINATE when the task has been completed.\n    '#
```

```python
async vs_transribe_video_screenshot(video_path: str, timestamp: float) → str[source]#
```

【中文翻译】Transcribes the video screenshot at a specific timestamp.

Parameters:

video_path (str) – Path to the video file.
timestamp (float) – Timestamp to take the screenshot.


Returns:
str – Transcription of the video screenshot.

【中文翻译】previous

【中文翻译】autogen_ext.agents.file_surfer

【中文翻译】next

【中文翻译】autogen_ext.agents.video_surfer.tools

### autogen_ext.agents.video_surfer.tools {autogen_extagentsvideo_surfertools}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html)

```python
extract_audio(video_path: str, audio_output_path: str) → str[source]#
```

【中文翻译】Extracts audio from a video file and saves it as an MP3 file.

Parameters:

video_path – Path to the video file.
audio_output_path – Path to save the extracted audio file.


Returns:
Confirmation message with the path to the saved audio file.

```python
get_screenshot_at(video_path: str, timestamps: List[float]) → List[Tuple[float, ndarray[Any, Any]]][source]#
```

【中文翻译】Captures screenshots at the specified timestamps and returns them as Python objects.

Parameters:

video_path – Path to the video file.
timestamps – List of timestamps in seconds.


Returns:
List of tuples containing timestamp and the corresponding frame (image).
Each frame is a NumPy array (height x width x channels).

```python
get_video_length(video_path: str) → str[source]#
```

【中文翻译】Returns the length of the video in seconds.

Parameters:
video_path – Path to the video file.

Returns:
Duration of the video in seconds.

```python
save_screenshot(video_path: str, timestamp: float, output_path: str) → None[source]#
```

【中文翻译】Captures a screenshot at the specified timestamp and saves it to the output path.

Parameters:

video_path – Path to the video file.
timestamp – Timestamp in seconds.
output_path – Path to save the screenshot. The file format is determined by the extension in the path.

```python
transcribe_audio_with_timestamps(audio_path: str) → str[source]#
```

【中文翻译】Transcribes the audio file with timestamps using the Whisper model.

Parameters:
audio_path – Path to the audio file.

Returns:
Transcription with timestamps.

```python
async transcribe_video_screenshot(video_path: str, timestamp: float, model_client: ChatCompletionClient) → str[source]#
```

【中文翻译】Transcribes the content of a video screenshot captured at the specified timestamp using OpenAI API.

Parameters:

video_path – Path to the video file.
timestamp – Timestamp in seconds.
model_client – ChatCompletionClient instance.


Returns:
Description of the screenshot content.

【中文翻译】previous

【中文翻译】autogen_ext.agents.video_surfer

【中文翻译】next

【中文翻译】autogen_ext.teams.magentic_one

### autogen_ext.agents.video_surfer.tools {autogen_extagentsvideo_surfertools}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.tools.html)

```python
extract_audio(video_path: str, audio_output_path: str) → str[source]#
```

【中文翻译】Extracts audio from a video file and saves it as an MP3 file.

Parameters:

video_path – Path to the video file.
audio_output_path – Path to save the extracted audio file.


Returns:
Confirmation message with the path to the saved audio file.

```python
get_screenshot_at(video_path: str, timestamps: List[float]) → List[Tuple[float, ndarray[Any, Any]]][source]#
```

【中文翻译】Captures screenshots at the specified timestamps and returns them as Python objects.

Parameters:

video_path – Path to the video file.
timestamps – List of timestamps in seconds.


Returns:
List of tuples containing timestamp and the corresponding frame (image).
Each frame is a NumPy array (height x width x channels).

```python
get_video_length(video_path: str) → str[source]#
```

【中文翻译】Returns the length of the video in seconds.

Parameters:
video_path – Path to the video file.

Returns:
Duration of the video in seconds.

```python
save_screenshot(video_path: str, timestamp: float, output_path: str) → None[source]#
```

【中文翻译】Captures a screenshot at the specified timestamp and saves it to the output path.

Parameters:

video_path – Path to the video file.
timestamp – Timestamp in seconds.
output_path – Path to save the screenshot. The file format is determined by the extension in the path.

```python
transcribe_audio_with_timestamps(audio_path: str) → str[source]#
```

【中文翻译】Transcribes the audio file with timestamps using the Whisper model.

Parameters:
audio_path – Path to the audio file.

Returns:
Transcription with timestamps.

```python
async transcribe_video_screenshot(video_path: str, timestamp: float, model_client: ChatCompletionClient) → str[source]#
```

【中文翻译】Transcribes the content of a video screenshot captured at the specified timestamp using OpenAI API.

Parameters:

video_path – Path to the video file.
timestamp – Timestamp in seconds.
model_client – ChatCompletionClient instance.


Returns:
Description of the screenshot content.

【中文翻译】previous

【中文翻译】autogen_ext.agents.video_surfer

【中文翻译】next

【中文翻译】autogen_ext.teams.magentic_one

### autogen_ext.agents.web_surfer {autogen_extagentsweb_surfer}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html)

```python
class MultimodalWebSurfer(name: str, model_client: ChatCompletionClient, downloads_folder: str | None = None, description: str = DEFAULT_DESCRIPTION, debug_dir: str | None = None, headless: bool = True, start_page: str | None = DEFAULT_START_PAGE, animate_actions: bool = False, to_save_screenshots: bool = False, use_ocr: bool = False, browser_channel: str | None = None, browser_data_dir: str | None = None, to_resize_viewport: bool = True, playwright: Playwright | None = None, context: BrowserContext | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[MultimodalWebSurferConfig]
MultimodalWebSurfer is a multimodal agent that acts as a web surfer that can search the web and visit web pages.
Installation:
pip install "autogen-ext[web-surfer]"


It launches a chromium browser and allows the playwright to interact with the web browser and can perform a variety of actions. The browser is launched on the first call to the agent and is reused for subsequent calls.
It must be used with a multimodal model client that supports function/tool calling, ideally GPT-4o currently.

When on_messages() or on_messages_stream() is called, the following occurs:
If this is the first call, the browser is initialized and the page is loaded. This is done in _lazy_init(). The browser is only closed when close() is called.
The method _generate_reply() is called, which then creates the final response as below.
The agent takes a screenshot of the page, extracts the interactive elements, and prepares a set-of-mark screenshot with bounding boxes around the interactive elements.

The agent makes a call to the model_client with the SOM screenshot, history of messages, and the list of available tools.
If the model returns a string, the agent returns the string as the final response.
If the model returns a list of tool calls, the agent executes the tool calls with _execute_tool() using _playwright_controller.
The agent returns a final response which includes a screenshot of the page, page metadata, description of the action taken and the inner text of the webpage.




If at any point the agent encounters an error, it returns the error message as the final response.




Note
Please note that using the MultimodalWebSurfer involves interacting with a digital world designed for humans, which carries inherent risks.
Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences.
Moreover, be cautious that MultimodalWebSurfer may be susceptible to prompt injection attacks from webpages.


Note
On Windows, the event loop policy must be set to WindowsProactorEventLoopPolicy to avoid issues with subprocesses.
import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())




Parameters:

name (str) – The name of the agent.
model_client (ChatCompletionClient) – The model client used by the agent. Must be multimodal and support function calling.
downloads_folder (str, optional) – The folder where downloads are saved. Defaults to None, no downloads are saved.
description (str, optional) – The description of the agent. Defaults to MultimodalWebSurfer.DEFAULT_DESCRIPTION.
debug_dir (str, optional) – The directory where debug information is saved. Defaults to None.
headless (bool, optional) – Whether the browser should be headless. Defaults to True.
start_page (str, optional) – The start page for the browser. Defaults to MultimodalWebSurfer.DEFAULT_START_PAGE.
animate_actions (bool, optional) – Whether to animate actions. Defaults to False.
to_save_screenshots (bool, optional) – Whether to save screenshots. Defaults to False.
use_ocr (bool, optional) – Whether to use OCR. Defaults to False.
browser_channel (str, optional) – The browser channel. Defaults to None.
browser_data_dir (str, optional) – The browser data directory. Defaults to None.
to_resize_viewport (bool, optional) – Whether to resize the viewport. Defaults to True.
playwright (Playwright, optional) – The playwright instance. Defaults to None.
context (BrowserContext, optional) – The browser context. Defaults to None.



Example usage:
The following example demonstrates how to create a web surfing agent with
a model client and run it for multiple turns.

import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.web_surfer import MultimodalWebSurfer


async def main() -> None:
    # Define an agent
    web_surfer_agent = MultimodalWebSurfer(
        name="MultimodalWebSurfer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06"),
    )

    # Define a team
    agent_team = RoundRobinGroupChat([web_surfer_agent], max_turns=3)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="Navigate to the AutoGen readme on GitHub.")
    await Console(stream)
    # Close the browser controlled by the agent
    await web_surfer_agent.close()


asyncio.run(main())





DEFAULT_DESCRIPTION = '\n    A helpful assistant with access to a web browser.\n    Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, filling in form fields, etc.).\n    It can also summarize the entire page, or answer questions based on the content of the page.\n    It can also be asked to sleep and wait for pages to load, in cases where the page seems not yet fully loaded.\n    '#



DEFAULT_START_PAGE = 'https://www.bing.com/'#



MLM_HEIGHT = 765#



MLM_WIDTH = 1224#



SCREENSHOT_TOKENS = 1105#



VIEWPORT_HEIGHT = 900#



VIEWPORT_WIDTH = 1440#



classmethod _from_config(config: MultimodalWebSurferConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → MultimodalWebSurferConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





async close() → None[source]#
Close the browser and the page.
Should be called when the agent is no longer needed.



component_config_schema#
alias of MultimodalWebSurferConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.web_surfer.MultimodalWebSurfer'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'agent'#
The logical type of the component.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

**示例**:
```python
pip install "autogen-ext[web-surfer]"

```

**示例**:
```python
import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

```

**示例**:
```python
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.web_surfer import MultimodalWebSurfer


async def main() -> None:
    # Define an agent
    web_surfer_agent = MultimodalWebSurfer(
        name="MultimodalWebSurfer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06"),
    )

    # Define a team
    agent_team = RoundRobinGroupChat([web_surfer_agent], max_turns=3)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="Navigate to the AutoGen readme on GitHub.")
    await Console(stream)
    # Close the browser controlled by the agent
    await web_surfer_agent.close()


asyncio.run(main())

```

```python
DEFAULT_DESCRIPTION = '\n    A helpful assistant with access to a web browser.\n    Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, filling in form fields, etc.).\n    It can also summarize the entire page, or answer questions based on the content of the page.\n    It can also be asked to sleep and wait for pages to load, in cases where the page seems not yet fully loaded.\n    '#
```

```python
DEFAULT_START_PAGE = 'https://www.bing.com/'#
```

```python
MLM_HEIGHT = 765#
```

```python
MLM_WIDTH = 1224#
```

```python
SCREENSHOT_TOKENS = 1105#
```

```python
VIEWPORT_HEIGHT = 900#
```

```python
VIEWPORT_WIDTH = 1440#
```

```python
classmethod _from_config(config: MultimodalWebSurferConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → MultimodalWebSurferConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
async close() → None[source]#
```

【中文翻译】Close the browser and the page.
Should be called when the agent is no longer needed.

```python
component_config_schema#
```

【中文翻译】alias of MultimodalWebSurferConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.web_surfer.MultimodalWebSurfer'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'agent'#
```

【中文翻译】The logical type of the component.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
class PlaywrightController(downloads_folder: str | None = None, animate_actions: bool = False, viewport_width: int = 1440, viewport_height: int = 900, _download_handler: Callable[[Download], None] | None = None, to_resize_viewport: bool = True)[source]#
```

【中文翻译】Bases: object
A helper class to allow Playwright to interact with web pages to perform actions such as clicking, filling, and scrolling.

Parameters:

downloads_folder (str | None) – The folder to save downloads to. If None, downloads are not saved.
animate_actions (bool) – Whether to animate the actions (create fake cursor to click).
viewport_width (int) – The width of the viewport.
viewport_height (int) – The height of the viewport.
_download_handler (Optional[Callable[[Download], None]]) – A function to handle downloads.
to_resize_viewport (bool) – Whether to resize the viewport





async add_cursor_box(page: Page, identifier: str) → None[source]#
Add a red cursor box around the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.






async back(page: Page) → None[source]#
Navigate back to the previous page.

Parameters:
page (Page) – The Playwright page object.





async click_id(page: Page, identifier: str) → Page | None[source]#
Click the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.


Returns:
Page | None – The new page if a new page is opened, otherwise None.





async fill_id(page: Page, identifier: str, value: str, press_enter: bool = True) → None[source]#
Fill the element with the given identifier with the specified value.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.
value (str) – The value to fill.






async get_focused_rect_id(page: Page) → str | None[source]#
Retrieve the ID of the currently focused element.

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The ID of the focused element or None if no control has focus.





async get_interactive_rects(page: Page) → Dict[str, InteractiveRegion][source]#
Retrieve interactive regions from the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
Dict[str, InteractiveRegion] – A dictionary of interactive regions.





async get_page_markdown(page: Page) → str[source]#
Retrieve the markdown content of the web page.
Currently not implemented.

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The markdown content of the page.





async get_page_metadata(page: Page) → Dict[str, Any][source]#
Retrieve metadata from the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
Dict[str, Any] – A dictionary of page metadata.





async get_visible_text(page: Page) → str[source]#
Retrieve the text content of the browser viewport (approximately).

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The text content of the page.





async get_visual_viewport(page: Page) → VisualViewport[source]#
Retrieve the visual viewport of the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
VisualViewport – The visual viewport of the page.





async get_webpage_text(page: Page, n_lines: int = 50) → str[source]#
Retrieve the text content of the web page.

Parameters:

page (Page) – The Playwright page object.
n_lines (int) – The number of lines to return from the page inner text.


Returns:
str – The text content of the page.





async gradual_cursor_animation(page: Page, start_x: float, start_y: float, end_x: float, end_y: float) → None[source]#
Animate the cursor movement gradually from start to end coordinates.

Parameters:

page (Page) – The Playwright page object.
start_x (float) – The starting x-coordinate.
start_y (float) – The starting y-coordinate.
end_x (float) – The ending x-coordinate.
end_y (float) – The ending y-coordinate.






async hover_id(page: Page, identifier: str) → None[source]#
Hover the mouse over the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.






async on_new_page(page: Page) → None[source]#
Handle actions to perform on a new page.

Parameters:
page (Page) – The Playwright page object.





async page_down(page: Page) → None[source]#
Scroll the page down by one viewport height minus 50 pixels.

Parameters:
page (Page) – The Playwright page object.





async page_up(page: Page) → None[source]#
Scroll the page up by one viewport height minus 50 pixels.

Parameters:
page (Page) – The Playwright page object.





async remove_cursor_box(page: Page, identifier: str) → None[source]#
Remove the red cursor box around the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.






async scroll_id(page: Page, identifier: str, direction: str) → None[source]#
Scroll the element with the given identifier in the specified direction.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.
direction (str) – The direction to scroll (“up” or “down”).






async sleep(page: Page, duration: int | float) → None[source]#
Pause the execution for a specified duration.

Parameters:

page (Page) – The Playwright page object.
duration (Union[int, float]) – The duration to sleep in milliseconds.






async visit_page(page: Page, url: str) → Tuple[bool, bool][source]#
Visit a specified URL.

Parameters:

page (Page) – The Playwright page object.
url (str) – The URL to visit.


Returns:
Tuple[bool, bool] – A tuple indicating whether to reset prior metadata hash and last download.

```python
async add_cursor_box(page: Page, identifier: str) → None[source]#
```

【中文翻译】Add a red cursor box around the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.

```python
async back(page: Page) → None[source]#
```

【中文翻译】Navigate back to the previous page.

Parameters:
page (Page) – The Playwright page object.

```python
async click_id(page: Page, identifier: str) → Page | None[source]#
```

【中文翻译】Click the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.


Returns:
Page | None – The new page if a new page is opened, otherwise None.

```python
async fill_id(page: Page, identifier: str, value: str, press_enter: bool = True) → None[source]#
```

【中文翻译】Fill the element with the given identifier with the specified value.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.
value (str) – The value to fill.

```python
async get_focused_rect_id(page: Page) → str | None[source]#
```

【中文翻译】Retrieve the ID of the currently focused element.

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The ID of the focused element or None if no control has focus.

```python
async get_interactive_rects(page: Page) → Dict[str, InteractiveRegion][source]#
```

【中文翻译】Retrieve interactive regions from the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
Dict[str, InteractiveRegion] – A dictionary of interactive regions.

```python
async get_page_markdown(page: Page) → str[source]#
```

【中文翻译】Retrieve the markdown content of the web page.
Currently not implemented.

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The markdown content of the page.

```python
async get_page_metadata(page: Page) → Dict[str, Any][source]#
```

【中文翻译】Retrieve metadata from the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
Dict[str, Any] – A dictionary of page metadata.

```python
async get_visible_text(page: Page) → str[source]#
```

【中文翻译】Retrieve the text content of the browser viewport (approximately).

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The text content of the page.

```python
async get_visual_viewport(page: Page) → VisualViewport[source]#
```

【中文翻译】Retrieve the visual viewport of the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
VisualViewport – The visual viewport of the page.

```python
async get_webpage_text(page: Page, n_lines: int = 50) → str[source]#
```

【中文翻译】Retrieve the text content of the web page.

Parameters:

page (Page) – The Playwright page object.
n_lines (int) – The number of lines to return from the page inner text.


Returns:
str – The text content of the page.

```python
async gradual_cursor_animation(page: Page, start_x: float, start_y: float, end_x: float, end_y: float) → None[source]#
```

【中文翻译】Animate the cursor movement gradually from start to end coordinates.

Parameters:

page (Page) – The Playwright page object.
start_x (float) – The starting x-coordinate.
start_y (float) – The starting y-coordinate.
end_x (float) – The ending x-coordinate.
end_y (float) – The ending y-coordinate.

```python
async hover_id(page: Page, identifier: str) → None[source]#
```

【中文翻译】Hover the mouse over the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.

```python
async on_new_page(page: Page) → None[source]#
```

【中文翻译】Handle actions to perform on a new page.

Parameters:
page (Page) – The Playwright page object.

```python
async page_down(page: Page) → None[source]#
```

【中文翻译】Scroll the page down by one viewport height minus 50 pixels.

Parameters:
page (Page) – The Playwright page object.

```python
async page_up(page: Page) → None[source]#
```

【中文翻译】Scroll the page up by one viewport height minus 50 pixels.

Parameters:
page (Page) – The Playwright page object.

```python
async remove_cursor_box(page: Page, identifier: str) → None[source]#
```

【中文翻译】Remove the red cursor box around the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.

```python
async scroll_id(page: Page, identifier: str, direction: str) → None[source]#
```

【中文翻译】Scroll the element with the given identifier in the specified direction.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.
direction (str) – The direction to scroll (“up” or “down”).

```python
async sleep(page: Page, duration: int | float) → None[source]#
```

【中文翻译】Pause the execution for a specified duration.

Parameters:

page (Page) – The Playwright page object.
duration (Union[int, float]) – The duration to sleep in milliseconds.

```python
async visit_page(page: Page, url: str) → Tuple[bool, bool][source]#
```

【中文翻译】Visit a specified URL.

Parameters:

page (Page) – The Playwright page object.
url (str) – The URL to visit.


Returns:
Tuple[bool, bool] – A tuple indicating whether to reset prior metadata hash and last download.

【中文翻译】previous

【中文翻译】autogen_ext.agents.openai

【中文翻译】next

【中文翻译】autogen_ext.agents.file_surfer

### autogen_ext.agents.web_surfer {autogen_extagentsweb_surfer}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html)

```python
class MultimodalWebSurfer(name: str, model_client: ChatCompletionClient, downloads_folder: str | None = None, description: str = DEFAULT_DESCRIPTION, debug_dir: str | None = None, headless: bool = True, start_page: str | None = DEFAULT_START_PAGE, animate_actions: bool = False, to_save_screenshots: bool = False, use_ocr: bool = False, browser_channel: str | None = None, browser_data_dir: str | None = None, to_resize_viewport: bool = True, playwright: Playwright | None = None, context: BrowserContext | None = None)[source]#
```

【中文翻译】Bases: BaseChatAgent, Component[MultimodalWebSurferConfig]
MultimodalWebSurfer is a multimodal agent that acts as a web surfer that can search the web and visit web pages.
Installation:
pip install "autogen-ext[web-surfer]"


It launches a chromium browser and allows the playwright to interact with the web browser and can perform a variety of actions. The browser is launched on the first call to the agent and is reused for subsequent calls.
It must be used with a multimodal model client that supports function/tool calling, ideally GPT-4o currently.

When on_messages() or on_messages_stream() is called, the following occurs:
If this is the first call, the browser is initialized and the page is loaded. This is done in _lazy_init(). The browser is only closed when close() is called.
The method _generate_reply() is called, which then creates the final response as below.
The agent takes a screenshot of the page, extracts the interactive elements, and prepares a set-of-mark screenshot with bounding boxes around the interactive elements.

The agent makes a call to the model_client with the SOM screenshot, history of messages, and the list of available tools.
If the model returns a string, the agent returns the string as the final response.
If the model returns a list of tool calls, the agent executes the tool calls with _execute_tool() using _playwright_controller.
The agent returns a final response which includes a screenshot of the page, page metadata, description of the action taken and the inner text of the webpage.




If at any point the agent encounters an error, it returns the error message as the final response.




Note
Please note that using the MultimodalWebSurfer involves interacting with a digital world designed for humans, which carries inherent risks.
Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences.
Moreover, be cautious that MultimodalWebSurfer may be susceptible to prompt injection attacks from webpages.


Note
On Windows, the event loop policy must be set to WindowsProactorEventLoopPolicy to avoid issues with subprocesses.
import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())




Parameters:

name (str) – The name of the agent.
model_client (ChatCompletionClient) – The model client used by the agent. Must be multimodal and support function calling.
downloads_folder (str, optional) – The folder where downloads are saved. Defaults to None, no downloads are saved.
description (str, optional) – The description of the agent. Defaults to MultimodalWebSurfer.DEFAULT_DESCRIPTION.
debug_dir (str, optional) – The directory where debug information is saved. Defaults to None.
headless (bool, optional) – Whether the browser should be headless. Defaults to True.
start_page (str, optional) – The start page for the browser. Defaults to MultimodalWebSurfer.DEFAULT_START_PAGE.
animate_actions (bool, optional) – Whether to animate actions. Defaults to False.
to_save_screenshots (bool, optional) – Whether to save screenshots. Defaults to False.
use_ocr (bool, optional) – Whether to use OCR. Defaults to False.
browser_channel (str, optional) – The browser channel. Defaults to None.
browser_data_dir (str, optional) – The browser data directory. Defaults to None.
to_resize_viewport (bool, optional) – Whether to resize the viewport. Defaults to True.
playwright (Playwright, optional) – The playwright instance. Defaults to None.
context (BrowserContext, optional) – The browser context. Defaults to None.



Example usage:
The following example demonstrates how to create a web surfing agent with
a model client and run it for multiple turns.

import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.web_surfer import MultimodalWebSurfer


async def main() -> None:
    # Define an agent
    web_surfer_agent = MultimodalWebSurfer(
        name="MultimodalWebSurfer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06"),
    )

    # Define a team
    agent_team = RoundRobinGroupChat([web_surfer_agent], max_turns=3)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="Navigate to the AutoGen readme on GitHub.")
    await Console(stream)
    # Close the browser controlled by the agent
    await web_surfer_agent.close()


asyncio.run(main())





DEFAULT_DESCRIPTION = '\n    A helpful assistant with access to a web browser.\n    Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, filling in form fields, etc.).\n    It can also summarize the entire page, or answer questions based on the content of the page.\n    It can also be asked to sleep and wait for pages to load, in cases where the page seems not yet fully loaded.\n    '#



DEFAULT_START_PAGE = 'https://www.bing.com/'#



MLM_HEIGHT = 765#



MLM_WIDTH = 1224#



SCREENSHOT_TOKENS = 1105#



VIEWPORT_HEIGHT = 900#



VIEWPORT_WIDTH = 1440#



classmethod _from_config(config: MultimodalWebSurferConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → MultimodalWebSurferConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





async close() → None[source]#
Close the browser and the page.
Should be called when the agent is no longer needed.



component_config_schema#
alias of MultimodalWebSurferConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.web_surfer.MultimodalWebSurfer'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'agent'#
The logical type of the component.



async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.




async on_reset(cancellation_token: CancellationToken) → None[source]#
Resets the agent to its initialization state.



property produced_message_types: Sequence[type[BaseChatMessage]]#
The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

**示例**:
```python
pip install "autogen-ext[web-surfer]"

```

**示例**:
```python
import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

```

**示例**:
```python
import asyncio
from autogen_agentchat.ui import Console
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.web_surfer import MultimodalWebSurfer


async def main() -> None:
    # Define an agent
    web_surfer_agent = MultimodalWebSurfer(
        name="MultimodalWebSurfer",
        model_client=OpenAIChatCompletionClient(model="gpt-4o-2024-08-06"),
    )

    # Define a team
    agent_team = RoundRobinGroupChat([web_surfer_agent], max_turns=3)

    # Run the team and stream messages to the console
    stream = agent_team.run_stream(task="Navigate to the AutoGen readme on GitHub.")
    await Console(stream)
    # Close the browser controlled by the agent
    await web_surfer_agent.close()


asyncio.run(main())

```

```python
DEFAULT_DESCRIPTION = '\n    A helpful assistant with access to a web browser.\n    Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, filling in form fields, etc.).\n    It can also summarize the entire page, or answer questions based on the content of the page.\n    It can also be asked to sleep and wait for pages to load, in cases where the page seems not yet fully loaded.\n    '#
```

```python
DEFAULT_START_PAGE = 'https://www.bing.com/'#
```

```python
MLM_HEIGHT = 765#
```

```python
MLM_WIDTH = 1224#
```

```python
SCREENSHOT_TOKENS = 1105#
```

```python
VIEWPORT_HEIGHT = 900#
```

```python
VIEWPORT_WIDTH = 1440#
```

```python
classmethod _from_config(config: MultimodalWebSurferConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → MultimodalWebSurferConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
async close() → None[source]#
```

【中文翻译】Close the browser and the page.
Should be called when the agent is no longer needed.

```python
component_config_schema#
```

【中文翻译】alias of MultimodalWebSurferConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.agents.web_surfer.MultimodalWebSurfer'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'agent'#
```

【中文翻译】The logical type of the component.

```python
async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]#
```

【中文翻译】Handles incoming messages and returns a response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]#
```

【中文翻译】Handles incoming messages and returns a stream of messages and
and the final item is the response. The base implementation in
BaseChatAgent simply calls on_messages() and yields
the messages in the response.

Note
Agents are stateful and the messages passed to this method should
be the new messages since the last call to this method. The agent
should maintain its state between calls to this method. For example,
if the agent needs to remember the previous messages to respond to
the current message, it should store the previous messages in the
agent state.

```python
async on_reset(cancellation_token: CancellationToken) → None[source]#
```

【中文翻译】Resets the agent to its initialization state.

```python
property produced_message_types: Sequence[type[BaseChatMessage]]#
```

【中文翻译】The types of messages that the agent produces in the
Response.chat_message field. They must be BaseChatMessage types.

```python
class PlaywrightController(downloads_folder: str | None = None, animate_actions: bool = False, viewport_width: int = 1440, viewport_height: int = 900, _download_handler: Callable[[Download], None] | None = None, to_resize_viewport: bool = True)[source]#
```

【中文翻译】Bases: object
A helper class to allow Playwright to interact with web pages to perform actions such as clicking, filling, and scrolling.

Parameters:

downloads_folder (str | None) – The folder to save downloads to. If None, downloads are not saved.
animate_actions (bool) – Whether to animate the actions (create fake cursor to click).
viewport_width (int) – The width of the viewport.
viewport_height (int) – The height of the viewport.
_download_handler (Optional[Callable[[Download], None]]) – A function to handle downloads.
to_resize_viewport (bool) – Whether to resize the viewport





async add_cursor_box(page: Page, identifier: str) → None[source]#
Add a red cursor box around the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.






async back(page: Page) → None[source]#
Navigate back to the previous page.

Parameters:
page (Page) – The Playwright page object.





async click_id(page: Page, identifier: str) → Page | None[source]#
Click the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.


Returns:
Page | None – The new page if a new page is opened, otherwise None.





async fill_id(page: Page, identifier: str, value: str, press_enter: bool = True) → None[source]#
Fill the element with the given identifier with the specified value.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.
value (str) – The value to fill.






async get_focused_rect_id(page: Page) → str | None[source]#
Retrieve the ID of the currently focused element.

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The ID of the focused element or None if no control has focus.





async get_interactive_rects(page: Page) → Dict[str, InteractiveRegion][source]#
Retrieve interactive regions from the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
Dict[str, InteractiveRegion] – A dictionary of interactive regions.





async get_page_markdown(page: Page) → str[source]#
Retrieve the markdown content of the web page.
Currently not implemented.

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The markdown content of the page.





async get_page_metadata(page: Page) → Dict[str, Any][source]#
Retrieve metadata from the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
Dict[str, Any] – A dictionary of page metadata.





async get_visible_text(page: Page) → str[source]#
Retrieve the text content of the browser viewport (approximately).

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The text content of the page.





async get_visual_viewport(page: Page) → VisualViewport[source]#
Retrieve the visual viewport of the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
VisualViewport – The visual viewport of the page.





async get_webpage_text(page: Page, n_lines: int = 50) → str[source]#
Retrieve the text content of the web page.

Parameters:

page (Page) – The Playwright page object.
n_lines (int) – The number of lines to return from the page inner text.


Returns:
str – The text content of the page.





async gradual_cursor_animation(page: Page, start_x: float, start_y: float, end_x: float, end_y: float) → None[source]#
Animate the cursor movement gradually from start to end coordinates.

Parameters:

page (Page) – The Playwright page object.
start_x (float) – The starting x-coordinate.
start_y (float) – The starting y-coordinate.
end_x (float) – The ending x-coordinate.
end_y (float) – The ending y-coordinate.






async hover_id(page: Page, identifier: str) → None[source]#
Hover the mouse over the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.






async on_new_page(page: Page) → None[source]#
Handle actions to perform on a new page.

Parameters:
page (Page) – The Playwright page object.





async page_down(page: Page) → None[source]#
Scroll the page down by one viewport height minus 50 pixels.

Parameters:
page (Page) – The Playwright page object.





async page_up(page: Page) → None[source]#
Scroll the page up by one viewport height minus 50 pixels.

Parameters:
page (Page) – The Playwright page object.





async remove_cursor_box(page: Page, identifier: str) → None[source]#
Remove the red cursor box around the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.






async scroll_id(page: Page, identifier: str, direction: str) → None[source]#
Scroll the element with the given identifier in the specified direction.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.
direction (str) – The direction to scroll (“up” or “down”).






async sleep(page: Page, duration: int | float) → None[source]#
Pause the execution for a specified duration.

Parameters:

page (Page) – The Playwright page object.
duration (Union[int, float]) – The duration to sleep in milliseconds.






async visit_page(page: Page, url: str) → Tuple[bool, bool][source]#
Visit a specified URL.

Parameters:

page (Page) – The Playwright page object.
url (str) – The URL to visit.


Returns:
Tuple[bool, bool] – A tuple indicating whether to reset prior metadata hash and last download.

```python
async add_cursor_box(page: Page, identifier: str) → None[source]#
```

【中文翻译】Add a red cursor box around the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.

```python
async back(page: Page) → None[source]#
```

【中文翻译】Navigate back to the previous page.

Parameters:
page (Page) – The Playwright page object.

```python
async click_id(page: Page, identifier: str) → Page | None[source]#
```

【中文翻译】Click the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.


Returns:
Page | None – The new page if a new page is opened, otherwise None.

```python
async fill_id(page: Page, identifier: str, value: str, press_enter: bool = True) → None[source]#
```

【中文翻译】Fill the element with the given identifier with the specified value.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.
value (str) – The value to fill.

```python
async get_focused_rect_id(page: Page) → str | None[source]#
```

【中文翻译】Retrieve the ID of the currently focused element.

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The ID of the focused element or None if no control has focus.

```python
async get_interactive_rects(page: Page) → Dict[str, InteractiveRegion][source]#
```

【中文翻译】Retrieve interactive regions from the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
Dict[str, InteractiveRegion] – A dictionary of interactive regions.

```python
async get_page_markdown(page: Page) → str[source]#
```

【中文翻译】Retrieve the markdown content of the web page.
Currently not implemented.

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The markdown content of the page.

```python
async get_page_metadata(page: Page) → Dict[str, Any][source]#
```

【中文翻译】Retrieve metadata from the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
Dict[str, Any] – A dictionary of page metadata.

```python
async get_visible_text(page: Page) → str[source]#
```

【中文翻译】Retrieve the text content of the browser viewport (approximately).

Parameters:
page (Page) – The Playwright page object.

Returns:
str – The text content of the page.

```python
async get_visual_viewport(page: Page) → VisualViewport[source]#
```

【中文翻译】Retrieve the visual viewport of the web page.

Parameters:
page (Page) – The Playwright page object.

Returns:
VisualViewport – The visual viewport of the page.

```python
async get_webpage_text(page: Page, n_lines: int = 50) → str[source]#
```

【中文翻译】Retrieve the text content of the web page.

Parameters:

page (Page) – The Playwright page object.
n_lines (int) – The number of lines to return from the page inner text.


Returns:
str – The text content of the page.

```python
async gradual_cursor_animation(page: Page, start_x: float, start_y: float, end_x: float, end_y: float) → None[source]#
```

【中文翻译】Animate the cursor movement gradually from start to end coordinates.

Parameters:

page (Page) – The Playwright page object.
start_x (float) – The starting x-coordinate.
start_y (float) – The starting y-coordinate.
end_x (float) – The ending x-coordinate.
end_y (float) – The ending y-coordinate.

```python
async hover_id(page: Page, identifier: str) → None[source]#
```

【中文翻译】Hover the mouse over the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.

```python
async on_new_page(page: Page) → None[source]#
```

【中文翻译】Handle actions to perform on a new page.

Parameters:
page (Page) – The Playwright page object.

```python
async page_down(page: Page) → None[source]#
```

【中文翻译】Scroll the page down by one viewport height minus 50 pixels.

Parameters:
page (Page) – The Playwright page object.

```python
async page_up(page: Page) → None[source]#
```

【中文翻译】Scroll the page up by one viewport height minus 50 pixels.

Parameters:
page (Page) – The Playwright page object.

```python
async remove_cursor_box(page: Page, identifier: str) → None[source]#
```

【中文翻译】Remove the red cursor box around the element with the given identifier.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.

```python
async scroll_id(page: Page, identifier: str, direction: str) → None[source]#
```

【中文翻译】Scroll the element with the given identifier in the specified direction.

Parameters:

page (Page) – The Playwright page object.
identifier (str) – The element identifier.
direction (str) – The direction to scroll (“up” or “down”).

```python
async sleep(page: Page, duration: int | float) → None[source]#
```

【中文翻译】Pause the execution for a specified duration.

Parameters:

page (Page) – The Playwright page object.
duration (Union[int, float]) – The duration to sleep in milliseconds.

```python
async visit_page(page: Page, url: str) → Tuple[bool, bool][source]#
```

【中文翻译】Visit a specified URL.

Parameters:

page (Page) – The Playwright page object.
url (str) – The URL to visit.


Returns:
Tuple[bool, bool] – A tuple indicating whether to reset prior metadata hash and last download.

【中文翻译】previous

【中文翻译】autogen_ext.agents.openai

【中文翻译】next

【中文翻译】autogen_ext.agents.file_surfer

### autogen_ext.auth.azure {autogen_extauthazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html)

```python
class AzureTokenProvider(credential: TokenCredential | SupportsTokenInfo, *scopes: str)[source]#
```

【中文翻译】Bases: ComponentBase[TokenProviderConfig], Component[TokenProviderConfig]


component_config_schema#
alias of TokenProviderConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.auth.azure.AzureTokenProvider'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'token_provider'#
The logical type of the component.

```python
component_config_schema#
```

【中文翻译】alias of TokenProviderConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.auth.azure.AzureTokenProvider'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'token_provider'#
```

【中文翻译】The logical type of the component.

```python
pydantic model TokenProviderConfig[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "TokenProviderConfig",
   "type": "object",
   "properties": {
      "provider_kind": {
         "title": "Provider Kind",
         "type": "string"
      },
      "scopes": {
         "items": {
            "type": "string"
         },
         "title": "Scopes",
         "type": "array"
      }
   },
   "required": [
      "provider_kind",
      "scopes"
   ]
}



Fields:

provider_kind (str)
scopes (List[str])





field provider_kind: str [Required]#



field scopes: List[str] [Required]#

**示例**:
```python
{
   "title": "TokenProviderConfig",
   "type": "object",
   "properties": {
      "provider_kind": {
         "title": "Provider Kind",
         "type": "string"
      },
      "scopes": {
         "items": {
            "type": "string"
         },
         "title": "Scopes",
         "type": "array"
      }
   },
   "required": [
      "provider_kind",
      "scopes"
   ]
}

```

```python
field provider_kind: str [Required]#
```

```python
field scopes: List[str] [Required]#
```

【中文翻译】previous

【中文翻译】autogen_ext.runtimes.grpc

【中文翻译】next

【中文翻译】autogen_ext.experimental.task_centric_memory

### autogen_ext.auth.azure {autogen_extauthazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.auth.azure.html)

```python
class AzureTokenProvider(credential: TokenCredential | SupportsTokenInfo, *scopes: str)[source]#
```

【中文翻译】Bases: ComponentBase[TokenProviderConfig], Component[TokenProviderConfig]


component_config_schema#
alias of TokenProviderConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.auth.azure.AzureTokenProvider'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'token_provider'#
The logical type of the component.

```python
component_config_schema#
```

【中文翻译】alias of TokenProviderConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.auth.azure.AzureTokenProvider'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'token_provider'#
```

【中文翻译】The logical type of the component.

```python
pydantic model TokenProviderConfig[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "TokenProviderConfig",
   "type": "object",
   "properties": {
      "provider_kind": {
         "title": "Provider Kind",
         "type": "string"
      },
      "scopes": {
         "items": {
            "type": "string"
         },
         "title": "Scopes",
         "type": "array"
      }
   },
   "required": [
      "provider_kind",
      "scopes"
   ]
}



Fields:

provider_kind (str)
scopes (List[str])





field provider_kind: str [Required]#



field scopes: List[str] [Required]#

**示例**:
```python
{
   "title": "TokenProviderConfig",
   "type": "object",
   "properties": {
      "provider_kind": {
         "title": "Provider Kind",
         "type": "string"
      },
      "scopes": {
         "items": {
            "type": "string"
         },
         "title": "Scopes",
         "type": "array"
      }
   },
   "required": [
      "provider_kind",
      "scopes"
   ]
}

```

```python
field provider_kind: str [Required]#
```

```python
field scopes: List[str] [Required]#
```

【中文翻译】previous

【中文翻译】autogen_ext.runtimes.grpc

【中文翻译】next

【中文翻译】autogen_ext.experimental.task_centric_memory

### autogen_ext.cache_store.diskcache {autogen_extcache_storediskcache}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html)

```python
class DiskCacheStore(cache_instance: Cache)[source]#
```

【中文翻译】Bases: CacheStore[T], Component[DiskCacheStoreConfig]
A typed CacheStore implementation that uses diskcache as the underlying storage.
See ChatCompletionCache for an example of usage.

Parameters:
cache_instance – An instance of diskcache.Cache.
The user is responsible for managing the DiskCache instance’s lifetime.




classmethod _from_config(config: DiskCacheStoreConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → DiskCacheStoreConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of DiskCacheStoreConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.cache_store.diskcache.DiskCacheStore'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



get(key: str, default: T | None = None) → T | None[source]#
Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.





set(key: str, value: T) → None[source]#
Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
classmethod _from_config(config: DiskCacheStoreConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → DiskCacheStoreConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of DiskCacheStoreConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.cache_store.diskcache.DiskCacheStore'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
get(key: str, default: T | None = None) → T | None[source]#
```

【中文翻译】Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.

```python
set(key: str, value: T) → None[source]#
```

【中文翻译】Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
pydantic model DiskCacheStoreConfig[source]#
```

【中文翻译】Bases: BaseModel
Configuration for DiskCacheStore

Show JSON schema{
   "title": "DiskCacheStoreConfig",
   "description": "Configuration for DiskCacheStore",
   "type": "object",
   "properties": {
      "directory": {
         "title": "Directory",
         "type": "string"
      }
   },
   "required": [
      "directory"
   ]
}



Fields:

directory (str)





field directory: str [Required]#

**示例**:
```python
{
   "title": "DiskCacheStoreConfig",
   "description": "Configuration for DiskCacheStore",
   "type": "object",
   "properties": {
      "directory": {
         "title": "Directory",
         "type": "string"
      }
   },
   "required": [
      "directory"
   ]
}

```

```python
field directory: str [Required]#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.azure

【中文翻译】next

【中文翻译】autogen_ext.cache_store.redis

### autogen_ext.cache_store.diskcache {autogen_extcache_storediskcache}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.diskcache.html)

```python
class DiskCacheStore(cache_instance: Cache)[source]#
```

【中文翻译】Bases: CacheStore[T], Component[DiskCacheStoreConfig]
A typed CacheStore implementation that uses diskcache as the underlying storage.
See ChatCompletionCache for an example of usage.

Parameters:
cache_instance – An instance of diskcache.Cache.
The user is responsible for managing the DiskCache instance’s lifetime.




classmethod _from_config(config: DiskCacheStoreConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → DiskCacheStoreConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of DiskCacheStoreConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.cache_store.diskcache.DiskCacheStore'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



get(key: str, default: T | None = None) → T | None[source]#
Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.





set(key: str, value: T) → None[source]#
Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
classmethod _from_config(config: DiskCacheStoreConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → DiskCacheStoreConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of DiskCacheStoreConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.cache_store.diskcache.DiskCacheStore'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
get(key: str, default: T | None = None) → T | None[source]#
```

【中文翻译】Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.

```python
set(key: str, value: T) → None[source]#
```

【中文翻译】Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
pydantic model DiskCacheStoreConfig[source]#
```

【中文翻译】Bases: BaseModel
Configuration for DiskCacheStore

Show JSON schema{
   "title": "DiskCacheStoreConfig",
   "description": "Configuration for DiskCacheStore",
   "type": "object",
   "properties": {
      "directory": {
         "title": "Directory",
         "type": "string"
      }
   },
   "required": [
      "directory"
   ]
}



Fields:

directory (str)





field directory: str [Required]#

**示例**:
```python
{
   "title": "DiskCacheStoreConfig",
   "description": "Configuration for DiskCacheStore",
   "type": "object",
   "properties": {
      "directory": {
         "title": "Directory",
         "type": "string"
      }
   },
   "required": [
      "directory"
   ]
}

```

```python
field directory: str [Required]#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.azure

【中文翻译】next

【中文翻译】autogen_ext.cache_store.redis

### autogen_ext.cache_store.redis {autogen_extcache_storeredis}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html)

```python
class RedisStore(redis_instance: Redis)[source]#
```

【中文翻译】Bases: CacheStore[T], Component[RedisStoreConfig]
A typed CacheStore implementation that uses redis as the underlying storage.
See ChatCompletionCache for an example of usage.

Parameters:
cache_instance – An instance of redis.Redis.
The user is responsible for managing the Redis instance’s lifetime.




classmethod _from_config(config: RedisStoreConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → RedisStoreConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of RedisStoreConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.cache_store.redis.RedisStore'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



get(key: str, default: T | None = None) → T | None[source]#
Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.





set(key: str, value: T) → None[source]#
Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
classmethod _from_config(config: RedisStoreConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → RedisStoreConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of RedisStoreConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.cache_store.redis.RedisStore'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
get(key: str, default: T | None = None) → T | None[source]#
```

【中文翻译】Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.

```python
set(key: str, value: T) → None[source]#
```

【中文翻译】Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
pydantic model RedisStoreConfig[source]#
```

【中文翻译】Bases: BaseModel
Configuration for RedisStore

Show JSON schema{
   "title": "RedisStoreConfig",
   "description": "Configuration for RedisStore",
   "type": "object",
   "properties": {
      "host": {
         "default": "localhost",
         "title": "Host",
         "type": "string"
      },
      "port": {
         "default": 6379,
         "title": "Port",
         "type": "integer"
      },
      "db": {
         "default": 0,
         "title": "Db",
         "type": "integer"
      },
      "username": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Username"
      },
      "password": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Password"
      },
      "ssl": {
         "default": false,
         "title": "Ssl",
         "type": "boolean"
      },
      "socket_timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Socket Timeout"
      }
   }
}



Fields:

db (int)
host (str)
password (str | None)
port (int)
socket_timeout (float | None)
ssl (bool)
username (str | None)





field db: int = 0#



field host: str = 'localhost'#



field password: str | None = None#



field port: int = 6379#



field socket_timeout: float | None = None#



field ssl: bool = False#



field username: str | None = None#

**示例**:
```python
{
   "title": "RedisStoreConfig",
   "description": "Configuration for RedisStore",
   "type": "object",
   "properties": {
      "host": {
         "default": "localhost",
         "title": "Host",
         "type": "string"
      },
      "port": {
         "default": 6379,
         "title": "Port",
         "type": "integer"
      },
      "db": {
         "default": 0,
         "title": "Db",
         "type": "integer"
      },
      "username": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Username"
      },
      "password": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Password"
      },
      "ssl": {
         "default": false,
         "title": "Ssl",
         "type": "boolean"
      },
      "socket_timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Socket Timeout"
      }
   }
}

```

```python
field db: int = 0#
```

```python
field host: str = 'localhost'#
```

```python
field password: str | None = None#
```

```python
field port: int = 6379#
```

```python
field socket_timeout: float | None = None#
```

```python
field ssl: bool = False#
```

```python
field username: str | None = None#
```

【中文翻译】previous

【中文翻译】autogen_ext.cache_store.diskcache

【中文翻译】next

【中文翻译】autogen_ext.runtimes.grpc

### autogen_ext.cache_store.redis {autogen_extcache_storeredis}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html)

```python
class RedisStore(redis_instance: Redis)[source]#
```

【中文翻译】Bases: CacheStore[T], Component[RedisStoreConfig]
A typed CacheStore implementation that uses redis as the underlying storage.
See ChatCompletionCache for an example of usage.

Parameters:
cache_instance – An instance of redis.Redis.
The user is responsible for managing the Redis instance’s lifetime.




classmethod _from_config(config: RedisStoreConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → RedisStoreConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of RedisStoreConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.cache_store.redis.RedisStore'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



get(key: str, default: T | None = None) → T | None[source]#
Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.





set(key: str, value: T) → None[source]#
Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
classmethod _from_config(config: RedisStoreConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → RedisStoreConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of RedisStoreConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.cache_store.redis.RedisStore'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
get(key: str, default: T | None = None) → T | None[source]#
```

【中文翻译】Retrieve an item from the store.

Parameters:

key – The key identifying the item in the store.
default (optional) – The default value to return if the key is not found.
Defaults to None.


Returns:
The value associated with the key if found, else the default value.

```python
set(key: str, value: T) → None[source]#
```

【中文翻译】Set an item in the store.

Parameters:

key – The key under which the item is to be stored.
value – The value to be stored in the store.

```python
pydantic model RedisStoreConfig[source]#
```

【中文翻译】Bases: BaseModel
Configuration for RedisStore

Show JSON schema{
   "title": "RedisStoreConfig",
   "description": "Configuration for RedisStore",
   "type": "object",
   "properties": {
      "host": {
         "default": "localhost",
         "title": "Host",
         "type": "string"
      },
      "port": {
         "default": 6379,
         "title": "Port",
         "type": "integer"
      },
      "db": {
         "default": 0,
         "title": "Db",
         "type": "integer"
      },
      "username": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Username"
      },
      "password": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Password"
      },
      "ssl": {
         "default": false,
         "title": "Ssl",
         "type": "boolean"
      },
      "socket_timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Socket Timeout"
      }
   }
}



Fields:

db (int)
host (str)
password (str | None)
port (int)
socket_timeout (float | None)
ssl (bool)
username (str | None)





field db: int = 0#



field host: str = 'localhost'#



field password: str | None = None#



field port: int = 6379#



field socket_timeout: float | None = None#



field ssl: bool = False#



field username: str | None = None#

**示例**:
```python
{
   "title": "RedisStoreConfig",
   "description": "Configuration for RedisStore",
   "type": "object",
   "properties": {
      "host": {
         "default": "localhost",
         "title": "Host",
         "type": "string"
      },
      "port": {
         "default": 6379,
         "title": "Port",
         "type": "integer"
      },
      "db": {
         "default": 0,
         "title": "Db",
         "type": "integer"
      },
      "username": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Username"
      },
      "password": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Password"
      },
      "ssl": {
         "default": false,
         "title": "Ssl",
         "type": "boolean"
      },
      "socket_timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Socket Timeout"
      }
   }
}

```

```python
field db: int = 0#
```

```python
field host: str = 'localhost'#
```

```python
field password: str | None = None#
```

```python
field port: int = 6379#
```

```python
field socket_timeout: float | None = None#
```

```python
field ssl: bool = False#
```

```python
field username: str | None = None#
```

【中文翻译】previous

【中文翻译】autogen_ext.cache_store.diskcache

【中文翻译】next

【中文翻译】autogen_ext.runtimes.grpc

### autogen_ext.code_executors.azure {autogen_extcode_executorsazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html)

```python
class ACADynamicSessionsCodeExecutor(pool_management_endpoint: str, credential: TokenProvider, timeout: int = 60, work_dir: Path | str | None = None, functions: Sequence[FunctionWithRequirements[Any, A] | Callable[[...], Any] | FunctionWithRequirementsStr] = [], functions_module: str = 'functions', suppress_result_output: bool = False, session_id: str | None = None)[source]#
```

【中文翻译】Bases: CodeExecutor
(Experimental) A code executor class that executes code through a an Azure
Container Apps Dynamic Sessions instance.

Note
This class requires the azure extra for the autogen-ext package:
pip install "autogen-ext[azure]"




Caution
This will execute LLM generated code on an Azure dynamic code container.

The execution environment is similar to that of a jupyter notebook which allows for incremental code execution. The parameter functions are executed in order once at the beginning of each session. Each code block is then executed serially and in the order they are received. Each environment has a statically defined set of available packages which cannot be changed.
Currently, attempting to use packages beyond what is available on the environment will result in an error. To get the list of supported packages, call the get_available_packages function.
Currently the only supported language is Python.
For Python code, use the language “python” for the code block.

Parameters:

pool_management_endpoint (str) – The azure container apps dynamic sessions endpoint.
credential (TokenProvider) – An object that implements the get_token function.
timeout (int) – The timeout for the execution of any single code block. Default is 60.
work_dir (str) – The working directory for the code execution. If None,
a default working directory will be used. The default working
directory is a temporal directory.
functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]]) – A list of functions that are available to the code executor. Default is an empty list.
bool (suppress_result_output) – By default the executor will attach any result info in the execution response to the result outpu. Set this to True to prevent this.
session_id (str) – The session id for the code execution (passed to Dynamic Sessions). If None, a new session id will be generated. Default is None. Note this value will be reset when calling restart




Note
Using the current directory (“.”) as working directory is deprecated. Using it will raise a deprecation warning.



FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions.\n\n$functions'#



SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['python']#



async download_files(files: List[Path | str], cancellation_token: CancellationToken) → List[str][source]#



async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
(Experimental) Execute the code blocks and return the result.

Parameters:

code_blocks (List[CodeBlock]) – The code blocks to execute.
cancellation_token (CancellationToken) – a token to cancel the operation
input_files (Optional[Union[Path, str]]) – Any files the code blocks will need to access


Returns:
CodeResult – The result of the code execution.





format_functions_for_prompt(prompt_template: str = FUNCTION_PROMPT_TEMPLATE) → str[source]#
(Experimental) Format the functions for a prompt.
The template includes one variable:
- $functions: The functions formatted as stubs with two newlines between each function.

Parameters:
prompt_template (str) – The prompt template. Default is the class default.

Returns:
str – The formatted prompt.





property functions: List[str]#



property functions_module: str#
(Experimental) The module name for the functions.



async get_available_packages(cancellation_token: CancellationToken) → set[str][source]#



async get_file_list(cancellation_token: CancellationToken) → List[str][source]#



async restart() → None[source]#
(Experimental) Restart the code executor.
Resets the internal state of the executor by generating a new session ID and resetting the setup variables.
This causes the next code execution to reinitialize the environment and re-run any setup code.



async start() → None[source]#
(Experimental) Start the code executor.
Marks the code executor as started.



async stop() → None[source]#
(Experimental) Stop the code executor.
Stops the code executor after cleaning up the temporary working directory (if it was created).



property timeout: int#
(Experimental) The timeout for code execution.



async upload_files(files: List[Path | str], cancellation_token: CancellationToken) → None[source]#



property work_dir: Path#

**示例**:
```python
pip install "autogen-ext[azure]"

```

```python
FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions.\n\n$functions'#
```

```python
SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['python']#
```

```python
async download_files(files: List[Path | str], cancellation_token: CancellationToken) → List[str][source]#
```

```python
async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
```

【中文翻译】(Experimental) Execute the code blocks and return the result.

Parameters:

code_blocks (List[CodeBlock]) – The code blocks to execute.
cancellation_token (CancellationToken) – a token to cancel the operation
input_files (Optional[Union[Path, str]]) – Any files the code blocks will need to access


Returns:
CodeResult – The result of the code execution.

```python
format_functions_for_prompt(prompt_template: str = FUNCTION_PROMPT_TEMPLATE) → str[source]#
```

【中文翻译】(Experimental) Format the functions for a prompt.
The template includes one variable:
- $functions: The functions formatted as stubs with two newlines between each function.

Parameters:
prompt_template (str) – The prompt template. Default is the class default.

Returns:
str – The formatted prompt.

```python
property functions: List[str]#
```

```python
property functions_module: str#
```

【中文翻译】(Experimental) The module name for the functions.

```python
async get_available_packages(cancellation_token: CancellationToken) → set[str][source]#
```

```python
async get_file_list(cancellation_token: CancellationToken) → List[str][source]#
```

```python
async restart() → None[source]#
```

【中文翻译】(Experimental) Restart the code executor.
Resets the internal state of the executor by generating a new session ID and resetting the setup variables.
This causes the next code execution to reinitialize the environment and re-run any setup code.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start the code executor.
Marks the code executor as started.

```python
async stop() → None[source]#
```

【中文翻译】(Experimental) Stop the code executor.
Stops the code executor after cleaning up the temporary working directory (if it was created).

```python
property timeout: int#
```

【中文翻译】(Experimental) The timeout for code execution.

```python
async upload_files(files: List[Path | str], cancellation_token: CancellationToken) → None[source]#
```

```python
property work_dir: Path#
```

```python
class TokenProvider(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


get_token(*scopes: str, claims: str | None = None, tenant_id: str | None = None, **kwargs: Any) → AccessToken[source]#

```python
get_token(*scopes: str, claims: str | None = None, tenant_id: str | None = None, **kwargs: Any) → AccessToken[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.docker_jupyter

【中文翻译】next

【中文翻译】autogen_ext.cache_store.diskcache

### autogen_ext.code_executors.azure {autogen_extcode_executorsazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html)

```python
class ACADynamicSessionsCodeExecutor(pool_management_endpoint: str, credential: TokenProvider, timeout: int = 60, work_dir: Path | str | None = None, functions: Sequence[FunctionWithRequirements[Any, A] | Callable[[...], Any] | FunctionWithRequirementsStr] = [], functions_module: str = 'functions', suppress_result_output: bool = False, session_id: str | None = None)[source]#
```

【中文翻译】Bases: CodeExecutor
(Experimental) A code executor class that executes code through a an Azure
Container Apps Dynamic Sessions instance.

Note
This class requires the azure extra for the autogen-ext package:
pip install "autogen-ext[azure]"




Caution
This will execute LLM generated code on an Azure dynamic code container.

The execution environment is similar to that of a jupyter notebook which allows for incremental code execution. The parameter functions are executed in order once at the beginning of each session. Each code block is then executed serially and in the order they are received. Each environment has a statically defined set of available packages which cannot be changed.
Currently, attempting to use packages beyond what is available on the environment will result in an error. To get the list of supported packages, call the get_available_packages function.
Currently the only supported language is Python.
For Python code, use the language “python” for the code block.

Parameters:

pool_management_endpoint (str) – The azure container apps dynamic sessions endpoint.
credential (TokenProvider) – An object that implements the get_token function.
timeout (int) – The timeout for the execution of any single code block. Default is 60.
work_dir (str) – The working directory for the code execution. If None,
a default working directory will be used. The default working
directory is a temporal directory.
functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]]) – A list of functions that are available to the code executor. Default is an empty list.
bool (suppress_result_output) – By default the executor will attach any result info in the execution response to the result outpu. Set this to True to prevent this.
session_id (str) – The session id for the code execution (passed to Dynamic Sessions). If None, a new session id will be generated. Default is None. Note this value will be reset when calling restart




Note
Using the current directory (“.”) as working directory is deprecated. Using it will raise a deprecation warning.



FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions.\n\n$functions'#



SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['python']#



async download_files(files: List[Path | str], cancellation_token: CancellationToken) → List[str][source]#



async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
(Experimental) Execute the code blocks and return the result.

Parameters:

code_blocks (List[CodeBlock]) – The code blocks to execute.
cancellation_token (CancellationToken) – a token to cancel the operation
input_files (Optional[Union[Path, str]]) – Any files the code blocks will need to access


Returns:
CodeResult – The result of the code execution.





format_functions_for_prompt(prompt_template: str = FUNCTION_PROMPT_TEMPLATE) → str[source]#
(Experimental) Format the functions for a prompt.
The template includes one variable:
- $functions: The functions formatted as stubs with two newlines between each function.

Parameters:
prompt_template (str) – The prompt template. Default is the class default.

Returns:
str – The formatted prompt.





property functions: List[str]#



property functions_module: str#
(Experimental) The module name for the functions.



async get_available_packages(cancellation_token: CancellationToken) → set[str][source]#



async get_file_list(cancellation_token: CancellationToken) → List[str][source]#



async restart() → None[source]#
(Experimental) Restart the code executor.
Resets the internal state of the executor by generating a new session ID and resetting the setup variables.
This causes the next code execution to reinitialize the environment and re-run any setup code.



async start() → None[source]#
(Experimental) Start the code executor.
Marks the code executor as started.



async stop() → None[source]#
(Experimental) Stop the code executor.
Stops the code executor after cleaning up the temporary working directory (if it was created).



property timeout: int#
(Experimental) The timeout for code execution.



async upload_files(files: List[Path | str], cancellation_token: CancellationToken) → None[source]#



property work_dir: Path#

**示例**:
```python
pip install "autogen-ext[azure]"

```

```python
FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions.\n\n$functions'#
```

```python
SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['python']#
```

```python
async download_files(files: List[Path | str], cancellation_token: CancellationToken) → List[str][source]#
```

```python
async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]#
```

【中文翻译】(Experimental) Execute the code blocks and return the result.

Parameters:

code_blocks (List[CodeBlock]) – The code blocks to execute.
cancellation_token (CancellationToken) – a token to cancel the operation
input_files (Optional[Union[Path, str]]) – Any files the code blocks will need to access


Returns:
CodeResult – The result of the code execution.

```python
format_functions_for_prompt(prompt_template: str = FUNCTION_PROMPT_TEMPLATE) → str[source]#
```

【中文翻译】(Experimental) Format the functions for a prompt.
The template includes one variable:
- $functions: The functions formatted as stubs with two newlines between each function.

Parameters:
prompt_template (str) – The prompt template. Default is the class default.

Returns:
str – The formatted prompt.

```python
property functions: List[str]#
```

```python
property functions_module: str#
```

【中文翻译】(Experimental) The module name for the functions.

```python
async get_available_packages(cancellation_token: CancellationToken) → set[str][source]#
```

```python
async get_file_list(cancellation_token: CancellationToken) → List[str][source]#
```

```python
async restart() → None[source]#
```

【中文翻译】(Experimental) Restart the code executor.
Resets the internal state of the executor by generating a new session ID and resetting the setup variables.
This causes the next code execution to reinitialize the environment and re-run any setup code.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start the code executor.
Marks the code executor as started.

```python
async stop() → None[source]#
```

【中文翻译】(Experimental) Stop the code executor.
Stops the code executor after cleaning up the temporary working directory (if it was created).

```python
property timeout: int#
```

【中文翻译】(Experimental) The timeout for code execution.

```python
async upload_files(files: List[Path | str], cancellation_token: CancellationToken) → None[source]#
```

```python
property work_dir: Path#
```

```python
class TokenProvider(*args, **kwargs)[source]#
```

【中文翻译】Bases: Protocol


get_token(*scopes: str, claims: str | None = None, tenant_id: str | None = None, **kwargs: Any) → AccessToken[source]#

```python
get_token(*scopes: str, claims: str | None = None, tenant_id: str | None = None, **kwargs: Any) → AccessToken[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.docker_jupyter

【中文翻译】next

【中文翻译】autogen_ext.cache_store.diskcache

### autogen_ext.code_executors.docker {autogen_extcode_executorsdocker}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html)

```python
class DockerCommandLineCodeExecutor(image: str = 'python:3-slim', container_name: str | None = None, *, timeout: int = 60, work_dir: Path | str | None = None, bind_dir: Path | str | None = None, auto_remove: bool = True, stop_container: bool = True, device_requests: List[DeviceRequest] | None = None, functions: Sequence[FunctionWithRequirements[Any, A] | Callable[[...], Any] | FunctionWithRequirementsStr] = [], functions_module: str = 'functions', extra_volumes: Dict[str, Dict[str, str]] | None = None, extra_hosts: Dict[str, str] | None = None, init_command: str | None = None, delete_tmp_files: bool = False)[source]#
```

【中文翻译】Bases: CodeExecutor, Component[DockerCommandLineCodeExecutorConfig]
Executes code through a command line environment in a Docker container.

Note
This class requires the docker extra for the autogen-ext package:
pip install "autogen-ext[docker]"



The executor first saves each code block in a file in the working
directory, and then executes the code file in the container.
The executor executes the code blocks in the order they are received.
Currently, the executor only supports Python and shell scripts.
For Python code, use the language “python” for the code block.
For shell scripts, use the language “bash”, “shell”, “sh”, “pwsh”, “powershell”, or “ps1” for the code block.

Parameters:

image (_type_, optional) – Docker image to use for code execution.
Defaults to “python:3-slim”.
container_name (Optional[str], optional) – Name of the Docker container
which is created. If None, will autogenerate a name. Defaults to None.
timeout (int, optional) – The timeout for code execution. Defaults to 60.
work_dir (Union[Path, str], optional) – The working directory for the code
execution. Defaults to temporary directory.
bind_dir (Union[Path, str], optional) – The directory that will be bound
spawn (to the code executor container. Useful for cases where you want to)
work_dir. (the container from within a container. Defaults to)
auto_remove (bool, optional) – If true, will automatically remove the Docker
container when it is stopped. Defaults to True.
stop_container (bool, optional) – If true, will automatically stop the
container when stop is called, when the context manager exits or when
the Python process exits with atext. Defaults to True.
device_requests (Optional[List[DeviceRequest]], optional) – A list of device request instances to add to the container for exposing GPUs (e.g., [docker.types.DeviceRequest(count=-1, capabilities=[[‘gpu’]])]). Defaults to None.
functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]]) – A list of functions that are available to the code executor. Default is an empty list.
functions_module (str, optional) – The name of the module that will be created to store the functions. Defaults to “functions”.
extra_volumes (Optional[Dict[str, Dict[str, str]]], optional) – A dictionary of extra volumes (beyond the work_dir) to mount to the container;
key is host source path and value ‘bind’ is the container path. See  Defaults to None.
Example: extra_volumes = {‘/home/user1/’: {‘bind’: ‘/mnt/vol2’, ‘mode’: ‘rw’}, ‘/var/www’: {‘bind’: ‘/mnt/vol1’, ‘mode’: ‘ro’}}
extra_hosts (Optional[Dict[str, str]], optional) – A dictionary of host mappings to add to the container. (See Docker docs on extra_hosts) Defaults to None.
Example: extra_hosts = {“kubernetes.docker.internal”: “host-gateway”}
init_command (Optional[str], optional) – A shell command to run before each shell operation execution. Defaults to None.
Example: init_command=”kubectl config use-context docker-hub”
delete_tmp_files (bool, optional) – If true, will delete temporary files after execution. Defaults to False.




Note
Using the current directory (“.”) as working directory is deprecated. Using it will raise a deprecation warning.



FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module_name import foo`\n\n$functions'#



SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']#



property bind_dir: Path#



component_config_schema#
alias of DockerCommandLineCodeExecutorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CommandLineCodeResult[source]#
(Experimental) Execute the code blocks and return the result.

Parameters:
code_blocks (List[CodeBlock]) – The code blocks to execute.

Returns:
CommandlineCodeResult – The result of the code execution.





async restart() → None[source]#
(Experimental) Restart the Docker container code executor.



async start() → None[source]#
(Experimental) Start the code executor.
This method sets the working environment variables, connects to Docker and starts the code executor.
If no working directory was provided to the code executor, it creates a temporary directory and sets it as the code executor working directory.



async stop() → None[source]#
(Experimental) Stop the code executor.
Stops the Docker container and cleans up any temporary files (if they were created), along with the temporary directory.
The method first waits for all cancellation tasks to finish before stopping the container. Finally it marks the executor as not running.
If the container is not running, the method does nothing.



property timeout: int#
(Experimental) The timeout for code execution.



property work_dir: Path#

**示例**:
```python
pip install "autogen-ext[docker]"

```

```python
FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module_name import foo`\n\n$functions'#
```

```python
SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']#
```

```python
property bind_dir: Path#
```

```python
component_config_schema#
```

【中文翻译】alias of DockerCommandLineCodeExecutorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CommandLineCodeResult[source]#
```

【中文翻译】(Experimental) Execute the code blocks and return the result.

Parameters:
code_blocks (List[CodeBlock]) – The code blocks to execute.

Returns:
CommandlineCodeResult – The result of the code execution.

```python
async restart() → None[source]#
```

【中文翻译】(Experimental) Restart the Docker container code executor.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start the code executor.
This method sets the working environment variables, connects to Docker and starts the code executor.
If no working directory was provided to the code executor, it creates a temporary directory and sets it as the code executor working directory.

```python
async stop() → None[source]#
```

【中文翻译】(Experimental) Stop the code executor.
Stops the Docker container and cleans up any temporary files (if they were created), along with the temporary directory.
The method first waits for all cancellation tasks to finish before stopping the container. Finally it marks the executor as not running.
If the container is not running, the method does nothing.

```python
property timeout: int#
```

【中文翻译】(Experimental) The timeout for code execution.

```python
property work_dir: Path#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.local

【中文翻译】next

【中文翻译】autogen_ext.code_executors.jupyter

### autogen_ext.code_executors.docker {autogen_extcode_executorsdocker}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html)

```python
class DockerCommandLineCodeExecutor(image: str = 'python:3-slim', container_name: str | None = None, *, timeout: int = 60, work_dir: Path | str | None = None, bind_dir: Path | str | None = None, auto_remove: bool = True, stop_container: bool = True, device_requests: List[DeviceRequest] | None = None, functions: Sequence[FunctionWithRequirements[Any, A] | Callable[[...], Any] | FunctionWithRequirementsStr] = [], functions_module: str = 'functions', extra_volumes: Dict[str, Dict[str, str]] | None = None, extra_hosts: Dict[str, str] | None = None, init_command: str | None = None, delete_tmp_files: bool = False)[source]#
```

【中文翻译】Bases: CodeExecutor, Component[DockerCommandLineCodeExecutorConfig]
Executes code through a command line environment in a Docker container.

Note
This class requires the docker extra for the autogen-ext package:
pip install "autogen-ext[docker]"



The executor first saves each code block in a file in the working
directory, and then executes the code file in the container.
The executor executes the code blocks in the order they are received.
Currently, the executor only supports Python and shell scripts.
For Python code, use the language “python” for the code block.
For shell scripts, use the language “bash”, “shell”, “sh”, “pwsh”, “powershell”, or “ps1” for the code block.

Parameters:

image (_type_, optional) – Docker image to use for code execution.
Defaults to “python:3-slim”.
container_name (Optional[str], optional) – Name of the Docker container
which is created. If None, will autogenerate a name. Defaults to None.
timeout (int, optional) – The timeout for code execution. Defaults to 60.
work_dir (Union[Path, str], optional) – The working directory for the code
execution. Defaults to temporary directory.
bind_dir (Union[Path, str], optional) – The directory that will be bound
spawn (to the code executor container. Useful for cases where you want to)
work_dir. (the container from within a container. Defaults to)
auto_remove (bool, optional) – If true, will automatically remove the Docker
container when it is stopped. Defaults to True.
stop_container (bool, optional) – If true, will automatically stop the
container when stop is called, when the context manager exits or when
the Python process exits with atext. Defaults to True.
device_requests (Optional[List[DeviceRequest]], optional) – A list of device request instances to add to the container for exposing GPUs (e.g., [docker.types.DeviceRequest(count=-1, capabilities=[[‘gpu’]])]). Defaults to None.
functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]]) – A list of functions that are available to the code executor. Default is an empty list.
functions_module (str, optional) – The name of the module that will be created to store the functions. Defaults to “functions”.
extra_volumes (Optional[Dict[str, Dict[str, str]]], optional) – A dictionary of extra volumes (beyond the work_dir) to mount to the container;
key is host source path and value ‘bind’ is the container path. See  Defaults to None.
Example: extra_volumes = {‘/home/user1/’: {‘bind’: ‘/mnt/vol2’, ‘mode’: ‘rw’}, ‘/var/www’: {‘bind’: ‘/mnt/vol1’, ‘mode’: ‘ro’}}
extra_hosts (Optional[Dict[str, str]], optional) – A dictionary of host mappings to add to the container. (See Docker docs on extra_hosts) Defaults to None.
Example: extra_hosts = {“kubernetes.docker.internal”: “host-gateway”}
init_command (Optional[str], optional) – A shell command to run before each shell operation execution. Defaults to None.
Example: init_command=”kubectl config use-context docker-hub”
delete_tmp_files (bool, optional) – If true, will delete temporary files after execution. Defaults to False.




Note
Using the current directory (“.”) as working directory is deprecated. Using it will raise a deprecation warning.



FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module_name import foo`\n\n$functions'#



SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']#



property bind_dir: Path#



component_config_schema#
alias of DockerCommandLineCodeExecutorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CommandLineCodeResult[source]#
(Experimental) Execute the code blocks and return the result.

Parameters:
code_blocks (List[CodeBlock]) – The code blocks to execute.

Returns:
CommandlineCodeResult – The result of the code execution.





async restart() → None[source]#
(Experimental) Restart the Docker container code executor.



async start() → None[source]#
(Experimental) Start the code executor.
This method sets the working environment variables, connects to Docker and starts the code executor.
If no working directory was provided to the code executor, it creates a temporary directory and sets it as the code executor working directory.



async stop() → None[source]#
(Experimental) Stop the code executor.
Stops the Docker container and cleans up any temporary files (if they were created), along with the temporary directory.
The method first waits for all cancellation tasks to finish before stopping the container. Finally it marks the executor as not running.
If the container is not running, the method does nothing.



property timeout: int#
(Experimental) The timeout for code execution.



property work_dir: Path#

**示例**:
```python
pip install "autogen-ext[docker]"

```

```python
FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module_name import foo`\n\n$functions'#
```

```python
SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']#
```

```python
property bind_dir: Path#
```

```python
component_config_schema#
```

【中文翻译】alias of DockerCommandLineCodeExecutorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CommandLineCodeResult[source]#
```

【中文翻译】(Experimental) Execute the code blocks and return the result.

Parameters:
code_blocks (List[CodeBlock]) – The code blocks to execute.

Returns:
CommandlineCodeResult – The result of the code execution.

```python
async restart() → None[source]#
```

【中文翻译】(Experimental) Restart the Docker container code executor.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start the code executor.
This method sets the working environment variables, connects to Docker and starts the code executor.
If no working directory was provided to the code executor, it creates a temporary directory and sets it as the code executor working directory.

```python
async stop() → None[source]#
```

【中文翻译】(Experimental) Stop the code executor.
Stops the Docker container and cleans up any temporary files (if they were created), along with the temporary directory.
The method first waits for all cancellation tasks to finish before stopping the container. Finally it marks the executor as not running.
If the container is not running, the method does nothing.

```python
property timeout: int#
```

【中文翻译】(Experimental) The timeout for code execution.

```python
property work_dir: Path#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.local

【中文翻译】next

【中文翻译】autogen_ext.code_executors.jupyter

### autogen_ext.code_executors.docker_jupyter {autogen_extcode_executorsdocker_jupyter}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html)

```python
class DockerJupyterCodeExecutor(jupyter_server: JupyterConnectable | JupyterConnectionInfo, kernel_name: str = 'python3', timeout: int = 60, output_dir: Path | None = None)[source]#
```

【中文翻译】Bases: CodeExecutor, Component[DockerJupyterCodeExecutorConfig]
(Experimental) A code executor class that executes code statefully using
a Jupyter server supplied to this class.
Each execution is stateful and can access variables created from previous
executions in the same session.
To use this, you need to install the following dependencies:
pip install "autogen-ext[docker-jupyter-executor]"



Parameters:

jupyter_server (Union[JupyterConnectable, JupyterConnectionInfo]) – The Jupyter server to use.
kernel_name (str) – The kernel name to use. Make sure it is installed.
By default, it is “python3”.
timeout (int) – The timeout for code execution, by default 60.
output_dir (str) – The directory to save output files, by default None.



Example of using it directly:
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
            print(code_result)


asyncio.run(main())


Example of using it with your own jupyter image:
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


async def main() -> None:
    async with DockerJupyterServer(custom_image_name="your_custom_images_name", expose_port=8888) as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
            print(code_result)


asyncio.run(main())


Example of using it with PythonCodeExecutionTool:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            tool = PythonCodeExecutionTool(executor)
            model_client = OpenAIChatCompletionClient(model="gpt-4o")
            agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
            result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
            print(result)


asyncio.run(main())


Example of using it inside a CodeExecutorAgent:
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
from autogen_core import CancellationToken


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
            task = TextMessage(
                content='''Here is some code
        ```python
        print('Hello world')
        ```
        ''',
                source="user",
            )
            response = await code_executor_agent.on_messages([task], CancellationToken())
            print(response.chat_message)


asyncio.run(main())




component_config_schema#
alias of DockerJupyterCodeExecutorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.docker_jupyter.DockerJupyterCodeExecutor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → DockerJupyterCodeResult[source]#
(Experimental) Execute a list of code blocks and return the result.
This method executes a list of code blocks as cells in the Jupyter kernel.
See: https://jupyter-client.readthedocs.io/en/stable/messaging.html
for the message protocol.

Parameters:
code_blocks (List[CodeBlock]) – A list of code blocks to execute.

Returns:
DockerJupyterCodeResult – The result of the code execution.





async restart() → None[source]#
(Experimental) Restart a new session.



async start() → None[source]#
(Experimental) Start a new session.



async stop() → None[source]#
Stop the kernel.

**示例**:
```python
pip install "autogen-ext[docker-jupyter-executor]"

```

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
            print(code_result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


async def main() -> None:
    async with DockerJupyterServer(custom_image_name="your_custom_images_name", expose_port=8888) as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
            print(code_result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            tool = PythonCodeExecutionTool(executor)
            model_client = OpenAIChatCompletionClient(model="gpt-4o")
            agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
            result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
            print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
from autogen_core import CancellationToken


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
            task = TextMessage(
                content='''Here is some code
        ```python
        print('Hello world')
        ```
        ''',
                source="user",
            )
            response = await code_executor_agent.on_messages([task], CancellationToken())
            print(response.chat_message)


asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of DockerJupyterCodeExecutorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.docker_jupyter.DockerJupyterCodeExecutor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → DockerJupyterCodeResult[source]#
```

【中文翻译】(Experimental) Execute a list of code blocks and return the result.
This method executes a list of code blocks as cells in the Jupyter kernel.
See: https://jupyter-client.readthedocs.io/en/stable/messaging.html
for the message protocol.

Parameters:
code_blocks (List[CodeBlock]) – A list of code blocks to execute.

Returns:
DockerJupyterCodeResult – The result of the code execution.

```python
async restart() → None[source]#
```

【中文翻译】(Experimental) Restart a new session.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start a new session.

```python
async stop() → None[source]#
```

【中文翻译】Stop the kernel.

```python
class DockerJupyterCodeResult(exit_code: int, output: str, output_files: list[Path])[source]#
```

【中文翻译】Bases: CodeResult
(Experimental) A code result class for IPython code executor.


output_files: list[Path]#

```python
output_files: list[Path]#
```

```python
class DockerJupyterServer(*, custom_image_name: str | None = None, container_name: str | None = None, auto_remove: bool = True, stop_container: bool = True, docker_env: Dict[str, str] | None = None, expose_port: int = 8888, token: str | GenerateToken | None = None, work_dir: Path | str = '/workspace', bind_dir: Path | str | None = None)[source]#
```

【中文翻译】Bases: JupyterConnectable


DEFAULT_DOCKERFILE = 'FROM quay.io/jupyter/docker-stacks-foundation\n\n        SHELL ["/bin/bash", "-o", "pipefail", "-c"]\n\n        USER ${NB_UID}\n        RUN mamba install --yes jupyter_kernel_gateway ipykernel &&             mamba clean --all -f -y &&             fix-permissions "${CONDA_DIR}" &&             fix-permissions "/home/${NB_USER}"\n\n        ENV TOKEN="UNSET"\n        CMD python -m jupyter kernelgateway --KernelGatewayApp.ip=0.0.0.0             --KernelGatewayApp.port=8888             --KernelGatewayApp.auth_token="${TOKEN}"             --JupyterApp.answer_yes=true             --JupyterWebsocketPersonality.list_kernels=true\n\n        EXPOSE 8888\n\n        WORKDIR "${HOME}"\n        '#



class GenerateToken[source]#
Bases: object



property connection_info: JupyterConnectionInfo#
Return the connection information for this connectable.



async get_client() → JupyterClient[source]#



async stop() → None[source]#

```python
DEFAULT_DOCKERFILE = 'FROM quay.io/jupyter/docker-stacks-foundation\n\n        SHELL ["/bin/bash", "-o", "pipefail", "-c"]\n\n        USER ${NB_UID}\n        RUN mamba install --yes jupyter_kernel_gateway ipykernel &&             mamba clean --all -f -y &&             fix-permissions "${CONDA_DIR}" &&             fix-permissions "/home/${NB_USER}"\n\n        ENV TOKEN="UNSET"\n        CMD python -m jupyter kernelgateway --KernelGatewayApp.ip=0.0.0.0             --KernelGatewayApp.port=8888             --KernelGatewayApp.auth_token="${TOKEN}"             --JupyterApp.answer_yes=true             --JupyterWebsocketPersonality.list_kernels=true\n\n        EXPOSE 8888\n\n        WORKDIR "${HOME}"\n        '#
```

```python
class GenerateToken[source]#
```

【中文翻译】Bases: object

```python
property connection_info: JupyterConnectionInfo#
```

【中文翻译】Return the connection information for this connectable.

```python
async get_client() → JupyterClient[source]#
```

```python
async stop() → None[source]#
```

```python
class JupyterClient(connection_info: JupyterConnectionInfo)[source]#
```

【中文翻译】Bases: object


async close() → None[source]#
Close the async session



async delete_kernel(kernel_id: str) → None[source]#



async get_kernel_client(kernel_id: str) → JupyterKernelClient[source]#



async list_kernel_specs() → Dict[str, Dict[str, str]][source]#



async list_kernels() → List[Dict[str, str]][source]#



async restart_kernel(kernel_id: str) → None[source]#



async start_kernel(kernel_spec_name: str) → str[source]#
Start a new kernel asynchronously.

Parameters:
kernel_spec_name (str) – Name of the kernel spec to start

Returns:
str – ID of the started kernel

```python
async close() → None[source]#
```

【中文翻译】Close the async session

```python
async delete_kernel(kernel_id: str) → None[source]#
```

```python
async get_kernel_client(kernel_id: str) → JupyterKernelClient[source]#
```

```python
async list_kernel_specs() → Dict[str, Dict[str, str]][source]#
```

```python
async list_kernels() → List[Dict[str, str]][source]#
```

```python
async restart_kernel(kernel_id: str) → None[source]#
```

```python
async start_kernel(kernel_spec_name: str) → str[source]#
```

【中文翻译】Start a new kernel asynchronously.

Parameters:
kernel_spec_name (str) – Name of the kernel spec to start

Returns:
str – ID of the started kernel

```python
class JupyterKernelClient(websocket: ClientConnection)[source]#
```

【中文翻译】Bases: object
An asynchronous client for communicating with a Jupyter kernel.


async execute(code: str, timeout_seconds: float | None = None) → ExecutionResult[source]#



async stop() → None[source]#



async wait_for_ready(timeout_seconds: float | None = None) → bool[source]#

```python
async execute(code: str, timeout_seconds: float | None = None) → ExecutionResult[source]#
```

```python
async stop() → None[source]#
```

```python
async wait_for_ready(timeout_seconds: float | None = None) → bool[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.jupyter

【中文翻译】next

【中文翻译】autogen_ext.code_executors.azure

### autogen_ext.code_executors.docker_jupyter {autogen_extcode_executorsdocker_jupyter}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html)

```python
class DockerJupyterCodeExecutor(jupyter_server: JupyterConnectable | JupyterConnectionInfo, kernel_name: str = 'python3', timeout: int = 60, output_dir: Path | None = None)[source]#
```

【中文翻译】Bases: CodeExecutor, Component[DockerJupyterCodeExecutorConfig]
(Experimental) A code executor class that executes code statefully using
a Jupyter server supplied to this class.
Each execution is stateful and can access variables created from previous
executions in the same session.
To use this, you need to install the following dependencies:
pip install "autogen-ext[docker-jupyter-executor]"



Parameters:

jupyter_server (Union[JupyterConnectable, JupyterConnectionInfo]) – The Jupyter server to use.
kernel_name (str) – The kernel name to use. Make sure it is installed.
By default, it is “python3”.
timeout (int) – The timeout for code execution, by default 60.
output_dir (str) – The directory to save output files, by default None.



Example of using it directly:
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
            print(code_result)


asyncio.run(main())


Example of using it with your own jupyter image:
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


async def main() -> None:
    async with DockerJupyterServer(custom_image_name="your_custom_images_name", expose_port=8888) as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
            print(code_result)


asyncio.run(main())


Example of using it with PythonCodeExecutionTool:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            tool = PythonCodeExecutionTool(executor)
            model_client = OpenAIChatCompletionClient(model="gpt-4o")
            agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
            result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
            print(result)


asyncio.run(main())


Example of using it inside a CodeExecutorAgent:
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
from autogen_core import CancellationToken


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
            task = TextMessage(
                content='''Here is some code
        ```python
        print('Hello world')
        ```
        ''',
                source="user",
            )
            response = await code_executor_agent.on_messages([task], CancellationToken())
            print(response.chat_message)


asyncio.run(main())




component_config_schema#
alias of DockerJupyterCodeExecutorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.docker_jupyter.DockerJupyterCodeExecutor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → DockerJupyterCodeResult[source]#
(Experimental) Execute a list of code blocks and return the result.
This method executes a list of code blocks as cells in the Jupyter kernel.
See: https://jupyter-client.readthedocs.io/en/stable/messaging.html
for the message protocol.

Parameters:
code_blocks (List[CodeBlock]) – A list of code blocks to execute.

Returns:
DockerJupyterCodeResult – The result of the code execution.





async restart() → None[source]#
(Experimental) Restart a new session.



async start() → None[source]#
(Experimental) Start a new session.



async stop() → None[source]#
Stop the kernel.

**示例**:
```python
pip install "autogen-ext[docker-jupyter-executor]"

```

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
            print(code_result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer


async def main() -> None:
    async with DockerJupyterServer(custom_image_name="your_custom_images_name", expose_port=8888) as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
            code_result = await executor.execute_code_blocks(code_blocks, cancellation_token=CancellationToken())
            print(code_result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            tool = PythonCodeExecutionTool(executor)
            model_client = OpenAIChatCompletionClient(model="gpt-4o")
            agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
            result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
            print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.docker_jupyter import DockerJupyterCodeExecutor, DockerJupyterServer
from autogen_core import CancellationToken


async def main() -> None:
    async with DockerJupyterServer() as jupyter_server:
        async with DockerJupyterCodeExecutor(jupyter_server=jupyter_server) as executor:
            code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
            task = TextMessage(
                content='''Here is some code
        ```python
        print('Hello world')
        ```
        ''',
                source="user",
            )
            response = await code_executor_agent.on_messages([task], CancellationToken())
            print(response.chat_message)


asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of DockerJupyterCodeExecutorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.docker_jupyter.DockerJupyterCodeExecutor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → DockerJupyterCodeResult[source]#
```

【中文翻译】(Experimental) Execute a list of code blocks and return the result.
This method executes a list of code blocks as cells in the Jupyter kernel.
See: https://jupyter-client.readthedocs.io/en/stable/messaging.html
for the message protocol.

Parameters:
code_blocks (List[CodeBlock]) – A list of code blocks to execute.

Returns:
DockerJupyterCodeResult – The result of the code execution.

```python
async restart() → None[source]#
```

【中文翻译】(Experimental) Restart a new session.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start a new session.

```python
async stop() → None[source]#
```

【中文翻译】Stop the kernel.

```python
class DockerJupyterCodeResult(exit_code: int, output: str, output_files: list[Path])[source]#
```

【中文翻译】Bases: CodeResult
(Experimental) A code result class for IPython code executor.


output_files: list[Path]#

```python
output_files: list[Path]#
```

```python
class DockerJupyterServer(*, custom_image_name: str | None = None, container_name: str | None = None, auto_remove: bool = True, stop_container: bool = True, docker_env: Dict[str, str] | None = None, expose_port: int = 8888, token: str | GenerateToken | None = None, work_dir: Path | str = '/workspace', bind_dir: Path | str | None = None)[source]#
```

【中文翻译】Bases: JupyterConnectable


DEFAULT_DOCKERFILE = 'FROM quay.io/jupyter/docker-stacks-foundation\n\n        SHELL ["/bin/bash", "-o", "pipefail", "-c"]\n\n        USER ${NB_UID}\n        RUN mamba install --yes jupyter_kernel_gateway ipykernel &&             mamba clean --all -f -y &&             fix-permissions "${CONDA_DIR}" &&             fix-permissions "/home/${NB_USER}"\n\n        ENV TOKEN="UNSET"\n        CMD python -m jupyter kernelgateway --KernelGatewayApp.ip=0.0.0.0             --KernelGatewayApp.port=8888             --KernelGatewayApp.auth_token="${TOKEN}"             --JupyterApp.answer_yes=true             --JupyterWebsocketPersonality.list_kernels=true\n\n        EXPOSE 8888\n\n        WORKDIR "${HOME}"\n        '#



class GenerateToken[source]#
Bases: object



property connection_info: JupyterConnectionInfo#
Return the connection information for this connectable.



async get_client() → JupyterClient[source]#



async stop() → None[source]#

```python
DEFAULT_DOCKERFILE = 'FROM quay.io/jupyter/docker-stacks-foundation\n\n        SHELL ["/bin/bash", "-o", "pipefail", "-c"]\n\n        USER ${NB_UID}\n        RUN mamba install --yes jupyter_kernel_gateway ipykernel &&             mamba clean --all -f -y &&             fix-permissions "${CONDA_DIR}" &&             fix-permissions "/home/${NB_USER}"\n\n        ENV TOKEN="UNSET"\n        CMD python -m jupyter kernelgateway --KernelGatewayApp.ip=0.0.0.0             --KernelGatewayApp.port=8888             --KernelGatewayApp.auth_token="${TOKEN}"             --JupyterApp.answer_yes=true             --JupyterWebsocketPersonality.list_kernels=true\n\n        EXPOSE 8888\n\n        WORKDIR "${HOME}"\n        '#
```

```python
class GenerateToken[source]#
```

【中文翻译】Bases: object

```python
property connection_info: JupyterConnectionInfo#
```

【中文翻译】Return the connection information for this connectable.

```python
async get_client() → JupyterClient[source]#
```

```python
async stop() → None[source]#
```

```python
class JupyterClient(connection_info: JupyterConnectionInfo)[source]#
```

【中文翻译】Bases: object


async close() → None[source]#
Close the async session



async delete_kernel(kernel_id: str) → None[source]#



async get_kernel_client(kernel_id: str) → JupyterKernelClient[source]#



async list_kernel_specs() → Dict[str, Dict[str, str]][source]#



async list_kernels() → List[Dict[str, str]][source]#



async restart_kernel(kernel_id: str) → None[source]#



async start_kernel(kernel_spec_name: str) → str[source]#
Start a new kernel asynchronously.

Parameters:
kernel_spec_name (str) – Name of the kernel spec to start

Returns:
str – ID of the started kernel

```python
async close() → None[source]#
```

【中文翻译】Close the async session

```python
async delete_kernel(kernel_id: str) → None[source]#
```

```python
async get_kernel_client(kernel_id: str) → JupyterKernelClient[source]#
```

```python
async list_kernel_specs() → Dict[str, Dict[str, str]][source]#
```

```python
async list_kernels() → List[Dict[str, str]][source]#
```

```python
async restart_kernel(kernel_id: str) → None[source]#
```

```python
async start_kernel(kernel_spec_name: str) → str[source]#
```

【中文翻译】Start a new kernel asynchronously.

Parameters:
kernel_spec_name (str) – Name of the kernel spec to start

Returns:
str – ID of the started kernel

```python
class JupyterKernelClient(websocket: ClientConnection)[source]#
```

【中文翻译】Bases: object
An asynchronous client for communicating with a Jupyter kernel.


async execute(code: str, timeout_seconds: float | None = None) → ExecutionResult[source]#



async stop() → None[source]#



async wait_for_ready(timeout_seconds: float | None = None) → bool[source]#

```python
async execute(code: str, timeout_seconds: float | None = None) → ExecutionResult[source]#
```

```python
async stop() → None[source]#
```

```python
async wait_for_ready(timeout_seconds: float | None = None) → bool[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.jupyter

【中文翻译】next

【中文翻译】autogen_ext.code_executors.azure

### autogen_ext.code_executors.jupyter {autogen_extcode_executorsjupyter}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html)

```python
class JupyterCodeExecutor(kernel_name: str = 'python3', timeout: int = 60, output_dir: Path | str | None = None)[source]#
```

【中文翻译】Bases: CodeExecutor, Component[JupyterCodeExecutorConfig]
A code executor class that executes code statefully using [nbclient](jupyter/nbclient).

Danger
This will execute code on the local machine. If being used with LLM generated code, caution should be used.

Example of using it directly:
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        cancel_token = CancellationToken()
        code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
        code_result = await executor.execute_code_blocks(code_blocks, cancel_token)
        print(code_result)


asyncio.run(main())


Example of using it with PythonCodeExecutionTool:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        tool = PythonCodeExecutionTool(executor)
        model_client = OpenAIChatCompletionClient(model="gpt-4o")
        agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
        result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
        print(result)


asyncio.run(main())


Example of using it inside a CodeExecutorAgent:
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
from autogen_core import CancellationToken


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
        task = TextMessage(
            content='''Here is some code
    ```python
    print('Hello world')
    ```
    ''',
            source="user",
        )
        response = await code_executor_agent.on_messages([task], CancellationToken())
        print(response.chat_message)


asyncio.run(main())



Parameters:

kernel_name (str) – The kernel name to use. By default, “python3”.
timeout (int) – The timeout for code execution, by default 60.
output_dir (Path) – The directory to save output files, by default a temporary directory.




Note
Using the current directory (“.”) as output directory is deprecated. Using it will raise a deprecation warning.



component_config_schema#
alias of JupyterCodeExecutorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.jupyter.JupyterCodeExecutor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_blocks(code_blocks: list[CodeBlock], cancellation_token: CancellationToken) → JupyterCodeResult[source]#
Execute code blocks and return the result.

Parameters:
code_blocks (list[CodeBlock]) – The code blocks to execute.

Returns:
JupyterCodeResult – The result of the code execution.





property output_dir: Path#



async restart() → None[source]#
Restart the code executor.



async start() → None[source]#
(Experimental) Start the code executor.
Initializes the Jupyter Notebook execution environment by creating a new notebook and setting it up with the specified Jupyter Kernel.
Marks the executor as started, allowing for code execution.
This method should be called before executing any code blocks.



async stop() → None[source]#
(Experimental) Stop the code executor.
Terminates the Jupyter Notebook execution by exiting the kernel context and cleaning up the associated resources.

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        cancel_token = CancellationToken()
        code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
        code_result = await executor.execute_code_blocks(code_blocks, cancel_token)
        print(code_result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        tool = PythonCodeExecutionTool(executor)
        model_client = OpenAIChatCompletionClient(model="gpt-4o")
        agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
        result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
        print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
from autogen_core import CancellationToken


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
        task = TextMessage(
            content='''Here is some code
    ```python
    print('Hello world')
    ```
    ''',
            source="user",
        )
        response = await code_executor_agent.on_messages([task], CancellationToken())
        print(response.chat_message)


asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of JupyterCodeExecutorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.jupyter.JupyterCodeExecutor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_blocks(code_blocks: list[CodeBlock], cancellation_token: CancellationToken) → JupyterCodeResult[source]#
```

【中文翻译】Execute code blocks and return the result.

Parameters:
code_blocks (list[CodeBlock]) – The code blocks to execute.

Returns:
JupyterCodeResult – The result of the code execution.

```python
property output_dir: Path#
```

```python
async restart() → None[source]#
```

【中文翻译】Restart the code executor.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start the code executor.
Initializes the Jupyter Notebook execution environment by creating a new notebook and setting it up with the specified Jupyter Kernel.
Marks the executor as started, allowing for code execution.
This method should be called before executing any code blocks.

```python
async stop() → None[source]#
```

【中文翻译】(Experimental) Stop the code executor.
Terminates the Jupyter Notebook execution by exiting the kernel context and cleaning up the associated resources.

```python
class JupyterCodeResult(exit_code: int, output: str, output_files: list[Path])[source]#
```

【中文翻译】Bases: CodeResult
A code result class for Jupyter code executor.


output_files: list[Path]#

```python
output_files: list[Path]#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.docker

【中文翻译】next

【中文翻译】autogen_ext.code_executors.docker_jupyter

### autogen_ext.code_executors.jupyter {autogen_extcode_executorsjupyter}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html)

```python
class JupyterCodeExecutor(kernel_name: str = 'python3', timeout: int = 60, output_dir: Path | str | None = None)[source]#
```

【中文翻译】Bases: CodeExecutor, Component[JupyterCodeExecutorConfig]
A code executor class that executes code statefully using [nbclient](jupyter/nbclient).

Danger
This will execute code on the local machine. If being used with LLM generated code, caution should be used.

Example of using it directly:
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        cancel_token = CancellationToken()
        code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
        code_result = await executor.execute_code_blocks(code_blocks, cancel_token)
        print(code_result)


asyncio.run(main())


Example of using it with PythonCodeExecutionTool:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        tool = PythonCodeExecutionTool(executor)
        model_client = OpenAIChatCompletionClient(model="gpt-4o")
        agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
        result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
        print(result)


asyncio.run(main())


Example of using it inside a CodeExecutorAgent:
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
from autogen_core import CancellationToken


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
        task = TextMessage(
            content='''Here is some code
    ```python
    print('Hello world')
    ```
    ''',
            source="user",
        )
        response = await code_executor_agent.on_messages([task], CancellationToken())
        print(response.chat_message)


asyncio.run(main())



Parameters:

kernel_name (str) – The kernel name to use. By default, “python3”.
timeout (int) – The timeout for code execution, by default 60.
output_dir (Path) – The directory to save output files, by default a temporary directory.




Note
Using the current directory (“.”) as output directory is deprecated. Using it will raise a deprecation warning.



component_config_schema#
alias of JupyterCodeExecutorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.jupyter.JupyterCodeExecutor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_blocks(code_blocks: list[CodeBlock], cancellation_token: CancellationToken) → JupyterCodeResult[source]#
Execute code blocks and return the result.

Parameters:
code_blocks (list[CodeBlock]) – The code blocks to execute.

Returns:
JupyterCodeResult – The result of the code execution.





property output_dir: Path#



async restart() → None[source]#
Restart the code executor.



async start() → None[source]#
(Experimental) Start the code executor.
Initializes the Jupyter Notebook execution environment by creating a new notebook and setting it up with the specified Jupyter Kernel.
Marks the executor as started, allowing for code execution.
This method should be called before executing any code blocks.



async stop() → None[source]#
(Experimental) Stop the code executor.
Terminates the Jupyter Notebook execution by exiting the kernel context and cleaning up the associated resources.

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        cancel_token = CancellationToken()
        code_blocks = [CodeBlock(code="print('hello world!')", language="python")]
        code_result = await executor.execute_code_blocks(code_blocks, cancel_token)
        print(code_result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        tool = PythonCodeExecutionTool(executor)
        model_client = OpenAIChatCompletionClient(model="gpt-4o")
        agent = AssistantAgent("assistant", model_client=model_client, tools=[tool])
        result = await agent.run(task="What is the 10th Fibonacci number? Use Python to calculate it.")
        print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
from autogen_core import CancellationToken


async def main() -> None:
    async with JupyterCodeExecutor() as executor:
        code_executor_agent = CodeExecutorAgent("code_executor", code_executor=executor)
        task = TextMessage(
            content='''Here is some code
    ```python
    print('Hello world')
    ```
    ''',
            source="user",
        )
        response = await code_executor_agent.on_messages([task], CancellationToken())
        print(response.chat_message)


asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of JupyterCodeExecutorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.jupyter.JupyterCodeExecutor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_blocks(code_blocks: list[CodeBlock], cancellation_token: CancellationToken) → JupyterCodeResult[source]#
```

【中文翻译】Execute code blocks and return the result.

Parameters:
code_blocks (list[CodeBlock]) – The code blocks to execute.

Returns:
JupyterCodeResult – The result of the code execution.

```python
property output_dir: Path#
```

```python
async restart() → None[source]#
```

【中文翻译】Restart the code executor.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start the code executor.
Initializes the Jupyter Notebook execution environment by creating a new notebook and setting it up with the specified Jupyter Kernel.
Marks the executor as started, allowing for code execution.
This method should be called before executing any code blocks.

```python
async stop() → None[source]#
```

【中文翻译】(Experimental) Stop the code executor.
Terminates the Jupyter Notebook execution by exiting the kernel context and cleaning up the associated resources.

```python
class JupyterCodeResult(exit_code: int, output: str, output_files: list[Path])[source]#
```

【中文翻译】Bases: CodeResult
A code result class for Jupyter code executor.


output_files: list[Path]#

```python
output_files: list[Path]#
```

【中文翻译】previous

【中文翻译】autogen_ext.code_executors.docker

【中文翻译】next

【中文翻译】autogen_ext.code_executors.docker_jupyter

### autogen_ext.code_executors.local {autogen_extcode_executorslocal}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html)

```python
class LocalCommandLineCodeExecutor(timeout: int = 60, work_dir: Path | str | None = None, functions: Sequence[FunctionWithRequirements[Any, A] | Callable[[...], Any] | FunctionWithRequirementsStr] = [], functions_module: str = 'functions', virtual_env_context: SimpleNamespace | None = None)[source]#
```

【中文翻译】Bases: CodeExecutor, Component[LocalCommandLineCodeExecutorConfig]
A code executor class that executes code through a local command line
environment.

Danger
This will execute code on the local machine. If being used with LLM generated code, caution should be used.

Each code block is saved as a file and executed in a separate process in
the working directory, and a unique file is generated and saved in the
working directory for each code block.
The code blocks are executed in the order they are received.
Command line code is sanitized using regular expression match against a list of dangerous commands in order to prevent self-destructive
commands from being executed which may potentially affect the users environment.
Currently the only supported languages is Python and shell scripts.
For Python code, use the language “python” for the code block.
For shell scripts, use the language “bash”, “shell”, “sh”, “pwsh”, “powershell”, or “ps1” for the code
block.

Note
On Windows, the event loop policy must be set to WindowsProactorEventLoopPolicy to avoid issues with subprocesses.
import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())




Parameters:

timeout (int) – The timeout for the execution of any single code block. Default is 60.
work_dir (str) – The working directory for the code execution. If None,
a default working directory will be used. The default working directory is a temporary directory.
functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]]) – A list of functions that are available to the code executor. Default is an empty list.
functions_module (str, optional) – The name of the module that will be created to store the functions. Defaults to “functions”.
virtual_env_context (Optional[SimpleNamespace], optional) – The virtual environment context. Defaults to None.




Note
Using the current directory (“.”) as working directory is deprecated. Using it will raise a deprecation warning.

Example:
How to use LocalCommandLineCodeExecutor with a virtual environment different from the one used to run the autogen application:
Set up a virtual environment using the venv module, and pass its context to the initializer of LocalCommandLineCodeExecutor. This way, the executor will run code within the new environment.

import venv
from pathlib import Path
import asyncio

from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor


async def example():
    work_dir = Path("coding")
    work_dir.mkdir(exist_ok=True)

    venv_dir = work_dir / ".venv"
    venv_builder = venv.EnvBuilder(with_pip=True)
    venv_builder.create(venv_dir)
    venv_context = venv_builder.ensure_directories(venv_dir)

    local_executor = LocalCommandLineCodeExecutor(work_dir=work_dir, virtual_env_context=venv_context)
    await local_executor.execute_code_blocks(
        code_blocks=[
            CodeBlock(language="bash", code="pip install matplotlib"),
        ],
        cancellation_token=CancellationToken(),
    )


asyncio.run(example())





FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module_name import foo`\n\n$functions'#



SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']#



classmethod _from_config(config: LocalCommandLineCodeExecutorConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → LocalCommandLineCodeExecutorConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of LocalCommandLineCodeExecutorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.local.LocalCommandLineCodeExecutor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CommandLineCodeResult[source]#
(Experimental) Execute the code blocks and return the result.

Parameters:

code_blocks (List[CodeBlock]) – The code blocks to execute.
cancellation_token (CancellationToken) – a token to cancel the operation


Returns:
CommandLineCodeResult – The result of the code execution.





format_functions_for_prompt(prompt_template: str = FUNCTION_PROMPT_TEMPLATE) → str[source]#
(Experimental) Format the functions for a prompt.
The template includes two variables:
- $module_name: The module name.
- $functions: The functions formatted as stubs with two newlines between each function.

Parameters:
prompt_template (str) – The prompt template. Default is the class default.

Returns:
str – The formatted prompt.





property functions: List[str]#



property functions_module: str#
(Experimental) The module name for the functions.



async restart() → None[source]#
(Experimental) Restart the code executor.



async start() → None[source]#
(Experimental) Start the code executor.
Initializes the local code executor and should be called before executing any code blocks.
It marks the executor internal state as started.
If no working directory is provided, the method creates a temporary directory for the executor to use.



async stop() → None[source]#
(Experimental) Stop the code executor.
Stops the local code executor and performs the cleanup of the temporary working directory (if it was created).
The executor’s internal state is markes as no longer started.



property timeout: int#
(Experimental) The timeout for code execution.



property work_dir: Path#
(Experimental) The working directory for the code execution.

**示例**:
```python
import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

```

**示例**:
```python
import venv
from pathlib import Path
import asyncio

from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor


async def example():
    work_dir = Path("coding")
    work_dir.mkdir(exist_ok=True)

    venv_dir = work_dir / ".venv"
    venv_builder = venv.EnvBuilder(with_pip=True)
    venv_builder.create(venv_dir)
    venv_context = venv_builder.ensure_directories(venv_dir)

    local_executor = LocalCommandLineCodeExecutor(work_dir=work_dir, virtual_env_context=venv_context)
    await local_executor.execute_code_blocks(
        code_blocks=[
            CodeBlock(language="bash", code="pip install matplotlib"),
        ],
        cancellation_token=CancellationToken(),
    )


asyncio.run(example())

```

```python
FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module_name import foo`\n\n$functions'#
```

```python
SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']#
```

```python
classmethod _from_config(config: LocalCommandLineCodeExecutorConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → LocalCommandLineCodeExecutorConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of LocalCommandLineCodeExecutorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.local.LocalCommandLineCodeExecutor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CommandLineCodeResult[source]#
```

【中文翻译】(Experimental) Execute the code blocks and return the result.

Parameters:

code_blocks (List[CodeBlock]) – The code blocks to execute.
cancellation_token (CancellationToken) – a token to cancel the operation


Returns:
CommandLineCodeResult – The result of the code execution.

```python
format_functions_for_prompt(prompt_template: str = FUNCTION_PROMPT_TEMPLATE) → str[source]#
```

【中文翻译】(Experimental) Format the functions for a prompt.
The template includes two variables:
- $module_name: The module name.
- $functions: The functions formatted as stubs with two newlines between each function.

Parameters:
prompt_template (str) – The prompt template. Default is the class default.

Returns:
str – The formatted prompt.

```python
property functions: List[str]#
```

```python
property functions_module: str#
```

【中文翻译】(Experimental) The module name for the functions.

```python
async restart() → None[source]#
```

【中文翻译】(Experimental) Restart the code executor.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start the code executor.
Initializes the local code executor and should be called before executing any code blocks.
It marks the executor internal state as started.
If no working directory is provided, the method creates a temporary directory for the executor to use.

```python
async stop() → None[source]#
```

【中文翻译】(Experimental) Stop the code executor.
Stops the local code executor and performs the cleanup of the temporary working directory (if it was created).
The executor’s internal state is markes as no longer started.

```python
property timeout: int#
```

【中文翻译】(Experimental) The timeout for code execution.

```python
property work_dir: Path#
```

【中文翻译】(Experimental) The working directory for the code execution.

【中文翻译】previous

【中文翻译】autogen_ext.tools.semantic_kernel

【中文翻译】next

【中文翻译】autogen_ext.code_executors.docker

### autogen_ext.code_executors.local {autogen_extcode_executorslocal}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.local.html)

```python
class LocalCommandLineCodeExecutor(timeout: int = 60, work_dir: Path | str | None = None, functions: Sequence[FunctionWithRequirements[Any, A] | Callable[[...], Any] | FunctionWithRequirementsStr] = [], functions_module: str = 'functions', virtual_env_context: SimpleNamespace | None = None)[source]#
```

【中文翻译】Bases: CodeExecutor, Component[LocalCommandLineCodeExecutorConfig]
A code executor class that executes code through a local command line
environment.

Danger
This will execute code on the local machine. If being used with LLM generated code, caution should be used.

Each code block is saved as a file and executed in a separate process in
the working directory, and a unique file is generated and saved in the
working directory for each code block.
The code blocks are executed in the order they are received.
Command line code is sanitized using regular expression match against a list of dangerous commands in order to prevent self-destructive
commands from being executed which may potentially affect the users environment.
Currently the only supported languages is Python and shell scripts.
For Python code, use the language “python” for the code block.
For shell scripts, use the language “bash”, “shell”, “sh”, “pwsh”, “powershell”, or “ps1” for the code
block.

Note
On Windows, the event loop policy must be set to WindowsProactorEventLoopPolicy to avoid issues with subprocesses.
import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())




Parameters:

timeout (int) – The timeout for the execution of any single code block. Default is 60.
work_dir (str) – The working directory for the code execution. If None,
a default working directory will be used. The default working directory is a temporary directory.
functions (List[Union[FunctionWithRequirements[Any, A], Callable[..., Any]]]) – A list of functions that are available to the code executor. Default is an empty list.
functions_module (str, optional) – The name of the module that will be created to store the functions. Defaults to “functions”.
virtual_env_context (Optional[SimpleNamespace], optional) – The virtual environment context. Defaults to None.




Note
Using the current directory (“.”) as working directory is deprecated. Using it will raise a deprecation warning.

Example:
How to use LocalCommandLineCodeExecutor with a virtual environment different from the one used to run the autogen application:
Set up a virtual environment using the venv module, and pass its context to the initializer of LocalCommandLineCodeExecutor. This way, the executor will run code within the new environment.

import venv
from pathlib import Path
import asyncio

from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor


async def example():
    work_dir = Path("coding")
    work_dir.mkdir(exist_ok=True)

    venv_dir = work_dir / ".venv"
    venv_builder = venv.EnvBuilder(with_pip=True)
    venv_builder.create(venv_dir)
    venv_context = venv_builder.ensure_directories(venv_dir)

    local_executor = LocalCommandLineCodeExecutor(work_dir=work_dir, virtual_env_context=venv_context)
    await local_executor.execute_code_blocks(
        code_blocks=[
            CodeBlock(language="bash", code="pip install matplotlib"),
        ],
        cancellation_token=CancellationToken(),
    )


asyncio.run(example())





FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module_name import foo`\n\n$functions'#



SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']#



classmethod _from_config(config: LocalCommandLineCodeExecutorConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → LocalCommandLineCodeExecutorConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of LocalCommandLineCodeExecutorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.local.LocalCommandLineCodeExecutor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CommandLineCodeResult[source]#
(Experimental) Execute the code blocks and return the result.

Parameters:

code_blocks (List[CodeBlock]) – The code blocks to execute.
cancellation_token (CancellationToken) – a token to cancel the operation


Returns:
CommandLineCodeResult – The result of the code execution.





format_functions_for_prompt(prompt_template: str = FUNCTION_PROMPT_TEMPLATE) → str[source]#
(Experimental) Format the functions for a prompt.
The template includes two variables:
- $module_name: The module name.
- $functions: The functions formatted as stubs with two newlines between each function.

Parameters:
prompt_template (str) – The prompt template. Default is the class default.

Returns:
str – The formatted prompt.





property functions: List[str]#



property functions_module: str#
(Experimental) The module name for the functions.



async restart() → None[source]#
(Experimental) Restart the code executor.



async start() → None[source]#
(Experimental) Start the code executor.
Initializes the local code executor and should be called before executing any code blocks.
It marks the executor internal state as started.
If no working directory is provided, the method creates a temporary directory for the executor to use.



async stop() → None[source]#
(Experimental) Stop the code executor.
Stops the local code executor and performs the cleanup of the temporary working directory (if it was created).
The executor’s internal state is markes as no longer started.



property timeout: int#
(Experimental) The timeout for code execution.



property work_dir: Path#
(Experimental) The working directory for the code execution.

**示例**:
```python
import sys
import asyncio

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

```

**示例**:
```python
import venv
from pathlib import Path
import asyncio

from autogen_core import CancellationToken
from autogen_core.code_executor import CodeBlock
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor


async def example():
    work_dir = Path("coding")
    work_dir.mkdir(exist_ok=True)

    venv_dir = work_dir / ".venv"
    venv_builder = venv.EnvBuilder(with_pip=True)
    venv_builder.create(venv_dir)
    venv_context = venv_builder.ensure_directories(venv_dir)

    local_executor = LocalCommandLineCodeExecutor(work_dir=work_dir, virtual_env_context=venv_context)
    await local_executor.execute_code_blocks(
        code_blocks=[
            CodeBlock(language="bash", code="pip install matplotlib"),
        ],
        cancellation_token=CancellationToken(),
    )


asyncio.run(example())

```

```python
FUNCTION_PROMPT_TEMPLATE: ClassVar[str] = 'You have access to the following user defined functions. They can be accessed from the module called `$module_name` by their function names.\n\nFor example, if there was a function called `foo` you could import it by writing `from $module_name import foo`\n\n$functions'#
```

```python
SUPPORTED_LANGUAGES: ClassVar[List[str]] = ['bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python']#
```

```python
classmethod _from_config(config: LocalCommandLineCodeExecutorConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → LocalCommandLineCodeExecutorConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of LocalCommandLineCodeExecutorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.code_executors.local.LocalCommandLineCodeExecutor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async execute_code_blocks(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CommandLineCodeResult[source]#
```

【中文翻译】(Experimental) Execute the code blocks and return the result.

Parameters:

code_blocks (List[CodeBlock]) – The code blocks to execute.
cancellation_token (CancellationToken) – a token to cancel the operation


Returns:
CommandLineCodeResult – The result of the code execution.

```python
format_functions_for_prompt(prompt_template: str = FUNCTION_PROMPT_TEMPLATE) → str[source]#
```

【中文翻译】(Experimental) Format the functions for a prompt.
The template includes two variables:
- $module_name: The module name.
- $functions: The functions formatted as stubs with two newlines between each function.

Parameters:
prompt_template (str) – The prompt template. Default is the class default.

Returns:
str – The formatted prompt.

```python
property functions: List[str]#
```

```python
property functions_module: str#
```

【中文翻译】(Experimental) The module name for the functions.

```python
async restart() → None[source]#
```

【中文翻译】(Experimental) Restart the code executor.

```python
async start() → None[source]#
```

【中文翻译】(Experimental) Start the code executor.
Initializes the local code executor and should be called before executing any code blocks.
It marks the executor internal state as started.
If no working directory is provided, the method creates a temporary directory for the executor to use.

```python
async stop() → None[source]#
```

【中文翻译】(Experimental) Stop the code executor.
Stops the local code executor and performs the cleanup of the temporary working directory (if it was created).
The executor’s internal state is markes as no longer started.

```python
property timeout: int#
```

【中文翻译】(Experimental) The timeout for code execution.

```python
property work_dir: Path#
```

【中文翻译】(Experimental) The working directory for the code execution.

【中文翻译】previous

【中文翻译】autogen_ext.tools.semantic_kernel

【中文翻译】next

【中文翻译】autogen_ext.code_executors.docker

### autogen_ext.experimental.task_centric_memory {autogen_extexperimentaltask_centric_memory}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html)

```python
class MemoryBankConfig[source]#
```

【中文翻译】Bases: TypedDict


distance_threshold: int#



n_results: int#



path: str#



relevance_conversion_threshold: float#

```python
distance_threshold: int#
```

```python
n_results: int#
```

```python
path: str#
```

```python
relevance_conversion_threshold: float#
```

```python
class MemoryController(reset: bool, client: ChatCompletionClient, task_assignment_callback: Callable[[str], Awaitable[Tuple[str, str]]] | None = None, config: MemoryControllerConfig | None = None, logger: PageLogger | None = None)[source]#
```

【中文翻译】Bases: object
(EXPERIMENTAL, RESEARCH IN PROGRESS)
Implements fast, memory-based learning, and manages the flow of information to and from a memory bank.

Parameters:

reset – True to empty the memory bank before starting.
client – The model client to use internally.
task_assignment_callback – An optional callback used to assign a task to any agent managed by the caller.
config – An optional dict that can be used to override the following values:

generalize_task: Whether to rewrite tasks in more general terms.
revise_generalized_task: Whether to critique then rewrite the generalized task.
generate_topics: Whether to base retrieval directly on tasks, or on topics extracted from tasks.
validate_memos: Whether to apply a final validation stage to retrieved memos.
max_memos_to_retrieve: The maximum number of memos to return from retrieve_relevant_memos().
max_train_trials: The maximum number of learning iterations to attempt when training on a task.
max_test_trials: The total number of attempts made when testing for failure on a task.
MemoryBank: A config dict passed to MemoryBank.


logger – An optional logger. If None, a default logger will be created.



Example
The task-centric-memory extra first needs to be installed:
pip install "autogen-ext[task-centric-memory]"


The following code snippet shows how to use this class for the most basic storage and retrieval of memories.:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.experimental.task_centric_memory import MemoryController
from autogen_ext.experimental.task_centric_memory.utils import PageLogger


async def main() -> None:
    client = OpenAIChatCompletionClient(model="gpt-4o")
    logger = PageLogger(config={"level": "DEBUG", "path": "./pagelogs/quickstart"})  # Optional, but very useful.
    memory_controller = MemoryController(reset=True, client=client, logger=logger)

    # Add a few task-insight pairs as memories, where an insight can be any string that may help solve the task.
    await memory_controller.add_memo(task="What color do I like?", insight="Deep blue is my favorite color")
    await memory_controller.add_memo(task="What's another color I like?", insight="I really like cyan")
    await memory_controller.add_memo(task="What's my favorite food?", insight="Halibut is my favorite")

    # Retrieve memories for a new task that's related to only two of the stored memories.
    memos = await memory_controller.retrieve_relevant_memos(task="What colors do I like most?")
    print("{} memories retrieved".format(len(memos)))
    for memo in memos:
        print("- " + memo.insight)


asyncio.run(main())




async add_memo(insight: str, task: None | str = None, index_on_both: bool = True) → None[source]#
Adds one insight to the memory bank, using the task (if provided) as context.



async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]#
Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
This is useful when the task-solution pair is an exemplar of solving a task related to some other task.



async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]#
Assigns a task to some agent through the task_assignment_callback, along with any relevant memories.



async consider_memo_storage(text: str) → str | None[source]#
Tries to extract any advice from the given text and add it to memory.



async handle_user_message(text: str, should_await: bool = True) → str[source]#
Handles a user message by extracting any advice as an insight to be stored in memory, and then calling assign_task().



reset_memory() → None[source]#
Empties the memory bank in RAM and on disk.



async retrieve_relevant_memos(task: str) → List[Memo][source]#
Retrieves any memos from memory that seem relevant to the task.



async test_on_task(task: str, expected_answer: str, num_trials: int = 1) → Tuple[str, int, int][source]#
Assigns a task to the agent, along with any relevant memos retrieved from memory.



async train_on_task(task: str, expected_answer: str) → None[source]#
Repeatedly assigns a task to the agent, and tries to learn from failures by creating useful insights as memories.

**示例**:
```python
pip install "autogen-ext[task-centric-memory]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.experimental.task_centric_memory import MemoryController
from autogen_ext.experimental.task_centric_memory.utils import PageLogger


async def main() -> None:
    client = OpenAIChatCompletionClient(model="gpt-4o")
    logger = PageLogger(config={"level": "DEBUG", "path": "./pagelogs/quickstart"})  # Optional, but very useful.
    memory_controller = MemoryController(reset=True, client=client, logger=logger)

    # Add a few task-insight pairs as memories, where an insight can be any string that may help solve the task.
    await memory_controller.add_memo(task="What color do I like?", insight="Deep blue is my favorite color")
    await memory_controller.add_memo(task="What's another color I like?", insight="I really like cyan")
    await memory_controller.add_memo(task="What's my favorite food?", insight="Halibut is my favorite")

    # Retrieve memories for a new task that's related to only two of the stored memories.
    memos = await memory_controller.retrieve_relevant_memos(task="What colors do I like most?")
    print("{} memories retrieved".format(len(memos)))
    for memo in memos:
        print("- " + memo.insight)


asyncio.run(main())

```

```python
async add_memo(insight: str, task: None | str = None, index_on_both: bool = True) → None[source]#
```

【中文翻译】Adds one insight to the memory bank, using the task (if provided) as context.

```python
async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]#
```

【中文翻译】Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
This is useful when the task-solution pair is an exemplar of solving a task related to some other task.

```python
async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]#
```

【中文翻译】Assigns a task to some agent through the task_assignment_callback, along with any relevant memories.

```python
async consider_memo_storage(text: str) → str | None[source]#
```

【中文翻译】Tries to extract any advice from the given text and add it to memory.

```python
async handle_user_message(text: str, should_await: bool = True) → str[source]#
```

【中文翻译】Handles a user message by extracting any advice as an insight to be stored in memory, and then calling assign_task().

```python
reset_memory() → None[source]#
```

【中文翻译】Empties the memory bank in RAM and on disk.

```python
async retrieve_relevant_memos(task: str) → List[Memo][source]#
```

【中文翻译】Retrieves any memos from memory that seem relevant to the task.

```python
async test_on_task(task: str, expected_answer: str, num_trials: int = 1) → Tuple[str, int, int][source]#
```

【中文翻译】Assigns a task to the agent, along with any relevant memos retrieved from memory.

```python
async train_on_task(task: str, expected_answer: str) → None[source]#
```

【中文翻译】Repeatedly assigns a task to the agent, and tries to learn from failures by creating useful insights as memories.

```python
class MemoryControllerConfig[source]#
```

【中文翻译】Bases: TypedDict


MemoryBank: MemoryBankConfig#



generalize_task: bool#



generate_topics: bool#



max_memos_to_retrieve: int#



max_test_trials: int#



max_train_trials: int#



revise_generalized_task: bool#



validate_memos: bool#

```python
MemoryBank: MemoryBankConfig#
```

```python
generalize_task: bool#
```

```python
generate_topics: bool#
```

```python
max_memos_to_retrieve: int#
```

```python
max_test_trials: int#
```

```python
max_train_trials: int#
```

```python
revise_generalized_task: bool#
```

```python
validate_memos: bool#
```

【中文翻译】previous

【中文翻译】autogen_ext.auth.azure

【中文翻译】next

【中文翻译】autogen_ext.experimental.task_centric_memory.utils

### autogen_ext.experimental.task_centric_memory {autogen_extexperimentaltask_centric_memory}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.html)

```python
class MemoryBankConfig[source]#
```

【中文翻译】Bases: TypedDict


distance_threshold: int#



n_results: int#



path: str#



relevance_conversion_threshold: float#

```python
distance_threshold: int#
```

```python
n_results: int#
```

```python
path: str#
```

```python
relevance_conversion_threshold: float#
```

```python
class MemoryController(reset: bool, client: ChatCompletionClient, task_assignment_callback: Callable[[str], Awaitable[Tuple[str, str]]] | None = None, config: MemoryControllerConfig | None = None, logger: PageLogger | None = None)[source]#
```

【中文翻译】Bases: object
(EXPERIMENTAL, RESEARCH IN PROGRESS)
Implements fast, memory-based learning, and manages the flow of information to and from a memory bank.

Parameters:

reset – True to empty the memory bank before starting.
client – The model client to use internally.
task_assignment_callback – An optional callback used to assign a task to any agent managed by the caller.
config – An optional dict that can be used to override the following values:

generalize_task: Whether to rewrite tasks in more general terms.
revise_generalized_task: Whether to critique then rewrite the generalized task.
generate_topics: Whether to base retrieval directly on tasks, or on topics extracted from tasks.
validate_memos: Whether to apply a final validation stage to retrieved memos.
max_memos_to_retrieve: The maximum number of memos to return from retrieve_relevant_memos().
max_train_trials: The maximum number of learning iterations to attempt when training on a task.
max_test_trials: The total number of attempts made when testing for failure on a task.
MemoryBank: A config dict passed to MemoryBank.


logger – An optional logger. If None, a default logger will be created.



Example
The task-centric-memory extra first needs to be installed:
pip install "autogen-ext[task-centric-memory]"


The following code snippet shows how to use this class for the most basic storage and retrieval of memories.:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.experimental.task_centric_memory import MemoryController
from autogen_ext.experimental.task_centric_memory.utils import PageLogger


async def main() -> None:
    client = OpenAIChatCompletionClient(model="gpt-4o")
    logger = PageLogger(config={"level": "DEBUG", "path": "./pagelogs/quickstart"})  # Optional, but very useful.
    memory_controller = MemoryController(reset=True, client=client, logger=logger)

    # Add a few task-insight pairs as memories, where an insight can be any string that may help solve the task.
    await memory_controller.add_memo(task="What color do I like?", insight="Deep blue is my favorite color")
    await memory_controller.add_memo(task="What's another color I like?", insight="I really like cyan")
    await memory_controller.add_memo(task="What's my favorite food?", insight="Halibut is my favorite")

    # Retrieve memories for a new task that's related to only two of the stored memories.
    memos = await memory_controller.retrieve_relevant_memos(task="What colors do I like most?")
    print("{} memories retrieved".format(len(memos)))
    for memo in memos:
        print("- " + memo.insight)


asyncio.run(main())




async add_memo(insight: str, task: None | str = None, index_on_both: bool = True) → None[source]#
Adds one insight to the memory bank, using the task (if provided) as context.



async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]#
Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
This is useful when the task-solution pair is an exemplar of solving a task related to some other task.



async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]#
Assigns a task to some agent through the task_assignment_callback, along with any relevant memories.



async consider_memo_storage(text: str) → str | None[source]#
Tries to extract any advice from the given text and add it to memory.



async handle_user_message(text: str, should_await: bool = True) → str[source]#
Handles a user message by extracting any advice as an insight to be stored in memory, and then calling assign_task().



reset_memory() → None[source]#
Empties the memory bank in RAM and on disk.



async retrieve_relevant_memos(task: str) → List[Memo][source]#
Retrieves any memos from memory that seem relevant to the task.



async test_on_task(task: str, expected_answer: str, num_trials: int = 1) → Tuple[str, int, int][source]#
Assigns a task to the agent, along with any relevant memos retrieved from memory.



async train_on_task(task: str, expected_answer: str) → None[source]#
Repeatedly assigns a task to the agent, and tries to learn from failures by creating useful insights as memories.

**示例**:
```python
pip install "autogen-ext[task-centric-memory]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.experimental.task_centric_memory import MemoryController
from autogen_ext.experimental.task_centric_memory.utils import PageLogger


async def main() -> None:
    client = OpenAIChatCompletionClient(model="gpt-4o")
    logger = PageLogger(config={"level": "DEBUG", "path": "./pagelogs/quickstart"})  # Optional, but very useful.
    memory_controller = MemoryController(reset=True, client=client, logger=logger)

    # Add a few task-insight pairs as memories, where an insight can be any string that may help solve the task.
    await memory_controller.add_memo(task="What color do I like?", insight="Deep blue is my favorite color")
    await memory_controller.add_memo(task="What's another color I like?", insight="I really like cyan")
    await memory_controller.add_memo(task="What's my favorite food?", insight="Halibut is my favorite")

    # Retrieve memories for a new task that's related to only two of the stored memories.
    memos = await memory_controller.retrieve_relevant_memos(task="What colors do I like most?")
    print("{} memories retrieved".format(len(memos)))
    for memo in memos:
        print("- " + memo.insight)


asyncio.run(main())

```

```python
async add_memo(insight: str, task: None | str = None, index_on_both: bool = True) → None[source]#
```

【中文翻译】Adds one insight to the memory bank, using the task (if provided) as context.

```python
async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]#
```

【中文翻译】Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
This is useful when the task-solution pair is an exemplar of solving a task related to some other task.

```python
async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]#
```

【中文翻译】Assigns a task to some agent through the task_assignment_callback, along with any relevant memories.

```python
async consider_memo_storage(text: str) → str | None[source]#
```

【中文翻译】Tries to extract any advice from the given text and add it to memory.

```python
async handle_user_message(text: str, should_await: bool = True) → str[source]#
```

【中文翻译】Handles a user message by extracting any advice as an insight to be stored in memory, and then calling assign_task().

```python
reset_memory() → None[source]#
```

【中文翻译】Empties the memory bank in RAM and on disk.

```python
async retrieve_relevant_memos(task: str) → List[Memo][source]#
```

【中文翻译】Retrieves any memos from memory that seem relevant to the task.

```python
async test_on_task(task: str, expected_answer: str, num_trials: int = 1) → Tuple[str, int, int][source]#
```

【中文翻译】Assigns a task to the agent, along with any relevant memos retrieved from memory.

```python
async train_on_task(task: str, expected_answer: str) → None[source]#
```

【中文翻译】Repeatedly assigns a task to the agent, and tries to learn from failures by creating useful insights as memories.

```python
class MemoryControllerConfig[source]#
```

【中文翻译】Bases: TypedDict


MemoryBank: MemoryBankConfig#



generalize_task: bool#



generate_topics: bool#



max_memos_to_retrieve: int#



max_test_trials: int#



max_train_trials: int#



revise_generalized_task: bool#



validate_memos: bool#

```python
MemoryBank: MemoryBankConfig#
```

```python
generalize_task: bool#
```

```python
generate_topics: bool#
```

```python
max_memos_to_retrieve: int#
```

```python
max_test_trials: int#
```

```python
max_train_trials: int#
```

```python
revise_generalized_task: bool#
```

```python
validate_memos: bool#
```

【中文翻译】previous

【中文翻译】autogen_ext.auth.azure

【中文翻译】next

【中文翻译】autogen_ext.experimental.task_centric_memory.utils

### autogen_ext.experimental.task_centric_memory.utils {autogen_extexperimentaltask_centric_memoryutils}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html)

```python
class Apprentice(client: ChatCompletionClient, config: ApprenticeConfig | None = None, logger: PageLogger | None = None)[source]#
```

【中文翻译】Bases: object
A minimal wrapper combining task-centric memory with an agent or team.
Applications may use the Apprentice class, or they may directly instantiate
and call the Memory Controller using this class as an example.

Parameters:

client – The client to call the model.
config – An optional dict that can be used to override the following values:

name_of_agent_or_team: The name of the target agent or team for assigning tasks to.
disable_prefix_caching: True to disable prefix caching by prepending random ints to the first message.
MemoryController: A config dict passed to MemoryController.


logger – An optional logger. If None, a default logger will be created.





async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]#
Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
This is useful when the insight is a demonstration of how to solve a given type of task.



async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]#
Assigns a task to the agent, along with any relevant insights/memories.



async assign_task_to_agent_or_team(task: str) → Tuple[str, str][source]#
Passes the given task to the target agent or team.



async handle_user_message(text: str, should_await: bool = True) → str[source]#
Handles a user message, extracting any advice and assigning a task to the agent.



reset_memory() → None[source]#
Resets the memory bank.



async train_on_task(task: str, expected_answer: str) → None[source]#
Repeatedly assigns a task to the completion agent, and tries to learn from failures by creating useful insights as memories.

```python
async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]#
```

【中文翻译】Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
This is useful when the insight is a demonstration of how to solve a given type of task.

```python
async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]#
```

【中文翻译】Assigns a task to the agent, along with any relevant insights/memories.

```python
async assign_task_to_agent_or_team(task: str) → Tuple[str, str][source]#
```

【中文翻译】Passes the given task to the target agent or team.

```python
async handle_user_message(text: str, should_await: bool = True) → str[source]#
```

【中文翻译】Handles a user message, extracting any advice and assigning a task to the agent.

```python
reset_memory() → None[source]#
```

【中文翻译】Resets the memory bank.

```python
async train_on_task(task: str, expected_answer: str) → None[source]#
```

【中文翻译】Repeatedly assigns a task to the completion agent, and tries to learn from failures by creating useful insights as memories.

```python
class ApprenticeConfig[source]#
```

【中文翻译】Bases: TypedDict


MemoryController: MemoryControllerConfig#



disable_prefix_caching: bool#



name_of_agent_or_team: str#

```python
MemoryController: MemoryControllerConfig#
```

```python
disable_prefix_caching: bool#
```

```python
name_of_agent_or_team: str#
```

```python
class ChatCompletionClientRecorder(client: ChatCompletionClient, mode: Literal['record', 'replay'], session_file_path: str, logger: PageLogger | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient
A chat completion client that supports fast, large-scale tests of code calling LLM clients.
Two modes are supported:


“record”: delegates to the underlying client while also recording the input messages and responses,
which are saved to disk when finalize() is called.
“replay”: loads previously recorded message and responses from disk, then on each call
checks that its message matches the recorded message, and returns the recorded response.


The recorded data is stored as a JSON list of records. Each record is a dictionary with a “mode”
field (either “create” or “create_stream”), a serialized list of messages, and either a “response” (for
create calls) or a “stream” (a list of streamed outputs for create_stream calls).
ReplayChatCompletionClient and ChatCompletionCache do similar things, but with significant differences:


ReplayChatCompletionClient replays pre-defined responses in a specified order without recording anything or checking the messages sent to the client.
ChatCompletionCache caches responses and replays them for messages that have been seen before, regardless of order, and calls the base client for any uncached messages.




actual_usage() → RequestUsage[source]#



property capabilities: ModelCapabilities#



async close() → None[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.





finalize() → None[source]#
In record mode, saves the accumulated records to disk.
In replay mode, makes sure all the records were checked.



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



total_usage() → RequestUsage[source]#

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelCapabilities#
```

```python
async close() → None[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.

```python
finalize() → None[source]#
```

【中文翻译】In record mode, saves the accumulated records to disk.
In replay mode, makes sure all the records were checked.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

```python
class Grader(client: ChatCompletionClient, logger: PageLogger | None = None)[source]#
```

【中文翻译】Bases: object
Runs basic tests, and determines task success without limitation to string matches.

Parameters:

client – The client to call the model.
logger – An optional logger. If None, no logging will be performed.





async call_model(summary: str, user_content: str | List[str | Image], system_message_content: str | None = None, keep_these_messages: bool = True) → str[source]#
Calls the model client with the given input and returns the response.



async is_response_correct(task_description: str, response_to_be_graded: str, correct_answer: str) → Tuple[bool, str][source]#
Determines whether the response is equivalent to the task’s correct answer.



async test_apprentice(apprentice: Apprentice, task_description: str, expected_answer: str, num_trials: int, use_memory: bool, client: ChatCompletionClient) → Tuple[int, int][source]#

```python
async call_model(summary: str, user_content: str | List[str | Image], system_message_content: str | None = None, keep_these_messages: bool = True) → str[source]#
```

【中文翻译】Calls the model client with the given input and returns the response.

```python
async is_response_correct(task_description: str, response_to_be_graded: str, correct_answer: str) → Tuple[bool, str][source]#
```

【中文翻译】Determines whether the response is equivalent to the task’s correct answer.

```python
async test_apprentice(apprentice: Apprentice, task_description: str, expected_answer: str, num_trials: int, use_memory: bool, client: ChatCompletionClient) → Tuple[int, int][source]#
```

```python
class PageLogger(config: PageLoggerConfig | None = None)[source]#
```

【中文翻译】Bases: object
Logs text and images to a set of HTML pages, one per function/method, linked to each other in a call tree.

Parameters:
config – An optional dict that can be used to override the following values:

level: The logging level, one of DEBUG, INFO, WARNING, ERROR, CRITICAL, or NONE.
path: The path to the directory where the log files will be written.






add_link_to_image(description: str, source_image_path: str) → None[source]#
Inserts a thumbnail link to an image to the page.



critical(line: str) → None[source]#
Adds CRITICAL text to the current page if debugging level <= CRITICAL.



debug(line: str) → None[source]#
Adds DEBUG text to the current page if debugging level <= DEBUG.



enter_function() → Page | None[source]#
Adds a new page corresponding to the current function call.



error(line: str) → None[source]#
Adds ERROR text to the current page if debugging level <= ERROR.



finalize() → None[source]#



flush(finished: bool = False) → None[source]#
Writes the current state of the log to disk.



info(line: str) → None[source]#
Adds INFO text to the current page if debugging level <= INFO.



leave_function() → None[source]#
Finishes the page corresponding to the current function call.



log_dict_list(content: List[Mapping[str, Any]], summary: str) → None[source]#
Adds a page containing a list of dicts.



log_link_to_local_file(file_path: str) → str[source]#
Returns a link to a local file in the log.



log_message_content(message_content: str | List[str | Image] | List[FunctionCall] | List[FunctionExecutionResult], summary: str) → None[source]#
Adds a page containing the message’s content, including any images.



log_model_call(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], response: CreateResult) → Page | None[source]#
Logs messages sent to a model and the TaskResult response to a new page.



log_model_task(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], task_result: TaskResult) → Page | None[source]#
Logs messages sent to a model and the TaskResult response to a new page.



warning(line: str) → None[source]#
Adds WARNING text to the current page if debugging level <= WARNING.

```python
add_link_to_image(description: str, source_image_path: str) → None[source]#
```

【中文翻译】Inserts a thumbnail link to an image to the page.

```python
critical(line: str) → None[source]#
```

【中文翻译】Adds CRITICAL text to the current page if debugging level <= CRITICAL.

```python
debug(line: str) → None[source]#
```

【中文翻译】Adds DEBUG text to the current page if debugging level <= DEBUG.

```python
enter_function() → Page | None[source]#
```

【中文翻译】Adds a new page corresponding to the current function call.

```python
error(line: str) → None[source]#
```

【中文翻译】Adds ERROR text to the current page if debugging level <= ERROR.

```python
finalize() → None[source]#
```

```python
flush(finished: bool = False) → None[source]#
```

【中文翻译】Writes the current state of the log to disk.

```python
info(line: str) → None[source]#
```

【中文翻译】Adds INFO text to the current page if debugging level <= INFO.

```python
leave_function() → None[source]#
```

【中文翻译】Finishes the page corresponding to the current function call.

```python
log_dict_list(content: List[Mapping[str, Any]], summary: str) → None[source]#
```

【中文翻译】Adds a page containing a list of dicts.

```python
log_link_to_local_file(file_path: str) → str[source]#
```

【中文翻译】Returns a link to a local file in the log.

```python
log_message_content(message_content: str | List[str | Image] | List[FunctionCall] | List[FunctionExecutionResult], summary: str) → None[source]#
```

【中文翻译】Adds a page containing the message’s content, including any images.

```python
log_model_call(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], response: CreateResult) → Page | None[source]#
```

【中文翻译】Logs messages sent to a model and the TaskResult response to a new page.

```python
log_model_task(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], task_result: TaskResult) → Page | None[source]#
```

【中文翻译】Logs messages sent to a model and the TaskResult response to a new page.

```python
warning(line: str) → None[source]#
```

【中文翻译】Adds WARNING text to the current page if debugging level <= WARNING.

```python
class PageLoggerConfig[source]#
```

【中文翻译】Bases: TypedDict


level: str#



path: str#

```python
level: str#
```

```python
path: str#
```

```python
class Teachability(memory_controller: MemoryController, name: str | None = None)[source]#
```

【中文翻译】Bases: Memory
Gives an AssistantAgent the ability to learn quickly from user teachings, hints, and advice.
Steps for usage:


Instantiate MemoryController.
Instantiate Teachability, passing the memory controller as a parameter.
Instantiate an AssistantAgent, passing the teachability instance (wrapped in a list) as the memory parameter.
Use the AssistantAgent as usual, such as for chatting with the user.




async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
Tries to extract any advice from the passed content and add it to memory.



async clear() → None[source]#
Clear all entries from memory.



async close() → None[source]#
Clean up memory resources.



property name: str#
Get the memory instance identifier.



async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
Returns any memories that seem relevant to the query.



async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
Extracts any advice from the last user turn to be stored in memory,
and adds any relevant memories to the model context.

```python
async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Tries to extract any advice from the passed content and add it to memory.

```python
async clear() → None[source]#
```

【中文翻译】Clear all entries from memory.

```python
async close() → None[source]#
```

【中文翻译】Clean up memory resources.

```python
property name: str#
```

【中文翻译】Get the memory instance identifier.

```python
async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
```

【中文翻译】Returns any memories that seem relevant to the query.

```python
async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
```

【中文翻译】Extracts any advice from the last user turn to be stored in memory,
and adds any relevant memories to the model context.

【中文翻译】previous

【中文翻译】autogen_ext.experimental.task_centric_memory

### autogen_ext.experimental.task_centric_memory.utils {autogen_extexperimentaltask_centric_memoryutils}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.experimental.task_centric_memory.utils.html)

```python
class Apprentice(client: ChatCompletionClient, config: ApprenticeConfig | None = None, logger: PageLogger | None = None)[source]#
```

【中文翻译】Bases: object
A minimal wrapper combining task-centric memory with an agent or team.
Applications may use the Apprentice class, or they may directly instantiate
and call the Memory Controller using this class as an example.

Parameters:

client – The client to call the model.
config – An optional dict that can be used to override the following values:

name_of_agent_or_team: The name of the target agent or team for assigning tasks to.
disable_prefix_caching: True to disable prefix caching by prepending random ints to the first message.
MemoryController: A config dict passed to MemoryController.


logger – An optional logger. If None, a default logger will be created.





async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]#
Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
This is useful when the insight is a demonstration of how to solve a given type of task.



async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]#
Assigns a task to the agent, along with any relevant insights/memories.



async assign_task_to_agent_or_team(task: str) → Tuple[str, str][source]#
Passes the given task to the target agent or team.



async handle_user_message(text: str, should_await: bool = True) → str[source]#
Handles a user message, extracting any advice and assigning a task to the agent.



reset_memory() → None[source]#
Resets the memory bank.



async train_on_task(task: str, expected_answer: str) → None[source]#
Repeatedly assigns a task to the completion agent, and tries to learn from failures by creating useful insights as memories.

```python
async add_task_solution_pair_to_memory(task: str, solution: str) → None[source]#
```

【中文翻译】Adds a task-solution pair to the memory bank, to be retrieved together later as a combined insight.
This is useful when the insight is a demonstration of how to solve a given type of task.

```python
async assign_task(task: str, use_memory: bool = True, should_await: bool = True) → str[source]#
```

【中文翻译】Assigns a task to the agent, along with any relevant insights/memories.

```python
async assign_task_to_agent_or_team(task: str) → Tuple[str, str][source]#
```

【中文翻译】Passes the given task to the target agent or team.

```python
async handle_user_message(text: str, should_await: bool = True) → str[source]#
```

【中文翻译】Handles a user message, extracting any advice and assigning a task to the agent.

```python
reset_memory() → None[source]#
```

【中文翻译】Resets the memory bank.

```python
async train_on_task(task: str, expected_answer: str) → None[source]#
```

【中文翻译】Repeatedly assigns a task to the completion agent, and tries to learn from failures by creating useful insights as memories.

```python
class ApprenticeConfig[source]#
```

【中文翻译】Bases: TypedDict


MemoryController: MemoryControllerConfig#



disable_prefix_caching: bool#



name_of_agent_or_team: str#

```python
MemoryController: MemoryControllerConfig#
```

```python
disable_prefix_caching: bool#
```

```python
name_of_agent_or_team: str#
```

```python
class ChatCompletionClientRecorder(client: ChatCompletionClient, mode: Literal['record', 'replay'], session_file_path: str, logger: PageLogger | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient
A chat completion client that supports fast, large-scale tests of code calling LLM clients.
Two modes are supported:


“record”: delegates to the underlying client while also recording the input messages and responses,
which are saved to disk when finalize() is called.
“replay”: loads previously recorded message and responses from disk, then on each call
checks that its message matches the recorded message, and returns the recorded response.


The recorded data is stored as a JSON list of records. Each record is a dictionary with a “mode”
field (either “create” or “create_stream”), a serialized list of messages, and either a “response” (for
create calls) or a “stream” (a list of streamed outputs for create_stream calls).
ReplayChatCompletionClient and ChatCompletionCache do similar things, but with significant differences:


ReplayChatCompletionClient replays pre-defined responses in a specified order without recording anything or checking the messages sent to the client.
ChatCompletionCache caches responses and replays them for messages that have been seen before, regardless of order, and calls the base client for any uncached messages.




actual_usage() → RequestUsage[source]#



property capabilities: ModelCapabilities#



async close() → None[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.





finalize() → None[source]#
In record mode, saves the accumulated records to disk.
In replay mode, makes sure all the records were checked.



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



total_usage() → RequestUsage[source]#

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelCapabilities#
```

```python
async close() → None[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.

```python
finalize() → None[source]#
```

【中文翻译】In record mode, saves the accumulated records to disk.
In replay mode, makes sure all the records were checked.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

```python
class Grader(client: ChatCompletionClient, logger: PageLogger | None = None)[source]#
```

【中文翻译】Bases: object
Runs basic tests, and determines task success without limitation to string matches.

Parameters:

client – The client to call the model.
logger – An optional logger. If None, no logging will be performed.





async call_model(summary: str, user_content: str | List[str | Image], system_message_content: str | None = None, keep_these_messages: bool = True) → str[source]#
Calls the model client with the given input and returns the response.



async is_response_correct(task_description: str, response_to_be_graded: str, correct_answer: str) → Tuple[bool, str][source]#
Determines whether the response is equivalent to the task’s correct answer.



async test_apprentice(apprentice: Apprentice, task_description: str, expected_answer: str, num_trials: int, use_memory: bool, client: ChatCompletionClient) → Tuple[int, int][source]#

```python
async call_model(summary: str, user_content: str | List[str | Image], system_message_content: str | None = None, keep_these_messages: bool = True) → str[source]#
```

【中文翻译】Calls the model client with the given input and returns the response.

```python
async is_response_correct(task_description: str, response_to_be_graded: str, correct_answer: str) → Tuple[bool, str][source]#
```

【中文翻译】Determines whether the response is equivalent to the task’s correct answer.

```python
async test_apprentice(apprentice: Apprentice, task_description: str, expected_answer: str, num_trials: int, use_memory: bool, client: ChatCompletionClient) → Tuple[int, int][source]#
```

```python
class PageLogger(config: PageLoggerConfig | None = None)[source]#
```

【中文翻译】Bases: object
Logs text and images to a set of HTML pages, one per function/method, linked to each other in a call tree.

Parameters:
config – An optional dict that can be used to override the following values:

level: The logging level, one of DEBUG, INFO, WARNING, ERROR, CRITICAL, or NONE.
path: The path to the directory where the log files will be written.






add_link_to_image(description: str, source_image_path: str) → None[source]#
Inserts a thumbnail link to an image to the page.



critical(line: str) → None[source]#
Adds CRITICAL text to the current page if debugging level <= CRITICAL.



debug(line: str) → None[source]#
Adds DEBUG text to the current page if debugging level <= DEBUG.



enter_function() → Page | None[source]#
Adds a new page corresponding to the current function call.



error(line: str) → None[source]#
Adds ERROR text to the current page if debugging level <= ERROR.



finalize() → None[source]#



flush(finished: bool = False) → None[source]#
Writes the current state of the log to disk.



info(line: str) → None[source]#
Adds INFO text to the current page if debugging level <= INFO.



leave_function() → None[source]#
Finishes the page corresponding to the current function call.



log_dict_list(content: List[Mapping[str, Any]], summary: str) → None[source]#
Adds a page containing a list of dicts.



log_link_to_local_file(file_path: str) → str[source]#
Returns a link to a local file in the log.



log_message_content(message_content: str | List[str | Image] | List[FunctionCall] | List[FunctionExecutionResult], summary: str) → None[source]#
Adds a page containing the message’s content, including any images.



log_model_call(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], response: CreateResult) → Page | None[source]#
Logs messages sent to a model and the TaskResult response to a new page.



log_model_task(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], task_result: TaskResult) → Page | None[source]#
Logs messages sent to a model and the TaskResult response to a new page.



warning(line: str) → None[source]#
Adds WARNING text to the current page if debugging level <= WARNING.

```python
add_link_to_image(description: str, source_image_path: str) → None[source]#
```

【中文翻译】Inserts a thumbnail link to an image to the page.

```python
critical(line: str) → None[source]#
```

【中文翻译】Adds CRITICAL text to the current page if debugging level <= CRITICAL.

```python
debug(line: str) → None[source]#
```

【中文翻译】Adds DEBUG text to the current page if debugging level <= DEBUG.

```python
enter_function() → Page | None[source]#
```

【中文翻译】Adds a new page corresponding to the current function call.

```python
error(line: str) → None[source]#
```

【中文翻译】Adds ERROR text to the current page if debugging level <= ERROR.

```python
finalize() → None[source]#
```

```python
flush(finished: bool = False) → None[source]#
```

【中文翻译】Writes the current state of the log to disk.

```python
info(line: str) → None[source]#
```

【中文翻译】Adds INFO text to the current page if debugging level <= INFO.

```python
leave_function() → None[source]#
```

【中文翻译】Finishes the page corresponding to the current function call.

```python
log_dict_list(content: List[Mapping[str, Any]], summary: str) → None[source]#
```

【中文翻译】Adds a page containing a list of dicts.

```python
log_link_to_local_file(file_path: str) → str[source]#
```

【中文翻译】Returns a link to a local file in the log.

```python
log_message_content(message_content: str | List[str | Image] | List[FunctionCall] | List[FunctionExecutionResult], summary: str) → None[source]#
```

【中文翻译】Adds a page containing the message’s content, including any images.

```python
log_model_call(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], response: CreateResult) → Page | None[source]#
```

【中文翻译】Logs messages sent to a model and the TaskResult response to a new page.

```python
log_model_task(summary: str, input_messages: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], task_result: TaskResult) → Page | None[source]#
```

【中文翻译】Logs messages sent to a model and the TaskResult response to a new page.

```python
warning(line: str) → None[source]#
```

【中文翻译】Adds WARNING text to the current page if debugging level <= WARNING.

```python
class PageLoggerConfig[source]#
```

【中文翻译】Bases: TypedDict


level: str#



path: str#

```python
level: str#
```

```python
path: str#
```

```python
class Teachability(memory_controller: MemoryController, name: str | None = None)[source]#
```

【中文翻译】Bases: Memory
Gives an AssistantAgent the ability to learn quickly from user teachings, hints, and advice.
Steps for usage:


Instantiate MemoryController.
Instantiate Teachability, passing the memory controller as a parameter.
Instantiate an AssistantAgent, passing the teachability instance (wrapped in a list) as the memory parameter.
Use the AssistantAgent as usual, such as for chatting with the user.




async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
Tries to extract any advice from the passed content and add it to memory.



async clear() → None[source]#
Clear all entries from memory.



async close() → None[source]#
Clean up memory resources.



property name: str#
Get the memory instance identifier.



async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
Returns any memories that seem relevant to the query.



async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
Extracts any advice from the last user turn to be stored in memory,
and adds any relevant memories to the model context.

```python
async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Tries to extract any advice from the passed content and add it to memory.

```python
async clear() → None[source]#
```

【中文翻译】Clear all entries from memory.

```python
async close() → None[source]#
```

【中文翻译】Clean up memory resources.

```python
property name: str#
```

【中文翻译】Get the memory instance identifier.

```python
async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
```

【中文翻译】Returns any memories that seem relevant to the query.

```python
async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
```

【中文翻译】Extracts any advice from the last user turn to be stored in memory,
and adds any relevant memories to the model context.

【中文翻译】previous

【中文翻译】autogen_ext.experimental.task_centric_memory

### autogen_ext.memory.canvas {autogen_extmemorycanvas}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html)

```python
class TextCanvas[source]#
```

【中文翻译】Bases: BaseCanvas
An in‑memory canvas that stores text files with full revision history.

Warning
This is an experimental API and may change in the future.

Besides the original CRUD‑like operations, this enhanced implementation adds:

apply_patch – applies patches using the unidiff library for accurate
hunk application and context line validation.
get_revision_content – random access to any historical revision.
get_revision_diffs – obtain the list of diffs applied between every
consecutive pair of revisions so that a caller can replay or audit the
full change history.



add_or_update_file(filename: str, new_content: str | bytes | Any) → None[source]#
Create filename or append a new revision containing new_content.



apply_patch(filename: str, patch_data: str | bytes | Any) → None[source]#
Apply patch_text (unified diff) to the latest revision and save a new revision.
Uses the unidiff library to accurately apply hunks and validate context lines.



get_all_contents_for_context() → str[source]#
Return a summarised view of every file and its latest revision.



get_diff(filename: str, from_revision: int, to_revision: int) → str[source]#
Return a unified diff between from_revision and to_revision.



get_latest_content(filename: str) → str[source]#
Return the most recent content or an empty string if the file is new.



get_revision_content(filename: str, revision: int) → str[source]#
Return the exact content stored in revision.
If the revision does not exist an empty string is returned so that
downstream code can handle the “not found” case without exceptions.



get_revision_diffs(filename: str) → List[str][source]#
Return a chronological list of unified‑diffs for filename.
Each element in the returned list represents the diff that transformed
revision n into revision n+1 (starting at revision 1 → 2).



list_files() → Dict[str, int][source]#
Return a mapping of filename → latest revision number.

```python
add_or_update_file(filename: str, new_content: str | bytes | Any) → None[source]#
```

【中文翻译】Create filename or append a new revision containing new_content.

```python
apply_patch(filename: str, patch_data: str | bytes | Any) → None[source]#
```

【中文翻译】Apply patch_text (unified diff) to the latest revision and save a new revision.
Uses the unidiff library to accurately apply hunks and validate context lines.

```python
get_all_contents_for_context() → str[source]#
```

【中文翻译】Return a summarised view of every file and its latest revision.

```python
get_diff(filename: str, from_revision: int, to_revision: int) → str[source]#
```

【中文翻译】Return a unified diff between from_revision and to_revision.

```python
get_latest_content(filename: str) → str[source]#
```

【中文翻译】Return the most recent content or an empty string if the file is new.

```python
get_revision_content(filename: str, revision: int) → str[source]#
```

【中文翻译】Return the exact content stored in revision.
If the revision does not exist an empty string is returned so that
downstream code can handle the “not found” case without exceptions.

```python
get_revision_diffs(filename: str) → List[str][source]#
```

【中文翻译】Return a chronological list of unified‑diffs for filename.
Each element in the returned list represents the diff that transformed
revision n into revision n+1 (starting at revision 1 → 2).

```python
list_files() → Dict[str, int][source]#
```

【中文翻译】Return a mapping of filename → latest revision number.

```python
class TextCanvasMemory(canvas: TextCanvas | None = None)[source]#
```

【中文翻译】Bases: Memory
A memory implementation that uses a Canvas for storing file-like content.
Inserts the current state of the canvas into the ChatCompletionContext on each turn.

Warning
This is an experimental API and may change in the future.

The TextCanvasMemory provides a persistent, file-like storage mechanism that can be used
by agents to read and write content. It automatically injects the current state of all files
in the canvas into the model context before each inference.
This is particularly useful for:
- Allowing agents to create and modify documents over multiple turns
- Enabling collaborative document editing between multiple agents
- Maintaining persistent state across conversation turns
- Working with content too large to fit in a single message
The canvas provides tools for:
- Creating or updating files with new content
- Applying patches (unified diff format) to existing files
Examples
Example: Using TextCanvasMemory with an AssistantAgent
The following example demonstrates how to create a TextCanvasMemory and use it with
an AssistantAgent to write and update a story file.
import asyncio
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.memory.canvas import TextCanvasMemory


async def main():
    # Create a model client
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )

    # Create the canvas memory
    text_canvas_memory = TextCanvasMemory()

    # Get tools for working with the canvas
    update_file_tool = text_canvas_memory.get_update_file_tool()
    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()

    # Create an agent with the canvas memory and tools
    writer_agent = AssistantAgent(
        name="Writer",
        model_client=model_client,
        description="A writer agent that creates and updates stories.",
        system_message='''
        You are a Writer Agent. Your focus is to generate a story based on the user's request.

        Instructions for using the canvas:

        - The story should be stored on the canvas in a file named "story.md".
        - If "story.md" does not exist, create it by calling the 'update_file' tool.
        - If "story.md" already exists, generate a unified diff (patch) from the current
          content to the new version, and call the 'apply_patch' tool to apply the changes.

        IMPORTANT: Do not include the full story text in your chat messages.
        Only write the story content to the canvas using the tools.
        ''',
        tools=[update_file_tool, apply_patch_tool],
        memory=[text_canvas_memory],
    )

    # Send a message to the agent
    await writer_agent.on_messages(
        [TextMessage(content="Write a short story about a bunny and a sunflower.", source="user")],
        CancellationToken(),
    )

    # Retrieve the content from the canvas
    story_content = text_canvas_memory.canvas.get_latest_content("story.md")
    print("Story content from canvas:")
    print(story_content)


if __name__ == "__main__":
    asyncio.run(main())


Example: Using TextCanvasMemory with multiple agents
The following example shows how to use TextCanvasMemory with multiple agents
collaborating on the same document.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_ext.memory.canvas import TextCanvasMemory


async def main():
    # Create a model client
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )

    # Create the shared canvas memory
    text_canvas_memory = TextCanvasMemory()
    update_file_tool = text_canvas_memory.get_update_file_tool()
    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()

    # Create a writer agent
    writer_agent = AssistantAgent(
        name="Writer",
        model_client=model_client,
        description="A writer agent that creates stories.",
        system_message="You write children's stories on the canvas in story.md.",
        tools=[update_file_tool, apply_patch_tool],
        memory=[text_canvas_memory],
    )

    # Create a critique agent
    critique_agent = AssistantAgent(
        name="Critique",
        model_client=model_client,
        description="A critique agent that provides feedback on stories.",
        system_message="You review the story.md file and provide constructive feedback.",
        memory=[text_canvas_memory],
    )

    # Create a team with both agents
    team = RoundRobinGroupChat(
        participants=[writer_agent, critique_agent],
        termination_condition=TextMentionTermination("TERMINATE"),
        max_turns=10,
    )

    # Run the team on a task
    await team.run(task="Create a children's book about a bunny and a sunflower")

    # Get the final story
    story = text_canvas_memory.canvas.get_latest_content("story.md")
    print(story)


if __name__ == "__main__":
    asyncio.run(main())




async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
Example usage: Possibly interpret content as a patch or direct file update.
Could also be done by a specialized “CanvasTool” instead.



async clear() → None[source]#
Clear the entire canvas by replacing it with a new empty instance.



async close() → None[source]#
Clean up any resources used by the memory implementation.



get_apply_patch_tool() → ApplyPatchTool[source]#
Returns an ApplyPatchTool instance that works with this memory’s canvas.



get_update_file_tool() → UpdateFileTool[source]#
Returns an UpdateFileTool instance that works with this memory’s canvas.



async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
Potentially search for matching filenames or file content.
This example returns empty.



async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
Inject the entire canvas summary (or a selected subset) as reference data.
Here, we just put it into a system message, but you could customize.

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.memory.canvas import TextCanvasMemory


async def main():
    # Create a model client
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )

    # Create the canvas memory
    text_canvas_memory = TextCanvasMemory()

    # Get tools for working with the canvas
    update_file_tool = text_canvas_memory.get_update_file_tool()
    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()

    # Create an agent with the canvas memory and tools
    writer_agent = AssistantAgent(
        name="Writer",
        model_client=model_client,
        description="A writer agent that creates and updates stories.",
        system_message='''
        You are a Writer Agent. Your focus is to generate a story based on the user's request.

        Instructions for using the canvas:

        - The story should be stored on the canvas in a file named "story.md".
        - If "story.md" does not exist, create it by calling the 'update_file' tool.
        - If "story.md" already exists, generate a unified diff (patch) from the current
          content to the new version, and call the 'apply_patch' tool to apply the changes.

        IMPORTANT: Do not include the full story text in your chat messages.
        Only write the story content to the canvas using the tools.
        ''',
        tools=[update_file_tool, apply_patch_tool],
        memory=[text_canvas_memory],
    )

    # Send a message to the agent
    await writer_agent.on_messages(
        [TextMessage(content="Write a short story about a bunny and a sunflower.", source="user")],
        CancellationToken(),
    )

    # Retrieve the content from the canvas
    story_content = text_canvas_memory.canvas.get_latest_content("story.md")
    print("Story content from canvas:")
    print(story_content)


if __name__ == "__main__":
    asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_ext.memory.canvas import TextCanvasMemory


async def main():
    # Create a model client
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )

    # Create the shared canvas memory
    text_canvas_memory = TextCanvasMemory()
    update_file_tool = text_canvas_memory.get_update_file_tool()
    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()

    # Create a writer agent
    writer_agent = AssistantAgent(
        name="Writer",
        model_client=model_client,
        description="A writer agent that creates stories.",
        system_message="You write children's stories on the canvas in story.md.",
        tools=[update_file_tool, apply_patch_tool],
        memory=[text_canvas_memory],
    )

    # Create a critique agent
    critique_agent = AssistantAgent(
        name="Critique",
        model_client=model_client,
        description="A critique agent that provides feedback on stories.",
        system_message="You review the story.md file and provide constructive feedback.",
        memory=[text_canvas_memory],
    )

    # Create a team with both agents
    team = RoundRobinGroupChat(
        participants=[writer_agent, critique_agent],
        termination_condition=TextMentionTermination("TERMINATE"),
        max_turns=10,
    )

    # Run the team on a task
    await team.run(task="Create a children's book about a bunny and a sunflower")

    # Get the final story
    story = text_canvas_memory.canvas.get_latest_content("story.md")
    print(story)


if __name__ == "__main__":
    asyncio.run(main())

```

```python
async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Example usage: Possibly interpret content as a patch or direct file update.
Could also be done by a specialized “CanvasTool” instead.

```python
async clear() → None[source]#
```

【中文翻译】Clear the entire canvas by replacing it with a new empty instance.

```python
async close() → None[source]#
```

【中文翻译】Clean up any resources used by the memory implementation.

```python
get_apply_patch_tool() → ApplyPatchTool[source]#
```

【中文翻译】Returns an ApplyPatchTool instance that works with this memory’s canvas.

```python
get_update_file_tool() → UpdateFileTool[source]#
```

【中文翻译】Returns an UpdateFileTool instance that works with this memory’s canvas.

```python
async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
```

【中文翻译】Potentially search for matching filenames or file content.
This example returns empty.

```python
async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
```

【中文翻译】Inject the entire canvas summary (or a selected subset) as reference data.
Here, we just put it into a system message, but you could customize.

【中文翻译】previous

【中文翻译】autogen_ext.tools.mcp

【中文翻译】next

【中文翻译】autogen_ext.tools.semantic_kernel

### autogen_ext.memory.canvas {autogen_extmemorycanvas}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.memory.canvas.html)

```python
class TextCanvas[source]#
```

【中文翻译】Bases: BaseCanvas
An in‑memory canvas that stores text files with full revision history.

Warning
This is an experimental API and may change in the future.

Besides the original CRUD‑like operations, this enhanced implementation adds:

apply_patch – applies patches using the unidiff library for accurate
hunk application and context line validation.
get_revision_content – random access to any historical revision.
get_revision_diffs – obtain the list of diffs applied between every
consecutive pair of revisions so that a caller can replay or audit the
full change history.



add_or_update_file(filename: str, new_content: str | bytes | Any) → None[source]#
Create filename or append a new revision containing new_content.



apply_patch(filename: str, patch_data: str | bytes | Any) → None[source]#
Apply patch_text (unified diff) to the latest revision and save a new revision.
Uses the unidiff library to accurately apply hunks and validate context lines.



get_all_contents_for_context() → str[source]#
Return a summarised view of every file and its latest revision.



get_diff(filename: str, from_revision: int, to_revision: int) → str[source]#
Return a unified diff between from_revision and to_revision.



get_latest_content(filename: str) → str[source]#
Return the most recent content or an empty string if the file is new.



get_revision_content(filename: str, revision: int) → str[source]#
Return the exact content stored in revision.
If the revision does not exist an empty string is returned so that
downstream code can handle the “not found” case without exceptions.



get_revision_diffs(filename: str) → List[str][source]#
Return a chronological list of unified‑diffs for filename.
Each element in the returned list represents the diff that transformed
revision n into revision n+1 (starting at revision 1 → 2).



list_files() → Dict[str, int][source]#
Return a mapping of filename → latest revision number.

```python
add_or_update_file(filename: str, new_content: str | bytes | Any) → None[source]#
```

【中文翻译】Create filename or append a new revision containing new_content.

```python
apply_patch(filename: str, patch_data: str | bytes | Any) → None[source]#
```

【中文翻译】Apply patch_text (unified diff) to the latest revision and save a new revision.
Uses the unidiff library to accurately apply hunks and validate context lines.

```python
get_all_contents_for_context() → str[source]#
```

【中文翻译】Return a summarised view of every file and its latest revision.

```python
get_diff(filename: str, from_revision: int, to_revision: int) → str[source]#
```

【中文翻译】Return a unified diff between from_revision and to_revision.

```python
get_latest_content(filename: str) → str[source]#
```

【中文翻译】Return the most recent content or an empty string if the file is new.

```python
get_revision_content(filename: str, revision: int) → str[source]#
```

【中文翻译】Return the exact content stored in revision.
If the revision does not exist an empty string is returned so that
downstream code can handle the “not found” case without exceptions.

```python
get_revision_diffs(filename: str) → List[str][source]#
```

【中文翻译】Return a chronological list of unified‑diffs for filename.
Each element in the returned list represents the diff that transformed
revision n into revision n+1 (starting at revision 1 → 2).

```python
list_files() → Dict[str, int][source]#
```

【中文翻译】Return a mapping of filename → latest revision number.

```python
class TextCanvasMemory(canvas: TextCanvas | None = None)[source]#
```

【中文翻译】Bases: Memory
A memory implementation that uses a Canvas for storing file-like content.
Inserts the current state of the canvas into the ChatCompletionContext on each turn.

Warning
This is an experimental API and may change in the future.

The TextCanvasMemory provides a persistent, file-like storage mechanism that can be used
by agents to read and write content. It automatically injects the current state of all files
in the canvas into the model context before each inference.
This is particularly useful for:
- Allowing agents to create and modify documents over multiple turns
- Enabling collaborative document editing between multiple agents
- Maintaining persistent state across conversation turns
- Working with content too large to fit in a single message
The canvas provides tools for:
- Creating or updating files with new content
- Applying patches (unified diff format) to existing files
Examples
Example: Using TextCanvasMemory with an AssistantAgent
The following example demonstrates how to create a TextCanvasMemory and use it with
an AssistantAgent to write and update a story file.
import asyncio
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.memory.canvas import TextCanvasMemory


async def main():
    # Create a model client
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )

    # Create the canvas memory
    text_canvas_memory = TextCanvasMemory()

    # Get tools for working with the canvas
    update_file_tool = text_canvas_memory.get_update_file_tool()
    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()

    # Create an agent with the canvas memory and tools
    writer_agent = AssistantAgent(
        name="Writer",
        model_client=model_client,
        description="A writer agent that creates and updates stories.",
        system_message='''
        You are a Writer Agent. Your focus is to generate a story based on the user's request.

        Instructions for using the canvas:

        - The story should be stored on the canvas in a file named "story.md".
        - If "story.md" does not exist, create it by calling the 'update_file' tool.
        - If "story.md" already exists, generate a unified diff (patch) from the current
          content to the new version, and call the 'apply_patch' tool to apply the changes.

        IMPORTANT: Do not include the full story text in your chat messages.
        Only write the story content to the canvas using the tools.
        ''',
        tools=[update_file_tool, apply_patch_tool],
        memory=[text_canvas_memory],
    )

    # Send a message to the agent
    await writer_agent.on_messages(
        [TextMessage(content="Write a short story about a bunny and a sunflower.", source="user")],
        CancellationToken(),
    )

    # Retrieve the content from the canvas
    story_content = text_canvas_memory.canvas.get_latest_content("story.md")
    print("Story content from canvas:")
    print(story_content)


if __name__ == "__main__":
    asyncio.run(main())


Example: Using TextCanvasMemory with multiple agents
The following example shows how to use TextCanvasMemory with multiple agents
collaborating on the same document.
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_ext.memory.canvas import TextCanvasMemory


async def main():
    # Create a model client
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )

    # Create the shared canvas memory
    text_canvas_memory = TextCanvasMemory()
    update_file_tool = text_canvas_memory.get_update_file_tool()
    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()

    # Create a writer agent
    writer_agent = AssistantAgent(
        name="Writer",
        model_client=model_client,
        description="A writer agent that creates stories.",
        system_message="You write children's stories on the canvas in story.md.",
        tools=[update_file_tool, apply_patch_tool],
        memory=[text_canvas_memory],
    )

    # Create a critique agent
    critique_agent = AssistantAgent(
        name="Critique",
        model_client=model_client,
        description="A critique agent that provides feedback on stories.",
        system_message="You review the story.md file and provide constructive feedback.",
        memory=[text_canvas_memory],
    )

    # Create a team with both agents
    team = RoundRobinGroupChat(
        participants=[writer_agent, critique_agent],
        termination_condition=TextMentionTermination("TERMINATE"),
        max_turns=10,
    )

    # Run the team on a task
    await team.run(task="Create a children's book about a bunny and a sunflower")

    # Get the final story
    story = text_canvas_memory.canvas.get_latest_content("story.md")
    print(story)


if __name__ == "__main__":
    asyncio.run(main())




async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
Example usage: Possibly interpret content as a patch or direct file update.
Could also be done by a specialized “CanvasTool” instead.



async clear() → None[source]#
Clear the entire canvas by replacing it with a new empty instance.



async close() → None[source]#
Clean up any resources used by the memory implementation.



get_apply_patch_tool() → ApplyPatchTool[source]#
Returns an ApplyPatchTool instance that works with this memory’s canvas.



get_update_file_tool() → UpdateFileTool[source]#
Returns an UpdateFileTool instance that works with this memory’s canvas.



async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
Potentially search for matching filenames or file content.
This example returns empty.



async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
Inject the entire canvas summary (or a selected subset) as reference data.
Here, we just put it into a system message, but you could customize.

**示例**:
```python
import asyncio
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.memory.canvas import TextCanvasMemory


async def main():
    # Create a model client
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )

    # Create the canvas memory
    text_canvas_memory = TextCanvasMemory()

    # Get tools for working with the canvas
    update_file_tool = text_canvas_memory.get_update_file_tool()
    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()

    # Create an agent with the canvas memory and tools
    writer_agent = AssistantAgent(
        name="Writer",
        model_client=model_client,
        description="A writer agent that creates and updates stories.",
        system_message='''
        You are a Writer Agent. Your focus is to generate a story based on the user's request.

        Instructions for using the canvas:

        - The story should be stored on the canvas in a file named "story.md".
        - If "story.md" does not exist, create it by calling the 'update_file' tool.
        - If "story.md" already exists, generate a unified diff (patch) from the current
          content to the new version, and call the 'apply_patch' tool to apply the changes.

        IMPORTANT: Do not include the full story text in your chat messages.
        Only write the story content to the canvas using the tools.
        ''',
        tools=[update_file_tool, apply_patch_tool],
        memory=[text_canvas_memory],
    )

    # Send a message to the agent
    await writer_agent.on_messages(
        [TextMessage(content="Write a short story about a bunny and a sunflower.", source="user")],
        CancellationToken(),
    )

    # Retrieve the content from the canvas
    story_content = text_canvas_memory.canvas.get_latest_content("story.md")
    print("Story content from canvas:")
    print(story_content)


if __name__ == "__main__":
    asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_ext.memory.canvas import TextCanvasMemory


async def main():
    # Create a model client
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )

    # Create the shared canvas memory
    text_canvas_memory = TextCanvasMemory()
    update_file_tool = text_canvas_memory.get_update_file_tool()
    apply_patch_tool = text_canvas_memory.get_apply_patch_tool()

    # Create a writer agent
    writer_agent = AssistantAgent(
        name="Writer",
        model_client=model_client,
        description="A writer agent that creates stories.",
        system_message="You write children's stories on the canvas in story.md.",
        tools=[update_file_tool, apply_patch_tool],
        memory=[text_canvas_memory],
    )

    # Create a critique agent
    critique_agent = AssistantAgent(
        name="Critique",
        model_client=model_client,
        description="A critique agent that provides feedback on stories.",
        system_message="You review the story.md file and provide constructive feedback.",
        memory=[text_canvas_memory],
    )

    # Create a team with both agents
    team = RoundRobinGroupChat(
        participants=[writer_agent, critique_agent],
        termination_condition=TextMentionTermination("TERMINATE"),
        max_turns=10,
    )

    # Run the team on a task
    await team.run(task="Create a children's book about a bunny and a sunflower")

    # Get the final story
    story = text_canvas_memory.canvas.get_latest_content("story.md")
    print(story)


if __name__ == "__main__":
    asyncio.run(main())

```

```python
async add(content: MemoryContent, cancellation_token: CancellationToken | None = None) → None[source]#
```

【中文翻译】Example usage: Possibly interpret content as a patch or direct file update.
Could also be done by a specialized “CanvasTool” instead.

```python
async clear() → None[source]#
```

【中文翻译】Clear the entire canvas by replacing it with a new empty instance.

```python
async close() → None[source]#
```

【中文翻译】Clean up any resources used by the memory implementation.

```python
get_apply_patch_tool() → ApplyPatchTool[source]#
```

【中文翻译】Returns an ApplyPatchTool instance that works with this memory’s canvas.

```python
get_update_file_tool() → UpdateFileTool[source]#
```

【中文翻译】Returns an UpdateFileTool instance that works with this memory’s canvas.

```python
async query(query: str | MemoryContent, cancellation_token: CancellationToken | None = None, **kwargs: Any) → MemoryQueryResult[source]#
```

【中文翻译】Potentially search for matching filenames or file content.
This example returns empty.

```python
async update_context(model_context: ChatCompletionContext) → UpdateContextResult[source]#
```

【中文翻译】Inject the entire canvas summary (or a selected subset) as reference data.
Here, we just put it into a system message, but you could customize.

【中文翻译】previous

【中文翻译】autogen_ext.tools.mcp

【中文翻译】next

【中文翻译】autogen_ext.tools.semantic_kernel

### autogen_ext.models.anthropic {autogen_extmodelsanthropic}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html)

```python
class AnthropicBedrockClientConfiguration[source]#
```

【中文翻译】Bases: AnthropicClientConfiguration


api_key: str#



base_url: str | None#



bedrock_info: BedrockInfo#



default_headers: Dict[str, str] | None#



max_retries: int | None#



max_tokens: int | None#



metadata: Dict[str, str] | None#



model: str#



model_capabilities: ModelCapabilities#



model_info: ModelInfo#



response_format: ResponseFormat | None#



stop_sequences: List[str] | None#



temperature: float | None#



timeout: float | None#



tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None#



tools: List[Dict[str, Any]] | None#



top_k: int | None#



top_p: float | None#

```python
api_key: str#
```

```python
base_url: str | None#
```

```python
bedrock_info: BedrockInfo#
```

```python
default_headers: Dict[str, str] | None#
```

```python
max_retries: int | None#
```

```python
max_tokens: int | None#
```

```python
metadata: Dict[str, str] | None#
```

```python
model: str#
```

```python
model_capabilities: ModelCapabilities#
```

```python
model_info: ModelInfo#
```

```python
response_format: ResponseFormat | None#
```

```python
stop_sequences: List[str] | None#
```

```python
temperature: float | None#
```

```python
timeout: float | None#
```

```python
tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None#
```

```python
tools: List[Dict[str, Any]] | None#
```

```python
top_k: int | None#
```

```python
top_p: float | None#
```

```python
pydantic model AnthropicBedrockClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: AnthropicClientConfigurationConfigModel

Show JSON schema{
   "title": "AnthropicBedrockClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "tools": {
         "anyOf": [
            {
               "items": {
                  "type": "object"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tools"
      },
      "tool_choice": {
         "anyOf": [
            {
               "enum": [
                  "auto",
                  "any",
                  "none"
               ],
               "type": "string"
            },
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tool Choice"
      },
      "bedrock_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/BedrockInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "BedrockInfo": {
         "description": "BedrockInfo is a dictionary that contains information about a bedrock's properties.\nIt is expected to be used in the bedrock_info property of a model client.",
         "properties": {
            "aws_access_key": {
               "format": "password",
               "title": "Aws Access Key",
               "type": "string",
               "writeOnly": true
            },
            "aws_secret_key": {
               "format": "password",
               "title": "Aws Secret Key",
               "type": "string",
               "writeOnly": true
            },
            "aws_session_token": {
               "format": "password",
               "title": "Aws Session Token",
               "type": "string",
               "writeOnly": true
            },
            "aws_region": {
               "title": "Aws Region",
               "type": "string"
            }
         },
         "required": [
            "aws_access_key",
            "aws_secret_key",
            "aws_session_token",
            "aws_region"
         ],
         "title": "BedrockInfo",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

bedrock_info (autogen_ext.models.anthropic.config.BedrockInfo | None)





field bedrock_info: BedrockInfo | None = None#

**示例**:
```python
{
   "title": "AnthropicBedrockClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "tools": {
         "anyOf": [
            {
               "items": {
                  "type": "object"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tools"
      },
      "tool_choice": {
         "anyOf": [
            {
               "enum": [
                  "auto",
                  "any",
                  "none"
               ],
               "type": "string"
            },
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tool Choice"
      },
      "bedrock_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/BedrockInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "BedrockInfo": {
         "description": "BedrockInfo is a dictionary that contains information about a bedrock's properties.\nIt is expected to be used in the bedrock_info property of a model client.",
         "properties": {
            "aws_access_key": {
               "format": "password",
               "title": "Aws Access Key",
               "type": "string",
               "writeOnly": true
            },
            "aws_secret_key": {
               "format": "password",
               "title": "Aws Secret Key",
               "type": "string",
               "writeOnly": true
            },
            "aws_session_token": {
               "format": "password",
               "title": "Aws Session Token",
               "type": "string",
               "writeOnly": true
            },
            "aws_region": {
               "title": "Aws Region",
               "type": "string"
            }
         },
         "required": [
            "aws_access_key",
            "aws_secret_key",
            "aws_session_token",
            "aws_region"
         ],
         "title": "BedrockInfo",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field bedrock_info: BedrockInfo | None = None#
```

```python
class AnthropicChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: BaseAnthropicChatCompletionClient, Component[AnthropicClientConfigurationConfigModel]
Chat completion client for Anthropic’s Claude models.

Parameters:

model (str) – The Claude model to use (e.g., “claude-3-sonnet-20240229”, “claude-3-opus-20240229”)
api_key (str, optional) – Anthropic API key. Required if not in environment variables.
base_url (str, optional) – Override the default API endpoint.
max_tokens (int, optional) – Maximum tokens in the response. Default is 4096.
temperature (float, optional) – Controls randomness. Lower is more deterministic. Default is 1.0.
top_p (float, optional) – Controls diversity via nucleus sampling. Default is 1.0.
top_k (int, optional) – Controls diversity via top-k sampling. Default is -1 (disabled).
model_info (ModelInfo, optional) – The capabilities of the model. Required if using a custom model.



To use this client, you must install the Anthropic extension:
pip install "autogen-ext[anthropic]"


Example:
import asyncio
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
from autogen_core.models import UserMessage


async def main():
    anthropic_client = AnthropicChatCompletionClient(
        model="claude-3-sonnet-20240229",
        api_key="your-api-key",  # Optional if ANTHROPIC_API_KEY is set in environment
    )

    result = await anthropic_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
    print(result)


if __name__ == "__main__":
    asyncio.run(main())


To load the client from a configuration:
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "AnthropicChatCompletionClient",
    "config": {"model": "claude-3-sonnet-20240229"},
}

client = ChatCompletionClient.load_component(config)




classmethod _from_config(config: AnthropicClientConfigurationConfigModel) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → AnthropicClientConfigurationConfigModel[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of AnthropicClientConfigurationConfigModel



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.anthropic.AnthropicChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'model'#
The logical type of the component.

**示例**:
```python
pip install "autogen-ext[anthropic]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
from autogen_core.models import UserMessage


async def main():
    anthropic_client = AnthropicChatCompletionClient(
        model="claude-3-sonnet-20240229",
        api_key="your-api-key",  # Optional if ANTHROPIC_API_KEY is set in environment
    )

    result = await anthropic_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
    print(result)


if __name__ == "__main__":
    asyncio.run(main())

```

**示例**:
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "AnthropicChatCompletionClient",
    "config": {"model": "claude-3-sonnet-20240229"},
}

client = ChatCompletionClient.load_component(config)

```

```python
classmethod _from_config(config: AnthropicClientConfigurationConfigModel) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → AnthropicClientConfigurationConfigModel[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of AnthropicClientConfigurationConfigModel

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.anthropic.AnthropicChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'model'#
```

【中文翻译】The logical type of the component.

```python
class AnthropicClientConfiguration[source]#
```

【中文翻译】Bases: BaseAnthropicClientConfiguration


api_key: str#



base_url: str | None#



default_headers: Dict[str, str] | None#



max_retries: int | None#



max_tokens: int | None#



metadata: Dict[str, str] | None#



model: str#



model_capabilities: ModelCapabilities#



model_info: ModelInfo#



response_format: ResponseFormat | None#



stop_sequences: List[str] | None#



temperature: float | None#



timeout: float | None#



tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None#



tools: List[Dict[str, Any]] | None#



top_k: int | None#



top_p: float | None#

```python
api_key: str#
```

```python
base_url: str | None#
```

```python
default_headers: Dict[str, str] | None#
```

```python
max_retries: int | None#
```

```python
max_tokens: int | None#
```

```python
metadata: Dict[str, str] | None#
```

```python
model: str#
```

```python
model_capabilities: ModelCapabilities#
```

```python
model_info: ModelInfo#
```

```python
response_format: ResponseFormat | None#
```

```python
stop_sequences: List[str] | None#
```

```python
temperature: float | None#
```

```python
timeout: float | None#
```

```python
tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None#
```

```python
tools: List[Dict[str, Any]] | None#
```

```python
top_k: int | None#
```

```python
top_p: float | None#
```

```python
pydantic model AnthropicClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: BaseAnthropicClientConfigurationConfigModel

Show JSON schema{
   "title": "AnthropicClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "tools": {
         "anyOf": [
            {
               "items": {
                  "type": "object"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tools"
      },
      "tool_choice": {
         "anyOf": [
            {
               "enum": [
                  "auto",
                  "any",
                  "none"
               ],
               "type": "string"
            },
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tool Choice"
      }
   },
   "$defs": {
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

tool_choice (Literal['auto', 'any', 'none'] | Dict[str, Any] | None)
tools (List[Dict[str, Any]] | None)





field tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None = None#



field tools: List[Dict[str, Any]] | None = None#

**示例**:
```python
{
   "title": "AnthropicClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "tools": {
         "anyOf": [
            {
               "items": {
                  "type": "object"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tools"
      },
      "tool_choice": {
         "anyOf": [
            {
               "enum": [
                  "auto",
                  "any",
                  "none"
               ],
               "type": "string"
            },
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tool Choice"
      }
   },
   "$defs": {
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None = None#
```

```python
field tools: List[Dict[str, Any]] | None = None#
```

```python
class BaseAnthropicChatCompletionClient(client: Any, *, create_args: Dict[str, Any], model_info: ModelInfo | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient


actual_usage() → RequestUsage[source]#



property capabilities: ModelCapabilities#



async close() → None[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
Estimate the number of tokens used by messages and tools.
Note: This is an estimation based on common tokenization patterns and may not perfectly
match Anthropic’s exact token counting for Claude models.



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, max_consecutive_empty_chunk_tolerance: int = 0) → AsyncGenerator[str | CreateResult, None][source]#
Creates an AsyncGenerator that yields a stream of completions based on the provided messages and tools.



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
Calculate the remaining tokens based on the model’s token limit.



total_usage() → RequestUsage[source]#

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelCapabilities#
```

```python
async close() → None[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

【中文翻译】Estimate the number of tokens used by messages and tools.
Note: This is an estimation based on common tokenization patterns and may not perfectly
match Anthropic’s exact token counting for Claude models.

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, max_consecutive_empty_chunk_tolerance: int = 0) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates an AsyncGenerator that yields a stream of completions based on the provided messages and tools.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

【中文翻译】Calculate the remaining tokens based on the model’s token limit.

```python
total_usage() → RequestUsage[source]#
```

```python
class BedrockInfo[source]#
```

【中文翻译】Bases: TypedDict
BedrockInfo is a dictionary that contains information about a bedrock’s properties.
It is expected to be used in the bedrock_info property of a model client.


aws_access_key: Required[SecretStr]#
Access key for the aws account to gain bedrock model access



aws_region: Required[str]#
aws region for the aws account to gain bedrock model access



aws_secret_key: Required[SecretStr]#
Access secret key for the aws account to gain bedrock model access



aws_session_token: Required[SecretStr]#
aws session token for the aws account to gain bedrock model access

```python
aws_access_key: Required[SecretStr]#
```

【中文翻译】Access key for the aws account to gain bedrock model access

```python
aws_region: Required[str]#
```

【中文翻译】aws region for the aws account to gain bedrock model access

```python
aws_secret_key: Required[SecretStr]#
```

【中文翻译】Access secret key for the aws account to gain bedrock model access

```python
aws_session_token: Required[SecretStr]#
```

【中文翻译】aws session token for the aws account to gain bedrock model access

```python
pydantic model CreateArgumentsConfigModel[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      }
   },
   "$defs": {
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

max_tokens (int | None)
metadata (Dict[str, str] | None)
model (str)
response_format (autogen_ext.models.anthropic.config.ResponseFormat | None)
stop_sequences (List[str] | None)
temperature (float | None)
top_k (int | None)
top_p (float | None)





field max_tokens: int | None = 4096#



field metadata: Dict[str, str] | None = None#



field model: str [Required]#



field response_format: ResponseFormat | None = None#



field stop_sequences: List[str] | None = None#



field temperature: float | None = 1.0#



field top_k: int | None = None#



field top_p: float | None = None#

**示例**:
```python
{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      }
   },
   "$defs": {
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field max_tokens: int | None = 4096#
```

```python
field metadata: Dict[str, str] | None = None#
```

```python
field model: str [Required]#
```

```python
field response_format: ResponseFormat | None = None#
```

```python
field stop_sequences: List[str] | None = None#
```

```python
field temperature: float | None = 1.0#
```

```python
field top_k: int | None = None#
```

```python
field top_p: float | None = None#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.azure

【中文翻译】next

【中文翻译】autogen_ext.models.semantic_kernel

### autogen_ext.models.anthropic {autogen_extmodelsanthropic}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html)

```python
class AnthropicBedrockClientConfiguration[source]#
```

【中文翻译】Bases: AnthropicClientConfiguration


api_key: str#



base_url: str | None#



bedrock_info: BedrockInfo#



default_headers: Dict[str, str] | None#



max_retries: int | None#



max_tokens: int | None#



metadata: Dict[str, str] | None#



model: str#



model_capabilities: ModelCapabilities#



model_info: ModelInfo#



response_format: ResponseFormat | None#



stop_sequences: List[str] | None#



temperature: float | None#



timeout: float | None#



tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None#



tools: List[Dict[str, Any]] | None#



top_k: int | None#



top_p: float | None#

```python
api_key: str#
```

```python
base_url: str | None#
```

```python
bedrock_info: BedrockInfo#
```

```python
default_headers: Dict[str, str] | None#
```

```python
max_retries: int | None#
```

```python
max_tokens: int | None#
```

```python
metadata: Dict[str, str] | None#
```

```python
model: str#
```

```python
model_capabilities: ModelCapabilities#
```

```python
model_info: ModelInfo#
```

```python
response_format: ResponseFormat | None#
```

```python
stop_sequences: List[str] | None#
```

```python
temperature: float | None#
```

```python
timeout: float | None#
```

```python
tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None#
```

```python
tools: List[Dict[str, Any]] | None#
```

```python
top_k: int | None#
```

```python
top_p: float | None#
```

```python
pydantic model AnthropicBedrockClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: AnthropicClientConfigurationConfigModel

Show JSON schema{
   "title": "AnthropicBedrockClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "tools": {
         "anyOf": [
            {
               "items": {
                  "type": "object"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tools"
      },
      "tool_choice": {
         "anyOf": [
            {
               "enum": [
                  "auto",
                  "any",
                  "none"
               ],
               "type": "string"
            },
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tool Choice"
      },
      "bedrock_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/BedrockInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "BedrockInfo": {
         "description": "BedrockInfo is a dictionary that contains information about a bedrock's properties.\nIt is expected to be used in the bedrock_info property of a model client.",
         "properties": {
            "aws_access_key": {
               "format": "password",
               "title": "Aws Access Key",
               "type": "string",
               "writeOnly": true
            },
            "aws_secret_key": {
               "format": "password",
               "title": "Aws Secret Key",
               "type": "string",
               "writeOnly": true
            },
            "aws_session_token": {
               "format": "password",
               "title": "Aws Session Token",
               "type": "string",
               "writeOnly": true
            },
            "aws_region": {
               "title": "Aws Region",
               "type": "string"
            }
         },
         "required": [
            "aws_access_key",
            "aws_secret_key",
            "aws_session_token",
            "aws_region"
         ],
         "title": "BedrockInfo",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

bedrock_info (autogen_ext.models.anthropic.config.BedrockInfo | None)





field bedrock_info: BedrockInfo | None = None#

**示例**:
```python
{
   "title": "AnthropicBedrockClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "tools": {
         "anyOf": [
            {
               "items": {
                  "type": "object"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tools"
      },
      "tool_choice": {
         "anyOf": [
            {
               "enum": [
                  "auto",
                  "any",
                  "none"
               ],
               "type": "string"
            },
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tool Choice"
      },
      "bedrock_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/BedrockInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "BedrockInfo": {
         "description": "BedrockInfo is a dictionary that contains information about a bedrock's properties.\nIt is expected to be used in the bedrock_info property of a model client.",
         "properties": {
            "aws_access_key": {
               "format": "password",
               "title": "Aws Access Key",
               "type": "string",
               "writeOnly": true
            },
            "aws_secret_key": {
               "format": "password",
               "title": "Aws Secret Key",
               "type": "string",
               "writeOnly": true
            },
            "aws_session_token": {
               "format": "password",
               "title": "Aws Session Token",
               "type": "string",
               "writeOnly": true
            },
            "aws_region": {
               "title": "Aws Region",
               "type": "string"
            }
         },
         "required": [
            "aws_access_key",
            "aws_secret_key",
            "aws_session_token",
            "aws_region"
         ],
         "title": "BedrockInfo",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field bedrock_info: BedrockInfo | None = None#
```

```python
class AnthropicChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: BaseAnthropicChatCompletionClient, Component[AnthropicClientConfigurationConfigModel]
Chat completion client for Anthropic’s Claude models.

Parameters:

model (str) – The Claude model to use (e.g., “claude-3-sonnet-20240229”, “claude-3-opus-20240229”)
api_key (str, optional) – Anthropic API key. Required if not in environment variables.
base_url (str, optional) – Override the default API endpoint.
max_tokens (int, optional) – Maximum tokens in the response. Default is 4096.
temperature (float, optional) – Controls randomness. Lower is more deterministic. Default is 1.0.
top_p (float, optional) – Controls diversity via nucleus sampling. Default is 1.0.
top_k (int, optional) – Controls diversity via top-k sampling. Default is -1 (disabled).
model_info (ModelInfo, optional) – The capabilities of the model. Required if using a custom model.



To use this client, you must install the Anthropic extension:
pip install "autogen-ext[anthropic]"


Example:
import asyncio
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
from autogen_core.models import UserMessage


async def main():
    anthropic_client = AnthropicChatCompletionClient(
        model="claude-3-sonnet-20240229",
        api_key="your-api-key",  # Optional if ANTHROPIC_API_KEY is set in environment
    )

    result = await anthropic_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
    print(result)


if __name__ == "__main__":
    asyncio.run(main())


To load the client from a configuration:
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "AnthropicChatCompletionClient",
    "config": {"model": "claude-3-sonnet-20240229"},
}

client = ChatCompletionClient.load_component(config)




classmethod _from_config(config: AnthropicClientConfigurationConfigModel) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → AnthropicClientConfigurationConfigModel[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of AnthropicClientConfigurationConfigModel



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.anthropic.AnthropicChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'model'#
The logical type of the component.

**示例**:
```python
pip install "autogen-ext[anthropic]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
from autogen_core.models import UserMessage


async def main():
    anthropic_client = AnthropicChatCompletionClient(
        model="claude-3-sonnet-20240229",
        api_key="your-api-key",  # Optional if ANTHROPIC_API_KEY is set in environment
    )

    result = await anthropic_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
    print(result)


if __name__ == "__main__":
    asyncio.run(main())

```

**示例**:
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "AnthropicChatCompletionClient",
    "config": {"model": "claude-3-sonnet-20240229"},
}

client = ChatCompletionClient.load_component(config)

```

```python
classmethod _from_config(config: AnthropicClientConfigurationConfigModel) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → AnthropicClientConfigurationConfigModel[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of AnthropicClientConfigurationConfigModel

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.anthropic.AnthropicChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'model'#
```

【中文翻译】The logical type of the component.

```python
class AnthropicClientConfiguration[source]#
```

【中文翻译】Bases: BaseAnthropicClientConfiguration


api_key: str#



base_url: str | None#



default_headers: Dict[str, str] | None#



max_retries: int | None#



max_tokens: int | None#



metadata: Dict[str, str] | None#



model: str#



model_capabilities: ModelCapabilities#



model_info: ModelInfo#



response_format: ResponseFormat | None#



stop_sequences: List[str] | None#



temperature: float | None#



timeout: float | None#



tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None#



tools: List[Dict[str, Any]] | None#



top_k: int | None#



top_p: float | None#

```python
api_key: str#
```

```python
base_url: str | None#
```

```python
default_headers: Dict[str, str] | None#
```

```python
max_retries: int | None#
```

```python
max_tokens: int | None#
```

```python
metadata: Dict[str, str] | None#
```

```python
model: str#
```

```python
model_capabilities: ModelCapabilities#
```

```python
model_info: ModelInfo#
```

```python
response_format: ResponseFormat | None#
```

```python
stop_sequences: List[str] | None#
```

```python
temperature: float | None#
```

```python
timeout: float | None#
```

```python
tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None#
```

```python
tools: List[Dict[str, Any]] | None#
```

```python
top_k: int | None#
```

```python
top_p: float | None#
```

```python
pydantic model AnthropicClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: BaseAnthropicClientConfigurationConfigModel

Show JSON schema{
   "title": "AnthropicClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "tools": {
         "anyOf": [
            {
               "items": {
                  "type": "object"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tools"
      },
      "tool_choice": {
         "anyOf": [
            {
               "enum": [
                  "auto",
                  "any",
                  "none"
               ],
               "type": "string"
            },
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tool Choice"
      }
   },
   "$defs": {
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

tool_choice (Literal['auto', 'any', 'none'] | Dict[str, Any] | None)
tools (List[Dict[str, Any]] | None)





field tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None = None#



field tools: List[Dict[str, Any]] | None = None#

**示例**:
```python
{
   "title": "AnthropicClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "tools": {
         "anyOf": [
            {
               "items": {
                  "type": "object"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tools"
      },
      "tool_choice": {
         "anyOf": [
            {
               "enum": [
                  "auto",
                  "any",
                  "none"
               ],
               "type": "string"
            },
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Tool Choice"
      }
   },
   "$defs": {
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field tool_choice: Literal['auto', 'any', 'none'] | Dict[str, Any] | None = None#
```

```python
field tools: List[Dict[str, Any]] | None = None#
```

```python
class BaseAnthropicChatCompletionClient(client: Any, *, create_args: Dict[str, Any], model_info: ModelInfo | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient


actual_usage() → RequestUsage[source]#



property capabilities: ModelCapabilities#



async close() → None[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
Estimate the number of tokens used by messages and tools.
Note: This is an estimation based on common tokenization patterns and may not perfectly
match Anthropic’s exact token counting for Claude models.



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, max_consecutive_empty_chunk_tolerance: int = 0) → AsyncGenerator[str | CreateResult, None][source]#
Creates an AsyncGenerator that yields a stream of completions based on the provided messages and tools.



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
Calculate the remaining tokens based on the model’s token limit.



total_usage() → RequestUsage[source]#

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelCapabilities#
```

```python
async close() → None[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

【中文翻译】Estimate the number of tokens used by messages and tools.
Note: This is an estimation based on common tokenization patterns and may not perfectly
match Anthropic’s exact token counting for Claude models.

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, max_consecutive_empty_chunk_tolerance: int = 0) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates an AsyncGenerator that yields a stream of completions based on the provided messages and tools.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

【中文翻译】Calculate the remaining tokens based on the model’s token limit.

```python
total_usage() → RequestUsage[source]#
```

```python
class BedrockInfo[source]#
```

【中文翻译】Bases: TypedDict
BedrockInfo is a dictionary that contains information about a bedrock’s properties.
It is expected to be used in the bedrock_info property of a model client.


aws_access_key: Required[SecretStr]#
Access key for the aws account to gain bedrock model access



aws_region: Required[str]#
aws region for the aws account to gain bedrock model access



aws_secret_key: Required[SecretStr]#
Access secret key for the aws account to gain bedrock model access



aws_session_token: Required[SecretStr]#
aws session token for the aws account to gain bedrock model access

```python
aws_access_key: Required[SecretStr]#
```

【中文翻译】Access key for the aws account to gain bedrock model access

```python
aws_region: Required[str]#
```

【中文翻译】aws region for the aws account to gain bedrock model access

```python
aws_secret_key: Required[SecretStr]#
```

【中文翻译】Access secret key for the aws account to gain bedrock model access

```python
aws_session_token: Required[SecretStr]#
```

【中文翻译】aws session token for the aws account to gain bedrock model access

```python
pydantic model CreateArgumentsConfigModel[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      }
   },
   "$defs": {
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

max_tokens (int | None)
metadata (Dict[str, str] | None)
model (str)
response_format (autogen_ext.models.anthropic.config.ResponseFormat | None)
stop_sequences (List[str] | None)
temperature (float | None)
top_k (int | None)
top_p (float | None)





field max_tokens: int | None = 4096#



field metadata: Dict[str, str] | None = None#



field model: str [Required]#



field response_format: ResponseFormat | None = None#



field stop_sequences: List[str] | None = None#



field temperature: float | None = 1.0#



field top_k: int | None = None#



field top_p: float | None = None#

**示例**:
```python
{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 4096,
         "title": "Max Tokens"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": 1.0,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "top_k": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top K"
      },
      "stop_sequences": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Sequences"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Metadata"
      }
   },
   "$defs": {
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object"
               ],
               "title": "Type",
               "type": "string"
            }
         },
         "required": [
            "type"
         ],
         "title": "ResponseFormat",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field max_tokens: int | None = 4096#
```

```python
field metadata: Dict[str, str] | None = None#
```

```python
field model: str [Required]#
```

```python
field response_format: ResponseFormat | None = None#
```

```python
field stop_sequences: List[str] | None = None#
```

```python
field temperature: float | None = 1.0#
```

```python
field top_k: int | None = None#
```

```python
field top_p: float | None = None#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.azure

【中文翻译】next

【中文翻译】autogen_ext.models.semantic_kernel

### autogen_ext.models.azure {autogen_extmodelsazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html)

```python
class AzureAIChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: ChatCompletionClient
Chat completion client for models hosted on Azure AI Foundry or GitHub Models.
See here for more info.

Parameters:

endpoint (str) – The endpoint to use. Required.
credential (union, AzureKeyCredential, AsyncTokenCredential) – The credentials to use. Required
model_info (ModelInfo) – The model family and capabilities of the model. Required.
model (str) – The name of the model. Required if model is hosted on GitHub Models.
frequency_penalty – (optional,float)
presence_penalty – (optional,float)
temperature – (optional,float)
top_p – (optional,float)
max_tokens – (optional,int)
response_format – (optional, literal[“text”, “json_object”])
stop – (optional,List[str])
tools – (optional,List[ChatCompletionsToolDefinition])
tool_choice – (optional,Union[str, ChatCompletionsToolChoicePreset, ChatCompletionsNamedToolChoice]])
seed – (optional,int)
model_extras – (optional,Dict[str, Any])



To use this client, you must install the azure extra:
pip install "autogen-ext[azure]"


The following code snippet shows how to use the client with GitHub Models:
import asyncio
import os
from azure.core.credentials import AzureKeyCredential
from autogen_ext.models.azure import AzureAIChatCompletionClient
from autogen_core.models import UserMessage


async def main():
    client = AzureAIChatCompletionClient(
        model="Phi-4",
        endpoint="https://models.inference.ai.azure.com",
        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
        credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
        model_info={
            "json_output": False,
            "function_calling": False,
            "vision": False,
            "family": "unknown",
            "structured_output": False,
        },
    )

    result = await client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)

    # Close the client.
    await client.close()


if __name__ == "__main__":
    asyncio.run(main())


To use streaming, you can use the create_stream method:
import asyncio
import os

from autogen_core.models import UserMessage
from autogen_ext.models.azure import AzureAIChatCompletionClient
from azure.core.credentials import AzureKeyCredential


async def main():
    client = AzureAIChatCompletionClient(
        model="Phi-4",
        endpoint="https://models.inference.ai.azure.com",
        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
        credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
        model_info={
            "json_output": False,
            "function_calling": False,
            "vision": False,
            "family": "unknown",
            "structured_output": False,
        },
    )

    # Create a stream.
    stream = client.create_stream([UserMessage(content="Write a poem about the ocean", source="user")])
    async for chunk in stream:
        print(chunk, end="", flush=True)
    print()

    # Close the client.
    await client.close()


if __name__ == "__main__":
    asyncio.run(main())




actual_usage() → RequestUsage[source]#



add_usage(usage: RequestUsage) → None[source]#



property capabilities: ModelInfo#



async close() → None[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.





property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



total_usage() → RequestUsage[source]#

**示例**:
```python
pip install "autogen-ext[azure]"

```

**示例**:
```python
import asyncio
import os
from azure.core.credentials import AzureKeyCredential
from autogen_ext.models.azure import AzureAIChatCompletionClient
from autogen_core.models import UserMessage


async def main():
    client = AzureAIChatCompletionClient(
        model="Phi-4",
        endpoint="https://models.inference.ai.azure.com",
        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
        credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
        model_info={
            "json_output": False,
            "function_calling": False,
            "vision": False,
            "family": "unknown",
            "structured_output": False,
        },
    )

    result = await client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)

    # Close the client.
    await client.close()


if __name__ == "__main__":
    asyncio.run(main())

```

**示例**:
```python
import asyncio
import os

from autogen_core.models import UserMessage
from autogen_ext.models.azure import AzureAIChatCompletionClient
from azure.core.credentials import AzureKeyCredential


async def main():
    client = AzureAIChatCompletionClient(
        model="Phi-4",
        endpoint="https://models.inference.ai.azure.com",
        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
        credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
        model_info={
            "json_output": False,
            "function_calling": False,
            "vision": False,
            "family": "unknown",
            "structured_output": False,
        },
    )

    # Create a stream.
    stream = client.create_stream([UserMessage(content="Write a poem about the ocean", source="user")])
    async for chunk in stream:
        print(chunk, end="", flush=True)
    print()

    # Close the client.
    await client.close()


if __name__ == "__main__":
    asyncio.run(main())

```

```python
actual_usage() → RequestUsage[source]#
```

```python
add_usage(usage: RequestUsage) → None[source]#
```

```python
property capabilities: ModelInfo#
```

```python
async close() → None[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

```python
class AzureAIChatCompletionClientConfig[source]#
```

【中文翻译】Bases: dict


credential: AzureKeyCredential | AsyncTokenCredential#



endpoint: str#



frequency_penalty: float | None#



max_tokens: int | None#



model: str | None#



model_extras: Dict[str, Any] | None#



model_info: ModelInfo#



presence_penalty: float | None#



response_format: Literal['text', 'json_object'] | None#



seed: int | None#



stop: List[str] | None#



temperature: float | None#



tool_choice: str | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice | None#



tools: List[ChatCompletionsToolDefinition] | None#



top_p: float | None#

```python
credential: AzureKeyCredential | AsyncTokenCredential#
```

```python
endpoint: str#
```

```python
frequency_penalty: float | None#
```

```python
max_tokens: int | None#
```

```python
model: str | None#
```

```python
model_extras: Dict[str, Any] | None#
```

```python
model_info: ModelInfo#
```

```python
presence_penalty: float | None#
```

```python
response_format: Literal['text', 'json_object'] | None#
```

```python
seed: int | None#
```

```python
stop: List[str] | None#
```

```python
temperature: float | None#
```

```python
tool_choice: str | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice | None#
```

```python
tools: List[ChatCompletionsToolDefinition] | None#
```

```python
top_p: float | None#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.replay

【中文翻译】next

【中文翻译】autogen_ext.models.anthropic

### autogen_ext.models.azure {autogen_extmodelsazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html)

```python
class AzureAIChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: ChatCompletionClient
Chat completion client for models hosted on Azure AI Foundry or GitHub Models.
See here for more info.

Parameters:

endpoint (str) – The endpoint to use. Required.
credential (union, AzureKeyCredential, AsyncTokenCredential) – The credentials to use. Required
model_info (ModelInfo) – The model family and capabilities of the model. Required.
model (str) – The name of the model. Required if model is hosted on GitHub Models.
frequency_penalty – (optional,float)
presence_penalty – (optional,float)
temperature – (optional,float)
top_p – (optional,float)
max_tokens – (optional,int)
response_format – (optional, literal[“text”, “json_object”])
stop – (optional,List[str])
tools – (optional,List[ChatCompletionsToolDefinition])
tool_choice – (optional,Union[str, ChatCompletionsToolChoicePreset, ChatCompletionsNamedToolChoice]])
seed – (optional,int)
model_extras – (optional,Dict[str, Any])



To use this client, you must install the azure extra:
pip install "autogen-ext[azure]"


The following code snippet shows how to use the client with GitHub Models:
import asyncio
import os
from azure.core.credentials import AzureKeyCredential
from autogen_ext.models.azure import AzureAIChatCompletionClient
from autogen_core.models import UserMessage


async def main():
    client = AzureAIChatCompletionClient(
        model="Phi-4",
        endpoint="https://models.inference.ai.azure.com",
        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
        credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
        model_info={
            "json_output": False,
            "function_calling": False,
            "vision": False,
            "family": "unknown",
            "structured_output": False,
        },
    )

    result = await client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)

    # Close the client.
    await client.close()


if __name__ == "__main__":
    asyncio.run(main())


To use streaming, you can use the create_stream method:
import asyncio
import os

from autogen_core.models import UserMessage
from autogen_ext.models.azure import AzureAIChatCompletionClient
from azure.core.credentials import AzureKeyCredential


async def main():
    client = AzureAIChatCompletionClient(
        model="Phi-4",
        endpoint="https://models.inference.ai.azure.com",
        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
        credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
        model_info={
            "json_output": False,
            "function_calling": False,
            "vision": False,
            "family": "unknown",
            "structured_output": False,
        },
    )

    # Create a stream.
    stream = client.create_stream([UserMessage(content="Write a poem about the ocean", source="user")])
    async for chunk in stream:
        print(chunk, end="", flush=True)
    print()

    # Close the client.
    await client.close()


if __name__ == "__main__":
    asyncio.run(main())




actual_usage() → RequestUsage[source]#



add_usage(usage: RequestUsage) → None[source]#



property capabilities: ModelInfo#



async close() → None[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.





property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



total_usage() → RequestUsage[source]#

**示例**:
```python
pip install "autogen-ext[azure]"

```

**示例**:
```python
import asyncio
import os
from azure.core.credentials import AzureKeyCredential
from autogen_ext.models.azure import AzureAIChatCompletionClient
from autogen_core.models import UserMessage


async def main():
    client = AzureAIChatCompletionClient(
        model="Phi-4",
        endpoint="https://models.inference.ai.azure.com",
        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
        credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
        model_info={
            "json_output": False,
            "function_calling": False,
            "vision": False,
            "family": "unknown",
            "structured_output": False,
        },
    )

    result = await client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)

    # Close the client.
    await client.close()


if __name__ == "__main__":
    asyncio.run(main())

```

**示例**:
```python
import asyncio
import os

from autogen_core.models import UserMessage
from autogen_ext.models.azure import AzureAIChatCompletionClient
from azure.core.credentials import AzureKeyCredential


async def main():
    client = AzureAIChatCompletionClient(
        model="Phi-4",
        endpoint="https://models.inference.ai.azure.com",
        # To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
        # Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
        credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
        model_info={
            "json_output": False,
            "function_calling": False,
            "vision": False,
            "family": "unknown",
            "structured_output": False,
        },
    )

    # Create a stream.
    stream = client.create_stream([UserMessage(content="Write a poem about the ocean", source="user")])
    async for chunk in stream:
        print(chunk, end="", flush=True)
    print()

    # Close the client.
    await client.close()


if __name__ == "__main__":
    asyncio.run(main())

```

```python
actual_usage() → RequestUsage[source]#
```

```python
add_usage(usage: RequestUsage) → None[source]#
```

```python
property capabilities: ModelInfo#
```

```python
async close() → None[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

```python
class AzureAIChatCompletionClientConfig[source]#
```

【中文翻译】Bases: dict


credential: AzureKeyCredential | AsyncTokenCredential#



endpoint: str#



frequency_penalty: float | None#



max_tokens: int | None#



model: str | None#



model_extras: Dict[str, Any] | None#



model_info: ModelInfo#



presence_penalty: float | None#



response_format: Literal['text', 'json_object'] | None#



seed: int | None#



stop: List[str] | None#



temperature: float | None#



tool_choice: str | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice | None#



tools: List[ChatCompletionsToolDefinition] | None#



top_p: float | None#

```python
credential: AzureKeyCredential | AsyncTokenCredential#
```

```python
endpoint: str#
```

```python
frequency_penalty: float | None#
```

```python
max_tokens: int | None#
```

```python
model: str | None#
```

```python
model_extras: Dict[str, Any] | None#
```

```python
model_info: ModelInfo#
```

```python
presence_penalty: float | None#
```

```python
response_format: Literal['text', 'json_object'] | None#
```

```python
seed: int | None#
```

```python
stop: List[str] | None#
```

```python
temperature: float | None#
```

```python
tool_choice: str | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice | None#
```

```python
tools: List[ChatCompletionsToolDefinition] | None#
```

```python
top_p: float | None#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.replay

【中文翻译】next

【中文翻译】autogen_ext.models.anthropic

### autogen_ext.models.cache {autogen_extmodelscache}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html)

```python
class ChatCompletionCache(client: ChatCompletionClient, store: CacheStore[CreateResult | List[str | CreateResult]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient, Component[ChatCompletionCacheConfig]
A wrapper around a ChatCompletionClient that caches
creation results from an underlying client.
Cache hits do not contribute to token usage of the original client.
Typical Usage:
Lets use caching on disk with openai client as an example.
First install autogen-ext with the required packages:
pip install -U "autogen-ext[openai, diskcache]"


And use it as:
import asyncio
import tempfile

from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
from autogen_ext.cache_store.diskcache import DiskCacheStore
from diskcache import Cache


async def main():
    with tempfile.TemporaryDirectory() as tmpdirname:
        # Initialize the original client
        openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")

        # Then initialize the CacheStore, in this case with diskcache.Cache.
        # You can also use redis like:
        # from autogen_ext.cache_store.redis import RedisStore
        # import redis
        # redis_instance = redis.Redis()
        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)
        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
        cache_client = ChatCompletionCache(openai_model_client, cache_store)

        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print response from OpenAI
        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print cached response


asyncio.run(main())


You can now use the cached_client as you would the original client, but with caching enabled.

Parameters:

client (ChatCompletionClient) – The original ChatCompletionClient to wrap.
store (CacheStore) – A store object that implements get and set methods.
The user is responsible for managing the store’s lifecycle & clearing it (if needed).
Defaults to using in-memory cache.





classmethod _from_config(config: ChatCompletionCacheConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → ChatCompletionCacheConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





actual_usage() → RequestUsage[source]#



property capabilities: ModelCapabilities#



async close() → None[source]#



component_config_schema#
alias of ChatCompletionCacheConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.cache.ChatCompletionCache'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'chat_completion_cache'#
The logical type of the component.



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Cached version of ChatCompletionClient.create.
If the result of a call to create has been cached, it will be returned immediately
without invoking the underlying client.
NOTE: cancellation_token is ignored for cached results.



create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Cached version of ChatCompletionClient.create_stream.
If the result of a call to create_stream has been cached, it will be returned
without streaming from the underlying client.
NOTE: cancellation_token is ignored for cached results.



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



total_usage() → RequestUsage[source]#

**示例**:
```python
pip install -U "autogen-ext[openai, diskcache]"

```

**示例**:
```python
import asyncio
import tempfile

from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
from autogen_ext.cache_store.diskcache import DiskCacheStore
from diskcache import Cache


async def main():
    with tempfile.TemporaryDirectory() as tmpdirname:
        # Initialize the original client
        openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")

        # Then initialize the CacheStore, in this case with diskcache.Cache.
        # You can also use redis like:
        # from autogen_ext.cache_store.redis import RedisStore
        # import redis
        # redis_instance = redis.Redis()
        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)
        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
        cache_client = ChatCompletionCache(openai_model_client, cache_store)

        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print response from OpenAI
        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print cached response


asyncio.run(main())

```

```python
classmethod _from_config(config: ChatCompletionCacheConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → ChatCompletionCacheConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelCapabilities#
```

```python
async close() → None[source]#
```

```python
component_config_schema#
```

【中文翻译】alias of ChatCompletionCacheConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.cache.ChatCompletionCache'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'chat_completion_cache'#
```

【中文翻译】The logical type of the component.

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Cached version of ChatCompletionClient.create.
If the result of a call to create has been cached, it will be returned immediately
without invoking the underlying client.
NOTE: cancellation_token is ignored for cached results.

```python
create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Cached version of ChatCompletionClient.create_stream.
If the result of a call to create_stream has been cached, it will be returned
without streaming from the underlying client.
NOTE: cancellation_token is ignored for cached results.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.teams.magentic_one

【中文翻译】next

【中文翻译】autogen_ext.models.openai

### autogen_ext.models.cache {autogen_extmodelscache}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html)

```python
class ChatCompletionCache(client: ChatCompletionClient, store: CacheStore[CreateResult | List[str | CreateResult]] | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient, Component[ChatCompletionCacheConfig]
A wrapper around a ChatCompletionClient that caches
creation results from an underlying client.
Cache hits do not contribute to token usage of the original client.
Typical Usage:
Lets use caching on disk with openai client as an example.
First install autogen-ext with the required packages:
pip install -U "autogen-ext[openai, diskcache]"


And use it as:
import asyncio
import tempfile

from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
from autogen_ext.cache_store.diskcache import DiskCacheStore
from diskcache import Cache


async def main():
    with tempfile.TemporaryDirectory() as tmpdirname:
        # Initialize the original client
        openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")

        # Then initialize the CacheStore, in this case with diskcache.Cache.
        # You can also use redis like:
        # from autogen_ext.cache_store.redis import RedisStore
        # import redis
        # redis_instance = redis.Redis()
        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)
        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
        cache_client = ChatCompletionCache(openai_model_client, cache_store)

        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print response from OpenAI
        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print cached response


asyncio.run(main())


You can now use the cached_client as you would the original client, but with caching enabled.

Parameters:

client (ChatCompletionClient) – The original ChatCompletionClient to wrap.
store (CacheStore) – A store object that implements get and set methods.
The user is responsible for managing the store’s lifecycle & clearing it (if needed).
Defaults to using in-memory cache.





classmethod _from_config(config: ChatCompletionCacheConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → ChatCompletionCacheConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





actual_usage() → RequestUsage[source]#



property capabilities: ModelCapabilities#



async close() → None[source]#



component_config_schema#
alias of ChatCompletionCacheConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.cache.ChatCompletionCache'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'chat_completion_cache'#
The logical type of the component.



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Cached version of ChatCompletionClient.create.
If the result of a call to create has been cached, it will be returned immediately
without invoking the underlying client.
NOTE: cancellation_token is ignored for cached results.



create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Cached version of ChatCompletionClient.create_stream.
If the result of a call to create_stream has been cached, it will be returned
without streaming from the underlying client.
NOTE: cancellation_token is ignored for cached results.



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



total_usage() → RequestUsage[source]#

**示例**:
```python
pip install -U "autogen-ext[openai, diskcache]"

```

**示例**:
```python
import asyncio
import tempfile

from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
from autogen_ext.cache_store.diskcache import DiskCacheStore
from diskcache import Cache


async def main():
    with tempfile.TemporaryDirectory() as tmpdirname:
        # Initialize the original client
        openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")

        # Then initialize the CacheStore, in this case with diskcache.Cache.
        # You can also use redis like:
        # from autogen_ext.cache_store.redis import RedisStore
        # import redis
        # redis_instance = redis.Redis()
        # cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)
        cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
        cache_client = ChatCompletionCache(openai_model_client, cache_store)

        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print response from OpenAI
        response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
        print(response)  # Should print cached response


asyncio.run(main())

```

```python
classmethod _from_config(config: ChatCompletionCacheConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → ChatCompletionCacheConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelCapabilities#
```

```python
async close() → None[source]#
```

```python
component_config_schema#
```

【中文翻译】alias of ChatCompletionCacheConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.cache.ChatCompletionCache'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'chat_completion_cache'#
```

【中文翻译】The logical type of the component.

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Cached version of ChatCompletionClient.create.
If the result of a call to create has been cached, it will be returned immediately
without invoking the underlying client.
NOTE: cancellation_token is ignored for cached results.

```python
create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Cached version of ChatCompletionClient.create_stream.
If the result of a call to create_stream has been cached, it will be returned
without streaming from the underlying client.
NOTE: cancellation_token is ignored for cached results.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.teams.magentic_one

【中文翻译】next

【中文翻译】autogen_ext.models.openai

### autogen_ext.models.llama_cpp {autogen_extmodelsllama_cpp}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html)

```python
class LlamaCppChatCompletionClient(model_info: ModelInfo | None = None, **kwargs: Unpack)[source]#
```

【中文翻译】Bases: ChatCompletionClient
Chat completion client for LlamaCpp models.
To use this client, you must install the llama-cpp extra:
pip install "autogen-ext[llama-cpp]"


This client allows you to interact with LlamaCpp models, either by specifying a local model path or by downloading a model from Hugging Face Hub.

Parameters:

model_info (optional, ModelInfo) – The information about the model. Defaults to DEFAULT_MODEL_INFO.
model_path (optional, str) – The path to the LlamaCpp model file. Required if repo_id and filename are not provided.
repo_id (optional, str) – The Hugging Face Hub repository ID. Required if model_path is not provided.
filename (optional, str) – The filename of the model within the Hugging Face Hub repository. Required if model_path is not provided.
n_gpu_layers (optional, int) – The number of layers to put on the GPU.
n_ctx (optional, int) – The context size.
n_batch (optional, int) – The batch size.
verbose (optional, bool) – Whether to print verbose output.
**kwargs – Additional parameters to pass to the Llama class.



Examples
The following code snippet shows how to use the client with a local model file:
import asyncio

from autogen_core.models import UserMessage
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient


async def main():
    llama_client = LlamaCppChatCompletionClient(model_path="/path/to/your/model.gguf")
    result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)


asyncio.run(main())


The following code snippet shows how to use the client with a model from Hugging Face Hub:
import asyncio

from autogen_core.models import UserMessage
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient


async def main():
    llama_client = LlamaCppChatCompletionClient(
        repo_id="unsloth/phi-4-GGUF", filename="phi-4-Q2_K_L.gguf", n_gpu_layers=-1, seed=1337, n_ctx=5000
    )
    result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)


asyncio.run(main())




DEFAULT_MODEL_INFO: ModelInfo = {'family': 'unknown', 'function_calling': True, 'json_output': True, 'structured_output': True, 'vision': False}#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.





actual_usage() → RequestUsage[source]#



property capabilities: ModelInfo#



count_tokens(messages: Sequence[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage], **kwargs: Any) → int[source]#



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage], **kwargs: Any) → int[source]#



total_usage() → RequestUsage[source]#



async close() → None[source]#
Close the LlamaCpp client.

**示例**:
```python
pip install "autogen-ext[llama-cpp]"

```

**示例**:
```python
import asyncio

from autogen_core.models import UserMessage
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient


async def main():
    llama_client = LlamaCppChatCompletionClient(model_path="/path/to/your/model.gguf")
    result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_core.models import UserMessage
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient


async def main():
    llama_client = LlamaCppChatCompletionClient(
        repo_id="unsloth/phi-4-GGUF", filename="phi-4-Q2_K_L.gguf", n_gpu_layers=-1, seed=1337, n_ctx=5000
    )
    result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)


asyncio.run(main())

```

```python
DEFAULT_MODEL_INFO: ModelInfo = {'family': 'unknown', 'function_calling': True, 'json_output': True, 'structured_output': True, 'vision': False}#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelInfo#
```

```python
count_tokens(messages: Sequence[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage], **kwargs: Any) → int[source]#
```

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage], **kwargs: Any) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

```python
async close() → None[source]#
```

【中文翻译】Close the LlamaCpp client.

【中文翻译】previous

【中文翻译】autogen_ext.models.ollama

【中文翻译】next

【中文翻译】autogen_ext.tools.azure

### autogen_ext.models.llama_cpp {autogen_extmodelsllama_cpp}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html)

```python
class LlamaCppChatCompletionClient(model_info: ModelInfo | None = None, **kwargs: Unpack)[source]#
```

【中文翻译】Bases: ChatCompletionClient
Chat completion client for LlamaCpp models.
To use this client, you must install the llama-cpp extra:
pip install "autogen-ext[llama-cpp]"


This client allows you to interact with LlamaCpp models, either by specifying a local model path or by downloading a model from Hugging Face Hub.

Parameters:

model_info (optional, ModelInfo) – The information about the model. Defaults to DEFAULT_MODEL_INFO.
model_path (optional, str) – The path to the LlamaCpp model file. Required if repo_id and filename are not provided.
repo_id (optional, str) – The Hugging Face Hub repository ID. Required if model_path is not provided.
filename (optional, str) – The filename of the model within the Hugging Face Hub repository. Required if model_path is not provided.
n_gpu_layers (optional, int) – The number of layers to put on the GPU.
n_ctx (optional, int) – The context size.
n_batch (optional, int) – The batch size.
verbose (optional, bool) – Whether to print verbose output.
**kwargs – Additional parameters to pass to the Llama class.



Examples
The following code snippet shows how to use the client with a local model file:
import asyncio

from autogen_core.models import UserMessage
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient


async def main():
    llama_client = LlamaCppChatCompletionClient(model_path="/path/to/your/model.gguf")
    result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)


asyncio.run(main())


The following code snippet shows how to use the client with a model from Hugging Face Hub:
import asyncio

from autogen_core.models import UserMessage
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient


async def main():
    llama_client = LlamaCppChatCompletionClient(
        repo_id="unsloth/phi-4-GGUF", filename="phi-4-Q2_K_L.gguf", n_gpu_layers=-1, seed=1337, n_ctx=5000
    )
    result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)


asyncio.run(main())




DEFAULT_MODEL_INFO: ModelInfo = {'family': 'unknown', 'function_calling': True, 'json_output': True, 'structured_output': True, 'vision': False}#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.





actual_usage() → RequestUsage[source]#



property capabilities: ModelInfo#



count_tokens(messages: Sequence[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage], **kwargs: Any) → int[source]#



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage], **kwargs: Any) → int[source]#



total_usage() → RequestUsage[source]#



async close() → None[source]#
Close the LlamaCpp client.

**示例**:
```python
pip install "autogen-ext[llama-cpp]"

```

**示例**:
```python
import asyncio

from autogen_core.models import UserMessage
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient


async def main():
    llama_client = LlamaCppChatCompletionClient(model_path="/path/to/your/model.gguf")
    result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_core.models import UserMessage
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient


async def main():
    llama_client = LlamaCppChatCompletionClient(
        repo_id="unsloth/phi-4-GGUF", filename="phi-4-Q2_K_L.gguf", n_gpu_layers=-1, seed=1337, n_ctx=5000
    )
    result = await llama_client.create([UserMessage(content="What is the capital of France?", source="user")])
    print(result)


asyncio.run(main())

```

```python
DEFAULT_MODEL_INFO: ModelInfo = {'family': 'unknown', 'function_calling': True, 'json_output': True, 'structured_output': True, 'vision': False}#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Creates a stream of string chunks from the model ending with a CreateResult.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.

extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult.

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelInfo#
```

```python
count_tokens(messages: Sequence[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage], **kwargs: Any) → int[source]#
```

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage], **kwargs: Any) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

```python
async close() → None[source]#
```

【中文翻译】Close the LlamaCpp client.

【中文翻译】previous

【中文翻译】autogen_ext.models.ollama

【中文翻译】next

【中文翻译】autogen_ext.tools.azure

### autogen_ext.models.ollama {autogen_extmodelsollama}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html)

```python
pydantic model BaseOllamaClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: CreateArgumentsConfigModel

Show JSON schema{
   "title": "BaseOllamaClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "host": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Host"
      },
      "response_format": {
         "default": null,
         "title": "Response Format"
      },
      "follow_redirects": {
         "default": true,
         "title": "Follow Redirects",
         "type": "boolean"
      },
      "timeout": {
         "default": null,
         "title": "Timeout"
      },
      "headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Headers"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "options": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "$ref": "#/$defs/Options"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Options"
      }
   },
   "$defs": {
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "Options": {
         "properties": {
            "numa": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Numa"
            },
            "num_ctx": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Ctx"
            },
            "num_batch": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Batch"
            },
            "num_gpu": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Gpu"
            },
            "main_gpu": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Main Gpu"
            },
            "low_vram": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Low Vram"
            },
            "f16_kv": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "F16 Kv"
            },
            "logits_all": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Logits All"
            },
            "vocab_only": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Vocab Only"
            },
            "use_mmap": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Use Mmap"
            },
            "use_mlock": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Use Mlock"
            },
            "embedding_only": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Embedding Only"
            },
            "num_thread": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Thread"
            },
            "num_keep": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Keep"
            },
            "seed": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Seed"
            },
            "num_predict": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Predict"
            },
            "top_k": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top K"
            },
            "top_p": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top P"
            },
            "tfs_z": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Tfs Z"
            },
            "typical_p": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Typical P"
            },
            "repeat_last_n": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Repeat Last N"
            },
            "temperature": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Temperature"
            },
            "repeat_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Repeat Penalty"
            },
            "presence_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Presence Penalty"
            },
            "frequency_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Frequency Penalty"
            },
            "mirostat": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat"
            },
            "mirostat_tau": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat Tau"
            },
            "mirostat_eta": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat Eta"
            },
            "penalize_newline": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Penalize Newline"
            },
            "stop": {
               "anyOf": [
                  {
                     "items": {
                        "type": "string"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Stop"
            }
         },
         "title": "Options",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

follow_redirects (bool)
headers (Mapping[str, str] | None)
model_capabilities (autogen_core.models._model_client.ModelCapabilities | None)
model_info (autogen_core.models._model_client.ModelInfo | None)
options (Mapping[str, Any] | ollama._types.Options | None)
timeout (Any)





field follow_redirects: bool = True#



field headers: Mapping[str, str] | None = None#



field model_capabilities: ModelCapabilities | None = None#



field model_info: ModelInfo | None = None#



field options: Mapping[str, Any] | Options | None = None#



field timeout: Any = None#

**示例**:
```python
{
   "title": "BaseOllamaClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "host": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Host"
      },
      "response_format": {
         "default": null,
         "title": "Response Format"
      },
      "follow_redirects": {
         "default": true,
         "title": "Follow Redirects",
         "type": "boolean"
      },
      "timeout": {
         "default": null,
         "title": "Timeout"
      },
      "headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Headers"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "options": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "$ref": "#/$defs/Options"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Options"
      }
   },
   "$defs": {
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "Options": {
         "properties": {
            "numa": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Numa"
            },
            "num_ctx": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Ctx"
            },
            "num_batch": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Batch"
            },
            "num_gpu": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Gpu"
            },
            "main_gpu": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Main Gpu"
            },
            "low_vram": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Low Vram"
            },
            "f16_kv": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "F16 Kv"
            },
            "logits_all": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Logits All"
            },
            "vocab_only": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Vocab Only"
            },
            "use_mmap": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Use Mmap"
            },
            "use_mlock": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Use Mlock"
            },
            "embedding_only": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Embedding Only"
            },
            "num_thread": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Thread"
            },
            "num_keep": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Keep"
            },
            "seed": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Seed"
            },
            "num_predict": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Predict"
            },
            "top_k": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top K"
            },
            "top_p": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top P"
            },
            "tfs_z": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Tfs Z"
            },
            "typical_p": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Typical P"
            },
            "repeat_last_n": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Repeat Last N"
            },
            "temperature": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Temperature"
            },
            "repeat_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Repeat Penalty"
            },
            "presence_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Presence Penalty"
            },
            "frequency_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Frequency Penalty"
            },
            "mirostat": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat"
            },
            "mirostat_tau": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat Tau"
            },
            "mirostat_eta": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat Eta"
            },
            "penalize_newline": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Penalize Newline"
            },
            "stop": {
               "anyOf": [
                  {
                     "items": {
                        "type": "string"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Stop"
            }
         },
         "title": "Options",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field follow_redirects: bool = True#
```

```python
field headers: Mapping[str, str] | None = None#
```

```python
field model_capabilities: ModelCapabilities | None = None#
```

```python
field model_info: ModelInfo | None = None#
```

```python
field options: Mapping[str, Any] | Options | None = None#
```

```python
field timeout: Any = None#
```

```python
pydantic model CreateArgumentsConfigModel[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "host": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Host"
      },
      "response_format": {
         "default": null,
         "title": "Response Format"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

host (str | None)
model (str)
response_format (Any)





field host: str | None = None#



field model: str [Required]#



field response_format: Any = None#

**示例**:
```python
{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "host": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Host"
      },
      "response_format": {
         "default": null,
         "title": "Response Format"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field host: str | None = None#
```

```python
field model: str [Required]#
```

```python
field response_format: Any = None#
```

```python
class OllamaChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: BaseOllamaChatCompletionClient, Component[BaseOllamaClientConfigurationConfigModel]
Chat completion client for Ollama hosted models.
Ollama must be installed and the appropriate model pulled.

Parameters:

model (str) – Which Ollama model to use.
host (optional, str) – Model host url.
response_format (optional, pydantic.BaseModel) – The format of the response. If provided, the response will be parsed into this format as json.
options (optional, Mapping[str, Any] | Options) – Additional options to pass to the Ollama client.
model_info (optional, ModelInfo) – The capabilities of the model. Required if the model is not listed in the ollama model info.




Note
Only models with 200k+ downloads (as of Jan 21, 2025), + phi4, deepseek-r1 have pre-defined model infos. See this file for the full list. An entry for one model encompases all parameter variants of that model.

To use this client, you must install the ollama extension:
pip install "autogen-ext[ollama]"


The following code snippet shows how to use the client with an Ollama model:
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage

ollama_client = OllamaChatCompletionClient(
    model="llama3",
)

result = await ollama_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
print(result)


To load the client from a configuration, you can use the load_component method:
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OllamaChatCompletionClient",
    "config": {"model": "llama3"},
}

client = ChatCompletionClient.load_component(config)


To output structured data, you can use the response_format argument:
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage
from pydantic import BaseModel


class StructuredOutput(BaseModel):
    first_name: str
    last_name: str


ollama_client = OllamaChatCompletionClient(
    model="llama3",
    response_format=StructuredOutput,
)
result = await ollama_client.create([UserMessage(content="Who was the first man on the moon?", source="user")])  # type: ignore
print(result)



Note
Tool usage in ollama is stricter than in its OpenAI counterparts. While OpenAI accepts a map of [str, Any], Ollama requires a map of [str, Property] where Property is a typed object containing type and description fields. Therefore, only the keys type and description will be converted from the properties blob in the tool schema.

To view the full list of available configuration options, see the OllamaClientConfigurationConfigModel class.


classmethod _from_config(config: BaseOllamaClientConfigurationConfigModel) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → BaseOllamaClientConfigurationConfigModel[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of BaseOllamaClientConfigurationConfigModel



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.ollama.OllamaChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'model'#
The logical type of the component.

**示例**:
```python
pip install "autogen-ext[ollama]"

```

**示例**:
```python
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage

ollama_client = OllamaChatCompletionClient(
    model="llama3",
)

result = await ollama_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
print(result)

```

**示例**:
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OllamaChatCompletionClient",
    "config": {"model": "llama3"},
}

client = ChatCompletionClient.load_component(config)

```

**示例**:
```python
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage
from pydantic import BaseModel


class StructuredOutput(BaseModel):
    first_name: str
    last_name: str


ollama_client = OllamaChatCompletionClient(
    model="llama3",
    response_format=StructuredOutput,
)
result = await ollama_client.create([UserMessage(content="Who was the first man on the moon?", source="user")])  # type: ignore
print(result)

```

```python
classmethod _from_config(config: BaseOllamaClientConfigurationConfigModel) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → BaseOllamaClientConfigurationConfigModel[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of BaseOllamaClientConfigurationConfigModel

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.ollama.OllamaChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'model'#
```

【中文翻译】The logical type of the component.

【中文翻译】previous

【中文翻译】autogen_ext.models.semantic_kernel

【中文翻译】next

【中文翻译】autogen_ext.models.llama_cpp

### autogen_ext.models.ollama {autogen_extmodelsollama}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html)

```python
pydantic model BaseOllamaClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: CreateArgumentsConfigModel

Show JSON schema{
   "title": "BaseOllamaClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "host": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Host"
      },
      "response_format": {
         "default": null,
         "title": "Response Format"
      },
      "follow_redirects": {
         "default": true,
         "title": "Follow Redirects",
         "type": "boolean"
      },
      "timeout": {
         "default": null,
         "title": "Timeout"
      },
      "headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Headers"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "options": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "$ref": "#/$defs/Options"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Options"
      }
   },
   "$defs": {
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "Options": {
         "properties": {
            "numa": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Numa"
            },
            "num_ctx": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Ctx"
            },
            "num_batch": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Batch"
            },
            "num_gpu": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Gpu"
            },
            "main_gpu": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Main Gpu"
            },
            "low_vram": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Low Vram"
            },
            "f16_kv": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "F16 Kv"
            },
            "logits_all": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Logits All"
            },
            "vocab_only": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Vocab Only"
            },
            "use_mmap": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Use Mmap"
            },
            "use_mlock": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Use Mlock"
            },
            "embedding_only": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Embedding Only"
            },
            "num_thread": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Thread"
            },
            "num_keep": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Keep"
            },
            "seed": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Seed"
            },
            "num_predict": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Predict"
            },
            "top_k": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top K"
            },
            "top_p": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top P"
            },
            "tfs_z": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Tfs Z"
            },
            "typical_p": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Typical P"
            },
            "repeat_last_n": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Repeat Last N"
            },
            "temperature": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Temperature"
            },
            "repeat_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Repeat Penalty"
            },
            "presence_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Presence Penalty"
            },
            "frequency_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Frequency Penalty"
            },
            "mirostat": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat"
            },
            "mirostat_tau": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat Tau"
            },
            "mirostat_eta": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat Eta"
            },
            "penalize_newline": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Penalize Newline"
            },
            "stop": {
               "anyOf": [
                  {
                     "items": {
                        "type": "string"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Stop"
            }
         },
         "title": "Options",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

follow_redirects (bool)
headers (Mapping[str, str] | None)
model_capabilities (autogen_core.models._model_client.ModelCapabilities | None)
model_info (autogen_core.models._model_client.ModelInfo | None)
options (Mapping[str, Any] | ollama._types.Options | None)
timeout (Any)





field follow_redirects: bool = True#



field headers: Mapping[str, str] | None = None#



field model_capabilities: ModelCapabilities | None = None#



field model_info: ModelInfo | None = None#



field options: Mapping[str, Any] | Options | None = None#



field timeout: Any = None#

**示例**:
```python
{
   "title": "BaseOllamaClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "host": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Host"
      },
      "response_format": {
         "default": null,
         "title": "Response Format"
      },
      "follow_redirects": {
         "default": true,
         "title": "Follow Redirects",
         "type": "boolean"
      },
      "timeout": {
         "default": null,
         "title": "Timeout"
      },
      "headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Headers"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "options": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "$ref": "#/$defs/Options"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Options"
      }
   },
   "$defs": {
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "Options": {
         "properties": {
            "numa": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Numa"
            },
            "num_ctx": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Ctx"
            },
            "num_batch": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Batch"
            },
            "num_gpu": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Gpu"
            },
            "main_gpu": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Main Gpu"
            },
            "low_vram": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Low Vram"
            },
            "f16_kv": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "F16 Kv"
            },
            "logits_all": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Logits All"
            },
            "vocab_only": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Vocab Only"
            },
            "use_mmap": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Use Mmap"
            },
            "use_mlock": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Use Mlock"
            },
            "embedding_only": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Embedding Only"
            },
            "num_thread": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Thread"
            },
            "num_keep": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Keep"
            },
            "seed": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Seed"
            },
            "num_predict": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Num Predict"
            },
            "top_k": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top K"
            },
            "top_p": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Top P"
            },
            "tfs_z": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Tfs Z"
            },
            "typical_p": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Typical P"
            },
            "repeat_last_n": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Repeat Last N"
            },
            "temperature": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Temperature"
            },
            "repeat_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Repeat Penalty"
            },
            "presence_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Presence Penalty"
            },
            "frequency_penalty": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Frequency Penalty"
            },
            "mirostat": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat"
            },
            "mirostat_tau": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat Tau"
            },
            "mirostat_eta": {
               "anyOf": [
                  {
                     "type": "number"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Mirostat Eta"
            },
            "penalize_newline": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Penalize Newline"
            },
            "stop": {
               "anyOf": [
                  {
                     "items": {
                        "type": "string"
                     },
                     "type": "array"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Stop"
            }
         },
         "title": "Options",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field follow_redirects: bool = True#
```

```python
field headers: Mapping[str, str] | None = None#
```

```python
field model_capabilities: ModelCapabilities | None = None#
```

```python
field model_info: ModelInfo | None = None#
```

```python
field options: Mapping[str, Any] | Options | None = None#
```

```python
field timeout: Any = None#
```

```python
pydantic model CreateArgumentsConfigModel[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "host": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Host"
      },
      "response_format": {
         "default": null,
         "title": "Response Format"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

host (str | None)
model (str)
response_format (Any)





field host: str | None = None#



field model: str [Required]#



field response_format: Any = None#

**示例**:
```python
{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "model": {
         "title": "Model",
         "type": "string"
      },
      "host": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Host"
      },
      "response_format": {
         "default": null,
         "title": "Response Format"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field host: str | None = None#
```

```python
field model: str [Required]#
```

```python
field response_format: Any = None#
```

```python
class OllamaChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: BaseOllamaChatCompletionClient, Component[BaseOllamaClientConfigurationConfigModel]
Chat completion client for Ollama hosted models.
Ollama must be installed and the appropriate model pulled.

Parameters:

model (str) – Which Ollama model to use.
host (optional, str) – Model host url.
response_format (optional, pydantic.BaseModel) – The format of the response. If provided, the response will be parsed into this format as json.
options (optional, Mapping[str, Any] | Options) – Additional options to pass to the Ollama client.
model_info (optional, ModelInfo) – The capabilities of the model. Required if the model is not listed in the ollama model info.




Note
Only models with 200k+ downloads (as of Jan 21, 2025), + phi4, deepseek-r1 have pre-defined model infos. See this file for the full list. An entry for one model encompases all parameter variants of that model.

To use this client, you must install the ollama extension:
pip install "autogen-ext[ollama]"


The following code snippet shows how to use the client with an Ollama model:
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage

ollama_client = OllamaChatCompletionClient(
    model="llama3",
)

result = await ollama_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
print(result)


To load the client from a configuration, you can use the load_component method:
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OllamaChatCompletionClient",
    "config": {"model": "llama3"},
}

client = ChatCompletionClient.load_component(config)


To output structured data, you can use the response_format argument:
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage
from pydantic import BaseModel


class StructuredOutput(BaseModel):
    first_name: str
    last_name: str


ollama_client = OllamaChatCompletionClient(
    model="llama3",
    response_format=StructuredOutput,
)
result = await ollama_client.create([UserMessage(content="Who was the first man on the moon?", source="user")])  # type: ignore
print(result)



Note
Tool usage in ollama is stricter than in its OpenAI counterparts. While OpenAI accepts a map of [str, Any], Ollama requires a map of [str, Property] where Property is a typed object containing type and description fields. Therefore, only the keys type and description will be converted from the properties blob in the tool schema.

To view the full list of available configuration options, see the OllamaClientConfigurationConfigModel class.


classmethod _from_config(config: BaseOllamaClientConfigurationConfigModel) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → BaseOllamaClientConfigurationConfigModel[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of BaseOllamaClientConfigurationConfigModel



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.ollama.OllamaChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'model'#
The logical type of the component.

**示例**:
```python
pip install "autogen-ext[ollama]"

```

**示例**:
```python
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage

ollama_client = OllamaChatCompletionClient(
    model="llama3",
)

result = await ollama_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
print(result)

```

**示例**:
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OllamaChatCompletionClient",
    "config": {"model": "llama3"},
}

client = ChatCompletionClient.load_component(config)

```

**示例**:
```python
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.models import UserMessage
from pydantic import BaseModel


class StructuredOutput(BaseModel):
    first_name: str
    last_name: str


ollama_client = OllamaChatCompletionClient(
    model="llama3",
    response_format=StructuredOutput,
)
result = await ollama_client.create([UserMessage(content="Who was the first man on the moon?", source="user")])  # type: ignore
print(result)

```

```python
classmethod _from_config(config: BaseOllamaClientConfigurationConfigModel) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → BaseOllamaClientConfigurationConfigModel[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of BaseOllamaClientConfigurationConfigModel

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.ollama.OllamaChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'model'#
```

【中文翻译】The logical type of the component.

【中文翻译】previous

【中文翻译】autogen_ext.models.semantic_kernel

【中文翻译】next

【中文翻译】autogen_ext.models.llama_cpp

### autogen_ext.models.openai {autogen_extmodelsopenai}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html)

```python
class OpenAIChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: BaseOpenAIChatCompletionClient, Component[OpenAIClientConfigurationConfigModel]
Chat completion client for OpenAI hosted models.
To use this client, you must install the openai extra:
pip install "autogen-ext[openai]"


You can also use this client for OpenAI-compatible ChatCompletion endpoints.
Using this client for non-OpenAI models is not tested or guaranteed.
For non-OpenAI models, please first take a look at our community extensions
for additional model clients.

Parameters:

model (str) – Which OpenAI model to use.
api_key (optional, str) – The API key to use. Required if ‘OPENAI_API_KEY’ is not found in the environment variables.
organization (optional, str) – The organization ID to use.
base_url (optional, str) – The base URL to use. Required if the model is not hosted on OpenAI.
timeout – (optional, float): The timeout for the request in seconds.
max_retries (optional, int) – The maximum number of retries to attempt.
model_info (optional, ModelInfo) – The capabilities of the model. Required if the model name is not a valid OpenAI model.
frequency_penalty (optional, float)
logit_bias – (optional, dict[str, int]):
max_tokens (optional, int)
n (optional, int)
presence_penalty (optional, float)
response_format (optional, Dict[str, Any]) – the format of the response. Possible options are:
# Text response, this is the default.
{"type": "text"}


# JSON response, make sure to instruct the model to return JSON.
{"type": "json_object"}


# Structured output response, with a pre-defined JSON schema.
{
    "type": "json_schema",
    "json_schema": {
        "name": "name of the schema, must be an identifier.",
        "description": "description for the model.",
        # You can convert a Pydantic (v2) model to JSON schema
        # using the `model_json_schema()` method.
        "schema": "<the JSON schema itself>",
        # Whether to enable strict schema adherence when
        # generating the output. If set to true, the model will
        # always follow the exact schema defined in the
        # `schema` field. Only a subset of JSON Schema is
        # supported when `strict` is `true`.
        # To learn more, read
        # https://platform.openai.com/docs/guides/structured-outputs.
        "strict": False,  # or True
    },
}


It is recommended to use the json_output parameter in
create() or
create_stream()
methods instead of response_format for structured output.
The json_output parameter is more flexible and allows you to
specify a Pydantic model class directly.

seed (optional, int)
stop (optional, str | List[str])
temperature (optional, float)
top_p (optional, float)
user (optional, str)
default_headers (optional, dict[str, str]) – Custom headers; useful for authentication or other custom requirements.
add_name_prefixes (optional, bool) – Whether to prepend the source value
to each UserMessage content. E.g.,
“this is content” becomes “Reviewer said: this is content.”
This can be useful for models that do not support the name field in
message. Defaults to False.
stream_options (optional, dict) – Additional options for streaming. Currently only include_usage is supported.



Examples
The following code snippet shows how to use the client with an OpenAI model:
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import UserMessage

openai_client = OpenAIChatCompletionClient(
    model="gpt-4o-2024-08-06",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY environment variable set.
)

result = await openai_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
print(result)

# Close the client when done.
# await openai_client.close()


To use the client with a non-OpenAI model, you need to provide the base URL of the model and the model info.
For example, to use Ollama, you can use the following code snippet:
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import ModelFamily

custom_model_client = OpenAIChatCompletionClient(
    model="deepseek-r1:1.5b",
    base_url="http://localhost:11434/v1",
    api_key="placeholder",
    model_info={
        "vision": False,
        "function_calling": False,
        "json_output": False,
        "family": ModelFamily.R1,
        "structured_output": True,
    },
)

# Close the client when done.
# await custom_model_client.close()


To use streaming mode, you can use the following code snippet:
import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Similar for AzureOpenAIChatCompletionClient.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")  # assuming OPENAI_API_KEY is set in the environment.

    messages = [UserMessage(content="Write a very short story about a dragon.", source="user")]

    # Create a stream.
    stream = model_client.create_stream(messages=messages)

    # Iterate over the stream and print the responses.
    print("Streamed responses:")
    async for response in stream:
        if isinstance(response, str):
            # A partial response is a string.
            print(response, flush=True, end="")
        else:
            # The last response is a CreateResult object with the complete message.
            print("\n\n------------\n")
            print("The complete response:", flush=True)
            print(response.content, flush=True)

    # Close the client when done.
    await model_client.close()


asyncio.run(main())


To use structured output as well as function calling, you can use the following code snippet:
import asyncio
from typing import Literal

from autogen_core.models import (
    AssistantMessage,
    FunctionExecutionResult,
    FunctionExecutionResultMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel


# Define the structured output format.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]


# Define the function to be called as a tool.
def sentiment_analysis(text: str) -> str:
    """Given a text, return the sentiment."""
    return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"


# Create a FunctionTool instance with `strict=True`,
# which is required for structured output mode.
tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)


async def main() -> None:
    # Create an OpenAIChatCompletionClient instance.
    model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")

    # Generate a response using the tool.
    response1 = await model_client.create(
        messages=[
            SystemMessage(content="Analyze input text sentiment using the tool provided."),
            UserMessage(content="I am happy.", source="user"),
        ],
        tools=[tool],
    )
    print(response1.content)
    # Should be a list of tool calls.
    # [FunctionCall(name="sentiment_analysis", arguments={"text": "I am happy."}, ...)]

    assert isinstance(response1.content, list)
    response2 = await model_client.create(
        messages=[
            SystemMessage(content="Analyze input text sentiment using the tool provided."),
            UserMessage(content="I am happy.", source="user"),
            AssistantMessage(content=response1.content, source="assistant"),
            FunctionExecutionResultMessage(
                content=[FunctionExecutionResult(content="happy", call_id=response1.content[0].id, is_error=False, name="sentiment_analysis")]
            ),
        ],
        # Use the structured output format.
        json_output=AgentResponse,
    )
    print(response2.content)
    # Should be a structured output.
    # {"thoughts": "The user is happy.", "response": "happy"}

    # Close the client when done.
    await model_client.close()

asyncio.run(main())


To load the client from a configuration, you can use the load_component method:
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {"model": "gpt-4o", "api_key": "REPLACE_WITH_YOUR_API_KEY"},
}

client = ChatCompletionClient.load_component(config)


To view the full list of available configuration options, see the OpenAIClientConfigurationConfigModel class.


component_type: ClassVar[ComponentType] = 'model'#
The logical type of the component.



component_config_schema#
alias of OpenAIClientConfigurationConfigModel



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.openai.OpenAIChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



_to_config() → OpenAIClientConfigurationConfigModel[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





classmethod _from_config(config: OpenAIClientConfigurationConfigModel) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

**示例**:
```python
pip install "autogen-ext[openai]"

```

**示例**:
```python
# Text response, this is the default.
{"type": "text"}

```

**示例**:
```python
# JSON response, make sure to instruct the model to return JSON.
{"type": "json_object"}

```

**示例**:
```python
# Structured output response, with a pre-defined JSON schema.
{
    "type": "json_schema",
    "json_schema": {
        "name": "name of the schema, must be an identifier.",
        "description": "description for the model.",
        # You can convert a Pydantic (v2) model to JSON schema
        # using the `model_json_schema()` method.
        "schema": "<the JSON schema itself>",
        # Whether to enable strict schema adherence when
        # generating the output. If set to true, the model will
        # always follow the exact schema defined in the
        # `schema` field. Only a subset of JSON Schema is
        # supported when `strict` is `true`.
        # To learn more, read
        # https://platform.openai.com/docs/guides/structured-outputs.
        "strict": False,  # or True
    },
}

```

**示例**:
```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import UserMessage

openai_client = OpenAIChatCompletionClient(
    model="gpt-4o-2024-08-06",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY environment variable set.
)

result = await openai_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
print(result)

# Close the client when done.
# await openai_client.close()

```

**示例**:
```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import ModelFamily

custom_model_client = OpenAIChatCompletionClient(
    model="deepseek-r1:1.5b",
    base_url="http://localhost:11434/v1",
    api_key="placeholder",
    model_info={
        "vision": False,
        "function_calling": False,
        "json_output": False,
        "family": ModelFamily.R1,
        "structured_output": True,
    },
)

# Close the client when done.
# await custom_model_client.close()

```

**示例**:
```python
import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Similar for AzureOpenAIChatCompletionClient.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")  # assuming OPENAI_API_KEY is set in the environment.

    messages = [UserMessage(content="Write a very short story about a dragon.", source="user")]

    # Create a stream.
    stream = model_client.create_stream(messages=messages)

    # Iterate over the stream and print the responses.
    print("Streamed responses:")
    async for response in stream:
        if isinstance(response, str):
            # A partial response is a string.
            print(response, flush=True, end="")
        else:
            # The last response is a CreateResult object with the complete message.
            print("\n\n------------\n")
            print("The complete response:", flush=True)
            print(response.content, flush=True)

    # Close the client when done.
    await model_client.close()


asyncio.run(main())

```

**示例**:
```python
import asyncio
from typing import Literal

from autogen_core.models import (
    AssistantMessage,
    FunctionExecutionResult,
    FunctionExecutionResultMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel


# Define the structured output format.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]


# Define the function to be called as a tool.
def sentiment_analysis(text: str) -> str:
    """Given a text, return the sentiment."""
    return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"


# Create a FunctionTool instance with `strict=True`,
# which is required for structured output mode.
tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)


async def main() -> None:
    # Create an OpenAIChatCompletionClient instance.
    model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")

    # Generate a response using the tool.
    response1 = await model_client.create(
        messages=[
            SystemMessage(content="Analyze input text sentiment using the tool provided."),
            UserMessage(content="I am happy.", source="user"),
        ],
        tools=[tool],
    )
    print(response1.content)
    # Should be a list of tool calls.
    # [FunctionCall(name="sentiment_analysis", arguments={"text": "I am happy."}, ...)]

    assert isinstance(response1.content, list)
    response2 = await model_client.create(
        messages=[
            SystemMessage(content="Analyze input text sentiment using the tool provided."),
            UserMessage(content="I am happy.", source="user"),
            AssistantMessage(content=response1.content, source="assistant"),
            FunctionExecutionResultMessage(
                content=[FunctionExecutionResult(content="happy", call_id=response1.content[0].id, is_error=False, name="sentiment_analysis")]
            ),
        ],
        # Use the structured output format.
        json_output=AgentResponse,
    )
    print(response2.content)
    # Should be a structured output.
    # {"thoughts": "The user is happy.", "response": "happy"}

    # Close the client when done.
    await model_client.close()

asyncio.run(main())

```

**示例**:
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {"model": "gpt-4o", "api_key": "REPLACE_WITH_YOUR_API_KEY"},
}

client = ChatCompletionClient.load_component(config)

```

```python
component_type: ClassVar[ComponentType] = 'model'#
```

【中文翻译】The logical type of the component.

```python
component_config_schema#
```

【中文翻译】alias of OpenAIClientConfigurationConfigModel

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.openai.OpenAIChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
_to_config() → OpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
classmethod _from_config(config: OpenAIClientConfigurationConfigModel) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
class AzureOpenAIChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: BaseOpenAIChatCompletionClient, Component[AzureOpenAIClientConfigurationConfigModel]
Chat completion client for Azure OpenAI hosted models.
To use this client, you must install the azure and openai extensions:
pip install "autogen-ext[openai,azure]"



Parameters:

model (str) – Which OpenAI model to use.
azure_endpoint (str) – The endpoint for the Azure model. Required for Azure models.
azure_deployment (str) – Deployment name for the Azure model. Required for Azure models.
api_version (str) – The API version to use. Required for Azure models.
azure_ad_token (str) – The Azure AD token to use. Provide this or azure_ad_token_provider for token-based authentication.
azure_ad_token_provider (optional, Callable[[], Awaitable[str]] | AzureTokenProvider) – The Azure AD token provider to use. Provide this or azure_ad_token for token-based authentication.
api_key (optional, str) – The API key to use, use this if you are using key based authentication. It is optional if you are using Azure AD token based authentication or AZURE_OPENAI_API_KEY environment variable.
timeout – (optional, float): The timeout for the request in seconds.
max_retries (optional, int) – The maximum number of retries to attempt.
model_info (optional, ModelInfo) – The capabilities of the model. Required if the model name is not a valid OpenAI model.
frequency_penalty (optional, float)
logit_bias – (optional, dict[str, int]):
max_tokens (optional, int)
n (optional, int)
presence_penalty (optional, float)
response_format (optional, Dict[str, Any]) – the format of the response. Possible options are:
# Text response, this is the default.
{"type": "text"}


# JSON response, make sure to instruct the model to return JSON.
{"type": "json_object"}


# Structured output response, with a pre-defined JSON schema.
{
    "type": "json_schema",
    "json_schema": {
        "name": "name of the schema, must be an identifier.",
        "description": "description for the model.",
        # You can convert a Pydantic (v2) model to JSON schema
        # using the `model_json_schema()` method.
        "schema": "<the JSON schema itself>",
        # Whether to enable strict schema adherence when
        # generating the output. If set to true, the model will
        # always follow the exact schema defined in the
        # `schema` field. Only a subset of JSON Schema is
        # supported when `strict` is `true`.
        # To learn more, read
        # https://platform.openai.com/docs/guides/structured-outputs.
        "strict": False,  # or True
    },
}


It is recommended to use the json_output parameter in
create() or
create_stream()
methods instead of response_format for structured output.
The json_output parameter is more flexible and allows you to
specify a Pydantic model class directly.

seed (optional, int)
stop (optional, str | List[str])
temperature (optional, float)
top_p (optional, float)
user (optional, str)
default_headers (optional, dict[str, str]) – Custom headers; useful for authentication or other custom requirements.



To use the client, you need to provide your deployment name, Azure Cognitive Services endpoint, and api version.
For authentication, you can either provide an API key or an Azure Active Directory (AAD) token credential.
The following code snippet shows how to use AAD authentication.
The identity used must be assigned the Cognitive Services OpenAI User role.
from autogen_ext.auth.azure import AzureTokenProvider
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
from azure.identity import DefaultAzureCredential

# Create the token provider
token_provider = AzureTokenProvider(
    DefaultAzureCredential(),
    "https://cognitiveservices.azure.com/.default",
)

az_model_client = AzureOpenAIChatCompletionClient(
    azure_deployment="{your-azure-deployment}",
    model="{model-name, such as gpt-4o}",
    api_version="2024-06-01",
    azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
    azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.
    # api_key="sk-...", # For key-based authentication.
)


See other usage examples in the OpenAIChatCompletionClient class.
To load the client that uses identity based aith from a configuration, you can use the load_component method:
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "AzureOpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o-2024-05-13",
        "azure_endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
        "azure_deployment": "{your-azure-deployment}",
        "api_version": "2024-06-01",
        "azure_ad_token_provider": {
            "provider": "autogen_ext.auth.azure.AzureTokenProvider",
            "config": {
                "provider_kind": "DefaultAzureCredential",
                "scopes": ["https://cognitiveservices.azure.com/.default"],
            },
        },
    },
}

client = ChatCompletionClient.load_component(config)


To view the full list of available configuration options, see the AzureOpenAIClientConfigurationConfigModel class.

Note
Right now only DefaultAzureCredential is supported with no additional args passed to it.


Note
The Azure OpenAI client by default sets the User-Agent header to autogen-python/{version}. To override this, you can set the variable autogen_ext.models.openai.AZURE_OPENAI_USER_AGENT environment variable to an empty string.

See here for how to use the Azure client directly or for more info.


component_type: ClassVar[ComponentType] = 'model'#
The logical type of the component.



component_config_schema#
alias of AzureOpenAIClientConfigurationConfigModel



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.openai.AzureOpenAIChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



_to_config() → AzureOpenAIClientConfigurationConfigModel[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





classmethod _from_config(config: AzureOpenAIClientConfigurationConfigModel) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

**示例**:
```python
pip install "autogen-ext[openai,azure]"

```

**示例**:
```python
# Text response, this is the default.
{"type": "text"}

```

**示例**:
```python
# JSON response, make sure to instruct the model to return JSON.
{"type": "json_object"}

```

**示例**:
```python
# Structured output response, with a pre-defined JSON schema.
{
    "type": "json_schema",
    "json_schema": {
        "name": "name of the schema, must be an identifier.",
        "description": "description for the model.",
        # You can convert a Pydantic (v2) model to JSON schema
        # using the `model_json_schema()` method.
        "schema": "<the JSON schema itself>",
        # Whether to enable strict schema adherence when
        # generating the output. If set to true, the model will
        # always follow the exact schema defined in the
        # `schema` field. Only a subset of JSON Schema is
        # supported when `strict` is `true`.
        # To learn more, read
        # https://platform.openai.com/docs/guides/structured-outputs.
        "strict": False,  # or True
    },
}

```

**示例**:
```python
from autogen_ext.auth.azure import AzureTokenProvider
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
from azure.identity import DefaultAzureCredential

# Create the token provider
token_provider = AzureTokenProvider(
    DefaultAzureCredential(),
    "https://cognitiveservices.azure.com/.default",
)

az_model_client = AzureOpenAIChatCompletionClient(
    azure_deployment="{your-azure-deployment}",
    model="{model-name, such as gpt-4o}",
    api_version="2024-06-01",
    azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
    azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.
    # api_key="sk-...", # For key-based authentication.
)

```

**示例**:
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "AzureOpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o-2024-05-13",
        "azure_endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
        "azure_deployment": "{your-azure-deployment}",
        "api_version": "2024-06-01",
        "azure_ad_token_provider": {
            "provider": "autogen_ext.auth.azure.AzureTokenProvider",
            "config": {
                "provider_kind": "DefaultAzureCredential",
                "scopes": ["https://cognitiveservices.azure.com/.default"],
            },
        },
    },
}

client = ChatCompletionClient.load_component(config)

```

```python
component_type: ClassVar[ComponentType] = 'model'#
```

【中文翻译】The logical type of the component.

```python
component_config_schema#
```

【中文翻译】alias of AzureOpenAIClientConfigurationConfigModel

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.openai.AzureOpenAIChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
_to_config() → AzureOpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
classmethod _from_config(config: AzureOpenAIClientConfigurationConfigModel) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
class BaseOpenAIChatCompletionClient(client: AsyncOpenAI | AsyncAzureOpenAI, *, create_args: Dict[str, Any], model_capabilities: ModelCapabilities | None = None, model_info: ModelInfo | None = None, add_name_prefixes: bool = False)[source]#
```

【中文翻译】Bases: ChatCompletionClient


classmethod create_from_config(config: Dict[str, Any]) → ChatCompletionClient[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, max_consecutive_empty_chunk_tolerance: int = 0) → AsyncGenerator[str | CreateResult, None][source]#
Create a stream of string chunks from the model ending with a CreateResult.
Extends autogen_core.models.ChatCompletionClient.create_stream() to support OpenAI API.
In streaming, the default behaviour is not return token usage counts.
See: OpenAI API reference for possible args.
You can set extra_create_args={“stream_options”: {“include_usage”: True}}
(if supported by the accessed API) to
return a final chunk with usage set to a RequestUsage object
with prompt and completion token counts,
all preceding chunks will have usage as None.
See: OpenAI API reference for stream options.

Other examples of supported arguments that can be included in extra_create_args:
temperature (float): Controls the randomness of the output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
max_tokens (int): The maximum number of tokens to generate in the completion.
top_p (float): An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
frequency_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on their existing frequency in the text so far, decreasing the likelihood of repeated phrases.
presence_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on whether they appear in the text so far, encouraging the model to talk about new topics.






async close() → None[source]#



actual_usage() → RequestUsage[source]#



total_usage() → RequestUsage[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



property capabilities: ModelCapabilities#



property model_info: ModelInfo#

```python
classmethod create_from_config(config: Dict[str, Any]) → ChatCompletionClient[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, max_consecutive_empty_chunk_tolerance: int = 0) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Create a stream of string chunks from the model ending with a CreateResult.
Extends autogen_core.models.ChatCompletionClient.create_stream() to support OpenAI API.
In streaming, the default behaviour is not return token usage counts.
See: OpenAI API reference for possible args.
You can set extra_create_args={“stream_options”: {“include_usage”: True}}
(if supported by the accessed API) to
return a final chunk with usage set to a RequestUsage object
with prompt and completion token counts,
all preceding chunks will have usage as None.
See: OpenAI API reference for stream options.

Other examples of supported arguments that can be included in extra_create_args:
temperature (float): Controls the randomness of the output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
max_tokens (int): The maximum number of tokens to generate in the completion.
top_p (float): An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
frequency_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on their existing frequency in the text so far, decreasing the likelihood of repeated phrases.
presence_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on whether they appear in the text so far, encouraging the model to talk about new topics.

```python
async close() → None[source]#
```

```python
actual_usage() → RequestUsage[source]#
```

```python
total_usage() → RequestUsage[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
property capabilities: ModelCapabilities#
```

```python
property model_info: ModelInfo#
```

```python
pydantic model AzureOpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: BaseOpenAIClientConfigurationConfigModel

Show JSON schema{
   "title": "AzureOpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "azure_endpoint": {
         "title": "Azure Endpoint",
         "type": "string"
      },
      "azure_deployment": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Azure Deployment"
      },
      "api_version": {
         "title": "Api Version",
         "type": "string"
      },
      "azure_ad_token": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Azure Ad Token"
      },
      "azure_ad_token_provider": {
         "anyOf": [
            {
               "$ref": "#/$defs/ComponentModel"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "ComponentModel": {
         "description": "Model class for a component. Contains all information required to instantiate a component.",
         "properties": {
            "provider": {
               "title": "Provider",
               "type": "string"
            },
            "component_type": {
               "anyOf": [
                  {
                     "enum": [
                        "model",
                        "agent",
                        "tool",
                        "termination",
                        "token_provider",
                        "workbench"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Component Type"
            },
            "version": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Version"
            },
            "component_version": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Component Version"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "label": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Label"
            },
            "config": {
               "title": "Config",
               "type": "object"
            }
         },
         "required": [
            "provider",
            "config"
         ],
         "title": "ComponentModel",
         "type": "object"
      },
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model",
      "azure_endpoint",
      "api_version"
   ]
}



Fields:

api_version (str)
azure_ad_token (str | None)
azure_ad_token_provider (autogen_core._component_config.ComponentModel | None)
azure_deployment (str | None)
azure_endpoint (str)





field azure_endpoint: str [Required]#



field azure_deployment: str | None = None#



field api_version: str [Required]#



field azure_ad_token: str | None = None#



field azure_ad_token_provider: ComponentModel | None = None#

**示例**:
```python
{
   "title": "AzureOpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "azure_endpoint": {
         "title": "Azure Endpoint",
         "type": "string"
      },
      "azure_deployment": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Azure Deployment"
      },
      "api_version": {
         "title": "Api Version",
         "type": "string"
      },
      "azure_ad_token": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Azure Ad Token"
      },
      "azure_ad_token_provider": {
         "anyOf": [
            {
               "$ref": "#/$defs/ComponentModel"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "ComponentModel": {
         "description": "Model class for a component. Contains all information required to instantiate a component.",
         "properties": {
            "provider": {
               "title": "Provider",
               "type": "string"
            },
            "component_type": {
               "anyOf": [
                  {
                     "enum": [
                        "model",
                        "agent",
                        "tool",
                        "termination",
                        "token_provider",
                        "workbench"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Component Type"
            },
            "version": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Version"
            },
            "component_version": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Component Version"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "label": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Label"
            },
            "config": {
               "title": "Config",
               "type": "object"
            }
         },
         "required": [
            "provider",
            "config"
         ],
         "title": "ComponentModel",
         "type": "object"
      },
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model",
      "azure_endpoint",
      "api_version"
   ]
}

```

```python
field azure_endpoint: str [Required]#
```

```python
field azure_deployment: str | None = None#
```

```python
field api_version: str [Required]#
```

```python
field azure_ad_token: str | None = None#
```

```python
field azure_ad_token_provider: ComponentModel | None = None#
```

```python
pydantic model OpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: BaseOpenAIClientConfigurationConfigModel

Show JSON schema{
   "title": "OpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "organization": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Organization"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

base_url (str | None)
organization (str | None)





field organization: str | None = None#



field base_url: str | None = None#

**示例**:
```python
{
   "title": "OpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "organization": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Organization"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field organization: str | None = None#
```

```python
field base_url: str | None = None#
```

```python
pydantic model BaseOpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: CreateArgumentsConfigModel

Show JSON schema{
   "title": "BaseOpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

add_name_prefixes (bool | None)
api_key (pydantic.types.SecretStr | None)
default_headers (Dict[str, str] | None)
max_retries (int | None)
model (str)
model_capabilities (autogen_core.models._model_client.ModelCapabilities | None)
model_info (autogen_core.models._model_client.ModelInfo | None)
timeout (float | None)





field model: str [Required]#



field api_key: SecretStr | None = None#



field timeout: float | None = None#



field max_retries: int | None = None#



field model_capabilities: ModelCapabilities | None = None#



field model_info: ModelInfo | None = None#



field add_name_prefixes: bool | None = None#



field default_headers: Dict[str, str] | None = None#

**示例**:
```python
{
   "title": "BaseOpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field model: str [Required]#
```

```python
field api_key: SecretStr | None = None#
```

```python
field timeout: float | None = None#
```

```python
field max_retries: int | None = None#
```

```python
field model_capabilities: ModelCapabilities | None = None#
```

```python
field model_info: ModelInfo | None = None#
```

```python
field add_name_prefixes: bool | None = None#
```

```python
field default_headers: Dict[str, str] | None = None#
```

```python
pydantic model CreateArgumentsConfigModel[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   }
}



Fields:

frequency_penalty (float | None)
logit_bias (Dict[str, int] | None)
max_tokens (int | None)
n (int | None)
presence_penalty (float | None)
response_format (autogen_ext.models.openai.config.ResponseFormat | None)
seed (int | None)
stop (str | List[str] | None)
stream_options (autogen_ext.models.openai.config.StreamOptions | None)
temperature (float | None)
top_p (float | None)
user (str | None)





field frequency_penalty: float | None = None#



field logit_bias: Dict[str, int] | None = None#



field max_tokens: int | None = None#



field n: int | None = None#



field presence_penalty: float | None = None#



field response_format: ResponseFormat | None = None#



field seed: int | None = None#



field stop: str | List[str] | None = None#



field temperature: float | None = None#



field top_p: float | None = None#



field user: str | None = None#



field stream_options: StreamOptions | None = None#

**示例**:
```python
{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   }
}

```

```python
field frequency_penalty: float | None = None#
```

```python
field logit_bias: Dict[str, int] | None = None#
```

```python
field max_tokens: int | None = None#
```

```python
field n: int | None = None#
```

```python
field presence_penalty: float | None = None#
```

```python
field response_format: ResponseFormat | None = None#
```

```python
field seed: int | None = None#
```

```python
field stop: str | List[str] | None = None#
```

```python
field temperature: float | None = None#
```

```python
field top_p: float | None = None#
```

```python
field user: str | None = None#
```

```python
field stream_options: StreamOptions | None = None#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.cache

【中文翻译】next

【中文翻译】autogen_ext.models.replay

### autogen_ext.models.openai {autogen_extmodelsopenai}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html)

```python
class OpenAIChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: BaseOpenAIChatCompletionClient, Component[OpenAIClientConfigurationConfigModel]
Chat completion client for OpenAI hosted models.
To use this client, you must install the openai extra:
pip install "autogen-ext[openai]"


You can also use this client for OpenAI-compatible ChatCompletion endpoints.
Using this client for non-OpenAI models is not tested or guaranteed.
For non-OpenAI models, please first take a look at our community extensions
for additional model clients.

Parameters:

model (str) – Which OpenAI model to use.
api_key (optional, str) – The API key to use. Required if ‘OPENAI_API_KEY’ is not found in the environment variables.
organization (optional, str) – The organization ID to use.
base_url (optional, str) – The base URL to use. Required if the model is not hosted on OpenAI.
timeout – (optional, float): The timeout for the request in seconds.
max_retries (optional, int) – The maximum number of retries to attempt.
model_info (optional, ModelInfo) – The capabilities of the model. Required if the model name is not a valid OpenAI model.
frequency_penalty (optional, float)
logit_bias – (optional, dict[str, int]):
max_tokens (optional, int)
n (optional, int)
presence_penalty (optional, float)
response_format (optional, Dict[str, Any]) – the format of the response. Possible options are:
# Text response, this is the default.
{"type": "text"}


# JSON response, make sure to instruct the model to return JSON.
{"type": "json_object"}


# Structured output response, with a pre-defined JSON schema.
{
    "type": "json_schema",
    "json_schema": {
        "name": "name of the schema, must be an identifier.",
        "description": "description for the model.",
        # You can convert a Pydantic (v2) model to JSON schema
        # using the `model_json_schema()` method.
        "schema": "<the JSON schema itself>",
        # Whether to enable strict schema adherence when
        # generating the output. If set to true, the model will
        # always follow the exact schema defined in the
        # `schema` field. Only a subset of JSON Schema is
        # supported when `strict` is `true`.
        # To learn more, read
        # https://platform.openai.com/docs/guides/structured-outputs.
        "strict": False,  # or True
    },
}


It is recommended to use the json_output parameter in
create() or
create_stream()
methods instead of response_format for structured output.
The json_output parameter is more flexible and allows you to
specify a Pydantic model class directly.

seed (optional, int)
stop (optional, str | List[str])
temperature (optional, float)
top_p (optional, float)
user (optional, str)
default_headers (optional, dict[str, str]) – Custom headers; useful for authentication or other custom requirements.
add_name_prefixes (optional, bool) – Whether to prepend the source value
to each UserMessage content. E.g.,
“this is content” becomes “Reviewer said: this is content.”
This can be useful for models that do not support the name field in
message. Defaults to False.
stream_options (optional, dict) – Additional options for streaming. Currently only include_usage is supported.



Examples
The following code snippet shows how to use the client with an OpenAI model:
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import UserMessage

openai_client = OpenAIChatCompletionClient(
    model="gpt-4o-2024-08-06",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY environment variable set.
)

result = await openai_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
print(result)

# Close the client when done.
# await openai_client.close()


To use the client with a non-OpenAI model, you need to provide the base URL of the model and the model info.
For example, to use Ollama, you can use the following code snippet:
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import ModelFamily

custom_model_client = OpenAIChatCompletionClient(
    model="deepseek-r1:1.5b",
    base_url="http://localhost:11434/v1",
    api_key="placeholder",
    model_info={
        "vision": False,
        "function_calling": False,
        "json_output": False,
        "family": ModelFamily.R1,
        "structured_output": True,
    },
)

# Close the client when done.
# await custom_model_client.close()


To use streaming mode, you can use the following code snippet:
import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Similar for AzureOpenAIChatCompletionClient.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")  # assuming OPENAI_API_KEY is set in the environment.

    messages = [UserMessage(content="Write a very short story about a dragon.", source="user")]

    # Create a stream.
    stream = model_client.create_stream(messages=messages)

    # Iterate over the stream and print the responses.
    print("Streamed responses:")
    async for response in stream:
        if isinstance(response, str):
            # A partial response is a string.
            print(response, flush=True, end="")
        else:
            # The last response is a CreateResult object with the complete message.
            print("\n\n------------\n")
            print("The complete response:", flush=True)
            print(response.content, flush=True)

    # Close the client when done.
    await model_client.close()


asyncio.run(main())


To use structured output as well as function calling, you can use the following code snippet:
import asyncio
from typing import Literal

from autogen_core.models import (
    AssistantMessage,
    FunctionExecutionResult,
    FunctionExecutionResultMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel


# Define the structured output format.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]


# Define the function to be called as a tool.
def sentiment_analysis(text: str) -> str:
    """Given a text, return the sentiment."""
    return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"


# Create a FunctionTool instance with `strict=True`,
# which is required for structured output mode.
tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)


async def main() -> None:
    # Create an OpenAIChatCompletionClient instance.
    model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")

    # Generate a response using the tool.
    response1 = await model_client.create(
        messages=[
            SystemMessage(content="Analyze input text sentiment using the tool provided."),
            UserMessage(content="I am happy.", source="user"),
        ],
        tools=[tool],
    )
    print(response1.content)
    # Should be a list of tool calls.
    # [FunctionCall(name="sentiment_analysis", arguments={"text": "I am happy."}, ...)]

    assert isinstance(response1.content, list)
    response2 = await model_client.create(
        messages=[
            SystemMessage(content="Analyze input text sentiment using the tool provided."),
            UserMessage(content="I am happy.", source="user"),
            AssistantMessage(content=response1.content, source="assistant"),
            FunctionExecutionResultMessage(
                content=[FunctionExecutionResult(content="happy", call_id=response1.content[0].id, is_error=False, name="sentiment_analysis")]
            ),
        ],
        # Use the structured output format.
        json_output=AgentResponse,
    )
    print(response2.content)
    # Should be a structured output.
    # {"thoughts": "The user is happy.", "response": "happy"}

    # Close the client when done.
    await model_client.close()

asyncio.run(main())


To load the client from a configuration, you can use the load_component method:
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {"model": "gpt-4o", "api_key": "REPLACE_WITH_YOUR_API_KEY"},
}

client = ChatCompletionClient.load_component(config)


To view the full list of available configuration options, see the OpenAIClientConfigurationConfigModel class.


component_type: ClassVar[ComponentType] = 'model'#
The logical type of the component.



component_config_schema#
alias of OpenAIClientConfigurationConfigModel



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.openai.OpenAIChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



_to_config() → OpenAIClientConfigurationConfigModel[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





classmethod _from_config(config: OpenAIClientConfigurationConfigModel) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

**示例**:
```python
pip install "autogen-ext[openai]"

```

**示例**:
```python
# Text response, this is the default.
{"type": "text"}

```

**示例**:
```python
# JSON response, make sure to instruct the model to return JSON.
{"type": "json_object"}

```

**示例**:
```python
# Structured output response, with a pre-defined JSON schema.
{
    "type": "json_schema",
    "json_schema": {
        "name": "name of the schema, must be an identifier.",
        "description": "description for the model.",
        # You can convert a Pydantic (v2) model to JSON schema
        # using the `model_json_schema()` method.
        "schema": "<the JSON schema itself>",
        # Whether to enable strict schema adherence when
        # generating the output. If set to true, the model will
        # always follow the exact schema defined in the
        # `schema` field. Only a subset of JSON Schema is
        # supported when `strict` is `true`.
        # To learn more, read
        # https://platform.openai.com/docs/guides/structured-outputs.
        "strict": False,  # or True
    },
}

```

**示例**:
```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import UserMessage

openai_client = OpenAIChatCompletionClient(
    model="gpt-4o-2024-08-06",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY environment variable set.
)

result = await openai_client.create([UserMessage(content="What is the capital of France?", source="user")])  # type: ignore
print(result)

# Close the client when done.
# await openai_client.close()

```

**示例**:
```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import ModelFamily

custom_model_client = OpenAIChatCompletionClient(
    model="deepseek-r1:1.5b",
    base_url="http://localhost:11434/v1",
    api_key="placeholder",
    model_info={
        "vision": False,
        "function_calling": False,
        "json_output": False,
        "family": ModelFamily.R1,
        "structured_output": True,
    },
)

# Close the client when done.
# await custom_model_client.close()

```

**示例**:
```python
import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Similar for AzureOpenAIChatCompletionClient.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")  # assuming OPENAI_API_KEY is set in the environment.

    messages = [UserMessage(content="Write a very short story about a dragon.", source="user")]

    # Create a stream.
    stream = model_client.create_stream(messages=messages)

    # Iterate over the stream and print the responses.
    print("Streamed responses:")
    async for response in stream:
        if isinstance(response, str):
            # A partial response is a string.
            print(response, flush=True, end="")
        else:
            # The last response is a CreateResult object with the complete message.
            print("\n\n------------\n")
            print("The complete response:", flush=True)
            print(response.content, flush=True)

    # Close the client when done.
    await model_client.close()


asyncio.run(main())

```

**示例**:
```python
import asyncio
from typing import Literal

from autogen_core.models import (
    AssistantMessage,
    FunctionExecutionResult,
    FunctionExecutionResultMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
from pydantic import BaseModel


# Define the structured output format.
class AgentResponse(BaseModel):
    thoughts: str
    response: Literal["happy", "sad", "neutral"]


# Define the function to be called as a tool.
def sentiment_analysis(text: str) -> str:
    """Given a text, return the sentiment."""
    return "happy" if "happy" in text else "sad" if "sad" in text else "neutral"


# Create a FunctionTool instance with `strict=True`,
# which is required for structured output mode.
tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True)


async def main() -> None:
    # Create an OpenAIChatCompletionClient instance.
    model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")

    # Generate a response using the tool.
    response1 = await model_client.create(
        messages=[
            SystemMessage(content="Analyze input text sentiment using the tool provided."),
            UserMessage(content="I am happy.", source="user"),
        ],
        tools=[tool],
    )
    print(response1.content)
    # Should be a list of tool calls.
    # [FunctionCall(name="sentiment_analysis", arguments={"text": "I am happy."}, ...)]

    assert isinstance(response1.content, list)
    response2 = await model_client.create(
        messages=[
            SystemMessage(content="Analyze input text sentiment using the tool provided."),
            UserMessage(content="I am happy.", source="user"),
            AssistantMessage(content=response1.content, source="assistant"),
            FunctionExecutionResultMessage(
                content=[FunctionExecutionResult(content="happy", call_id=response1.content[0].id, is_error=False, name="sentiment_analysis")]
            ),
        ],
        # Use the structured output format.
        json_output=AgentResponse,
    )
    print(response2.content)
    # Should be a structured output.
    # {"thoughts": "The user is happy.", "response": "happy"}

    # Close the client when done.
    await model_client.close()

asyncio.run(main())

```

**示例**:
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {"model": "gpt-4o", "api_key": "REPLACE_WITH_YOUR_API_KEY"},
}

client = ChatCompletionClient.load_component(config)

```

```python
component_type: ClassVar[ComponentType] = 'model'#
```

【中文翻译】The logical type of the component.

```python
component_config_schema#
```

【中文翻译】alias of OpenAIClientConfigurationConfigModel

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.openai.OpenAIChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
_to_config() → OpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
classmethod _from_config(config: OpenAIClientConfigurationConfigModel) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
class AzureOpenAIChatCompletionClient(**kwargs: Unpack)[source]#
```

【中文翻译】Bases: BaseOpenAIChatCompletionClient, Component[AzureOpenAIClientConfigurationConfigModel]
Chat completion client for Azure OpenAI hosted models.
To use this client, you must install the azure and openai extensions:
pip install "autogen-ext[openai,azure]"



Parameters:

model (str) – Which OpenAI model to use.
azure_endpoint (str) – The endpoint for the Azure model. Required for Azure models.
azure_deployment (str) – Deployment name for the Azure model. Required for Azure models.
api_version (str) – The API version to use. Required for Azure models.
azure_ad_token (str) – The Azure AD token to use. Provide this or azure_ad_token_provider for token-based authentication.
azure_ad_token_provider (optional, Callable[[], Awaitable[str]] | AzureTokenProvider) – The Azure AD token provider to use. Provide this or azure_ad_token for token-based authentication.
api_key (optional, str) – The API key to use, use this if you are using key based authentication. It is optional if you are using Azure AD token based authentication or AZURE_OPENAI_API_KEY environment variable.
timeout – (optional, float): The timeout for the request in seconds.
max_retries (optional, int) – The maximum number of retries to attempt.
model_info (optional, ModelInfo) – The capabilities of the model. Required if the model name is not a valid OpenAI model.
frequency_penalty (optional, float)
logit_bias – (optional, dict[str, int]):
max_tokens (optional, int)
n (optional, int)
presence_penalty (optional, float)
response_format (optional, Dict[str, Any]) – the format of the response. Possible options are:
# Text response, this is the default.
{"type": "text"}


# JSON response, make sure to instruct the model to return JSON.
{"type": "json_object"}


# Structured output response, with a pre-defined JSON schema.
{
    "type": "json_schema",
    "json_schema": {
        "name": "name of the schema, must be an identifier.",
        "description": "description for the model.",
        # You can convert a Pydantic (v2) model to JSON schema
        # using the `model_json_schema()` method.
        "schema": "<the JSON schema itself>",
        # Whether to enable strict schema adherence when
        # generating the output. If set to true, the model will
        # always follow the exact schema defined in the
        # `schema` field. Only a subset of JSON Schema is
        # supported when `strict` is `true`.
        # To learn more, read
        # https://platform.openai.com/docs/guides/structured-outputs.
        "strict": False,  # or True
    },
}


It is recommended to use the json_output parameter in
create() or
create_stream()
methods instead of response_format for structured output.
The json_output parameter is more flexible and allows you to
specify a Pydantic model class directly.

seed (optional, int)
stop (optional, str | List[str])
temperature (optional, float)
top_p (optional, float)
user (optional, str)
default_headers (optional, dict[str, str]) – Custom headers; useful for authentication or other custom requirements.



To use the client, you need to provide your deployment name, Azure Cognitive Services endpoint, and api version.
For authentication, you can either provide an API key or an Azure Active Directory (AAD) token credential.
The following code snippet shows how to use AAD authentication.
The identity used must be assigned the Cognitive Services OpenAI User role.
from autogen_ext.auth.azure import AzureTokenProvider
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
from azure.identity import DefaultAzureCredential

# Create the token provider
token_provider = AzureTokenProvider(
    DefaultAzureCredential(),
    "https://cognitiveservices.azure.com/.default",
)

az_model_client = AzureOpenAIChatCompletionClient(
    azure_deployment="{your-azure-deployment}",
    model="{model-name, such as gpt-4o}",
    api_version="2024-06-01",
    azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
    azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.
    # api_key="sk-...", # For key-based authentication.
)


See other usage examples in the OpenAIChatCompletionClient class.
To load the client that uses identity based aith from a configuration, you can use the load_component method:
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "AzureOpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o-2024-05-13",
        "azure_endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
        "azure_deployment": "{your-azure-deployment}",
        "api_version": "2024-06-01",
        "azure_ad_token_provider": {
            "provider": "autogen_ext.auth.azure.AzureTokenProvider",
            "config": {
                "provider_kind": "DefaultAzureCredential",
                "scopes": ["https://cognitiveservices.azure.com/.default"],
            },
        },
    },
}

client = ChatCompletionClient.load_component(config)


To view the full list of available configuration options, see the AzureOpenAIClientConfigurationConfigModel class.

Note
Right now only DefaultAzureCredential is supported with no additional args passed to it.


Note
The Azure OpenAI client by default sets the User-Agent header to autogen-python/{version}. To override this, you can set the variable autogen_ext.models.openai.AZURE_OPENAI_USER_AGENT environment variable to an empty string.

See here for how to use the Azure client directly or for more info.


component_type: ClassVar[ComponentType] = 'model'#
The logical type of the component.



component_config_schema#
alias of AzureOpenAIClientConfigurationConfigModel



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.openai.AzureOpenAIChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



_to_config() → AzureOpenAIClientConfigurationConfigModel[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





classmethod _from_config(config: AzureOpenAIClientConfigurationConfigModel) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

**示例**:
```python
pip install "autogen-ext[openai,azure]"

```

**示例**:
```python
# Text response, this is the default.
{"type": "text"}

```

**示例**:
```python
# JSON response, make sure to instruct the model to return JSON.
{"type": "json_object"}

```

**示例**:
```python
# Structured output response, with a pre-defined JSON schema.
{
    "type": "json_schema",
    "json_schema": {
        "name": "name of the schema, must be an identifier.",
        "description": "description for the model.",
        # You can convert a Pydantic (v2) model to JSON schema
        # using the `model_json_schema()` method.
        "schema": "<the JSON schema itself>",
        # Whether to enable strict schema adherence when
        # generating the output. If set to true, the model will
        # always follow the exact schema defined in the
        # `schema` field. Only a subset of JSON Schema is
        # supported when `strict` is `true`.
        # To learn more, read
        # https://platform.openai.com/docs/guides/structured-outputs.
        "strict": False,  # or True
    },
}

```

**示例**:
```python
from autogen_ext.auth.azure import AzureTokenProvider
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
from azure.identity import DefaultAzureCredential

# Create the token provider
token_provider = AzureTokenProvider(
    DefaultAzureCredential(),
    "https://cognitiveservices.azure.com/.default",
)

az_model_client = AzureOpenAIChatCompletionClient(
    azure_deployment="{your-azure-deployment}",
    model="{model-name, such as gpt-4o}",
    api_version="2024-06-01",
    azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
    azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.
    # api_key="sk-...", # For key-based authentication.
)

```

**示例**:
```python
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "AzureOpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o-2024-05-13",
        "azure_endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
        "azure_deployment": "{your-azure-deployment}",
        "api_version": "2024-06-01",
        "azure_ad_token_provider": {
            "provider": "autogen_ext.auth.azure.AzureTokenProvider",
            "config": {
                "provider_kind": "DefaultAzureCredential",
                "scopes": ["https://cognitiveservices.azure.com/.default"],
            },
        },
    },
}

client = ChatCompletionClient.load_component(config)

```

```python
component_type: ClassVar[ComponentType] = 'model'#
```

【中文翻译】The logical type of the component.

```python
component_config_schema#
```

【中文翻译】alias of AzureOpenAIClientConfigurationConfigModel

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.openai.AzureOpenAIChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
_to_config() → AzureOpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
classmethod _from_config(config: AzureOpenAIClientConfigurationConfigModel) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
class BaseOpenAIChatCompletionClient(client: AsyncOpenAI | AsyncAzureOpenAI, *, create_args: Dict[str, Any], model_capabilities: ModelCapabilities | None = None, model_info: ModelInfo | None = None, add_name_prefixes: bool = False)[source]#
```

【中文翻译】Bases: ChatCompletionClient


classmethod create_from_config(config: Dict[str, Any]) → ChatCompletionClient[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, max_consecutive_empty_chunk_tolerance: int = 0) → AsyncGenerator[str | CreateResult, None][source]#
Create a stream of string chunks from the model ending with a CreateResult.
Extends autogen_core.models.ChatCompletionClient.create_stream() to support OpenAI API.
In streaming, the default behaviour is not return token usage counts.
See: OpenAI API reference for possible args.
You can set extra_create_args={“stream_options”: {“include_usage”: True}}
(if supported by the accessed API) to
return a final chunk with usage set to a RequestUsage object
with prompt and completion token counts,
all preceding chunks will have usage as None.
See: OpenAI API reference for stream options.

Other examples of supported arguments that can be included in extra_create_args:
temperature (float): Controls the randomness of the output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
max_tokens (int): The maximum number of tokens to generate in the completion.
top_p (float): An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
frequency_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on their existing frequency in the text so far, decreasing the likelihood of repeated phrases.
presence_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on whether they appear in the text so far, encouraging the model to talk about new topics.






async close() → None[source]#



actual_usage() → RequestUsage[source]#



total_usage() → RequestUsage[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



property capabilities: ModelCapabilities#



property model_info: ModelInfo#

```python
classmethod create_from_config(config: Dict[str, Any]) → ChatCompletionClient[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Creates a single response from the model.

Parameters:

messages (Sequence[LLMMessage]) – The messages to send to the model.
tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to [].
json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither.
Defaults to None. If set to a Pydantic BaseModel type,
it will be used as the output type for structured output.
If set to a boolean, it will be used to determine whether to use JSON mode or not.
If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt.
extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}.
cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None.


Returns:
CreateResult – The result of the model call.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None, max_consecutive_empty_chunk_tolerance: int = 0) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Create a stream of string chunks from the model ending with a CreateResult.
Extends autogen_core.models.ChatCompletionClient.create_stream() to support OpenAI API.
In streaming, the default behaviour is not return token usage counts.
See: OpenAI API reference for possible args.
You can set extra_create_args={“stream_options”: {“include_usage”: True}}
(if supported by the accessed API) to
return a final chunk with usage set to a RequestUsage object
with prompt and completion token counts,
all preceding chunks will have usage as None.
See: OpenAI API reference for stream options.

Other examples of supported arguments that can be included in extra_create_args:
temperature (float): Controls the randomness of the output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
max_tokens (int): The maximum number of tokens to generate in the completion.
top_p (float): An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
frequency_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on their existing frequency in the text so far, decreasing the likelihood of repeated phrases.
presence_penalty (float): A value between -2.0 and 2.0 that penalizes new tokens based on whether they appear in the text so far, encouraging the model to talk about new topics.

```python
async close() → None[source]#
```

```python
actual_usage() → RequestUsage[source]#
```

```python
total_usage() → RequestUsage[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
property capabilities: ModelCapabilities#
```

```python
property model_info: ModelInfo#
```

```python
pydantic model AzureOpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: BaseOpenAIClientConfigurationConfigModel

Show JSON schema{
   "title": "AzureOpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "azure_endpoint": {
         "title": "Azure Endpoint",
         "type": "string"
      },
      "azure_deployment": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Azure Deployment"
      },
      "api_version": {
         "title": "Api Version",
         "type": "string"
      },
      "azure_ad_token": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Azure Ad Token"
      },
      "azure_ad_token_provider": {
         "anyOf": [
            {
               "$ref": "#/$defs/ComponentModel"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "ComponentModel": {
         "description": "Model class for a component. Contains all information required to instantiate a component.",
         "properties": {
            "provider": {
               "title": "Provider",
               "type": "string"
            },
            "component_type": {
               "anyOf": [
                  {
                     "enum": [
                        "model",
                        "agent",
                        "tool",
                        "termination",
                        "token_provider",
                        "workbench"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Component Type"
            },
            "version": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Version"
            },
            "component_version": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Component Version"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "label": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Label"
            },
            "config": {
               "title": "Config",
               "type": "object"
            }
         },
         "required": [
            "provider",
            "config"
         ],
         "title": "ComponentModel",
         "type": "object"
      },
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model",
      "azure_endpoint",
      "api_version"
   ]
}



Fields:

api_version (str)
azure_ad_token (str | None)
azure_ad_token_provider (autogen_core._component_config.ComponentModel | None)
azure_deployment (str | None)
azure_endpoint (str)





field azure_endpoint: str [Required]#



field azure_deployment: str | None = None#



field api_version: str [Required]#



field azure_ad_token: str | None = None#



field azure_ad_token_provider: ComponentModel | None = None#

**示例**:
```python
{
   "title": "AzureOpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "azure_endpoint": {
         "title": "Azure Endpoint",
         "type": "string"
      },
      "azure_deployment": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Azure Deployment"
      },
      "api_version": {
         "title": "Api Version",
         "type": "string"
      },
      "azure_ad_token": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Azure Ad Token"
      },
      "azure_ad_token_provider": {
         "anyOf": [
            {
               "$ref": "#/$defs/ComponentModel"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "ComponentModel": {
         "description": "Model class for a component. Contains all information required to instantiate a component.",
         "properties": {
            "provider": {
               "title": "Provider",
               "type": "string"
            },
            "component_type": {
               "anyOf": [
                  {
                     "enum": [
                        "model",
                        "agent",
                        "tool",
                        "termination",
                        "token_provider",
                        "workbench"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Component Type"
            },
            "version": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Version"
            },
            "component_version": {
               "anyOf": [
                  {
                     "type": "integer"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Component Version"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "label": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Label"
            },
            "config": {
               "title": "Config",
               "type": "object"
            }
         },
         "required": [
            "provider",
            "config"
         ],
         "title": "ComponentModel",
         "type": "object"
      },
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model",
      "azure_endpoint",
      "api_version"
   ]
}

```

```python
field azure_endpoint: str [Required]#
```

```python
field azure_deployment: str | None = None#
```

```python
field api_version: str [Required]#
```

```python
field azure_ad_token: str | None = None#
```

```python
field azure_ad_token_provider: ComponentModel | None = None#
```

```python
pydantic model OpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: BaseOpenAIClientConfigurationConfigModel

Show JSON schema{
   "title": "OpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "organization": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Organization"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

base_url (str | None)
organization (str | None)





field organization: str | None = None#



field base_url: str | None = None#

**示例**:
```python
{
   "title": "OpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      },
      "organization": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Organization"
      },
      "base_url": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Base Url"
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field organization: str | None = None#
```

```python
field base_url: str | None = None#
```

```python
pydantic model BaseOpenAIClientConfigurationConfigModel[source]#
```

【中文翻译】Bases: CreateArgumentsConfigModel

Show JSON schema{
   "title": "BaseOpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}



Fields:

add_name_prefixes (bool | None)
api_key (pydantic.types.SecretStr | None)
default_headers (Dict[str, str] | None)
max_retries (int | None)
model (str)
model_capabilities (autogen_core.models._model_client.ModelCapabilities | None)
model_info (autogen_core.models._model_client.ModelInfo | None)
timeout (float | None)





field model: str [Required]#



field api_key: SecretStr | None = None#



field timeout: float | None = None#



field max_retries: int | None = None#



field model_capabilities: ModelCapabilities | None = None#



field model_info: ModelInfo | None = None#



field add_name_prefixes: bool | None = None#



field default_headers: Dict[str, str] | None = None#

**示例**:
```python
{
   "title": "BaseOpenAIClientConfigurationConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model": {
         "title": "Model",
         "type": "string"
      },
      "api_key": {
         "anyOf": [
            {
               "format": "password",
               "type": "string",
               "writeOnly": true
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Api Key"
      },
      "timeout": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Timeout"
      },
      "max_retries": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Retries"
      },
      "model_capabilities": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelCapabilities"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "model_info": {
         "anyOf": [
            {
               "$ref": "#/$defs/ModelInfo"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "add_name_prefixes": {
         "anyOf": [
            {
               "type": "boolean"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Add Name Prefixes"
      },
      "default_headers": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Default Headers"
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ModelCapabilities": {
         "deprecated": true,
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output"
         ],
         "title": "ModelCapabilities",
         "type": "object"
      },
      "ModelInfo": {
         "description": "ModelInfo is a dictionary that contains information about a model's properties.\nIt is expected to be used in the model_info property of a model client.\n\nWe are expecting this to grow over time as we add more features.",
         "properties": {
            "vision": {
               "title": "Vision",
               "type": "boolean"
            },
            "function_calling": {
               "title": "Function Calling",
               "type": "boolean"
            },
            "json_output": {
               "title": "Json Output",
               "type": "boolean"
            },
            "family": {
               "anyOf": [
                  {
                     "enum": [
                        "gpt-41",
                        "gpt-45",
                        "gpt-4o",
                        "o1",
                        "o3",
                        "o4",
                        "gpt-4",
                        "gpt-35",
                        "r1",
                        "gemini-1.5-flash",
                        "gemini-1.5-pro",
                        "gemini-2.0-flash",
                        "gemini-2.5-pro",
                        "claude-3-haiku",
                        "claude-3-sonnet",
                        "claude-3-opus",
                        "claude-3-5-haiku",
                        "claude-3-5-sonnet",
                        "claude-3-7-sonnet",
                        "unknown"
                     ],
                     "type": "string"
                  },
                  {
                     "type": "string"
                  }
               ],
               "title": "Family"
            },
            "structured_output": {
               "title": "Structured Output",
               "type": "boolean"
            },
            "multiple_system_messages": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Multiple System Messages"
            }
         },
         "required": [
            "vision",
            "function_calling",
            "json_output",
            "family",
            "structured_output"
         ],
         "title": "ModelInfo",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   },
   "required": [
      "model"
   ]
}

```

```python
field model: str [Required]#
```

```python
field api_key: SecretStr | None = None#
```

```python
field timeout: float | None = None#
```

```python
field max_retries: int | None = None#
```

```python
field model_capabilities: ModelCapabilities | None = None#
```

```python
field model_info: ModelInfo | None = None#
```

```python
field add_name_prefixes: bool | None = None#
```

```python
field default_headers: Dict[str, str] | None = None#
```

```python
pydantic model CreateArgumentsConfigModel[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   }
}



Fields:

frequency_penalty (float | None)
logit_bias (Dict[str, int] | None)
max_tokens (int | None)
n (int | None)
presence_penalty (float | None)
response_format (autogen_ext.models.openai.config.ResponseFormat | None)
seed (int | None)
stop (str | List[str] | None)
stream_options (autogen_ext.models.openai.config.StreamOptions | None)
temperature (float | None)
top_p (float | None)
user (str | None)





field frequency_penalty: float | None = None#



field logit_bias: Dict[str, int] | None = None#



field max_tokens: int | None = None#



field n: int | None = None#



field presence_penalty: float | None = None#



field response_format: ResponseFormat | None = None#



field seed: int | None = None#



field stop: str | List[str] | None = None#



field temperature: float | None = None#



field top_p: float | None = None#



field user: str | None = None#



field stream_options: StreamOptions | None = None#

**示例**:
```python
{
   "title": "CreateArgumentsConfigModel",
   "type": "object",
   "properties": {
      "frequency_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Frequency Penalty"
      },
      "logit_bias": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "integer"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Logit Bias"
      },
      "max_tokens": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Max Tokens"
      },
      "n": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "N"
      },
      "presence_penalty": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Presence Penalty"
      },
      "response_format": {
         "anyOf": [
            {
               "$ref": "#/$defs/ResponseFormat"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "seed": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Seed"
      },
      "stop": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop"
      },
      "temperature": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Temperature"
      },
      "top_p": {
         "anyOf": [
            {
               "type": "number"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Top P"
      },
      "user": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "User"
      },
      "stream_options": {
         "anyOf": [
            {
               "$ref": "#/$defs/StreamOptions"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      }
   },
   "$defs": {
      "JSONSchema": {
         "properties": {
            "name": {
               "title": "Name",
               "type": "string"
            },
            "description": {
               "title": "Description",
               "type": "string"
            },
            "schema": {
               "title": "Schema",
               "type": "object"
            },
            "strict": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Strict"
            }
         },
         "required": [
            "name"
         ],
         "title": "JSONSchema",
         "type": "object"
      },
      "ResponseFormat": {
         "properties": {
            "type": {
               "enum": [
                  "text",
                  "json_object",
                  "json_schema"
               ],
               "title": "Type",
               "type": "string"
            },
            "json_schema": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/JSONSchema"
                  },
                  {
                     "type": "null"
                  }
               ]
            }
         },
         "required": [
            "type",
            "json_schema"
         ],
         "title": "ResponseFormat",
         "type": "object"
      },
      "StreamOptions": {
         "properties": {
            "include_usage": {
               "title": "Include Usage",
               "type": "boolean"
            }
         },
         "required": [
            "include_usage"
         ],
         "title": "StreamOptions",
         "type": "object"
      }
   }
}

```

```python
field frequency_penalty: float | None = None#
```

```python
field logit_bias: Dict[str, int] | None = None#
```

```python
field max_tokens: int | None = None#
```

```python
field n: int | None = None#
```

```python
field presence_penalty: float | None = None#
```

```python
field response_format: ResponseFormat | None = None#
```

```python
field seed: int | None = None#
```

```python
field stop: str | List[str] | None = None#
```

```python
field temperature: float | None = None#
```

```python
field top_p: float | None = None#
```

```python
field user: str | None = None#
```

```python
field stream_options: StreamOptions | None = None#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.cache

【中文翻译】next

【中文翻译】autogen_ext.models.replay

### autogen_ext.models.replay {autogen_extmodelsreplay}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html)

```python
class ReplayChatCompletionClient(chat_completions: Sequence[str | CreateResult], model_info: ModelInfo | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient, Component[ReplayChatCompletionClientConfig]
A mock chat completion client that replays predefined responses using an index-based approach.
This class simulates a chat completion client by replaying a predefined list of responses. It supports both single completion and streaming responses. The responses can be either strings or CreateResult objects. The client now uses an index-based approach to access the responses, allowing for resetting the state.

Note
The responses can be either strings or CreateResult objects.


Parameters:
chat_completions (Sequence[Union[str, CreateResult]]) – A list of predefined responses to replay.

Raises:
ValueError("No more mock responses available") – If the list of provided outputs are exhausted.


Examples:
Simple chat completion client to return pre-defined responses.

from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
        "I'm happy to help with any questions you have.",
        "Is there anything else I can assist you with?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?"



Simple streaming chat completion client to return pre-defined responses

import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
        "I'm happy to help with any questions you have.",
        "Is there anything else I can assist you with?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]

    async for token in client.create_stream(messages):
        print(token, end="")  # Output: "Hello, how can I assist you today?"

    async for token in client.create_stream(messages):
        print(token, end="")  # Output: "I'm happy to help with any questions you have."

    asyncio.run(example())



Using .reset to reset the chat client state

import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?"

    response = await client.create(messages)  # Raises ValueError("No more mock responses available")

    client.reset()  # Reset the client state (current index of message and token usages)
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?" again


asyncio.run(example())





classmethod _from_config(config: ReplayChatCompletionClientConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → ReplayChatCompletionClientConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





actual_usage() → RequestUsage[source]#



property capabilities: ModelCapabilities#
Return mock capabilities.



async close() → None[source]#



component_config_schema#
alias of ReplayChatCompletionClientConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.replay.ReplayChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'replay_chat_completion_client'#
The logical type of the component.



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Return the next completion from the list.



property create_calls: List[Dict[str, Any]]#
Return the arguments of the calls made to the create method.



async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Return the next completion as a stream.



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



reset() → None[source]#
Reset the client state and usage to its initial state.



set_cached_bool_value(value: bool) → None[source]#



total_usage() → RequestUsage[source]#

**示例**:
```python
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
        "I'm happy to help with any questions you have.",
        "Is there anything else I can assist you with?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?"

```

**示例**:
```python
import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
        "I'm happy to help with any questions you have.",
        "Is there anything else I can assist you with?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]

    async for token in client.create_stream(messages):
        print(token, end="")  # Output: "Hello, how can I assist you today?"

    async for token in client.create_stream(messages):
        print(token, end="")  # Output: "I'm happy to help with any questions you have."

    asyncio.run(example())

```

**示例**:
```python
import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?"

    response = await client.create(messages)  # Raises ValueError("No more mock responses available")

    client.reset()  # Reset the client state (current index of message and token usages)
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?" again


asyncio.run(example())

```

```python
classmethod _from_config(config: ReplayChatCompletionClientConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → ReplayChatCompletionClientConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelCapabilities#
```

【中文翻译】Return mock capabilities.

```python
async close() → None[source]#
```

```python
component_config_schema#
```

【中文翻译】alias of ReplayChatCompletionClientConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.replay.ReplayChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'replay_chat_completion_client'#
```

【中文翻译】The logical type of the component.

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Return the next completion from the list.

```python
property create_calls: List[Dict[str, Any]]#
```

【中文翻译】Return the arguments of the calls made to the create method.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Return the next completion as a stream.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
reset() → None[source]#
```

【中文翻译】Reset the client state and usage to its initial state.

```python
set_cached_bool_value(value: bool) → None[source]#
```

```python
total_usage() → RequestUsage[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.openai

【中文翻译】next

【中文翻译】autogen_ext.models.azure

### autogen_ext.models.replay {autogen_extmodelsreplay}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html)

```python
class ReplayChatCompletionClient(chat_completions: Sequence[str | CreateResult], model_info: ModelInfo | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient, Component[ReplayChatCompletionClientConfig]
A mock chat completion client that replays predefined responses using an index-based approach.
This class simulates a chat completion client by replaying a predefined list of responses. It supports both single completion and streaming responses. The responses can be either strings or CreateResult objects. The client now uses an index-based approach to access the responses, allowing for resetting the state.

Note
The responses can be either strings or CreateResult objects.


Parameters:
chat_completions (Sequence[Union[str, CreateResult]]) – A list of predefined responses to replay.

Raises:
ValueError("No more mock responses available") – If the list of provided outputs are exhausted.


Examples:
Simple chat completion client to return pre-defined responses.

from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
        "I'm happy to help with any questions you have.",
        "Is there anything else I can assist you with?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?"



Simple streaming chat completion client to return pre-defined responses

import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
        "I'm happy to help with any questions you have.",
        "Is there anything else I can assist you with?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]

    async for token in client.create_stream(messages):
        print(token, end="")  # Output: "Hello, how can I assist you today?"

    async for token in client.create_stream(messages):
        print(token, end="")  # Output: "I'm happy to help with any questions you have."

    asyncio.run(example())



Using .reset to reset the chat client state

import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?"

    response = await client.create(messages)  # Raises ValueError("No more mock responses available")

    client.reset()  # Reset the client state (current index of message and token usages)
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?" again


asyncio.run(example())





classmethod _from_config(config: ReplayChatCompletionClientConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → ReplayChatCompletionClientConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





actual_usage() → RequestUsage[source]#



property capabilities: ModelCapabilities#
Return mock capabilities.



async close() → None[source]#



component_config_schema#
alias of ReplayChatCompletionClientConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.models.replay.ReplayChatCompletionClient'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'replay_chat_completion_client'#
The logical type of the component.



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Return the next completion from the list.



property create_calls: List[Dict[str, Any]]#
Return the arguments of the calls made to the create method.



async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Return the next completion as a stream.



property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



reset() → None[source]#
Reset the client state and usage to its initial state.



set_cached_bool_value(value: bool) → None[source]#



total_usage() → RequestUsage[source]#

**示例**:
```python
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
        "I'm happy to help with any questions you have.",
        "Is there anything else I can assist you with?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?"

```

**示例**:
```python
import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
        "I'm happy to help with any questions you have.",
        "Is there anything else I can assist you with?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]

    async for token in client.create_stream(messages):
        print(token, end="")  # Output: "Hello, how can I assist you today?"

    async for token in client.create_stream(messages):
        print(token, end="")  # Output: "I'm happy to help with any questions you have."

    asyncio.run(example())

```

**示例**:
```python
import asyncio
from autogen_core.models import UserMessage
from autogen_ext.models.replay import ReplayChatCompletionClient


async def example():
    chat_completions = [
        "Hello, how can I assist you today?",
    ]
    client = ReplayChatCompletionClient(chat_completions)
    messages = [UserMessage(content="What can you do?", source="user")]
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?"

    response = await client.create(messages)  # Raises ValueError("No more mock responses available")

    client.reset()  # Reset the client state (current index of message and token usages)
    response = await client.create(messages)
    print(response.content)  # Output: "Hello, how can I assist you today?" again


asyncio.run(example())

```

```python
classmethod _from_config(config: ReplayChatCompletionClientConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → ReplayChatCompletionClientConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelCapabilities#
```

【中文翻译】Return mock capabilities.

```python
async close() → None[source]#
```

```python
component_config_schema#
```

【中文翻译】alias of ReplayChatCompletionClientConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.models.replay.ReplayChatCompletionClient'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'replay_chat_completion_client'#
```

【中文翻译】The logical type of the component.

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Return the next completion from the list.

```python
property create_calls: List[Dict[str, Any]]#
```

【中文翻译】Return the arguments of the calls made to the create method.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Return the next completion as a stream.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
reset() → None[source]#
```

【中文翻译】Reset the client state and usage to its initial state.

```python
set_cached_bool_value(value: bool) → None[source]#
```

```python
total_usage() → RequestUsage[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.openai

【中文翻译】next

【中文翻译】autogen_ext.models.azure

### autogen_ext.models.semantic_kernel {autogen_extmodelssemantic_kernel}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html)

```python
class SKChatCompletionAdapter(sk_client: ChatCompletionClientBase, kernel: Kernel | None = None, prompt_settings: PromptExecutionSettings | None = None, model_info: ModelInfo | None = None, service_id: str | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient
SKChatCompletionAdapter is an adapter that allows using Semantic Kernel model clients
as Autogen ChatCompletion clients. This makes it possible to seamlessly integrate
Semantic Kernel connectors (e.g., Azure OpenAI, Google Gemini, Ollama, etc.) into
Autogen agents that rely on a ChatCompletionClient interface.
By leveraging this adapter, you can:

Pass in a Kernel and any supported Semantic Kernel ChatCompletionClientBase connector.
Provide tools (via Autogen Tool or ToolSchema) for function calls during chat completion.
Stream responses or retrieve them in a single request.

Provide prompt settings to control the chat completion behavior either globally through the constructoror on a per-request basis through the extra_create_args dictionary.




The list of extras that can be installed:

semantic-kernel-anthropic: Install this extra to use Anthropic models.
semantic-kernel-google: Install this extra to use Google Gemini models.
semantic-kernel-ollama: Install this extra to use Ollama models.
semantic-kernel-mistralai: Install this extra to use MistralAI models.
semantic-kernel-aws: Install this extra to use AWS models.
semantic-kernel-hugging-face: Install this extra to use Hugging Face models.


Parameters:

sk_client (ChatCompletionClientBase) – The Semantic Kernel client to wrap (e.g., AzureChatCompletion, GoogleAIChatCompletion, OllamaChatCompletion).
kernel (Optional[Kernel]) – The Semantic Kernel instance to use for executing requests. If not provided, one must be passed
in the extra_create_args for each request.
prompt_settings (Optional[PromptExecutionSettings]) – Default prompt execution settings to use. Can be overridden per request.
model_info (Optional[ModelInfo]) – Information about the model’s capabilities.
service_id (Optional[str]) – Optional service identifier.



Examples
Anthropic models with function calling:
pip install "autogen-ext[semantic-kernel-anthropic]"


import asyncio
import os

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.models import ModelFamily, UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion, AnthropicChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory


async def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is 75 degrees."


async def main() -> None:
    sk_client = AnthropicChatCompletion(
        ai_model_id="claude-3-5-sonnet-20241022",
        api_key=os.environ["ANTHROPIC_API_KEY"],
        service_id="my-service-id",  # Optional; for targeting specific services within Semantic Kernel
    )
    settings = AnthropicChatPromptExecutionSettings(
        temperature=0.2,
    )

    model_client = SKChatCompletionAdapter(
        sk_client,
        kernel=Kernel(memory=NullMemory()),
        prompt_settings=settings,
        model_info={
            "function_calling": True,
            "json_output": True,
            "vision": True,
            "family": ModelFamily.CLAUDE_3_5_SONNET,
            "structured_output": True,
        },
    )

    # Call the model directly.
    response = await model_client.create([UserMessage(content="What is the capital of France?", source="test")])
    print(response)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent(
        "assistant", model_client=model_client, system_message="You are a helpful assistant.", tools=[get_weather]
    )
    # Call the assistant with a task.
    await Console(assistant.run_stream(task="What is the weather in Paris and London?"))


asyncio.run(main())


Google Gemini models with function calling:
pip install "autogen-ext[semantic-kernel-google]"


import asyncio
import os

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.models import UserMessage, ModelFamily
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.google.google_ai import (
    GoogleAIChatCompletion,
    GoogleAIChatPromptExecutionSettings,
)
from semantic_kernel.memory.null_memory import NullMemory


def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is 75 degrees."


async def main() -> None:
    sk_client = GoogleAIChatCompletion(
        gemini_model_id="gemini-2.0-flash",
        api_key=os.environ["GEMINI_API_KEY"],
    )
    settings = GoogleAIChatPromptExecutionSettings(
        temperature=0.2,
    )

    kernel = Kernel(memory=NullMemory())

    model_client = SKChatCompletionAdapter(
        sk_client,
        kernel=kernel,
        prompt_settings=settings,
        model_info={
            "family": ModelFamily.GEMINI_2_0_FLASH,
            "function_calling": True,
            "json_output": True,
            "vision": True,
            "structured_output": True,
        },
    )

    # Call the model directly.
    model_result = await model_client.create(
        messages=[UserMessage(content="What is the capital of France?", source="User")]
    )
    print(model_result)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent(
        "assistant", model_client=model_client, tools=[get_weather], system_message="You are a helpful assistant."
    )
    # Call the assistant with a task.
    stream = assistant.run_stream(task="What is the weather in Paris and London?")
    await Console(stream)


asyncio.run(main())


Ollama models:
pip install "autogen-ext[semantic-kernel-ollama]"


import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.models import UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion, OllamaChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory


async def main() -> None:
    sk_client = OllamaChatCompletion(
        host="http://localhost:11434",
        ai_model_id="llama3.2:latest",
    )
    ollama_settings = OllamaChatPromptExecutionSettings(
        options={"temperature": 0.5},
    )

    model_client = SKChatCompletionAdapter(
        sk_client, kernel=Kernel(memory=NullMemory()), prompt_settings=ollama_settings
    )

    # Call the model directly.
    model_result = await model_client.create(
        messages=[UserMessage(content="What is the capital of France?", source="User")]
    )
    print(model_result)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent("assistant", model_client=model_client)
    # Call the assistant with a task.
    result = await assistant.run(task="What is the capital of France?")
    print(result)


asyncio.run(main())




actual_usage() → RequestUsage[source]#



property capabilities: ModelInfo#



async close() → None[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Create a chat completion using the Semantic Kernel client.
The extra_create_args dictionary can include two special keys:


“kernel” (optional):An instance of semantic_kernel.Kernel used to execute the request.
If not provided either in constructor or extra_create_args, a ValueError is raised.




“prompt_execution_settings” (optional):An instance of a PromptExecutionSettings subclass corresponding to the
underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
prompt settings will be used.





Parameters:

messages – The list of LLM messages to send.
tools – The tools that may be invoked during the chat.
json_output – Whether the model is expected to return JSON.
extra_create_args – Additional arguments to control the chat completion behavior.
cancellation_token – Token allowing cancellation of the request.


Returns:
CreateResult – The result of the chat completion.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Create a streaming chat completion using the Semantic Kernel client.
The extra_create_args dictionary can include two special keys:


“kernel” (optional):An instance of semantic_kernel.Kernel used to execute the request.
If not provided either in constructor or extra_create_args, a ValueError is raised.




“prompt_execution_settings” (optional):An instance of a PromptExecutionSettings subclass corresponding to the
underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
prompt settings will be used.





Parameters:

messages – The list of LLM messages to send.
tools – The tools that may be invoked during the chat.
json_output – Whether the model is expected to return JSON.
extra_create_args – Additional arguments to control the chat completion behavior.
cancellation_token – Token allowing cancellation of the request.


Yields:
Union[str, CreateResult] – Either a string chunk of the response or a CreateResult containing function calls.





property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



total_usage() → RequestUsage[source]#

**示例**:
```python
pip install "autogen-ext[semantic-kernel-anthropic]"

```

**示例**:
```python
import asyncio
import os

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.models import ModelFamily, UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion, AnthropicChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory


async def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is 75 degrees."


async def main() -> None:
    sk_client = AnthropicChatCompletion(
        ai_model_id="claude-3-5-sonnet-20241022",
        api_key=os.environ["ANTHROPIC_API_KEY"],
        service_id="my-service-id",  # Optional; for targeting specific services within Semantic Kernel
    )
    settings = AnthropicChatPromptExecutionSettings(
        temperature=0.2,
    )

    model_client = SKChatCompletionAdapter(
        sk_client,
        kernel=Kernel(memory=NullMemory()),
        prompt_settings=settings,
        model_info={
            "function_calling": True,
            "json_output": True,
            "vision": True,
            "family": ModelFamily.CLAUDE_3_5_SONNET,
            "structured_output": True,
        },
    )

    # Call the model directly.
    response = await model_client.create([UserMessage(content="What is the capital of France?", source="test")])
    print(response)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent(
        "assistant", model_client=model_client, system_message="You are a helpful assistant.", tools=[get_weather]
    )
    # Call the assistant with a task.
    await Console(assistant.run_stream(task="What is the weather in Paris and London?"))


asyncio.run(main())

```

**示例**:
```python
pip install "autogen-ext[semantic-kernel-google]"

```

**示例**:
```python
import asyncio
import os

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.models import UserMessage, ModelFamily
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.google.google_ai import (
    GoogleAIChatCompletion,
    GoogleAIChatPromptExecutionSettings,
)
from semantic_kernel.memory.null_memory import NullMemory


def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is 75 degrees."


async def main() -> None:
    sk_client = GoogleAIChatCompletion(
        gemini_model_id="gemini-2.0-flash",
        api_key=os.environ["GEMINI_API_KEY"],
    )
    settings = GoogleAIChatPromptExecutionSettings(
        temperature=0.2,
    )

    kernel = Kernel(memory=NullMemory())

    model_client = SKChatCompletionAdapter(
        sk_client,
        kernel=kernel,
        prompt_settings=settings,
        model_info={
            "family": ModelFamily.GEMINI_2_0_FLASH,
            "function_calling": True,
            "json_output": True,
            "vision": True,
            "structured_output": True,
        },
    )

    # Call the model directly.
    model_result = await model_client.create(
        messages=[UserMessage(content="What is the capital of France?", source="User")]
    )
    print(model_result)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent(
        "assistant", model_client=model_client, tools=[get_weather], system_message="You are a helpful assistant."
    )
    # Call the assistant with a task.
    stream = assistant.run_stream(task="What is the weather in Paris and London?")
    await Console(stream)


asyncio.run(main())

```

**示例**:
```python
pip install "autogen-ext[semantic-kernel-ollama]"

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.models import UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion, OllamaChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory


async def main() -> None:
    sk_client = OllamaChatCompletion(
        host="http://localhost:11434",
        ai_model_id="llama3.2:latest",
    )
    ollama_settings = OllamaChatPromptExecutionSettings(
        options={"temperature": 0.5},
    )

    model_client = SKChatCompletionAdapter(
        sk_client, kernel=Kernel(memory=NullMemory()), prompt_settings=ollama_settings
    )

    # Call the model directly.
    model_result = await model_client.create(
        messages=[UserMessage(content="What is the capital of France?", source="User")]
    )
    print(model_result)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent("assistant", model_client=model_client)
    # Call the assistant with a task.
    result = await assistant.run(task="What is the capital of France?")
    print(result)


asyncio.run(main())

```

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelInfo#
```

```python
async close() → None[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Create a chat completion using the Semantic Kernel client.
The extra_create_args dictionary can include two special keys:


“kernel” (optional):An instance of semantic_kernel.Kernel used to execute the request.
If not provided either in constructor or extra_create_args, a ValueError is raised.




“prompt_execution_settings” (optional):An instance of a PromptExecutionSettings subclass corresponding to the
underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
prompt settings will be used.





Parameters:

messages – The list of LLM messages to send.
tools – The tools that may be invoked during the chat.
json_output – Whether the model is expected to return JSON.
extra_create_args – Additional arguments to control the chat completion behavior.
cancellation_token – Token allowing cancellation of the request.


Returns:
CreateResult – The result of the chat completion.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Create a streaming chat completion using the Semantic Kernel client.
The extra_create_args dictionary can include two special keys:


“kernel” (optional):An instance of semantic_kernel.Kernel used to execute the request.
If not provided either in constructor or extra_create_args, a ValueError is raised.




“prompt_execution_settings” (optional):An instance of a PromptExecutionSettings subclass corresponding to the
underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
prompt settings will be used.





Parameters:

messages – The list of LLM messages to send.
tools – The tools that may be invoked during the chat.
json_output – Whether the model is expected to return JSON.
extra_create_args – Additional arguments to control the chat completion behavior.
cancellation_token – Token allowing cancellation of the request.


Yields:
Union[str, CreateResult] – Either a string chunk of the response or a CreateResult containing function calls.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.anthropic

【中文翻译】next

【中文翻译】autogen_ext.models.ollama

### autogen_ext.models.semantic_kernel {autogen_extmodelssemantic_kernel}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html)

```python
class SKChatCompletionAdapter(sk_client: ChatCompletionClientBase, kernel: Kernel | None = None, prompt_settings: PromptExecutionSettings | None = None, model_info: ModelInfo | None = None, service_id: str | None = None)[source]#
```

【中文翻译】Bases: ChatCompletionClient
SKChatCompletionAdapter is an adapter that allows using Semantic Kernel model clients
as Autogen ChatCompletion clients. This makes it possible to seamlessly integrate
Semantic Kernel connectors (e.g., Azure OpenAI, Google Gemini, Ollama, etc.) into
Autogen agents that rely on a ChatCompletionClient interface.
By leveraging this adapter, you can:

Pass in a Kernel and any supported Semantic Kernel ChatCompletionClientBase connector.
Provide tools (via Autogen Tool or ToolSchema) for function calls during chat completion.
Stream responses or retrieve them in a single request.

Provide prompt settings to control the chat completion behavior either globally through the constructoror on a per-request basis through the extra_create_args dictionary.




The list of extras that can be installed:

semantic-kernel-anthropic: Install this extra to use Anthropic models.
semantic-kernel-google: Install this extra to use Google Gemini models.
semantic-kernel-ollama: Install this extra to use Ollama models.
semantic-kernel-mistralai: Install this extra to use MistralAI models.
semantic-kernel-aws: Install this extra to use AWS models.
semantic-kernel-hugging-face: Install this extra to use Hugging Face models.


Parameters:

sk_client (ChatCompletionClientBase) – The Semantic Kernel client to wrap (e.g., AzureChatCompletion, GoogleAIChatCompletion, OllamaChatCompletion).
kernel (Optional[Kernel]) – The Semantic Kernel instance to use for executing requests. If not provided, one must be passed
in the extra_create_args for each request.
prompt_settings (Optional[PromptExecutionSettings]) – Default prompt execution settings to use. Can be overridden per request.
model_info (Optional[ModelInfo]) – Information about the model’s capabilities.
service_id (Optional[str]) – Optional service identifier.



Examples
Anthropic models with function calling:
pip install "autogen-ext[semantic-kernel-anthropic]"


import asyncio
import os

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.models import ModelFamily, UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion, AnthropicChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory


async def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is 75 degrees."


async def main() -> None:
    sk_client = AnthropicChatCompletion(
        ai_model_id="claude-3-5-sonnet-20241022",
        api_key=os.environ["ANTHROPIC_API_KEY"],
        service_id="my-service-id",  # Optional; for targeting specific services within Semantic Kernel
    )
    settings = AnthropicChatPromptExecutionSettings(
        temperature=0.2,
    )

    model_client = SKChatCompletionAdapter(
        sk_client,
        kernel=Kernel(memory=NullMemory()),
        prompt_settings=settings,
        model_info={
            "function_calling": True,
            "json_output": True,
            "vision": True,
            "family": ModelFamily.CLAUDE_3_5_SONNET,
            "structured_output": True,
        },
    )

    # Call the model directly.
    response = await model_client.create([UserMessage(content="What is the capital of France?", source="test")])
    print(response)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent(
        "assistant", model_client=model_client, system_message="You are a helpful assistant.", tools=[get_weather]
    )
    # Call the assistant with a task.
    await Console(assistant.run_stream(task="What is the weather in Paris and London?"))


asyncio.run(main())


Google Gemini models with function calling:
pip install "autogen-ext[semantic-kernel-google]"


import asyncio
import os

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.models import UserMessage, ModelFamily
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.google.google_ai import (
    GoogleAIChatCompletion,
    GoogleAIChatPromptExecutionSettings,
)
from semantic_kernel.memory.null_memory import NullMemory


def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is 75 degrees."


async def main() -> None:
    sk_client = GoogleAIChatCompletion(
        gemini_model_id="gemini-2.0-flash",
        api_key=os.environ["GEMINI_API_KEY"],
    )
    settings = GoogleAIChatPromptExecutionSettings(
        temperature=0.2,
    )

    kernel = Kernel(memory=NullMemory())

    model_client = SKChatCompletionAdapter(
        sk_client,
        kernel=kernel,
        prompt_settings=settings,
        model_info={
            "family": ModelFamily.GEMINI_2_0_FLASH,
            "function_calling": True,
            "json_output": True,
            "vision": True,
            "structured_output": True,
        },
    )

    # Call the model directly.
    model_result = await model_client.create(
        messages=[UserMessage(content="What is the capital of France?", source="User")]
    )
    print(model_result)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent(
        "assistant", model_client=model_client, tools=[get_weather], system_message="You are a helpful assistant."
    )
    # Call the assistant with a task.
    stream = assistant.run_stream(task="What is the weather in Paris and London?")
    await Console(stream)


asyncio.run(main())


Ollama models:
pip install "autogen-ext[semantic-kernel-ollama]"


import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.models import UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion, OllamaChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory


async def main() -> None:
    sk_client = OllamaChatCompletion(
        host="http://localhost:11434",
        ai_model_id="llama3.2:latest",
    )
    ollama_settings = OllamaChatPromptExecutionSettings(
        options={"temperature": 0.5},
    )

    model_client = SKChatCompletionAdapter(
        sk_client, kernel=Kernel(memory=NullMemory()), prompt_settings=ollama_settings
    )

    # Call the model directly.
    model_result = await model_client.create(
        messages=[UserMessage(content="What is the capital of France?", source="User")]
    )
    print(model_result)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent("assistant", model_client=model_client)
    # Call the assistant with a task.
    result = await assistant.run(task="What is the capital of France?")
    print(result)


asyncio.run(main())




actual_usage() → RequestUsage[source]#



property capabilities: ModelInfo#



async close() → None[source]#



count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
Create a chat completion using the Semantic Kernel client.
The extra_create_args dictionary can include two special keys:


“kernel” (optional):An instance of semantic_kernel.Kernel used to execute the request.
If not provided either in constructor or extra_create_args, a ValueError is raised.




“prompt_execution_settings” (optional):An instance of a PromptExecutionSettings subclass corresponding to the
underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
prompt settings will be used.





Parameters:

messages – The list of LLM messages to send.
tools – The tools that may be invoked during the chat.
json_output – Whether the model is expected to return JSON.
extra_create_args – Additional arguments to control the chat completion behavior.
cancellation_token – Token allowing cancellation of the request.


Returns:
CreateResult – The result of the chat completion.





async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
Create a streaming chat completion using the Semantic Kernel client.
The extra_create_args dictionary can include two special keys:


“kernel” (optional):An instance of semantic_kernel.Kernel used to execute the request.
If not provided either in constructor or extra_create_args, a ValueError is raised.




“prompt_execution_settings” (optional):An instance of a PromptExecutionSettings subclass corresponding to the
underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
prompt settings will be used.





Parameters:

messages – The list of LLM messages to send.
tools – The tools that may be invoked during the chat.
json_output – Whether the model is expected to return JSON.
extra_create_args – Additional arguments to control the chat completion behavior.
cancellation_token – Token allowing cancellation of the request.


Yields:
Union[str, CreateResult] – Either a string chunk of the response or a CreateResult containing function calls.





property model_info: ModelInfo#



remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#



total_usage() → RequestUsage[source]#

**示例**:
```python
pip install "autogen-ext[semantic-kernel-anthropic]"

```

**示例**:
```python
import asyncio
import os

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.models import ModelFamily, UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion, AnthropicChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory


async def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is 75 degrees."


async def main() -> None:
    sk_client = AnthropicChatCompletion(
        ai_model_id="claude-3-5-sonnet-20241022",
        api_key=os.environ["ANTHROPIC_API_KEY"],
        service_id="my-service-id",  # Optional; for targeting specific services within Semantic Kernel
    )
    settings = AnthropicChatPromptExecutionSettings(
        temperature=0.2,
    )

    model_client = SKChatCompletionAdapter(
        sk_client,
        kernel=Kernel(memory=NullMemory()),
        prompt_settings=settings,
        model_info={
            "function_calling": True,
            "json_output": True,
            "vision": True,
            "family": ModelFamily.CLAUDE_3_5_SONNET,
            "structured_output": True,
        },
    )

    # Call the model directly.
    response = await model_client.create([UserMessage(content="What is the capital of France?", source="test")])
    print(response)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent(
        "assistant", model_client=model_client, system_message="You are a helpful assistant.", tools=[get_weather]
    )
    # Call the assistant with a task.
    await Console(assistant.run_stream(task="What is the weather in Paris and London?"))


asyncio.run(main())

```

**示例**:
```python
pip install "autogen-ext[semantic-kernel-google]"

```

**示例**:
```python
import asyncio
import os

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.models import UserMessage, ModelFamily
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.google.google_ai import (
    GoogleAIChatCompletion,
    GoogleAIChatPromptExecutionSettings,
)
from semantic_kernel.memory.null_memory import NullMemory


def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is 75 degrees."


async def main() -> None:
    sk_client = GoogleAIChatCompletion(
        gemini_model_id="gemini-2.0-flash",
        api_key=os.environ["GEMINI_API_KEY"],
    )
    settings = GoogleAIChatPromptExecutionSettings(
        temperature=0.2,
    )

    kernel = Kernel(memory=NullMemory())

    model_client = SKChatCompletionAdapter(
        sk_client,
        kernel=kernel,
        prompt_settings=settings,
        model_info={
            "family": ModelFamily.GEMINI_2_0_FLASH,
            "function_calling": True,
            "json_output": True,
            "vision": True,
            "structured_output": True,
        },
    )

    # Call the model directly.
    model_result = await model_client.create(
        messages=[UserMessage(content="What is the capital of France?", source="User")]
    )
    print(model_result)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent(
        "assistant", model_client=model_client, tools=[get_weather], system_message="You are a helpful assistant."
    )
    # Call the assistant with a task.
    stream = assistant.run_stream(task="What is the weather in Paris and London?")
    await Console(stream)


asyncio.run(main())

```

**示例**:
```python
pip install "autogen-ext[semantic-kernel-ollama]"

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_core.models import UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion, OllamaChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory


async def main() -> None:
    sk_client = OllamaChatCompletion(
        host="http://localhost:11434",
        ai_model_id="llama3.2:latest",
    )
    ollama_settings = OllamaChatPromptExecutionSettings(
        options={"temperature": 0.5},
    )

    model_client = SKChatCompletionAdapter(
        sk_client, kernel=Kernel(memory=NullMemory()), prompt_settings=ollama_settings
    )

    # Call the model directly.
    model_result = await model_client.create(
        messages=[UserMessage(content="What is the capital of France?", source="User")]
    )
    print(model_result)

    # Create an assistant agent with the model client.
    assistant = AssistantAgent("assistant", model_client=model_client)
    # Call the assistant with a task.
    result = await assistant.run(task="What is the capital of France?")
    print(result)


asyncio.run(main())

```

```python
actual_usage() → RequestUsage[source]#
```

```python
property capabilities: ModelInfo#
```

```python
async close() → None[source]#
```

```python
count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]#
```

【中文翻译】Create a chat completion using the Semantic Kernel client.
The extra_create_args dictionary can include two special keys:


“kernel” (optional):An instance of semantic_kernel.Kernel used to execute the request.
If not provided either in constructor or extra_create_args, a ValueError is raised.




“prompt_execution_settings” (optional):An instance of a PromptExecutionSettings subclass corresponding to the
underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
prompt settings will be used.





Parameters:

messages – The list of LLM messages to send.
tools – The tools that may be invoked during the chat.
json_output – Whether the model is expected to return JSON.
extra_create_args – Additional arguments to control the chat completion behavior.
cancellation_token – Token allowing cancellation of the request.


Returns:
CreateResult – The result of the chat completion.

```python
async create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]#
```

【中文翻译】Create a streaming chat completion using the Semantic Kernel client.
The extra_create_args dictionary can include two special keys:


“kernel” (optional):An instance of semantic_kernel.Kernel used to execute the request.
If not provided either in constructor or extra_create_args, a ValueError is raised.




“prompt_execution_settings” (optional):An instance of a PromptExecutionSettings subclass corresponding to the
underlying Semantic Kernel client (e.g., AzureChatPromptExecutionSettings,
GoogleAIChatPromptExecutionSettings). If not provided, the adapter’s default
prompt settings will be used.





Parameters:

messages – The list of LLM messages to send.
tools – The tools that may be invoked during the chat.
json_output – Whether the model is expected to return JSON.
extra_create_args – Additional arguments to control the chat completion behavior.
cancellation_token – Token allowing cancellation of the request.


Yields:
Union[str, CreateResult] – Either a string chunk of the response or a CreateResult containing function calls.

```python
property model_info: ModelInfo#
```

```python
remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]#
```

```python
total_usage() → RequestUsage[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.models.anthropic

【中文翻译】next

【中文翻译】autogen_ext.models.ollama

### autogen_ext.runtimes.grpc {autogen_extruntimesgrpc}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html)

```python
class GrpcWorkerAgentRuntime(host_address: str, tracer_provider: TracerProvider | None = None, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None, payload_serialization_format: str = JSON_DATA_CONTENT_TYPE)[source]#
```

【中文翻译】Bases: AgentRuntime
An agent runtime for running remote or cross-language agents.
Agent messaging uses protobufs from agent_worker.proto and CloudEvent from cloudevent.proto.
Cross-language agents will additionally require all agents use shared protobuf schemas for any message types that are sent between agents.


add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add





async add_subscription(subscription: Subscription) → None[source]#
Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add





async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.






async agent_metadata(agent: AgentId) → AgentMetadata[source]#
Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.





async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.





async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId[source]#



async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by save_state().

Parameters:
state (Mapping[str, Any]) – The saved state.





async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.





async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.






async remove_subscription(id: str) → None[source]#
Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist





async save_state() → Mapping[str, Any][source]#
Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to load_state().
The structure of the state is implementation defined and can be any JSON serializable object.

Returns:
Mapping[str, Any] – The saved state.





async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.





async start() → None[source]#
Start the runtime in a background task.



async stop() → None[source]#
Stop the runtime immediately.



async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) → None[source]#
Stop the runtime when a signal is received.



async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
```

【中文翻译】Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

```python
async add_subscription(subscription: Subscription) → None[source]#
```

【中文翻译】Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add

```python
async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.

```python
async agent_metadata(agent: AgentId) → AgentMetadata[source]#
```

【中文翻译】Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.

```python
async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
```

【中文翻译】Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.

```python
async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId[source]#
```

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by save_state().

Parameters:
state (Mapping[str, Any]) – The saved state.

```python
async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
```

【中文翻译】Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.

```python
async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
```

【中文翻译】Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
async remove_subscription(id: str) → None[source]#
```

【中文翻译】Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to load_state().
The structure of the state is implementation defined and can be any JSON serializable object.

Returns:
Mapping[str, Any] – The saved state.

```python
async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

【中文翻译】Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.

```python
async start() → None[source]#
```

【中文翻译】Start the runtime in a background task.

```python
async stop() → None[source]#
```

【中文翻译】Stop the runtime immediately.

```python
async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) → None[source]#
```

【中文翻译】Stop the runtime when a signal is received.

```python
async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
```

【中文翻译】Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.

```python
class GrpcWorkerAgentRuntimeHost(address: str, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None)[source]#
```

【中文翻译】Bases: object


start() → None[source]#
Start the server in a background task.



async stop(grace: int = 5) → None[source]#
Stop the server.



async stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) → None[source]#
Stop the server when a signal is received.

```python
start() → None[source]#
```

【中文翻译】Start the server in a background task.

```python
async stop(grace: int = 5) → None[source]#
```

【中文翻译】Stop the server.

```python
async stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) → None[source]#
```

【中文翻译】Stop the server when a signal is received.

```python
class GrpcWorkerAgentRuntimeHostServicer[source]#
```

【中文翻译】Bases: AgentRpcServicer
A gRPC servicer that hosts message delivery service for agents.


async AddSubscription(request: AddSubscriptionRequest, context: ServicerContext[AddSubscriptionRequest, AddSubscriptionResponse]) → AddSubscriptionResponse[source]#
Missing associated documentation comment in .proto file.



async GetSubscriptions(request: GetSubscriptionsRequest, context: ServicerContext[GetSubscriptionsRequest, GetSubscriptionsResponse]) → GetSubscriptionsResponse[source]#
Missing associated documentation comment in .proto file.



async OpenChannel(request_iterator: AsyncIterator[Message], context: ServicerContext[Message, Message]) → AsyncIterator[Message][source]#
Missing associated documentation comment in .proto file.



async OpenControlChannel(request_iterator: AsyncIterator[ControlMessage], context: ServicerContext[ControlMessage, ControlMessage]) → AsyncIterator[ControlMessage][source]#
Missing associated documentation comment in .proto file.



async RegisterAgent(request: RegisterAgentTypeRequest, context: ServicerContext[RegisterAgentTypeRequest, RegisterAgentTypeResponse]) → RegisterAgentTypeResponse[source]#
Missing associated documentation comment in .proto file.



async RemoveSubscription(request: RemoveSubscriptionRequest, context: ServicerContext[RemoveSubscriptionRequest, RemoveSubscriptionResponse]) → RemoveSubscriptionResponse[source]#
Missing associated documentation comment in .proto file.

```python
async AddSubscription(request: AddSubscriptionRequest, context: ServicerContext[AddSubscriptionRequest, AddSubscriptionResponse]) → AddSubscriptionResponse[source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async GetSubscriptions(request: GetSubscriptionsRequest, context: ServicerContext[GetSubscriptionsRequest, GetSubscriptionsResponse]) → GetSubscriptionsResponse[source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async OpenChannel(request_iterator: AsyncIterator[Message], context: ServicerContext[Message, Message]) → AsyncIterator[Message][source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async OpenControlChannel(request_iterator: AsyncIterator[ControlMessage], context: ServicerContext[ControlMessage, ControlMessage]) → AsyncIterator[ControlMessage][source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async RegisterAgent(request: RegisterAgentTypeRequest, context: ServicerContext[RegisterAgentTypeRequest, RegisterAgentTypeResponse]) → RegisterAgentTypeResponse[source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async RemoveSubscription(request: RemoveSubscriptionRequest, context: ServicerContext[RemoveSubscriptionRequest, RemoveSubscriptionResponse]) → RemoveSubscriptionResponse[source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

【中文翻译】previous

【中文翻译】autogen_ext.cache_store.redis

【中文翻译】next

【中文翻译】autogen_ext.auth.azure

### autogen_ext.runtimes.grpc {autogen_extruntimesgrpc}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html)

```python
class GrpcWorkerAgentRuntime(host_address: str, tracer_provider: TracerProvider | None = None, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None, payload_serialization_format: str = JSON_DATA_CONTENT_TYPE)[source]#
```

【中文翻译】Bases: AgentRuntime
An agent runtime for running remote or cross-language agents.
Agent messaging uses protobufs from agent_worker.proto and CloudEvent from cloudevent.proto.
Cross-language agents will additionally require all agents use shared protobuf schemas for any message types that are sent between agents.


add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add





async add_subscription(subscription: Subscription) → None[source]#
Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add





async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.






async agent_metadata(agent: AgentId) → AgentMetadata[source]#
Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.





async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.





async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId[source]#



async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by save_state().

Parameters:
state (Mapping[str, Any]) – The saved state.





async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.





async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.






async remove_subscription(id: str) → None[source]#
Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist





async save_state() → Mapping[str, Any][source]#
Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to load_state().
The structure of the state is implementation defined and can be any JSON serializable object.

Returns:
Mapping[str, Any] – The saved state.





async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.





async start() → None[source]#
Start the runtime in a background task.



async stop() → None[source]#
Stop the runtime immediately.



async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) → None[source]#
Stop the runtime when a signal is received.



async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
add_message_serializer(serializer: MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) → None[source]#
```

【中文翻译】Add a new message serialization serializer to the runtime
Note: This will deduplicate serializers based on the type_name and data_content_type properties

Parameters:
serializer (MessageSerializer[Any] | Sequence[MessageSerializer[Any]]) – The serializer/s to add

```python
async add_subscription(subscription: Subscription) → None[source]#
```

【中文翻译】Add a new subscription that the runtime should fulfill when processing published messages

Parameters:
subscription (Subscription) – The subscription to add

```python
async agent_load_state(agent: AgentId, state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of a single agent.

Parameters:

agent (AgentId) – The agent id.
state (Mapping[str, Any]) – The saved state.

```python
async agent_metadata(agent: AgentId) → AgentMetadata[source]#
```

【中文翻译】Get the metadata for an agent.

Parameters:
agent (AgentId) – The agent id.

Returns:
AgentMetadata – The agent metadata.

```python
async agent_save_state(agent: AgentId) → Mapping[str, Any][source]#
```

【中文翻译】Save the state of a single agent.
The structure of the state is implementation defined and can be any JSON serializable object.

Parameters:
agent (AgentId) – The agent id.

Returns:
Mapping[str, Any] – The saved state.

```python
async get(id_or_type: AgentId | AgentType | str, /, key: str = 'default', *, lazy: bool = True) → AgentId[source]#
```

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the entire runtime, including all hosted agents. The state should be the same as the one returned by save_state().

Parameters:
state (Mapping[str, Any]) – The saved state.

```python
async publish_message(message: Any, topic_id: TopicId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → None[source]#
```

【中文翻译】Publish a message to all agents in the given namespace, or if no namespace is provided, the namespace of the sender.
No responses are expected from publishing.

Parameters:

message (Any) – The message to publish.
topic_id (TopicId) – The topic to publish the message to.
sender (AgentId | None, optional) – The agent which sent the message. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress. Defaults to None.
message_id (str | None, optional) – The message id. If None, a new message id will be generated. Defaults to None. This message id must be unique. and is recommended to be a UUID.


Raises:
UndeliverableException – If the message cannot be delivered.

```python
async register_factory(type: str | AgentType, agent_factory: Callable[[], T | Awaitable[T]], *, expected_class: type[T] | None = None) → AgentType[source]#
```

【中文翻译】Register an agent factory with the runtime associated with a specific type. The type must be unique. This API does not add any subscriptions.

Note
This is a low level API and usually the agent class’s register method should be used instead, as this also handles subscriptions automatically.

Example:
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())



Parameters:

type (str) – The type of agent this factory creates. It is not the same as agent class name. The type parameter is used to differentiate between different factory functions rather than agent classes.
agent_factory (Callable[[], T]) – The factory that creates the agent, where T is a concrete Agent type. Inside the factory, use autogen_core.AgentInstantiationContext to access variables like the current runtime and agent ID.
expected_class (type[T] | None, optional) – The expected class of the agent, used for runtime validation of the factory. Defaults to None. If None, no validation is performed.

**示例**:
```python
from dataclasses import dataclass

from autogen_core import AgentRuntime, MessageContext, RoutedAgent, event
from autogen_core.models import UserMessage


@dataclass
class MyMessage:
    content: str


class MyAgent(RoutedAgent):
    def __init__(self) -> None:
        super().__init__("My core agent")

    @event
    async def handler(self, message: UserMessage, context: MessageContext) -> None:
        print("Event received: ", message.content)


async def my_agent_factory():
    return MyAgent()


async def main() -> None:
    runtime: AgentRuntime = ...  # type: ignore
    await runtime.register_factory("my_agent", lambda: MyAgent())


import asyncio

asyncio.run(main())

```

```python
async remove_subscription(id: str) → None[source]#
```

【中文翻译】Remove a subscription from the runtime

Parameters:
id (str) – id of the subscription to remove

Raises:
LookupError – If the subscription does not exist

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the entire runtime, including all hosted agents. The only way to restore the state is to pass it to load_state().
The structure of the state is implementation defined and can be any JSON serializable object.

Returns:
Mapping[str, Any] – The saved state.

```python
async send_message(message: Any, recipient: AgentId, *, sender: AgentId | None = None, cancellation_token: CancellationToken | None = None, message_id: str | None = None) → Any[source]#
```

【中文翻译】Send a message to an agent and get a response.

Parameters:

message (Any) – The message to send.
recipient (AgentId) – The agent to send the message to.
sender (AgentId | None, optional) – Agent which sent the message. Should only be None if this was sent from no agent, such as directly to the runtime externally. Defaults to None.
cancellation_token (CancellationToken | None, optional) – Token used to cancel an in progress . Defaults to None.


Raises:

CantHandleException – If the recipient cannot handle the message.
UndeliverableException – If the message cannot be delivered.
Other – Any other exception raised by the recipient.


Returns:
Any – The response from the agent.

```python
async start() → None[source]#
```

【中文翻译】Start the runtime in a background task.

```python
async stop() → None[source]#
```

【中文翻译】Stop the runtime immediately.

```python
async stop_when_signal(signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) → None[source]#
```

【中文翻译】Stop the runtime when a signal is received.

```python
async try_get_underlying_agent_instance(id: AgentId, type: Type[T] = Agent) → T[source]#
```

【中文翻译】Try to get the underlying agent instance by name and namespace. This is generally discouraged (hence the long name), but can be useful in some cases.
If the underlying agent is not accessible, this will raise an exception.

Parameters:

id (AgentId) – The agent id.
type (Type[T], optional) – The expected type of the agent. Defaults to Agent.


Returns:
T – The concrete agent instance.

Raises:

LookupError – If the agent is not found.
NotAccessibleError – If the agent is not accessible, for example if it is located remotely.
TypeError – If the agent is not of the expected type.

```python
class GrpcWorkerAgentRuntimeHost(address: str, extra_grpc_config: Sequence[Tuple[str, Any]] | None = None)[source]#
```

【中文翻译】Bases: object


start() → None[source]#
Start the server in a background task.



async stop(grace: int = 5) → None[source]#
Stop the server.



async stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) → None[source]#
Stop the server when a signal is received.

```python
start() → None[source]#
```

【中文翻译】Start the server in a background task.

```python
async stop(grace: int = 5) → None[source]#
```

【中文翻译】Stop the server.

```python
async stop_when_signal(grace: int = 5, signals: Sequence[Signals] = (signal.SIGTERM, signal.SIGINT)) → None[source]#
```

【中文翻译】Stop the server when a signal is received.

```python
class GrpcWorkerAgentRuntimeHostServicer[source]#
```

【中文翻译】Bases: AgentRpcServicer
A gRPC servicer that hosts message delivery service for agents.


async AddSubscription(request: AddSubscriptionRequest, context: ServicerContext[AddSubscriptionRequest, AddSubscriptionResponse]) → AddSubscriptionResponse[source]#
Missing associated documentation comment in .proto file.



async GetSubscriptions(request: GetSubscriptionsRequest, context: ServicerContext[GetSubscriptionsRequest, GetSubscriptionsResponse]) → GetSubscriptionsResponse[source]#
Missing associated documentation comment in .proto file.



async OpenChannel(request_iterator: AsyncIterator[Message], context: ServicerContext[Message, Message]) → AsyncIterator[Message][source]#
Missing associated documentation comment in .proto file.



async OpenControlChannel(request_iterator: AsyncIterator[ControlMessage], context: ServicerContext[ControlMessage, ControlMessage]) → AsyncIterator[ControlMessage][source]#
Missing associated documentation comment in .proto file.



async RegisterAgent(request: RegisterAgentTypeRequest, context: ServicerContext[RegisterAgentTypeRequest, RegisterAgentTypeResponse]) → RegisterAgentTypeResponse[source]#
Missing associated documentation comment in .proto file.



async RemoveSubscription(request: RemoveSubscriptionRequest, context: ServicerContext[RemoveSubscriptionRequest, RemoveSubscriptionResponse]) → RemoveSubscriptionResponse[source]#
Missing associated documentation comment in .proto file.

```python
async AddSubscription(request: AddSubscriptionRequest, context: ServicerContext[AddSubscriptionRequest, AddSubscriptionResponse]) → AddSubscriptionResponse[source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async GetSubscriptions(request: GetSubscriptionsRequest, context: ServicerContext[GetSubscriptionsRequest, GetSubscriptionsResponse]) → GetSubscriptionsResponse[source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async OpenChannel(request_iterator: AsyncIterator[Message], context: ServicerContext[Message, Message]) → AsyncIterator[Message][source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async OpenControlChannel(request_iterator: AsyncIterator[ControlMessage], context: ServicerContext[ControlMessage, ControlMessage]) → AsyncIterator[ControlMessage][source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async RegisterAgent(request: RegisterAgentTypeRequest, context: ServicerContext[RegisterAgentTypeRequest, RegisterAgentTypeResponse]) → RegisterAgentTypeResponse[source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

```python
async RemoveSubscription(request: RemoveSubscriptionRequest, context: ServicerContext[RemoveSubscriptionRequest, RemoveSubscriptionResponse]) → RemoveSubscriptionResponse[source]#
```

【中文翻译】Missing associated documentation comment in .proto file.

【中文翻译】previous

【中文翻译】autogen_ext.cache_store.redis

【中文翻译】next

【中文翻译】autogen_ext.auth.azure

### autogen_ext.teams.magentic_one {autogen_extteamsmagentic_one}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html)

```python
class MagenticOne(client: ChatCompletionClient, hil_mode: bool = False, input_func: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]] | None = None, code_executor: CodeExecutor | None = None)[source]#
```

【中文翻译】Bases: MagenticOneGroupChat
MagenticOne is a specialized group chat class that integrates various agents
such as FileSurfer, WebSurfer, Coder, and Executor to solve complex tasks.
To read more about the science behind Magentic-One, see the full blog post: Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks and the references below.
Installation:
pip install "autogen-ext[magentic-one]"



Parameters:

client (ChatCompletionClient) – The client used for model interactions.
hil_mode (bool) – Optional; If set to True, adds the UserProxyAgent to the list of agents.




Warning
Using Magentic-One involves interacting with a digital world designed for humans, which carries inherent risks. To minimize these risks, consider the following precautions:

Use Containers: Run all tasks in docker containers to isolate the agents and prevent direct system attacks.
Virtual Environment: Use a virtual environment to run the agents and prevent them from accessing sensitive data.
Monitor Logs: Closely monitor logs during and after execution to detect and mitigate risky behavior.
Human Oversight: Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.
Limit Access: Restrict the agents’ access to the internet and other resources to prevent unauthorized actions.
Safeguard Data: Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents.

Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.

Architecture:
Magentic-One is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step towards developing agents that can complete tasks that people encounter in their work and personal lives.
Magentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents, and tracking task progress. The Orchestrator begins by creating a plan to tackle the task, gathering needed facts and educated guesses in a Task Ledger that is maintained. At each step of its plan, the Orchestrator creates a Progress Ledger where it self-reflects on task progress and checks whether the task is completed. If the task is not yet completed, it assigns one of Magentic-One’s other agents a subtask to complete. After the assigned agent completes its subtask, the Orchestrator updates the Progress Ledger and continues in this way until the task is complete. If the Orchestrator finds that progress is not being made for enough steps, it can update the Task Ledger and create a new plan.
Overall, Magentic-One consists of the following agents:

Orchestrator: The lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed.
WebSurfer: An LLM-based agent proficient in commanding and managing the state of a Chromium-based web browser. It performs actions on the browser and reports on the new state of the web page.
FileSurfer: An LLM-based agent that commands a markdown-based file preview application to read local files of most types. It can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.
Coder: An LLM-based agent specialized in writing code, analyzing information collected from other agents, or creating new artifacts.
ComputerTerminal: Provides the team with access to a console shell where the Coder’s programs can be executed, and where new programming libraries can be installed.

Together, Magentic-One’s agents provide the Orchestrator with the tools and capabilities needed to solve a broad variety of open-ended problems, as well as the ability to autonomously adapt to, and act in, dynamic and ever-changing web and file-system environments.
Examples
# Autonomously complete a coding task:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console


async def example_usage():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    m1 = MagenticOne(client=client)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)


if __name__ == "__main__":
    asyncio.run(example_usage())


# Enable human-in-the-loop mode
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console


async def example_usage_hil():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    # to enable human-in-the-loop mode, set hil_mode=True
    m1 = MagenticOne(client=client, hil_mode=True)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)


if __name__ == "__main__":
    asyncio.run(example_usage_hil())


References
@article{fourney2024magentic,
    title={Magentic-one: A generalist multi-agent system for solving complex tasks},
    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
    journal={arXiv preprint arXiv:2411.04468},
    year={2024},
    url={https://arxiv.org/abs/2411.04468}
}

**示例**:
```python
pip install "autogen-ext[magentic-one]"

```

**示例**:
```python
# Autonomously complete a coding task:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console


async def example_usage():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    m1 = MagenticOne(client=client)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)


if __name__ == "__main__":
    asyncio.run(example_usage())

```

**示例**:
```python
# Enable human-in-the-loop mode
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console


async def example_usage_hil():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    # to enable human-in-the-loop mode, set hil_mode=True
    m1 = MagenticOne(client=client, hil_mode=True)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)


if __name__ == "__main__":
    asyncio.run(example_usage_hil())

```

**示例**:
```python
@article{fourney2024magentic,
    title={Magentic-one: A generalist multi-agent system for solving complex tasks},
    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
    journal={arXiv preprint arXiv:2411.04468},
    year={2024},
    url={https://arxiv.org/abs/2411.04468}
}

```

【中文翻译】previous

【中文翻译】autogen_ext.agents.video_surfer.tools

【中文翻译】next

【中文翻译】autogen_ext.models.cache

### autogen_ext.teams.magentic_one {autogen_extteamsmagentic_one}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.teams.magentic_one.html)

```python
class MagenticOne(client: ChatCompletionClient, hil_mode: bool = False, input_func: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]] | None = None, code_executor: CodeExecutor | None = None)[source]#
```

【中文翻译】Bases: MagenticOneGroupChat
MagenticOne is a specialized group chat class that integrates various agents
such as FileSurfer, WebSurfer, Coder, and Executor to solve complex tasks.
To read more about the science behind Magentic-One, see the full blog post: Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks and the references below.
Installation:
pip install "autogen-ext[magentic-one]"



Parameters:

client (ChatCompletionClient) – The client used for model interactions.
hil_mode (bool) – Optional; If set to True, adds the UserProxyAgent to the list of agents.




Warning
Using Magentic-One involves interacting with a digital world designed for humans, which carries inherent risks. To minimize these risks, consider the following precautions:

Use Containers: Run all tasks in docker containers to isolate the agents and prevent direct system attacks.
Virtual Environment: Use a virtual environment to run the agents and prevent them from accessing sensitive data.
Monitor Logs: Closely monitor logs during and after execution to detect and mitigate risky behavior.
Human Oversight: Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.
Limit Access: Restrict the agents’ access to the internet and other resources to prevent unauthorized actions.
Safeguard Data: Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents.

Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.

Architecture:
Magentic-One is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step towards developing agents that can complete tasks that people encounter in their work and personal lives.
Magentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents, and tracking task progress. The Orchestrator begins by creating a plan to tackle the task, gathering needed facts and educated guesses in a Task Ledger that is maintained. At each step of its plan, the Orchestrator creates a Progress Ledger where it self-reflects on task progress and checks whether the task is completed. If the task is not yet completed, it assigns one of Magentic-One’s other agents a subtask to complete. After the assigned agent completes its subtask, the Orchestrator updates the Progress Ledger and continues in this way until the task is complete. If the Orchestrator finds that progress is not being made for enough steps, it can update the Task Ledger and create a new plan.
Overall, Magentic-One consists of the following agents:

Orchestrator: The lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed.
WebSurfer: An LLM-based agent proficient in commanding and managing the state of a Chromium-based web browser. It performs actions on the browser and reports on the new state of the web page.
FileSurfer: An LLM-based agent that commands a markdown-based file preview application to read local files of most types. It can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.
Coder: An LLM-based agent specialized in writing code, analyzing information collected from other agents, or creating new artifacts.
ComputerTerminal: Provides the team with access to a console shell where the Coder’s programs can be executed, and where new programming libraries can be installed.

Together, Magentic-One’s agents provide the Orchestrator with the tools and capabilities needed to solve a broad variety of open-ended problems, as well as the ability to autonomously adapt to, and act in, dynamic and ever-changing web and file-system environments.
Examples
# Autonomously complete a coding task:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console


async def example_usage():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    m1 = MagenticOne(client=client)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)


if __name__ == "__main__":
    asyncio.run(example_usage())


# Enable human-in-the-loop mode
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console


async def example_usage_hil():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    # to enable human-in-the-loop mode, set hil_mode=True
    m1 = MagenticOne(client=client, hil_mode=True)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)


if __name__ == "__main__":
    asyncio.run(example_usage_hil())


References
@article{fourney2024magentic,
    title={Magentic-one: A generalist multi-agent system for solving complex tasks},
    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
    journal={arXiv preprint arXiv:2411.04468},
    year={2024},
    url={https://arxiv.org/abs/2411.04468}
}

**示例**:
```python
pip install "autogen-ext[magentic-one]"

```

**示例**:
```python
# Autonomously complete a coding task:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console


async def example_usage():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    m1 = MagenticOne(client=client)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)


if __name__ == "__main__":
    asyncio.run(example_usage())

```

**示例**:
```python
# Enable human-in-the-loop mode
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.teams.magentic_one import MagenticOne
from autogen_agentchat.ui import Console


async def example_usage_hil():
    client = OpenAIChatCompletionClient(model="gpt-4o")
    # to enable human-in-the-loop mode, set hil_mode=True
    m1 = MagenticOne(client=client, hil_mode=True)
    task = "Write a Python script to fetch data from an API."
    result = await Console(m1.run_stream(task=task))
    print(result)


if __name__ == "__main__":
    asyncio.run(example_usage_hil())

```

**示例**:
```python
@article{fourney2024magentic,
    title={Magentic-one: A generalist multi-agent system for solving complex tasks},
    author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others},
    journal={arXiv preprint arXiv:2411.04468},
    year={2024},
    url={https://arxiv.org/abs/2411.04468}
}

```

【中文翻译】previous

【中文翻译】autogen_ext.agents.video_surfer.tools

【中文翻译】next

【中文翻译】autogen_ext.models.cache

### autogen_ext.tools.azure {autogen_exttoolsazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html)

```python
pydantic model AzureAISearchConfig[source]#
```

【中文翻译】Bases: BaseModel
Configuration for Azure AI Search tool.
This class defines the configuration parameters for AzureAISearchTool.
It provides options for customizing search behavior including query types,
field selection, authentication, retry policies, and caching strategies.

Note
This class requires the azure extra for the autogen-ext package.
pip install -U "autogen-ext[azure]"



Example
from azure.core.credentials import AzureKeyCredential
from autogen_ext.tools.azure import AzureAISearchConfig

config = AzureAISearchConfig(
    name="doc_search",
    endpoint="https://my-search.search.windows.net",
    index_name="my-index",
    credential=AzureKeyCredential("<your-key>"),
    query_type="vector",
    vector_fields=["embedding"],
)



For more details, see:
Azure AI Search Overview
Vector Search




Parameters:

name (str) – Name for the tool instance, used to identify it in the agent’s toolkit.
description (Optional[str]) – Human-readable description of what this tool does and how to use it.
endpoint (str) – The full URL of your Azure AI Search service, in the format
‘https://<service-name>.search.windows.net’.
index_name (str) – Name of the target search index in your Azure AI Search service.
The index must be pre-created and properly configured.
api_version (str) – Azure AI Search REST API version to use. Defaults to ‘2023-11-01’.
Only change if you need specific features from a different API version.
credential (Union[AzureKeyCredential, TokenCredential]) – Azure authentication credential:
- AzureKeyCredential: For API key authentication (admin/query key)
- TokenCredential: For Azure AD authentication (e.g., DefaultAzureCredential)
query_type (Literal["keyword", "fulltext", "vector", "semantic"]) – The search query mode to use:
- ‘keyword’: Basic keyword search (default)
- ‘fulltext’: Full Lucene query syntax
- ‘vector’: Vector similarity search
- ‘semantic’: Semantic search using semantic configuration
search_fields (Optional[List[str]]) – List of index fields to search within. If not specified,
searches all searchable fields. Example: [‘title’, ‘content’].
select_fields (Optional[List[str]]) – Fields to return in search results. If not specified,
returns all fields. Use to optimize response size.
vector_fields (Optional[List[str]]) – Vector field names for vector search. Must be configured
in your search index as vector fields. Required for vector search.
top (Optional[int]) – Maximum number of documents to return in search results.
Helps control response size and processing time.
retry_enabled (bool) – Whether to enable retry policy for transient errors. Defaults to True.
retry_max_attempts (Optional[int]) – Maximum number of retry attempts for failed requests. Defaults to 3.
retry_mode (Literal["fixed", "exponential"]) – Retry backoff strategy: fixed or exponential. Defaults to “exponential”.
enable_caching (bool) – Whether to enable client-side caching of search results. Defaults to False.
cache_ttl_seconds (int) – Time-to-live for cached search results in seconds. Defaults to 300 (5 minutes).
filter (Optional[str]) – OData filter expression to refine search results.




Show JSON schema{
   "title": "AzureAISearchConfig",
   "description": "Configuration for Azure AI Search tool.\n\nThis class defines the configuration parameters for :class:`AzureAISearchTool`.\nIt provides options for customizing search behavior including query types,\nfield selection, authentication, retry policies, and caching strategies.\n\n.. note::\n\n    This class requires the :code:`azure` extra for the :code:`autogen-ext` package.\n\n    .. code-block:: bash\n\n        pip install -U \"autogen-ext[azure]\"\n\nExample:\n    .. code-block:: python\n\n        from azure.core.credentials import AzureKeyCredential\n        from autogen_ext.tools.azure import AzureAISearchConfig\n\n        config = AzureAISearchConfig(\n            name=\"doc_search\",\n            endpoint=\"https://my-search.search.windows.net\",\n            index_name=\"my-index\",\n            credential=AzureKeyCredential(\"<your-key>\"),\n            query_type=\"vector\",\n            vector_fields=[\"embedding\"],\n        )\n\nFor more details, see:\n    * `Azure AI Search Overview <https://learn.microsoft.com/azure/search/search-what-is-azure-search>`_\n    * `Vector Search <https://learn.microsoft.com/azure/search/vector-search-overview>`_\n\nArgs:\n    name (str): Name for the tool instance, used to identify it in the agent's toolkit.\n    description (Optional[str]): Human-readable description of what this tool does and how to use it.\n    endpoint (str): The full URL of your Azure AI Search service, in the format\n        'https://<service-name>.search.windows.net'.\n    index_name (str): Name of the target search index in your Azure AI Search service.\n        The index must be pre-created and properly configured.\n    api_version (str): Azure AI Search REST API version to use. Defaults to '2023-11-01'.\n        Only change if you need specific features from a different API version.\n    credential (Union[AzureKeyCredential, TokenCredential]): Azure authentication credential:\n        - AzureKeyCredential: For API key authentication (admin/query key)\n        - TokenCredential: For Azure AD authentication (e.g., DefaultAzureCredential)\n    query_type (Literal[\"keyword\", \"fulltext\", \"vector\", \"semantic\"]): The search query mode to use:\n        - 'keyword': Basic keyword search (default)\n        - 'fulltext': Full Lucene query syntax\n        - 'vector': Vector similarity search\n        - 'semantic': Semantic search using semantic configuration\n    search_fields (Optional[List[str]]): List of index fields to search within. If not specified,\n        searches all searchable fields. Example: ['title', 'content'].\n    select_fields (Optional[List[str]]): Fields to return in search results. If not specified,\n        returns all fields. Use to optimize response size.\n    vector_fields (Optional[List[str]]): Vector field names for vector search. Must be configured\n        in your search index as vector fields. Required for vector search.\n    top (Optional[int]): Maximum number of documents to return in search results.\n        Helps control response size and processing time.\n    retry_enabled (bool): Whether to enable retry policy for transient errors. Defaults to True.\n    retry_max_attempts (Optional[int]): Maximum number of retry attempts for failed requests. Defaults to 3.\n    retry_mode (Literal[\"fixed\", \"exponential\"]): Retry backoff strategy: fixed or exponential. Defaults to \"exponential\".\n    enable_caching (bool): Whether to enable client-side caching of search results. Defaults to False.\n    cache_ttl_seconds (int): Time-to-live for cached search results in seconds. Defaults to 300 (5 minutes).\n    filter (Optional[str]): OData filter expression to refine search results.",
   "type": "object",
   "properties": {
      "name": {
         "description": "The name of the tool",
         "title": "Name",
         "type": "string"
      },
      "description": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "A description of the tool",
         "title": "Description"
      },
      "endpoint": {
         "description": "The endpoint URL for your Azure AI Search service",
         "title": "Endpoint",
         "type": "string"
      },
      "index_name": {
         "description": "The name of the search index to query",
         "title": "Index Name",
         "type": "string"
      },
      "api_version": {
         "default": "2023-11-01",
         "description": "API version to use",
         "title": "Api Version",
         "type": "string"
      },
      "credential": {
         "anyOf": [],
         "description": "The credential to use for authentication",
         "title": "Credential"
      },
      "query_type": {
         "default": "keyword",
         "description": "Type of query to perform (keyword for classic, fulltext for Lucene, vector for embedding, semantic for semantic/AI search)",
         "enum": [
            "keyword",
            "fulltext",
            "vector",
            "semantic"
         ],
         "title": "Query Type",
         "type": "string"
      },
      "search_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of fields to search in",
         "title": "Search Fields"
      },
      "select_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of fields to return in results",
         "title": "Select Fields"
      },
      "vector_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of vector fields for vector search",
         "title": "Vector Fields"
      },
      "top": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional number of results to return",
         "title": "Top"
      },
      "filter": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional OData filter expression to refine search results",
         "title": "Filter"
      },
      "retry_enabled": {
         "default": true,
         "description": "Whether to enable retry policy for transient errors",
         "title": "Retry Enabled",
         "type": "boolean"
      },
      "retry_max_attempts": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 3,
         "description": "Maximum number of retry attempts for failed requests",
         "title": "Retry Max Attempts"
      },
      "retry_mode": {
         "default": "exponential",
         "description": "Retry backoff strategy: fixed or exponential",
         "enum": [
            "fixed",
            "exponential"
         ],
         "title": "Retry Mode",
         "type": "string"
      },
      "enable_caching": {
         "default": false,
         "description": "Whether to enable client-side caching of search results",
         "title": "Enable Caching",
         "type": "boolean"
      },
      "cache_ttl_seconds": {
         "default": 300,
         "description": "Time-to-live for cached search results in seconds",
         "title": "Cache Ttl Seconds",
         "type": "integer"
      },
      "embedding_provider": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Name of embedding provider to use (e.g., 'azure_openai', 'openai')",
         "title": "Embedding Provider"
      },
      "embedding_model": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Model name to use for generating embeddings",
         "title": "Embedding Model"
      },
      "embedding_dimension": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Dimension of embedding vectors produced by the model",
         "title": "Embedding Dimension"
      }
   },
   "required": [
      "name",
      "endpoint",
      "index_name",
      "credential"
   ]
}



Fields:

api_version (str)
cache_ttl_seconds (int)
credential (azure.core.credentials.AzureKeyCredential | azure.core.credentials.TokenCredential)
description (str | None)
embedding_dimension (int | None)
embedding_model (str | None)
embedding_provider (str | None)
enable_caching (bool)
endpoint (str)
filter (str | None)
index_name (str)
name (str)
query_type (Literal['keyword', 'fulltext', 'vector', 'semantic'])
retry_enabled (bool)
retry_max_attempts (int | None)
retry_mode (Literal['fixed', 'exponential'])
search_fields (List[str] | None)
select_fields (List[str] | None)
top (int | None)
vector_fields (List[str] | None)





field api_version: str = '2023-11-01'#
API version to use



field cache_ttl_seconds: int = 300#
Time-to-live for cached search results in seconds



field credential: AzureKeyCredential | TokenCredential [Required]#
The credential to use for authentication



field description: str | None = None#
A description of the tool



field embedding_dimension: int | None = None#
Dimension of embedding vectors produced by the model



field embedding_model: str | None = None#
Model name to use for generating embeddings



field embedding_provider: str | None = None#
Name of embedding provider to use (e.g., ‘azure_openai’, ‘openai’)



field enable_caching: bool = False#
Whether to enable client-side caching of search results



field endpoint: str [Required]#
The endpoint URL for your Azure AI Search service



field filter: str | None = None#
Optional OData filter expression to refine search results



field index_name: str [Required]#
The name of the search index to query



field name: str [Required]#
The name of the tool



field query_type: Literal['keyword', 'fulltext', 'vector', 'semantic'] = 'keyword'#
Type of query to perform (keyword for classic, fulltext for Lucene, vector for embedding, semantic for semantic/AI search)



field retry_enabled: bool = True#
Whether to enable retry policy for transient errors



field retry_max_attempts: int | None = 3#
Maximum number of retry attempts for failed requests



field retry_mode: Literal['fixed', 'exponential'] = 'exponential'#
Retry backoff strategy: fixed or exponential



field search_fields: List[str] | None = None#
Optional list of fields to search in



field select_fields: List[str] | None = None#
Optional list of fields to return in results



field top: int | None = None#
Optional number of results to return



field vector_fields: List[str] | None = None#
Optional list of vector fields for vector search



model_dump(**kwargs: Any) → Dict[str, Any][source]#
Custom model_dump to handle credentials.



classmethod validate_credentials(data: Any) → Any[source]#
Wrap a classmethod, staticmethod, property or unbound function
and act as a descriptor that allows us to detect decorated items
from the class’ attributes.
This class’ __get__ returns the wrapped item’s __get__ result,
which makes it transparent for classmethods and staticmethods.


wrapped#
The decorator that has to be wrapped.



decorator_info#
The decorator info.



shim#
A wrapper function to wrap V1 style function.

**示例**:
```python
pip install -U "autogen-ext[azure]"

```

**示例**:
```python
from azure.core.credentials import AzureKeyCredential
from autogen_ext.tools.azure import AzureAISearchConfig

config = AzureAISearchConfig(
    name="doc_search",
    endpoint="https://my-search.search.windows.net",
    index_name="my-index",
    credential=AzureKeyCredential("<your-key>"),
    query_type="vector",
    vector_fields=["embedding"],
)

```

**示例**:
```python
{
   "title": "AzureAISearchConfig",
   "description": "Configuration for Azure AI Search tool.\n\nThis class defines the configuration parameters for :class:`AzureAISearchTool`.\nIt provides options for customizing search behavior including query types,\nfield selection, authentication, retry policies, and caching strategies.\n\n.. note::\n\n    This class requires the :code:`azure` extra for the :code:`autogen-ext` package.\n\n    .. code-block:: bash\n\n        pip install -U \"autogen-ext[azure]\"\n\nExample:\n    .. code-block:: python\n\n        from azure.core.credentials import AzureKeyCredential\n        from autogen_ext.tools.azure import AzureAISearchConfig\n\n        config = AzureAISearchConfig(\n            name=\"doc_search\",\n            endpoint=\"https://my-search.search.windows.net\",\n            index_name=\"my-index\",\n            credential=AzureKeyCredential(\"<your-key>\"),\n            query_type=\"vector\",\n            vector_fields=[\"embedding\"],\n        )\n\nFor more details, see:\n    * `Azure AI Search Overview <https://learn.microsoft.com/azure/search/search-what-is-azure-search>`_\n    * `Vector Search <https://learn.microsoft.com/azure/search/vector-search-overview>`_\n\nArgs:\n    name (str): Name for the tool instance, used to identify it in the agent's toolkit.\n    description (Optional[str]): Human-readable description of what this tool does and how to use it.\n    endpoint (str): The full URL of your Azure AI Search service, in the format\n        'https://<service-name>.search.windows.net'.\n    index_name (str): Name of the target search index in your Azure AI Search service.\n        The index must be pre-created and properly configured.\n    api_version (str): Azure AI Search REST API version to use. Defaults to '2023-11-01'.\n        Only change if you need specific features from a different API version.\n    credential (Union[AzureKeyCredential, TokenCredential]): Azure authentication credential:\n        - AzureKeyCredential: For API key authentication (admin/query key)\n        - TokenCredential: For Azure AD authentication (e.g., DefaultAzureCredential)\n    query_type (Literal[\"keyword\", \"fulltext\", \"vector\", \"semantic\"]): The search query mode to use:\n        - 'keyword': Basic keyword search (default)\n        - 'fulltext': Full Lucene query syntax\n        - 'vector': Vector similarity search\n        - 'semantic': Semantic search using semantic configuration\n    search_fields (Optional[List[str]]): List of index fields to search within. If not specified,\n        searches all searchable fields. Example: ['title', 'content'].\n    select_fields (Optional[List[str]]): Fields to return in search results. If not specified,\n        returns all fields. Use to optimize response size.\n    vector_fields (Optional[List[str]]): Vector field names for vector search. Must be configured\n        in your search index as vector fields. Required for vector search.\n    top (Optional[int]): Maximum number of documents to return in search results.\n        Helps control response size and processing time.\n    retry_enabled (bool): Whether to enable retry policy for transient errors. Defaults to True.\n    retry_max_attempts (Optional[int]): Maximum number of retry attempts for failed requests. Defaults to 3.\n    retry_mode (Literal[\"fixed\", \"exponential\"]): Retry backoff strategy: fixed or exponential. Defaults to \"exponential\".\n    enable_caching (bool): Whether to enable client-side caching of search results. Defaults to False.\n    cache_ttl_seconds (int): Time-to-live for cached search results in seconds. Defaults to 300 (5 minutes).\n    filter (Optional[str]): OData filter expression to refine search results.",
   "type": "object",
   "properties": {
      "name": {
         "description": "The name of the tool",
         "title": "Name",
         "type": "string"
      },
      "description": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "A description of the tool",
         "title": "Description"
      },
      "endpoint": {
         "description": "The endpoint URL for your Azure AI Search service",
         "title": "Endpoint",
         "type": "string"
      },
      "index_name": {
         "description": "The name of the search index to query",
         "title": "Index Name",
         "type": "string"
      },
      "api_version": {
         "default": "2023-11-01",
         "description": "API version to use",
         "title": "Api Version",
         "type": "string"
      },
      "credential": {
         "anyOf": [],
         "description": "The credential to use for authentication",
         "title": "Credential"
      },
      "query_type": {
         "default": "keyword",
         "description": "Type of query to perform (keyword for classic, fulltext for Lucene, vector for embedding, semantic for semantic/AI search)",
         "enum": [
            "keyword",
            "fulltext",
            "vector",
            "semantic"
         ],
         "title": "Query Type",
         "type": "string"
      },
      "search_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of fields to search in",
         "title": "Search Fields"
      },
      "select_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of fields to return in results",
         "title": "Select Fields"
      },
      "vector_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of vector fields for vector search",
         "title": "Vector Fields"
      },
      "top": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional number of results to return",
         "title": "Top"
      },
      "filter": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional OData filter expression to refine search results",
         "title": "Filter"
      },
      "retry_enabled": {
         "default": true,
         "description": "Whether to enable retry policy for transient errors",
         "title": "Retry Enabled",
         "type": "boolean"
      },
      "retry_max_attempts": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 3,
         "description": "Maximum number of retry attempts for failed requests",
         "title": "Retry Max Attempts"
      },
      "retry_mode": {
         "default": "exponential",
         "description": "Retry backoff strategy: fixed or exponential",
         "enum": [
            "fixed",
            "exponential"
         ],
         "title": "Retry Mode",
         "type": "string"
      },
      "enable_caching": {
         "default": false,
         "description": "Whether to enable client-side caching of search results",
         "title": "Enable Caching",
         "type": "boolean"
      },
      "cache_ttl_seconds": {
         "default": 300,
         "description": "Time-to-live for cached search results in seconds",
         "title": "Cache Ttl Seconds",
         "type": "integer"
      },
      "embedding_provider": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Name of embedding provider to use (e.g., 'azure_openai', 'openai')",
         "title": "Embedding Provider"
      },
      "embedding_model": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Model name to use for generating embeddings",
         "title": "Embedding Model"
      },
      "embedding_dimension": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Dimension of embedding vectors produced by the model",
         "title": "Embedding Dimension"
      }
   },
   "required": [
      "name",
      "endpoint",
      "index_name",
      "credential"
   ]
}

```

```python
field api_version: str = '2023-11-01'#
```

【中文翻译】API version to use

```python
field cache_ttl_seconds: int = 300#
```

【中文翻译】Time-to-live for cached search results in seconds

```python
field credential: AzureKeyCredential | TokenCredential [Required]#
```

【中文翻译】The credential to use for authentication

```python
field description: str | None = None#
```

【中文翻译】A description of the tool

```python
field embedding_dimension: int | None = None#
```

【中文翻译】Dimension of embedding vectors produced by the model

```python
field embedding_model: str | None = None#
```

【中文翻译】Model name to use for generating embeddings

```python
field embedding_provider: str | None = None#
```

【中文翻译】Name of embedding provider to use (e.g., ‘azure_openai’, ‘openai’)

```python
field enable_caching: bool = False#
```

【中文翻译】Whether to enable client-side caching of search results

```python
field endpoint: str [Required]#
```

【中文翻译】The endpoint URL for your Azure AI Search service

```python
field filter: str | None = None#
```

【中文翻译】Optional OData filter expression to refine search results

```python
field index_name: str [Required]#
```

【中文翻译】The name of the search index to query

```python
field name: str [Required]#
```

【中文翻译】The name of the tool

```python
field query_type: Literal['keyword', 'fulltext', 'vector', 'semantic'] = 'keyword'#
```

【中文翻译】Type of query to perform (keyword for classic, fulltext for Lucene, vector for embedding, semantic for semantic/AI search)

```python
field retry_enabled: bool = True#
```

【中文翻译】Whether to enable retry policy for transient errors

```python
field retry_max_attempts: int | None = 3#
```

【中文翻译】Maximum number of retry attempts for failed requests

```python
field retry_mode: Literal['fixed', 'exponential'] = 'exponential'#
```

【中文翻译】Retry backoff strategy: fixed or exponential

```python
field search_fields: List[str] | None = None#
```

【中文翻译】Optional list of fields to search in

```python
field select_fields: List[str] | None = None#
```

【中文翻译】Optional list of fields to return in results

```python
field top: int | None = None#
```

【中文翻译】Optional number of results to return

```python
field vector_fields: List[str] | None = None#
```

【中文翻译】Optional list of vector fields for vector search

```python
model_dump(**kwargs: Any) → Dict[str, Any][source]#
```

【中文翻译】Custom model_dump to handle credentials.

```python
classmethod validate_credentials(data: Any) → Any[source]#
```

【中文翻译】Wrap a classmethod, staticmethod, property or unbound function
and act as a descriptor that allows us to detect decorated items
from the class’ attributes.
This class’ __get__ returns the wrapped item’s __get__ result,
which makes it transparent for classmethods and staticmethods.


wrapped#
The decorator that has to be wrapped.



decorator_info#
The decorator info.



shim#
A wrapper function to wrap V1 style function.

```python
wrapped#
```

【中文翻译】The decorator that has to be wrapped.

```python
decorator_info#
```

【中文翻译】The decorator info.

```python
shim#
```

【中文翻译】A wrapper function to wrap V1 style function.

```python
class AzureAISearchTool(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], query_type: Literal['keyword', 'fulltext', 'vector', 'semantic'], search_fields: List[str] | None = None, select_fields: List[str] | None = None, vector_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any)[source]#
```

【中文翻译】Bases: BaseAzureAISearchTool
Azure AI Search tool for querying Azure search indexes.
This tool provides a simplified interface for querying Azure AI Search indexes using
various search methods. The tool supports four main search types:

Keyword Search: Traditional text-based search using Azure’s text analysis
Full-Text Search: Enhanced text search with language-specific analyzers
Vector Search: Semantic similarity search using vector embeddings
Hybrid Search: Combines fulltext and vector search for comprehensive results

You should use the factory methods to create instances for specific search types:
- create_keyword_search()
- create_full_text_search()
- create_vector_search()
- create_hybrid_search()


classmethod create_full_text_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
Factory method to create a full-text search tool.
Full-text search uses advanced text analysis (stemming, lemmatization, etc.)
to provide more comprehensive text matching than basic keyword search.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
search_fields (Optional[List[str]]) – Fields to search within
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized full-text search tool



Example Usage:# type: ignore
# Example of using full-text search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a full-text search tool
full_text_search = AzureAISearchTool.create_full_text_search(
    name="document_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["title", "content", "category", "url"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[full_text_search])







classmethod create_hybrid_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], vector_fields: List[str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
Factory method to create a hybrid search tool (text + vector).
Hybrid search combines text search (fulltext or semantic) with vector similarity
search to provide more comprehensive results. This is the recommended entrypoint for hybrid (text + vector) search.
The query_type will be ‘semantic’ if semantic_config_name is provided, otherwise ‘fulltext’.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
vector_fields (List[str]) – Fields containing vector embeddings for similarity search
search_fields (Optional[List[str]]) – Fields to search within for text search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized hybrid search tool



Example Usage:# type: ignore
# Example of using hybrid search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a hybrid search tool
hybrid_search = AzureAISearchTool.create_hybrid_search(
    name="hybrid_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding_field"],
    search_fields=["title", "content"],
    select_fields=["title", "content", "url", "date"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("researcher", tools=[hybrid_search])



Warning
If you set query_type="semantic", you must also provide a valid semantic_config_name.
If you do not, the tool will default to the config name "semantic".






classmethod create_keyword_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
Factory method to create a keyword search tool.
Keyword search performs traditional text-based search, good for finding documents
containing specific terms or exact matches to your query.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
search_fields (Optional[List[str]]) – Fields to search within for text search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized keyword search tool



Example Usage:# type: ignore
# Example of using keyword search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a keyword search tool
keyword_search = AzureAISearchTool.create_keyword_search(
    name="keyword_search",
    endpoint="https://your-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["id", "title", "content", "category"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[keyword_search])







classmethod create_vector_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], vector_fields: List[str], select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
Factory method to create a vector search tool.
Vector search uses embedding vectors to find semantically similar content, enabling
the discovery of related information even when different terminology is used.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
vector_fields (List[str]) – Fields containing vector embeddings for similarity search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized vector search tool



Example Usage:# type: ignore
# Example of using vector search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a vector search tool
vector_search = AzureAISearchTool.create_vector_search(
    name="vector_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding"],
    select_fields=["title", "content", "url"],
    top=5,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[vector_search])







classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → AzureAISearchTool[source]#

classmethod load_component(model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType]) → ExpectedType
Load a component from a component model.

Parameters:

model – The component model or dictionary with configuration
expected – Optional expected return type


Returns:
An initialized AzureAISearchTool instance

**示例**:
```python
# type: ignore
# Example of using full-text search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a full-text search tool
full_text_search = AzureAISearchTool.create_full_text_search(
    name="document_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["title", "content", "category", "url"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[full_text_search])

```

**示例**:
```python
# type: ignore
# Example of using hybrid search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a hybrid search tool
hybrid_search = AzureAISearchTool.create_hybrid_search(
    name="hybrid_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding_field"],
    search_fields=["title", "content"],
    select_fields=["title", "content", "url", "date"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("researcher", tools=[hybrid_search])

```

**示例**:
```python
# type: ignore
# Example of using keyword search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a keyword search tool
keyword_search = AzureAISearchTool.create_keyword_search(
    name="keyword_search",
    endpoint="https://your-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["id", "title", "content", "category"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[keyword_search])

```

**示例**:
```python
# type: ignore
# Example of using vector search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a vector search tool
vector_search = AzureAISearchTool.create_vector_search(
    name="vector_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding"],
    select_fields=["title", "content", "url"],
    top=5,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[vector_search])

```

```python
classmethod create_full_text_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
```

【中文翻译】Factory method to create a full-text search tool.
Full-text search uses advanced text analysis (stemming, lemmatization, etc.)
to provide more comprehensive text matching than basic keyword search.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
search_fields (Optional[List[str]]) – Fields to search within
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized full-text search tool



Example Usage:# type: ignore
# Example of using full-text search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a full-text search tool
full_text_search = AzureAISearchTool.create_full_text_search(
    name="document_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["title", "content", "category", "url"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[full_text_search])

**示例**:
```python
# type: ignore
# Example of using full-text search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a full-text search tool
full_text_search = AzureAISearchTool.create_full_text_search(
    name="document_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["title", "content", "category", "url"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[full_text_search])

```

```python
classmethod create_hybrid_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], vector_fields: List[str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
```

【中文翻译】Factory method to create a hybrid search tool (text + vector).
Hybrid search combines text search (fulltext or semantic) with vector similarity
search to provide more comprehensive results. This is the recommended entrypoint for hybrid (text + vector) search.
The query_type will be ‘semantic’ if semantic_config_name is provided, otherwise ‘fulltext’.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
vector_fields (List[str]) – Fields containing vector embeddings for similarity search
search_fields (Optional[List[str]]) – Fields to search within for text search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized hybrid search tool



Example Usage:# type: ignore
# Example of using hybrid search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a hybrid search tool
hybrid_search = AzureAISearchTool.create_hybrid_search(
    name="hybrid_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding_field"],
    search_fields=["title", "content"],
    select_fields=["title", "content", "url", "date"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("researcher", tools=[hybrid_search])



Warning
If you set query_type="semantic", you must also provide a valid semantic_config_name.
If you do not, the tool will default to the config name "semantic".

**示例**:
```python
# type: ignore
# Example of using hybrid search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a hybrid search tool
hybrid_search = AzureAISearchTool.create_hybrid_search(
    name="hybrid_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding_field"],
    search_fields=["title", "content"],
    select_fields=["title", "content", "url", "date"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("researcher", tools=[hybrid_search])

```

```python
classmethod create_keyword_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
```

【中文翻译】Factory method to create a keyword search tool.
Keyword search performs traditional text-based search, good for finding documents
containing specific terms or exact matches to your query.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
search_fields (Optional[List[str]]) – Fields to search within for text search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized keyword search tool



Example Usage:# type: ignore
# Example of using keyword search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a keyword search tool
keyword_search = AzureAISearchTool.create_keyword_search(
    name="keyword_search",
    endpoint="https://your-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["id", "title", "content", "category"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[keyword_search])

**示例**:
```python
# type: ignore
# Example of using keyword search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a keyword search tool
keyword_search = AzureAISearchTool.create_keyword_search(
    name="keyword_search",
    endpoint="https://your-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["id", "title", "content", "category"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[keyword_search])

```

```python
classmethod create_vector_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], vector_fields: List[str], select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
```

【中文翻译】Factory method to create a vector search tool.
Vector search uses embedding vectors to find semantically similar content, enabling
the discovery of related information even when different terminology is used.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
vector_fields (List[str]) – Fields containing vector embeddings for similarity search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized vector search tool



Example Usage:# type: ignore
# Example of using vector search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a vector search tool
vector_search = AzureAISearchTool.create_vector_search(
    name="vector_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding"],
    select_fields=["title", "content", "url"],
    top=5,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[vector_search])

**示例**:
```python
# type: ignore
# Example of using vector search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a vector search tool
vector_search = AzureAISearchTool.create_vector_search(
    name="vector_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding"],
    select_fields=["title", "content", "url"],
    top=5,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[vector_search])

```

```python
classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → AzureAISearchTool[source]#
```

【中文翻译】Load a component from a component model.

Parameters:

model – The component model or dictionary with configuration
expected – Optional expected return type


Returns:
An initialized AzureAISearchTool instance

```python
class BaseAzureAISearchTool(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], description: str | None = None, api_version: str = '2023-11-01', query_type: Literal['keyword', 'fulltext', 'vector', 'semantic'] = 'keyword', search_fields: List[str] | None = None, select_fields: List[str] | None = None, vector_fields: List[str] | None = None, top: int | None = None, filter: str | None = None, semantic_config_name: str | None = None, enable_caching: bool = False, cache_ttl_seconds: int = 300)[source]#
```

【中文翻译】Bases: BaseTool[SearchQuery, SearchResults], ABC
Abstract base class for Azure AI Search tools.
This class defines the common interface and functionality for all Azure AI Search tools.
It handles configuration management, client initialization, and the abstract methods
that subclasses must implement.


search_config#
Configuration parameters for the search service.


Note
This is an abstract base class and should not be instantiated directly.
Use concrete implementations or the factory methods in AzureAISearchTool.



async close() → None[source]#
Explicitly close the Azure SearchClient if needed (for cleanup in long-running apps/tests).



dump_component() → ComponentModel[source]#
Serialize the tool to a component model.

Returns:
ComponentModel – A serialized representation of the tool





classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → BaseAzureAISearchTool[source]#

classmethod load_component(model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType]) → ExpectedType
Load the tool from a component model.

Parameters:

model (Union[ComponentModel, Dict[str, Any]]) – The component configuration.
expected (Optional[Type[ExpectedType]]) – Optional component class for deserialization.


Returns:
Union[BaseAzureAISearchTool, ExpectedType] – An instance of the tool.

Raises:
ValueError – If the component configuration is invalid.





return_value_as_string(value: SearchResults) → str[source]#
Convert the search results to a string representation.
This method is used to format the search results in a way that’s suitable
for display to the user or for consumption by language models.

Parameters:
value (List[SearchResult]) – The search results to convert.

Returns:
str – A formatted string representation of the search results.





async run(args: str | Dict[str, Any] | SearchQuery, cancellation_token: CancellationToken | None = None) → SearchResults[source]#
Execute a search against the Azure AI Search index.

Parameters:

args – Search query text or SearchQuery object
cancellation_token – Optional token to cancel the operation


Returns:
Search results





property schema: ToolSchema#
Return the schema for the tool.

```python
search_config#
```

【中文翻译】Configuration parameters for the search service.

```python
async close() → None[source]#
```

【中文翻译】Explicitly close the Azure SearchClient if needed (for cleanup in long-running apps/tests).

```python
dump_component() → ComponentModel[source]#
```

【中文翻译】Serialize the tool to a component model.

Returns:
ComponentModel – A serialized representation of the tool

```python
classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → BaseAzureAISearchTool[source]#
```

【中文翻译】Load the tool from a component model.

Parameters:

model (Union[ComponentModel, Dict[str, Any]]) – The component configuration.
expected (Optional[Type[ExpectedType]]) – Optional component class for deserialization.


Returns:
Union[BaseAzureAISearchTool, ExpectedType] – An instance of the tool.

Raises:
ValueError – If the component configuration is invalid.

```python
return_value_as_string(value: SearchResults) → str[source]#
```

【中文翻译】Convert the search results to a string representation.
This method is used to format the search results in a way that’s suitable
for display to the user or for consumption by language models.

Parameters:
value (List[SearchResult]) – The search results to convert.

Returns:
str – A formatted string representation of the search results.

```python
async run(args: str | Dict[str, Any] | SearchQuery, cancellation_token: CancellationToken | None = None) → SearchResults[source]#
```

【中文翻译】Execute a search against the Azure AI Search index.

Parameters:

args – Search query text or SearchQuery object
cancellation_token – Optional token to cancel the operation


Returns:
Search results

```python
property schema: ToolSchema#
```

【中文翻译】Return the schema for the tool.

```python
pydantic model SearchQuery[source]#
```

【中文翻译】Bases: BaseModel
Search query parameters.
This simplified interface only requires a search query string.
All other parameters (top, filters, vector fields, etc.) are specified during tool creation
rather than at query time, making it easier for language models to generate structured output.

Parameters:
query (str) – The search query text.



Show JSON schema{
   "title": "SearchQuery",
   "description": "Search query parameters.\n\nThis simplified interface only requires a search query string.\nAll other parameters (top, filters, vector fields, etc.) are specified during tool creation\nrather than at query time, making it easier for language models to generate structured output.\n\nArgs:\n    query (str): The search query text.",
   "type": "object",
   "properties": {
      "query": {
         "description": "Search query text",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}



Fields:

query (str)





field query: str [Required]#
Search query text

**示例**:
```python
{
   "title": "SearchQuery",
   "description": "Search query parameters.\n\nThis simplified interface only requires a search query string.\nAll other parameters (top, filters, vector fields, etc.) are specified during tool creation\nrather than at query time, making it easier for language models to generate structured output.\n\nArgs:\n    query (str): The search query text.",
   "type": "object",
   "properties": {
      "query": {
         "description": "Search query text",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}

```

```python
field query: str [Required]#
```

【中文翻译】Search query text

```python
pydantic model SearchResult[source]#
```

【中文翻译】Bases: BaseModel
Search result.

Parameters:

score (float) – The search score.
content (Dict[str, Any]) – The document content.
metadata (Dict[str, Any]) – Additional metadata about the document.




Show JSON schema{
   "title": "SearchResult",
   "description": "Search result.\n\nArgs:\n    score (float): The search score.\n    content (Dict[str, Any]): The document content.\n    metadata (Dict[str, Any]): Additional metadata about the document.",
   "type": "object",
   "properties": {
      "score": {
         "description": "The search score",
         "title": "Score",
         "type": "number"
      },
      "content": {
         "description": "The document content",
         "title": "Content",
         "type": "object"
      },
      "metadata": {
         "description": "Additional metadata about the document",
         "title": "Metadata",
         "type": "object"
      }
   },
   "required": [
      "score",
      "content",
      "metadata"
   ]
}



Fields:

content (Dict[str, Any])
metadata (Dict[str, Any])
score (float)





field content: Dict[str, Any] [Required]#
The document content



field metadata: Dict[str, Any] [Required]#
Additional metadata about the document



field score: float [Required]#
The search score

**示例**:
```python
{
   "title": "SearchResult",
   "description": "Search result.\n\nArgs:\n    score (float): The search score.\n    content (Dict[str, Any]): The document content.\n    metadata (Dict[str, Any]): Additional metadata about the document.",
   "type": "object",
   "properties": {
      "score": {
         "description": "The search score",
         "title": "Score",
         "type": "number"
      },
      "content": {
         "description": "The document content",
         "title": "Content",
         "type": "object"
      },
      "metadata": {
         "description": "Additional metadata about the document",
         "title": "Metadata",
         "type": "object"
      }
   },
   "required": [
      "score",
      "content",
      "metadata"
   ]
}

```

```python
field content: Dict[str, Any] [Required]#
```

【中文翻译】The document content

```python
field metadata: Dict[str, Any] [Required]#
```

【中文翻译】Additional metadata about the document

```python
field score: float [Required]#
```

【中文翻译】The search score

```python
pydantic model SearchResults[source]#
```

【中文翻译】Bases: BaseModel
Container for search results.

Parameters:
results (List[SearchResult]) – List of search results.



Show JSON schema{
   "title": "SearchResults",
   "description": "Container for search results.\n\nArgs:\n    results (List[SearchResult]): List of search results.",
   "type": "object",
   "properties": {
      "results": {
         "description": "List of search results",
         "items": {
            "$ref": "#/$defs/SearchResult"
         },
         "title": "Results",
         "type": "array"
      }
   },
   "$defs": {
      "SearchResult": {
         "description": "Search result.\n\nArgs:\n    score (float): The search score.\n    content (Dict[str, Any]): The document content.\n    metadata (Dict[str, Any]): Additional metadata about the document.",
         "properties": {
            "score": {
               "description": "The search score",
               "title": "Score",
               "type": "number"
            },
            "content": {
               "description": "The document content",
               "title": "Content",
               "type": "object"
            },
            "metadata": {
               "description": "Additional metadata about the document",
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "score",
            "content",
            "metadata"
         ],
         "title": "SearchResult",
         "type": "object"
      }
   },
   "required": [
      "results"
   ]
}



Fields:

results (List[autogen_ext.tools.azure._ai_search.SearchResult])





field results: List[SearchResult] [Required]#
List of search results

**示例**:
```python
{
   "title": "SearchResults",
   "description": "Container for search results.\n\nArgs:\n    results (List[SearchResult]): List of search results.",
   "type": "object",
   "properties": {
      "results": {
         "description": "List of search results",
         "items": {
            "$ref": "#/$defs/SearchResult"
         },
         "title": "Results",
         "type": "array"
      }
   },
   "$defs": {
      "SearchResult": {
         "description": "Search result.\n\nArgs:\n    score (float): The search score.\n    content (Dict[str, Any]): The document content.\n    metadata (Dict[str, Any]): Additional metadata about the document.",
         "properties": {
            "score": {
               "description": "The search score",
               "title": "Score",
               "type": "number"
            },
            "content": {
               "description": "The document content",
               "title": "Content",
               "type": "object"
            },
            "metadata": {
               "description": "Additional metadata about the document",
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "score",
            "content",
            "metadata"
         ],
         "title": "SearchResult",
         "type": "object"
      }
   },
   "required": [
      "results"
   ]
}

```

```python
field results: List[SearchResult] [Required]#
```

【中文翻译】List of search results

【中文翻译】previous

【中文翻译】autogen_ext.models.llama_cpp

【中文翻译】next

【中文翻译】autogen_ext.tools.code_execution

### autogen_ext.tools.azure {autogen_exttoolsazure}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html)

```python
pydantic model AzureAISearchConfig[source]#
```

【中文翻译】Bases: BaseModel
Configuration for Azure AI Search tool.
This class defines the configuration parameters for AzureAISearchTool.
It provides options for customizing search behavior including query types,
field selection, authentication, retry policies, and caching strategies.

Note
This class requires the azure extra for the autogen-ext package.
pip install -U "autogen-ext[azure]"



Example
from azure.core.credentials import AzureKeyCredential
from autogen_ext.tools.azure import AzureAISearchConfig

config = AzureAISearchConfig(
    name="doc_search",
    endpoint="https://my-search.search.windows.net",
    index_name="my-index",
    credential=AzureKeyCredential("<your-key>"),
    query_type="vector",
    vector_fields=["embedding"],
)



For more details, see:
Azure AI Search Overview
Vector Search




Parameters:

name (str) – Name for the tool instance, used to identify it in the agent’s toolkit.
description (Optional[str]) – Human-readable description of what this tool does and how to use it.
endpoint (str) – The full URL of your Azure AI Search service, in the format
‘https://<service-name>.search.windows.net’.
index_name (str) – Name of the target search index in your Azure AI Search service.
The index must be pre-created and properly configured.
api_version (str) – Azure AI Search REST API version to use. Defaults to ‘2023-11-01’.
Only change if you need specific features from a different API version.
credential (Union[AzureKeyCredential, TokenCredential]) – Azure authentication credential:
- AzureKeyCredential: For API key authentication (admin/query key)
- TokenCredential: For Azure AD authentication (e.g., DefaultAzureCredential)
query_type (Literal["keyword", "fulltext", "vector", "semantic"]) – The search query mode to use:
- ‘keyword’: Basic keyword search (default)
- ‘fulltext’: Full Lucene query syntax
- ‘vector’: Vector similarity search
- ‘semantic’: Semantic search using semantic configuration
search_fields (Optional[List[str]]) – List of index fields to search within. If not specified,
searches all searchable fields. Example: [‘title’, ‘content’].
select_fields (Optional[List[str]]) – Fields to return in search results. If not specified,
returns all fields. Use to optimize response size.
vector_fields (Optional[List[str]]) – Vector field names for vector search. Must be configured
in your search index as vector fields. Required for vector search.
top (Optional[int]) – Maximum number of documents to return in search results.
Helps control response size and processing time.
retry_enabled (bool) – Whether to enable retry policy for transient errors. Defaults to True.
retry_max_attempts (Optional[int]) – Maximum number of retry attempts for failed requests. Defaults to 3.
retry_mode (Literal["fixed", "exponential"]) – Retry backoff strategy: fixed or exponential. Defaults to “exponential”.
enable_caching (bool) – Whether to enable client-side caching of search results. Defaults to False.
cache_ttl_seconds (int) – Time-to-live for cached search results in seconds. Defaults to 300 (5 minutes).
filter (Optional[str]) – OData filter expression to refine search results.




Show JSON schema{
   "title": "AzureAISearchConfig",
   "description": "Configuration for Azure AI Search tool.\n\nThis class defines the configuration parameters for :class:`AzureAISearchTool`.\nIt provides options for customizing search behavior including query types,\nfield selection, authentication, retry policies, and caching strategies.\n\n.. note::\n\n    This class requires the :code:`azure` extra for the :code:`autogen-ext` package.\n\n    .. code-block:: bash\n\n        pip install -U \"autogen-ext[azure]\"\n\nExample:\n    .. code-block:: python\n\n        from azure.core.credentials import AzureKeyCredential\n        from autogen_ext.tools.azure import AzureAISearchConfig\n\n        config = AzureAISearchConfig(\n            name=\"doc_search\",\n            endpoint=\"https://my-search.search.windows.net\",\n            index_name=\"my-index\",\n            credential=AzureKeyCredential(\"<your-key>\"),\n            query_type=\"vector\",\n            vector_fields=[\"embedding\"],\n        )\n\nFor more details, see:\n    * `Azure AI Search Overview <https://learn.microsoft.com/azure/search/search-what-is-azure-search>`_\n    * `Vector Search <https://learn.microsoft.com/azure/search/vector-search-overview>`_\n\nArgs:\n    name (str): Name for the tool instance, used to identify it in the agent's toolkit.\n    description (Optional[str]): Human-readable description of what this tool does and how to use it.\n    endpoint (str): The full URL of your Azure AI Search service, in the format\n        'https://<service-name>.search.windows.net'.\n    index_name (str): Name of the target search index in your Azure AI Search service.\n        The index must be pre-created and properly configured.\n    api_version (str): Azure AI Search REST API version to use. Defaults to '2023-11-01'.\n        Only change if you need specific features from a different API version.\n    credential (Union[AzureKeyCredential, TokenCredential]): Azure authentication credential:\n        - AzureKeyCredential: For API key authentication (admin/query key)\n        - TokenCredential: For Azure AD authentication (e.g., DefaultAzureCredential)\n    query_type (Literal[\"keyword\", \"fulltext\", \"vector\", \"semantic\"]): The search query mode to use:\n        - 'keyword': Basic keyword search (default)\n        - 'fulltext': Full Lucene query syntax\n        - 'vector': Vector similarity search\n        - 'semantic': Semantic search using semantic configuration\n    search_fields (Optional[List[str]]): List of index fields to search within. If not specified,\n        searches all searchable fields. Example: ['title', 'content'].\n    select_fields (Optional[List[str]]): Fields to return in search results. If not specified,\n        returns all fields. Use to optimize response size.\n    vector_fields (Optional[List[str]]): Vector field names for vector search. Must be configured\n        in your search index as vector fields. Required for vector search.\n    top (Optional[int]): Maximum number of documents to return in search results.\n        Helps control response size and processing time.\n    retry_enabled (bool): Whether to enable retry policy for transient errors. Defaults to True.\n    retry_max_attempts (Optional[int]): Maximum number of retry attempts for failed requests. Defaults to 3.\n    retry_mode (Literal[\"fixed\", \"exponential\"]): Retry backoff strategy: fixed or exponential. Defaults to \"exponential\".\n    enable_caching (bool): Whether to enable client-side caching of search results. Defaults to False.\n    cache_ttl_seconds (int): Time-to-live for cached search results in seconds. Defaults to 300 (5 minutes).\n    filter (Optional[str]): OData filter expression to refine search results.",
   "type": "object",
   "properties": {
      "name": {
         "description": "The name of the tool",
         "title": "Name",
         "type": "string"
      },
      "description": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "A description of the tool",
         "title": "Description"
      },
      "endpoint": {
         "description": "The endpoint URL for your Azure AI Search service",
         "title": "Endpoint",
         "type": "string"
      },
      "index_name": {
         "description": "The name of the search index to query",
         "title": "Index Name",
         "type": "string"
      },
      "api_version": {
         "default": "2023-11-01",
         "description": "API version to use",
         "title": "Api Version",
         "type": "string"
      },
      "credential": {
         "anyOf": [],
         "description": "The credential to use for authentication",
         "title": "Credential"
      },
      "query_type": {
         "default": "keyword",
         "description": "Type of query to perform (keyword for classic, fulltext for Lucene, vector for embedding, semantic for semantic/AI search)",
         "enum": [
            "keyword",
            "fulltext",
            "vector",
            "semantic"
         ],
         "title": "Query Type",
         "type": "string"
      },
      "search_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of fields to search in",
         "title": "Search Fields"
      },
      "select_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of fields to return in results",
         "title": "Select Fields"
      },
      "vector_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of vector fields for vector search",
         "title": "Vector Fields"
      },
      "top": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional number of results to return",
         "title": "Top"
      },
      "filter": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional OData filter expression to refine search results",
         "title": "Filter"
      },
      "retry_enabled": {
         "default": true,
         "description": "Whether to enable retry policy for transient errors",
         "title": "Retry Enabled",
         "type": "boolean"
      },
      "retry_max_attempts": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 3,
         "description": "Maximum number of retry attempts for failed requests",
         "title": "Retry Max Attempts"
      },
      "retry_mode": {
         "default": "exponential",
         "description": "Retry backoff strategy: fixed or exponential",
         "enum": [
            "fixed",
            "exponential"
         ],
         "title": "Retry Mode",
         "type": "string"
      },
      "enable_caching": {
         "default": false,
         "description": "Whether to enable client-side caching of search results",
         "title": "Enable Caching",
         "type": "boolean"
      },
      "cache_ttl_seconds": {
         "default": 300,
         "description": "Time-to-live for cached search results in seconds",
         "title": "Cache Ttl Seconds",
         "type": "integer"
      },
      "embedding_provider": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Name of embedding provider to use (e.g., 'azure_openai', 'openai')",
         "title": "Embedding Provider"
      },
      "embedding_model": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Model name to use for generating embeddings",
         "title": "Embedding Model"
      },
      "embedding_dimension": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Dimension of embedding vectors produced by the model",
         "title": "Embedding Dimension"
      }
   },
   "required": [
      "name",
      "endpoint",
      "index_name",
      "credential"
   ]
}



Fields:

api_version (str)
cache_ttl_seconds (int)
credential (azure.core.credentials.AzureKeyCredential | azure.core.credentials.TokenCredential)
description (str | None)
embedding_dimension (int | None)
embedding_model (str | None)
embedding_provider (str | None)
enable_caching (bool)
endpoint (str)
filter (str | None)
index_name (str)
name (str)
query_type (Literal['keyword', 'fulltext', 'vector', 'semantic'])
retry_enabled (bool)
retry_max_attempts (int | None)
retry_mode (Literal['fixed', 'exponential'])
search_fields (List[str] | None)
select_fields (List[str] | None)
top (int | None)
vector_fields (List[str] | None)





field api_version: str = '2023-11-01'#
API version to use



field cache_ttl_seconds: int = 300#
Time-to-live for cached search results in seconds



field credential: AzureKeyCredential | TokenCredential [Required]#
The credential to use for authentication



field description: str | None = None#
A description of the tool



field embedding_dimension: int | None = None#
Dimension of embedding vectors produced by the model



field embedding_model: str | None = None#
Model name to use for generating embeddings



field embedding_provider: str | None = None#
Name of embedding provider to use (e.g., ‘azure_openai’, ‘openai’)



field enable_caching: bool = False#
Whether to enable client-side caching of search results



field endpoint: str [Required]#
The endpoint URL for your Azure AI Search service



field filter: str | None = None#
Optional OData filter expression to refine search results



field index_name: str [Required]#
The name of the search index to query



field name: str [Required]#
The name of the tool



field query_type: Literal['keyword', 'fulltext', 'vector', 'semantic'] = 'keyword'#
Type of query to perform (keyword for classic, fulltext for Lucene, vector for embedding, semantic for semantic/AI search)



field retry_enabled: bool = True#
Whether to enable retry policy for transient errors



field retry_max_attempts: int | None = 3#
Maximum number of retry attempts for failed requests



field retry_mode: Literal['fixed', 'exponential'] = 'exponential'#
Retry backoff strategy: fixed or exponential



field search_fields: List[str] | None = None#
Optional list of fields to search in



field select_fields: List[str] | None = None#
Optional list of fields to return in results



field top: int | None = None#
Optional number of results to return



field vector_fields: List[str] | None = None#
Optional list of vector fields for vector search



model_dump(**kwargs: Any) → Dict[str, Any][source]#
Custom model_dump to handle credentials.



classmethod validate_credentials(data: Any) → Any[source]#
Wrap a classmethod, staticmethod, property or unbound function
and act as a descriptor that allows us to detect decorated items
from the class’ attributes.
This class’ __get__ returns the wrapped item’s __get__ result,
which makes it transparent for classmethods and staticmethods.


wrapped#
The decorator that has to be wrapped.



decorator_info#
The decorator info.



shim#
A wrapper function to wrap V1 style function.

**示例**:
```python
pip install -U "autogen-ext[azure]"

```

**示例**:
```python
from azure.core.credentials import AzureKeyCredential
from autogen_ext.tools.azure import AzureAISearchConfig

config = AzureAISearchConfig(
    name="doc_search",
    endpoint="https://my-search.search.windows.net",
    index_name="my-index",
    credential=AzureKeyCredential("<your-key>"),
    query_type="vector",
    vector_fields=["embedding"],
)

```

**示例**:
```python
{
   "title": "AzureAISearchConfig",
   "description": "Configuration for Azure AI Search tool.\n\nThis class defines the configuration parameters for :class:`AzureAISearchTool`.\nIt provides options for customizing search behavior including query types,\nfield selection, authentication, retry policies, and caching strategies.\n\n.. note::\n\n    This class requires the :code:`azure` extra for the :code:`autogen-ext` package.\n\n    .. code-block:: bash\n\n        pip install -U \"autogen-ext[azure]\"\n\nExample:\n    .. code-block:: python\n\n        from azure.core.credentials import AzureKeyCredential\n        from autogen_ext.tools.azure import AzureAISearchConfig\n\n        config = AzureAISearchConfig(\n            name=\"doc_search\",\n            endpoint=\"https://my-search.search.windows.net\",\n            index_name=\"my-index\",\n            credential=AzureKeyCredential(\"<your-key>\"),\n            query_type=\"vector\",\n            vector_fields=[\"embedding\"],\n        )\n\nFor more details, see:\n    * `Azure AI Search Overview <https://learn.microsoft.com/azure/search/search-what-is-azure-search>`_\n    * `Vector Search <https://learn.microsoft.com/azure/search/vector-search-overview>`_\n\nArgs:\n    name (str): Name for the tool instance, used to identify it in the agent's toolkit.\n    description (Optional[str]): Human-readable description of what this tool does and how to use it.\n    endpoint (str): The full URL of your Azure AI Search service, in the format\n        'https://<service-name>.search.windows.net'.\n    index_name (str): Name of the target search index in your Azure AI Search service.\n        The index must be pre-created and properly configured.\n    api_version (str): Azure AI Search REST API version to use. Defaults to '2023-11-01'.\n        Only change if you need specific features from a different API version.\n    credential (Union[AzureKeyCredential, TokenCredential]): Azure authentication credential:\n        - AzureKeyCredential: For API key authentication (admin/query key)\n        - TokenCredential: For Azure AD authentication (e.g., DefaultAzureCredential)\n    query_type (Literal[\"keyword\", \"fulltext\", \"vector\", \"semantic\"]): The search query mode to use:\n        - 'keyword': Basic keyword search (default)\n        - 'fulltext': Full Lucene query syntax\n        - 'vector': Vector similarity search\n        - 'semantic': Semantic search using semantic configuration\n    search_fields (Optional[List[str]]): List of index fields to search within. If not specified,\n        searches all searchable fields. Example: ['title', 'content'].\n    select_fields (Optional[List[str]]): Fields to return in search results. If not specified,\n        returns all fields. Use to optimize response size.\n    vector_fields (Optional[List[str]]): Vector field names for vector search. Must be configured\n        in your search index as vector fields. Required for vector search.\n    top (Optional[int]): Maximum number of documents to return in search results.\n        Helps control response size and processing time.\n    retry_enabled (bool): Whether to enable retry policy for transient errors. Defaults to True.\n    retry_max_attempts (Optional[int]): Maximum number of retry attempts for failed requests. Defaults to 3.\n    retry_mode (Literal[\"fixed\", \"exponential\"]): Retry backoff strategy: fixed or exponential. Defaults to \"exponential\".\n    enable_caching (bool): Whether to enable client-side caching of search results. Defaults to False.\n    cache_ttl_seconds (int): Time-to-live for cached search results in seconds. Defaults to 300 (5 minutes).\n    filter (Optional[str]): OData filter expression to refine search results.",
   "type": "object",
   "properties": {
      "name": {
         "description": "The name of the tool",
         "title": "Name",
         "type": "string"
      },
      "description": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "A description of the tool",
         "title": "Description"
      },
      "endpoint": {
         "description": "The endpoint URL for your Azure AI Search service",
         "title": "Endpoint",
         "type": "string"
      },
      "index_name": {
         "description": "The name of the search index to query",
         "title": "Index Name",
         "type": "string"
      },
      "api_version": {
         "default": "2023-11-01",
         "description": "API version to use",
         "title": "Api Version",
         "type": "string"
      },
      "credential": {
         "anyOf": [],
         "description": "The credential to use for authentication",
         "title": "Credential"
      },
      "query_type": {
         "default": "keyword",
         "description": "Type of query to perform (keyword for classic, fulltext for Lucene, vector for embedding, semantic for semantic/AI search)",
         "enum": [
            "keyword",
            "fulltext",
            "vector",
            "semantic"
         ],
         "title": "Query Type",
         "type": "string"
      },
      "search_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of fields to search in",
         "title": "Search Fields"
      },
      "select_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of fields to return in results",
         "title": "Select Fields"
      },
      "vector_fields": {
         "anyOf": [
            {
               "items": {
                  "type": "string"
               },
               "type": "array"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional list of vector fields for vector search",
         "title": "Vector Fields"
      },
      "top": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional number of results to return",
         "title": "Top"
      },
      "filter": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Optional OData filter expression to refine search results",
         "title": "Filter"
      },
      "retry_enabled": {
         "default": true,
         "description": "Whether to enable retry policy for transient errors",
         "title": "Retry Enabled",
         "type": "boolean"
      },
      "retry_max_attempts": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": 3,
         "description": "Maximum number of retry attempts for failed requests",
         "title": "Retry Max Attempts"
      },
      "retry_mode": {
         "default": "exponential",
         "description": "Retry backoff strategy: fixed or exponential",
         "enum": [
            "fixed",
            "exponential"
         ],
         "title": "Retry Mode",
         "type": "string"
      },
      "enable_caching": {
         "default": false,
         "description": "Whether to enable client-side caching of search results",
         "title": "Enable Caching",
         "type": "boolean"
      },
      "cache_ttl_seconds": {
         "default": 300,
         "description": "Time-to-live for cached search results in seconds",
         "title": "Cache Ttl Seconds",
         "type": "integer"
      },
      "embedding_provider": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Name of embedding provider to use (e.g., 'azure_openai', 'openai')",
         "title": "Embedding Provider"
      },
      "embedding_model": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Model name to use for generating embeddings",
         "title": "Embedding Model"
      },
      "embedding_dimension": {
         "anyOf": [
            {
               "type": "integer"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "description": "Dimension of embedding vectors produced by the model",
         "title": "Embedding Dimension"
      }
   },
   "required": [
      "name",
      "endpoint",
      "index_name",
      "credential"
   ]
}

```

```python
field api_version: str = '2023-11-01'#
```

【中文翻译】API version to use

```python
field cache_ttl_seconds: int = 300#
```

【中文翻译】Time-to-live for cached search results in seconds

```python
field credential: AzureKeyCredential | TokenCredential [Required]#
```

【中文翻译】The credential to use for authentication

```python
field description: str | None = None#
```

【中文翻译】A description of the tool

```python
field embedding_dimension: int | None = None#
```

【中文翻译】Dimension of embedding vectors produced by the model

```python
field embedding_model: str | None = None#
```

【中文翻译】Model name to use for generating embeddings

```python
field embedding_provider: str | None = None#
```

【中文翻译】Name of embedding provider to use (e.g., ‘azure_openai’, ‘openai’)

```python
field enable_caching: bool = False#
```

【中文翻译】Whether to enable client-side caching of search results

```python
field endpoint: str [Required]#
```

【中文翻译】The endpoint URL for your Azure AI Search service

```python
field filter: str | None = None#
```

【中文翻译】Optional OData filter expression to refine search results

```python
field index_name: str [Required]#
```

【中文翻译】The name of the search index to query

```python
field name: str [Required]#
```

【中文翻译】The name of the tool

```python
field query_type: Literal['keyword', 'fulltext', 'vector', 'semantic'] = 'keyword'#
```

【中文翻译】Type of query to perform (keyword for classic, fulltext for Lucene, vector for embedding, semantic for semantic/AI search)

```python
field retry_enabled: bool = True#
```

【中文翻译】Whether to enable retry policy for transient errors

```python
field retry_max_attempts: int | None = 3#
```

【中文翻译】Maximum number of retry attempts for failed requests

```python
field retry_mode: Literal['fixed', 'exponential'] = 'exponential'#
```

【中文翻译】Retry backoff strategy: fixed or exponential

```python
field search_fields: List[str] | None = None#
```

【中文翻译】Optional list of fields to search in

```python
field select_fields: List[str] | None = None#
```

【中文翻译】Optional list of fields to return in results

```python
field top: int | None = None#
```

【中文翻译】Optional number of results to return

```python
field vector_fields: List[str] | None = None#
```

【中文翻译】Optional list of vector fields for vector search

```python
model_dump(**kwargs: Any) → Dict[str, Any][source]#
```

【中文翻译】Custom model_dump to handle credentials.

```python
classmethod validate_credentials(data: Any) → Any[source]#
```

【中文翻译】Wrap a classmethod, staticmethod, property or unbound function
and act as a descriptor that allows us to detect decorated items
from the class’ attributes.
This class’ __get__ returns the wrapped item’s __get__ result,
which makes it transparent for classmethods and staticmethods.


wrapped#
The decorator that has to be wrapped.



decorator_info#
The decorator info.



shim#
A wrapper function to wrap V1 style function.

```python
wrapped#
```

【中文翻译】The decorator that has to be wrapped.

```python
decorator_info#
```

【中文翻译】The decorator info.

```python
shim#
```

【中文翻译】A wrapper function to wrap V1 style function.

```python
class AzureAISearchTool(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], query_type: Literal['keyword', 'fulltext', 'vector', 'semantic'], search_fields: List[str] | None = None, select_fields: List[str] | None = None, vector_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any)[source]#
```

【中文翻译】Bases: BaseAzureAISearchTool
Azure AI Search tool for querying Azure search indexes.
This tool provides a simplified interface for querying Azure AI Search indexes using
various search methods. The tool supports four main search types:

Keyword Search: Traditional text-based search using Azure’s text analysis
Full-Text Search: Enhanced text search with language-specific analyzers
Vector Search: Semantic similarity search using vector embeddings
Hybrid Search: Combines fulltext and vector search for comprehensive results

You should use the factory methods to create instances for specific search types:
- create_keyword_search()
- create_full_text_search()
- create_vector_search()
- create_hybrid_search()


classmethod create_full_text_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
Factory method to create a full-text search tool.
Full-text search uses advanced text analysis (stemming, lemmatization, etc.)
to provide more comprehensive text matching than basic keyword search.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
search_fields (Optional[List[str]]) – Fields to search within
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized full-text search tool



Example Usage:# type: ignore
# Example of using full-text search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a full-text search tool
full_text_search = AzureAISearchTool.create_full_text_search(
    name="document_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["title", "content", "category", "url"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[full_text_search])







classmethod create_hybrid_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], vector_fields: List[str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
Factory method to create a hybrid search tool (text + vector).
Hybrid search combines text search (fulltext or semantic) with vector similarity
search to provide more comprehensive results. This is the recommended entrypoint for hybrid (text + vector) search.
The query_type will be ‘semantic’ if semantic_config_name is provided, otherwise ‘fulltext’.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
vector_fields (List[str]) – Fields containing vector embeddings for similarity search
search_fields (Optional[List[str]]) – Fields to search within for text search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized hybrid search tool



Example Usage:# type: ignore
# Example of using hybrid search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a hybrid search tool
hybrid_search = AzureAISearchTool.create_hybrid_search(
    name="hybrid_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding_field"],
    search_fields=["title", "content"],
    select_fields=["title", "content", "url", "date"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("researcher", tools=[hybrid_search])



Warning
If you set query_type="semantic", you must also provide a valid semantic_config_name.
If you do not, the tool will default to the config name "semantic".






classmethod create_keyword_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
Factory method to create a keyword search tool.
Keyword search performs traditional text-based search, good for finding documents
containing specific terms or exact matches to your query.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
search_fields (Optional[List[str]]) – Fields to search within for text search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized keyword search tool



Example Usage:# type: ignore
# Example of using keyword search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a keyword search tool
keyword_search = AzureAISearchTool.create_keyword_search(
    name="keyword_search",
    endpoint="https://your-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["id", "title", "content", "category"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[keyword_search])







classmethod create_vector_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], vector_fields: List[str], select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
Factory method to create a vector search tool.
Vector search uses embedding vectors to find semantically similar content, enabling
the discovery of related information even when different terminology is used.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
vector_fields (List[str]) – Fields containing vector embeddings for similarity search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized vector search tool



Example Usage:# type: ignore
# Example of using vector search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a vector search tool
vector_search = AzureAISearchTool.create_vector_search(
    name="vector_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding"],
    select_fields=["title", "content", "url"],
    top=5,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[vector_search])







classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → AzureAISearchTool[source]#

classmethod load_component(model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType]) → ExpectedType
Load a component from a component model.

Parameters:

model – The component model or dictionary with configuration
expected – Optional expected return type


Returns:
An initialized AzureAISearchTool instance

**示例**:
```python
# type: ignore
# Example of using full-text search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a full-text search tool
full_text_search = AzureAISearchTool.create_full_text_search(
    name="document_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["title", "content", "category", "url"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[full_text_search])

```

**示例**:
```python
# type: ignore
# Example of using hybrid search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a hybrid search tool
hybrid_search = AzureAISearchTool.create_hybrid_search(
    name="hybrid_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding_field"],
    search_fields=["title", "content"],
    select_fields=["title", "content", "url", "date"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("researcher", tools=[hybrid_search])

```

**示例**:
```python
# type: ignore
# Example of using keyword search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a keyword search tool
keyword_search = AzureAISearchTool.create_keyword_search(
    name="keyword_search",
    endpoint="https://your-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["id", "title", "content", "category"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[keyword_search])

```

**示例**:
```python
# type: ignore
# Example of using vector search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a vector search tool
vector_search = AzureAISearchTool.create_vector_search(
    name="vector_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding"],
    select_fields=["title", "content", "url"],
    top=5,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[vector_search])

```

```python
classmethod create_full_text_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
```

【中文翻译】Factory method to create a full-text search tool.
Full-text search uses advanced text analysis (stemming, lemmatization, etc.)
to provide more comprehensive text matching than basic keyword search.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
search_fields (Optional[List[str]]) – Fields to search within
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized full-text search tool



Example Usage:# type: ignore
# Example of using full-text search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a full-text search tool
full_text_search = AzureAISearchTool.create_full_text_search(
    name="document_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["title", "content", "category", "url"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[full_text_search])

**示例**:
```python
# type: ignore
# Example of using full-text search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a full-text search tool
full_text_search = AzureAISearchTool.create_full_text_search(
    name="document_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["title", "content", "category", "url"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[full_text_search])

```

```python
classmethod create_hybrid_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], vector_fields: List[str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
```

【中文翻译】Factory method to create a hybrid search tool (text + vector).
Hybrid search combines text search (fulltext or semantic) with vector similarity
search to provide more comprehensive results. This is the recommended entrypoint for hybrid (text + vector) search.
The query_type will be ‘semantic’ if semantic_config_name is provided, otherwise ‘fulltext’.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
vector_fields (List[str]) – Fields containing vector embeddings for similarity search
search_fields (Optional[List[str]]) – Fields to search within for text search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized hybrid search tool



Example Usage:# type: ignore
# Example of using hybrid search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a hybrid search tool
hybrid_search = AzureAISearchTool.create_hybrid_search(
    name="hybrid_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding_field"],
    search_fields=["title", "content"],
    select_fields=["title", "content", "url", "date"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("researcher", tools=[hybrid_search])



Warning
If you set query_type="semantic", you must also provide a valid semantic_config_name.
If you do not, the tool will default to the config name "semantic".

**示例**:
```python
# type: ignore
# Example of using hybrid search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a hybrid search tool
hybrid_search = AzureAISearchTool.create_hybrid_search(
    name="hybrid_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding_field"],
    search_fields=["title", "content"],
    select_fields=["title", "content", "url", "date"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("researcher", tools=[hybrid_search])

```

```python
classmethod create_keyword_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], search_fields: List[str] | None = None, select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
```

【中文翻译】Factory method to create a keyword search tool.
Keyword search performs traditional text-based search, good for finding documents
containing specific terms or exact matches to your query.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
search_fields (Optional[List[str]]) – Fields to search within for text search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized keyword search tool



Example Usage:# type: ignore
# Example of using keyword search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a keyword search tool
keyword_search = AzureAISearchTool.create_keyword_search(
    name="keyword_search",
    endpoint="https://your-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["id", "title", "content", "category"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[keyword_search])

**示例**:
```python
# type: ignore
# Example of using keyword search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a keyword search tool
keyword_search = AzureAISearchTool.create_keyword_search(
    name="keyword_search",
    endpoint="https://your-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    search_fields=["title", "content"],
    select_fields=["id", "title", "content", "category"],
    top=10,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[keyword_search])

```

```python
classmethod create_vector_search(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], vector_fields: List[str], select_fields: List[str] | None = None, filter: str | None = None, top: int | None = 5, **kwargs: Any) → AzureAISearchTool[source]#
```

【中文翻译】Factory method to create a vector search tool.
Vector search uses embedding vectors to find semantically similar content, enabling
the discovery of related information even when different terminology is used.

Parameters:

name (str) – The name of the tool
endpoint (str) – The URL of your Azure AI Search service
index_name (str) – The name of the search index
credential (Union[AzureKeyCredential, TokenCredential, Dict[str, str]]) – Authentication credentials
vector_fields (List[str]) – Fields containing vector embeddings for similarity search
select_fields (Optional[List[str]]) – Fields to include in results
filter (Optional[str]) – OData filter expression to filter results
top (Optional[int]) – Maximum number of results to return
**kwargs (Any) – Additional configuration options


Returns:
An initialized vector search tool



Example Usage:# type: ignore
# Example of using vector search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a vector search tool
vector_search = AzureAISearchTool.create_vector_search(
    name="vector_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding"],
    select_fields=["title", "content", "url"],
    top=5,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[vector_search])

**示例**:
```python
# type: ignore
# Example of using vector search with Azure AI Search
from autogen_ext.tools.azure import AzureAISearchTool
from azure.core.credentials import AzureKeyCredential

# Create a vector search tool
vector_search = AzureAISearchTool.create_vector_search(
    name="vector_search",
    endpoint="https://your-search-service.search.windows.net",
    index_name="your-index",
    credential=AzureKeyCredential("your-api-key"),
    vector_fields=["embedding"],
    select_fields=["title", "content", "url"],
    top=5,
)

# The search tool can be used with an Agent
# assistant = Agent("assistant", tools=[vector_search])

```

```python
classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → AzureAISearchTool[source]#
```

【中文翻译】Load a component from a component model.

Parameters:

model – The component model or dictionary with configuration
expected – Optional expected return type


Returns:
An initialized AzureAISearchTool instance

```python
class BaseAzureAISearchTool(name: str, endpoint: str, index_name: str, credential: AzureKeyCredential | TokenCredential | Dict[str, str], description: str | None = None, api_version: str = '2023-11-01', query_type: Literal['keyword', 'fulltext', 'vector', 'semantic'] = 'keyword', search_fields: List[str] | None = None, select_fields: List[str] | None = None, vector_fields: List[str] | None = None, top: int | None = None, filter: str | None = None, semantic_config_name: str | None = None, enable_caching: bool = False, cache_ttl_seconds: int = 300)[source]#
```

【中文翻译】Bases: BaseTool[SearchQuery, SearchResults], ABC
Abstract base class for Azure AI Search tools.
This class defines the common interface and functionality for all Azure AI Search tools.
It handles configuration management, client initialization, and the abstract methods
that subclasses must implement.


search_config#
Configuration parameters for the search service.


Note
This is an abstract base class and should not be instantiated directly.
Use concrete implementations or the factory methods in AzureAISearchTool.



async close() → None[source]#
Explicitly close the Azure SearchClient if needed (for cleanup in long-running apps/tests).



dump_component() → ComponentModel[source]#
Serialize the tool to a component model.

Returns:
ComponentModel – A serialized representation of the tool





classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → BaseAzureAISearchTool[source]#

classmethod load_component(model: ComponentModel | Dict[str, Any], expected: Type[ExpectedType]) → ExpectedType
Load the tool from a component model.

Parameters:

model (Union[ComponentModel, Dict[str, Any]]) – The component configuration.
expected (Optional[Type[ExpectedType]]) – Optional component class for deserialization.


Returns:
Union[BaseAzureAISearchTool, ExpectedType] – An instance of the tool.

Raises:
ValueError – If the component configuration is invalid.





return_value_as_string(value: SearchResults) → str[source]#
Convert the search results to a string representation.
This method is used to format the search results in a way that’s suitable
for display to the user or for consumption by language models.

Parameters:
value (List[SearchResult]) – The search results to convert.

Returns:
str – A formatted string representation of the search results.





async run(args: str | Dict[str, Any] | SearchQuery, cancellation_token: CancellationToken | None = None) → SearchResults[source]#
Execute a search against the Azure AI Search index.

Parameters:

args – Search query text or SearchQuery object
cancellation_token – Optional token to cancel the operation


Returns:
Search results





property schema: ToolSchema#
Return the schema for the tool.

```python
search_config#
```

【中文翻译】Configuration parameters for the search service.

```python
async close() → None[source]#
```

【中文翻译】Explicitly close the Azure SearchClient if needed (for cleanup in long-running apps/tests).

```python
dump_component() → ComponentModel[source]#
```

【中文翻译】Serialize the tool to a component model.

Returns:
ComponentModel – A serialized representation of the tool

```python
classmethod load_component(model: ComponentModel | Dict[str, Any], expected: None = None) → BaseAzureAISearchTool[source]#
```

【中文翻译】Load the tool from a component model.

Parameters:

model (Union[ComponentModel, Dict[str, Any]]) – The component configuration.
expected (Optional[Type[ExpectedType]]) – Optional component class for deserialization.


Returns:
Union[BaseAzureAISearchTool, ExpectedType] – An instance of the tool.

Raises:
ValueError – If the component configuration is invalid.

```python
return_value_as_string(value: SearchResults) → str[source]#
```

【中文翻译】Convert the search results to a string representation.
This method is used to format the search results in a way that’s suitable
for display to the user or for consumption by language models.

Parameters:
value (List[SearchResult]) – The search results to convert.

Returns:
str – A formatted string representation of the search results.

```python
async run(args: str | Dict[str, Any] | SearchQuery, cancellation_token: CancellationToken | None = None) → SearchResults[source]#
```

【中文翻译】Execute a search against the Azure AI Search index.

Parameters:

args – Search query text or SearchQuery object
cancellation_token – Optional token to cancel the operation


Returns:
Search results

```python
property schema: ToolSchema#
```

【中文翻译】Return the schema for the tool.

```python
pydantic model SearchQuery[source]#
```

【中文翻译】Bases: BaseModel
Search query parameters.
This simplified interface only requires a search query string.
All other parameters (top, filters, vector fields, etc.) are specified during tool creation
rather than at query time, making it easier for language models to generate structured output.

Parameters:
query (str) – The search query text.



Show JSON schema{
   "title": "SearchQuery",
   "description": "Search query parameters.\n\nThis simplified interface only requires a search query string.\nAll other parameters (top, filters, vector fields, etc.) are specified during tool creation\nrather than at query time, making it easier for language models to generate structured output.\n\nArgs:\n    query (str): The search query text.",
   "type": "object",
   "properties": {
      "query": {
         "description": "Search query text",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}



Fields:

query (str)





field query: str [Required]#
Search query text

**示例**:
```python
{
   "title": "SearchQuery",
   "description": "Search query parameters.\n\nThis simplified interface only requires a search query string.\nAll other parameters (top, filters, vector fields, etc.) are specified during tool creation\nrather than at query time, making it easier for language models to generate structured output.\n\nArgs:\n    query (str): The search query text.",
   "type": "object",
   "properties": {
      "query": {
         "description": "Search query text",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}

```

```python
field query: str [Required]#
```

【中文翻译】Search query text

```python
pydantic model SearchResult[source]#
```

【中文翻译】Bases: BaseModel
Search result.

Parameters:

score (float) – The search score.
content (Dict[str, Any]) – The document content.
metadata (Dict[str, Any]) – Additional metadata about the document.




Show JSON schema{
   "title": "SearchResult",
   "description": "Search result.\n\nArgs:\n    score (float): The search score.\n    content (Dict[str, Any]): The document content.\n    metadata (Dict[str, Any]): Additional metadata about the document.",
   "type": "object",
   "properties": {
      "score": {
         "description": "The search score",
         "title": "Score",
         "type": "number"
      },
      "content": {
         "description": "The document content",
         "title": "Content",
         "type": "object"
      },
      "metadata": {
         "description": "Additional metadata about the document",
         "title": "Metadata",
         "type": "object"
      }
   },
   "required": [
      "score",
      "content",
      "metadata"
   ]
}



Fields:

content (Dict[str, Any])
metadata (Dict[str, Any])
score (float)





field content: Dict[str, Any] [Required]#
The document content



field metadata: Dict[str, Any] [Required]#
Additional metadata about the document



field score: float [Required]#
The search score

**示例**:
```python
{
   "title": "SearchResult",
   "description": "Search result.\n\nArgs:\n    score (float): The search score.\n    content (Dict[str, Any]): The document content.\n    metadata (Dict[str, Any]): Additional metadata about the document.",
   "type": "object",
   "properties": {
      "score": {
         "description": "The search score",
         "title": "Score",
         "type": "number"
      },
      "content": {
         "description": "The document content",
         "title": "Content",
         "type": "object"
      },
      "metadata": {
         "description": "Additional metadata about the document",
         "title": "Metadata",
         "type": "object"
      }
   },
   "required": [
      "score",
      "content",
      "metadata"
   ]
}

```

```python
field content: Dict[str, Any] [Required]#
```

【中文翻译】The document content

```python
field metadata: Dict[str, Any] [Required]#
```

【中文翻译】Additional metadata about the document

```python
field score: float [Required]#
```

【中文翻译】The search score

```python
pydantic model SearchResults[source]#
```

【中文翻译】Bases: BaseModel
Container for search results.

Parameters:
results (List[SearchResult]) – List of search results.



Show JSON schema{
   "title": "SearchResults",
   "description": "Container for search results.\n\nArgs:\n    results (List[SearchResult]): List of search results.",
   "type": "object",
   "properties": {
      "results": {
         "description": "List of search results",
         "items": {
            "$ref": "#/$defs/SearchResult"
         },
         "title": "Results",
         "type": "array"
      }
   },
   "$defs": {
      "SearchResult": {
         "description": "Search result.\n\nArgs:\n    score (float): The search score.\n    content (Dict[str, Any]): The document content.\n    metadata (Dict[str, Any]): Additional metadata about the document.",
         "properties": {
            "score": {
               "description": "The search score",
               "title": "Score",
               "type": "number"
            },
            "content": {
               "description": "The document content",
               "title": "Content",
               "type": "object"
            },
            "metadata": {
               "description": "Additional metadata about the document",
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "score",
            "content",
            "metadata"
         ],
         "title": "SearchResult",
         "type": "object"
      }
   },
   "required": [
      "results"
   ]
}



Fields:

results (List[autogen_ext.tools.azure._ai_search.SearchResult])





field results: List[SearchResult] [Required]#
List of search results

**示例**:
```python
{
   "title": "SearchResults",
   "description": "Container for search results.\n\nArgs:\n    results (List[SearchResult]): List of search results.",
   "type": "object",
   "properties": {
      "results": {
         "description": "List of search results",
         "items": {
            "$ref": "#/$defs/SearchResult"
         },
         "title": "Results",
         "type": "array"
      }
   },
   "$defs": {
      "SearchResult": {
         "description": "Search result.\n\nArgs:\n    score (float): The search score.\n    content (Dict[str, Any]): The document content.\n    metadata (Dict[str, Any]): Additional metadata about the document.",
         "properties": {
            "score": {
               "description": "The search score",
               "title": "Score",
               "type": "number"
            },
            "content": {
               "description": "The document content",
               "title": "Content",
               "type": "object"
            },
            "metadata": {
               "description": "Additional metadata about the document",
               "title": "Metadata",
               "type": "object"
            }
         },
         "required": [
            "score",
            "content",
            "metadata"
         ],
         "title": "SearchResult",
         "type": "object"
      }
   },
   "required": [
      "results"
   ]
}

```

```python
field results: List[SearchResult] [Required]#
```

【中文翻译】List of search results

【中文翻译】previous

【中文翻译】autogen_ext.models.llama_cpp

【中文翻译】next

【中文翻译】autogen_ext.tools.code_execution

### autogen_ext.tools.code_execution {autogen_exttoolscode_execution}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html)

```python
pydantic model CodeExecutionInput[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CodeExecutionInput",
   "type": "object",
   "properties": {
      "code": {
         "description": "The contents of the Python code block that should be executed",
         "title": "Code",
         "type": "string"
      }
   },
   "required": [
      "code"
   ]
}



Fields:

code (str)





field code: str [Required]#
The contents of the Python code block that should be executed

**示例**:
```python
{
   "title": "CodeExecutionInput",
   "type": "object",
   "properties": {
      "code": {
         "description": "The contents of the Python code block that should be executed",
         "title": "Code",
         "type": "string"
      }
   },
   "required": [
      "code"
   ]
}

```

```python
field code: str [Required]#
```

【中文翻译】The contents of the Python code block that should be executed

```python
pydantic model CodeExecutionResult[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CodeExecutionResult",
   "type": "object",
   "properties": {
      "success": {
         "title": "Success",
         "type": "boolean"
      },
      "output": {
         "title": "Output",
         "type": "string"
      }
   },
   "required": [
      "success",
      "output"
   ]
}



Fields:

output (str)
success (bool)





field output: str [Required]#



field success: bool [Required]#



ser_model() → str[source]#

**示例**:
```python
{
   "title": "CodeExecutionResult",
   "type": "object",
   "properties": {
      "success": {
         "title": "Success",
         "type": "boolean"
      },
      "output": {
         "title": "Output",
         "type": "string"
      }
   },
   "required": [
      "success",
      "output"
   ]
}

```

```python
field output: str [Required]#
```

```python
field success: bool [Required]#
```

```python
ser_model() → str[source]#
```

```python
class PythonCodeExecutionTool(executor: CodeExecutor)[source]#
```

【中文翻译】Bases: BaseTool[CodeExecutionInput, CodeExecutionResult], Component[PythonCodeExecutionToolConfig]
A tool that executes Python code in a code executor and returns output.
Example executors:

autogen_ext.code_executors.local.LocalCommandLineCodeExecutor
autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor
autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor

Example usage:
pip install -U "autogen-agentchat" "autogen-ext[openai]" "yfinance" "matplotlib"


import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    tool = PythonCodeExecutionTool(LocalCommandLineCodeExecutor(work_dir="coding"))
    agent = AssistantAgent(
        "assistant", OpenAIChatCompletionClient(model="gpt-4o"), tools=[tool], reflect_on_tool_use=True
    )
    await Console(
        agent.run_stream(
            task="Create a plot of MSFT stock prices in 2024 and save it to a file. Use yfinance and matplotlib."
        )
    )


asyncio.run(main())



Parameters:
executor (CodeExecutor) – The code executor that will be used to execute the code blocks.




component_config_schema#
alias of PythonCodeExecutionToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.code_execution.PythonCodeExecutionTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async run(args: CodeExecutionInput, cancellation_token: CancellationToken) → CodeExecutionResult[source]#

**示例**:
```python
pip install -U "autogen-agentchat" "autogen-ext[openai]" "yfinance" "matplotlib"

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    tool = PythonCodeExecutionTool(LocalCommandLineCodeExecutor(work_dir="coding"))
    agent = AssistantAgent(
        "assistant", OpenAIChatCompletionClient(model="gpt-4o"), tools=[tool], reflect_on_tool_use=True
    )
    await Console(
        agent.run_stream(
            task="Create a plot of MSFT stock prices in 2024 and save it to a file. Use yfinance and matplotlib."
        )
    )


asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of PythonCodeExecutionToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.code_execution.PythonCodeExecutionTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async run(args: CodeExecutionInput, cancellation_token: CancellationToken) → CodeExecutionResult[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.tools.azure

【中文翻译】next

【中文翻译】autogen_ext.tools.graphrag

### autogen_ext.tools.code_execution {autogen_exttoolscode_execution}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html)

```python
pydantic model CodeExecutionInput[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CodeExecutionInput",
   "type": "object",
   "properties": {
      "code": {
         "description": "The contents of the Python code block that should be executed",
         "title": "Code",
         "type": "string"
      }
   },
   "required": [
      "code"
   ]
}



Fields:

code (str)





field code: str [Required]#
The contents of the Python code block that should be executed

**示例**:
```python
{
   "title": "CodeExecutionInput",
   "type": "object",
   "properties": {
      "code": {
         "description": "The contents of the Python code block that should be executed",
         "title": "Code",
         "type": "string"
      }
   },
   "required": [
      "code"
   ]
}

```

```python
field code: str [Required]#
```

【中文翻译】The contents of the Python code block that should be executed

```python
pydantic model CodeExecutionResult[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "CodeExecutionResult",
   "type": "object",
   "properties": {
      "success": {
         "title": "Success",
         "type": "boolean"
      },
      "output": {
         "title": "Output",
         "type": "string"
      }
   },
   "required": [
      "success",
      "output"
   ]
}



Fields:

output (str)
success (bool)





field output: str [Required]#



field success: bool [Required]#



ser_model() → str[source]#

**示例**:
```python
{
   "title": "CodeExecutionResult",
   "type": "object",
   "properties": {
      "success": {
         "title": "Success",
         "type": "boolean"
      },
      "output": {
         "title": "Output",
         "type": "string"
      }
   },
   "required": [
      "success",
      "output"
   ]
}

```

```python
field output: str [Required]#
```

```python
field success: bool [Required]#
```

```python
ser_model() → str[source]#
```

```python
class PythonCodeExecutionTool(executor: CodeExecutor)[source]#
```

【中文翻译】Bases: BaseTool[CodeExecutionInput, CodeExecutionResult], Component[PythonCodeExecutionToolConfig]
A tool that executes Python code in a code executor and returns output.
Example executors:

autogen_ext.code_executors.local.LocalCommandLineCodeExecutor
autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor
autogen_ext.code_executors.azure.ACADynamicSessionsCodeExecutor

Example usage:
pip install -U "autogen-agentchat" "autogen-ext[openai]" "yfinance" "matplotlib"


import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    tool = PythonCodeExecutionTool(LocalCommandLineCodeExecutor(work_dir="coding"))
    agent = AssistantAgent(
        "assistant", OpenAIChatCompletionClient(model="gpt-4o"), tools=[tool], reflect_on_tool_use=True
    )
    await Console(
        agent.run_stream(
            task="Create a plot of MSFT stock prices in 2024 and save it to a file. Use yfinance and matplotlib."
        )
    )


asyncio.run(main())



Parameters:
executor (CodeExecutor) – The code executor that will be used to execute the code blocks.




component_config_schema#
alias of PythonCodeExecutionToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.code_execution.PythonCodeExecutionTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async run(args: CodeExecutionInput, cancellation_token: CancellationToken) → CodeExecutionResult[source]#

**示例**:
```python
pip install -U "autogen-agentchat" "autogen-ext[openai]" "yfinance" "matplotlib"

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.tools.code_execution import PythonCodeExecutionTool


async def main() -> None:
    tool = PythonCodeExecutionTool(LocalCommandLineCodeExecutor(work_dir="coding"))
    agent = AssistantAgent(
        "assistant", OpenAIChatCompletionClient(model="gpt-4o"), tools=[tool], reflect_on_tool_use=True
    )
    await Console(
        agent.run_stream(
            task="Create a plot of MSFT stock prices in 2024 and save it to a file. Use yfinance and matplotlib."
        )
    )


asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of PythonCodeExecutionToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.code_execution.PythonCodeExecutionTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async run(args: CodeExecutionInput, cancellation_token: CancellationToken) → CodeExecutionResult[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.tools.azure

【中文翻译】next

【中文翻译】autogen_ext.tools.graphrag

### autogen_ext.tools.graphrag {autogen_exttoolsgraphrag}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html)

```python
pydantic model GlobalContextConfig[source]#
```

【中文翻译】Bases: ContextConfig

Show JSON schema{
   "title": "GlobalContextConfig",
   "type": "object",
   "properties": {
      "max_data_tokens": {
         "default": 12000,
         "title": "Max Data Tokens",
         "type": "integer"
      },
      "use_community_summary": {
         "default": false,
         "title": "Use Community Summary",
         "type": "boolean"
      },
      "shuffle_data": {
         "default": true,
         "title": "Shuffle Data",
         "type": "boolean"
      },
      "include_community_rank": {
         "default": true,
         "title": "Include Community Rank",
         "type": "boolean"
      },
      "min_community_rank": {
         "default": 0,
         "title": "Min Community Rank",
         "type": "integer"
      },
      "community_rank_name": {
         "default": "rank",
         "title": "Community Rank Name",
         "type": "string"
      },
      "include_community_weight": {
         "default": true,
         "title": "Include Community Weight",
         "type": "boolean"
      },
      "community_weight_name": {
         "default": "occurrence weight",
         "title": "Community Weight Name",
         "type": "string"
      },
      "normalize_community_weight": {
         "default": true,
         "title": "Normalize Community Weight",
         "type": "boolean"
      }
   }
}



Fields:

community_rank_name (str)
community_weight_name (str)
include_community_rank (bool)
include_community_weight (bool)
max_data_tokens (int)
min_community_rank (int)
normalize_community_weight (bool)
shuffle_data (bool)
use_community_summary (bool)





field community_rank_name: str = 'rank'#



field community_weight_name: str = 'occurrence weight'#



field include_community_rank: bool = True#



field include_community_weight: bool = True#



field max_data_tokens: int = 12000#



field min_community_rank: int = 0#



field normalize_community_weight: bool = True#



field shuffle_data: bool = True#



field use_community_summary: bool = False#

**示例**:
```python
{
   "title": "GlobalContextConfig",
   "type": "object",
   "properties": {
      "max_data_tokens": {
         "default": 12000,
         "title": "Max Data Tokens",
         "type": "integer"
      },
      "use_community_summary": {
         "default": false,
         "title": "Use Community Summary",
         "type": "boolean"
      },
      "shuffle_data": {
         "default": true,
         "title": "Shuffle Data",
         "type": "boolean"
      },
      "include_community_rank": {
         "default": true,
         "title": "Include Community Rank",
         "type": "boolean"
      },
      "min_community_rank": {
         "default": 0,
         "title": "Min Community Rank",
         "type": "integer"
      },
      "community_rank_name": {
         "default": "rank",
         "title": "Community Rank Name",
         "type": "string"
      },
      "include_community_weight": {
         "default": true,
         "title": "Include Community Weight",
         "type": "boolean"
      },
      "community_weight_name": {
         "default": "occurrence weight",
         "title": "Community Weight Name",
         "type": "string"
      },
      "normalize_community_weight": {
         "default": true,
         "title": "Normalize Community Weight",
         "type": "boolean"
      }
   }
}

```

```python
field community_rank_name: str = 'rank'#
```

```python
field community_weight_name: str = 'occurrence weight'#
```

```python
field include_community_rank: bool = True#
```

```python
field include_community_weight: bool = True#
```

```python
field max_data_tokens: int = 12000#
```

```python
field min_community_rank: int = 0#
```

```python
field normalize_community_weight: bool = True#
```

```python
field shuffle_data: bool = True#
```

```python
field use_community_summary: bool = False#
```

```python
pydantic model GlobalDataConfig[source]#
```

【中文翻译】Bases: DataConfig

Show JSON schema{
   "title": "GlobalDataConfig",
   "type": "object",
   "properties": {
      "input_dir": {
         "title": "Input Dir",
         "type": "string"
      },
      "entity_table": {
         "default": "create_final_nodes",
         "title": "Entity Table",
         "type": "string"
      },
      "entity_embedding_table": {
         "default": "create_final_entities",
         "title": "Entity Embedding Table",
         "type": "string"
      },
      "community_level": {
         "default": 2,
         "title": "Community Level",
         "type": "integer"
      },
      "community_table": {
         "default": "create_final_communities",
         "title": "Community Table",
         "type": "string"
      },
      "community_report_table": {
         "default": "create_final_community_reports",
         "title": "Community Report Table",
         "type": "string"
      }
   },
   "required": [
      "input_dir"
   ]
}



Fields:

community_report_table (str)
community_table (str)





field community_report_table: str = 'create_final_community_reports'#



field community_table: str = 'create_final_communities'#

**示例**:
```python
{
   "title": "GlobalDataConfig",
   "type": "object",
   "properties": {
      "input_dir": {
         "title": "Input Dir",
         "type": "string"
      },
      "entity_table": {
         "default": "create_final_nodes",
         "title": "Entity Table",
         "type": "string"
      },
      "entity_embedding_table": {
         "default": "create_final_entities",
         "title": "Entity Embedding Table",
         "type": "string"
      },
      "community_level": {
         "default": 2,
         "title": "Community Level",
         "type": "integer"
      },
      "community_table": {
         "default": "create_final_communities",
         "title": "Community Table",
         "type": "string"
      },
      "community_report_table": {
         "default": "create_final_community_reports",
         "title": "Community Report Table",
         "type": "string"
      }
   },
   "required": [
      "input_dir"
   ]
}

```

```python
field community_report_table: str = 'create_final_community_reports'#
```

```python
field community_table: str = 'create_final_communities'#
```

```python
class GlobalSearchTool(token_encoder: Encoding, llm: BaseLLM, data_config: GlobalDataConfig, context_config: GlobalContextConfig = _default_context_config, mapreduce_config: MapReduceConfig = _default_mapreduce_config)[source]#
```

【中文翻译】Bases: BaseTool[GlobalSearchToolArgs, GlobalSearchToolReturn]
Enables running GraphRAG global search queries as an AutoGen tool.
This tool allows you to perform semantic search over a corpus of documents using the GraphRAG framework.
The search combines graph-based document relationships with semantic embeddings to find relevant information.

Note
This tool requires the graphrag extra for the autogen-ext package.
To install:
pip install -U "autogen-agentchat" "autogen-ext[graphrag]"


Before using this tool, you must complete the GraphRAG setup and indexing process:

Follow the GraphRAG documentation to initialize your project and settings
Configure and tune your prompts for the specific use case
Run the indexing process to generate the required data files
Ensure you have the settings.yaml file from the setup process

Please refer to the [GraphRAG documentation](https://microsoft.github.io/graphrag/)
for detailed instructions on completing these prerequisite steps.

Example usage with AssistantAgent:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_ext.tools.graphrag import GlobalSearchTool
from autogen_agentchat.agents import AssistantAgent


async def main():
    # Initialize the OpenAI client
    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="<api-key>",
    )

    # Set up global search tool
    global_tool = GlobalSearchTool.from_settings(settings_path="./settings.yaml")

    # Create assistant agent with the global search tool
    assistant_agent = AssistantAgent(
        name="search_assistant",
        tools=[global_tool],
        model_client=openai_client,
        system_message=(
            "You are a tool selector AI assistant using the GraphRAG framework. "
            "Your primary task is to determine the appropriate search tool to call based on the user's query. "
            "For broader, abstract questions requiring a comprehensive understanding of the dataset, call the 'global_search' function."
        ),
    )

    # Run a sample query
    query = "What is the overall sentiment of the community reports?"
    await Console(assistant_agent.run_stream(task=query))


if __name__ == "__main__":
    asyncio.run(main())




classmethod from_settings(settings_path: str | Path) → GlobalSearchTool[source]#
Create a GlobalSearchTool instance from GraphRAG settings file.

Parameters:
settings_path – Path to the GraphRAG settings.yaml file

Returns:
An initialized GlobalSearchTool instance





async run(args: GlobalSearchToolArgs, cancellation_token: CancellationToken) → GlobalSearchToolReturn[source]#

**示例**:
```python
pip install -U "autogen-agentchat" "autogen-ext[graphrag]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_ext.tools.graphrag import GlobalSearchTool
from autogen_agentchat.agents import AssistantAgent


async def main():
    # Initialize the OpenAI client
    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="<api-key>",
    )

    # Set up global search tool
    global_tool = GlobalSearchTool.from_settings(settings_path="./settings.yaml")

    # Create assistant agent with the global search tool
    assistant_agent = AssistantAgent(
        name="search_assistant",
        tools=[global_tool],
        model_client=openai_client,
        system_message=(
            "You are a tool selector AI assistant using the GraphRAG framework. "
            "Your primary task is to determine the appropriate search tool to call based on the user's query. "
            "For broader, abstract questions requiring a comprehensive understanding of the dataset, call the 'global_search' function."
        ),
    )

    # Run a sample query
    query = "What is the overall sentiment of the community reports?"
    await Console(assistant_agent.run_stream(task=query))


if __name__ == "__main__":
    asyncio.run(main())

```

```python
classmethod from_settings(settings_path: str | Path) → GlobalSearchTool[source]#
```

【中文翻译】Create a GlobalSearchTool instance from GraphRAG settings file.

Parameters:
settings_path – Path to the GraphRAG settings.yaml file

Returns:
An initialized GlobalSearchTool instance

```python
async run(args: GlobalSearchToolArgs, cancellation_token: CancellationToken) → GlobalSearchToolReturn[source]#
```

```python
pydantic model GlobalSearchToolArgs[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "GlobalSearchToolArgs",
   "type": "object",
   "properties": {
      "query": {
         "description": "The user query to perform global search on.",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}



Fields:

query (str)





field query: str [Required]#
The user query to perform global search on.

**示例**:
```python
{
   "title": "GlobalSearchToolArgs",
   "type": "object",
   "properties": {
      "query": {
         "description": "The user query to perform global search on.",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}

```

```python
field query: str [Required]#
```

【中文翻译】The user query to perform global search on.

```python
pydantic model GlobalSearchToolReturn[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "GlobalSearchToolReturn",
   "type": "object",
   "properties": {
      "answer": {
         "title": "Answer",
         "type": "string"
      }
   },
   "required": [
      "answer"
   ]
}



Fields:

answer (str)





field answer: str [Required]#

**示例**:
```python
{
   "title": "GlobalSearchToolReturn",
   "type": "object",
   "properties": {
      "answer": {
         "title": "Answer",
         "type": "string"
      }
   },
   "required": [
      "answer"
   ]
}

```

```python
field answer: str [Required]#
```

```python
pydantic model LocalContextConfig[source]#
```

【中文翻译】Bases: ContextConfig

Show JSON schema{
   "title": "LocalContextConfig",
   "type": "object",
   "properties": {
      "max_data_tokens": {
         "default": 8000,
         "title": "Max Data Tokens",
         "type": "integer"
      },
      "text_unit_prop": {
         "default": 0.5,
         "title": "Text Unit Prop",
         "type": "number"
      },
      "community_prop": {
         "default": 0.25,
         "title": "Community Prop",
         "type": "number"
      },
      "include_entity_rank": {
         "default": true,
         "title": "Include Entity Rank",
         "type": "boolean"
      },
      "rank_description": {
         "default": "number of relationships",
         "title": "Rank Description",
         "type": "string"
      },
      "include_relationship_weight": {
         "default": true,
         "title": "Include Relationship Weight",
         "type": "boolean"
      },
      "relationship_ranking_attribute": {
         "default": "rank",
         "title": "Relationship Ranking Attribute",
         "type": "string"
      }
   }
}



Fields:

community_prop (float)
include_entity_rank (bool)
include_relationship_weight (bool)
rank_description (str)
relationship_ranking_attribute (str)
text_unit_prop (float)





field community_prop: float = 0.25#



field include_entity_rank: bool = True#



field include_relationship_weight: bool = True#



field rank_description: str = 'number of relationships'#



field relationship_ranking_attribute: str = 'rank'#



field text_unit_prop: float = 0.5#

**示例**:
```python
{
   "title": "LocalContextConfig",
   "type": "object",
   "properties": {
      "max_data_tokens": {
         "default": 8000,
         "title": "Max Data Tokens",
         "type": "integer"
      },
      "text_unit_prop": {
         "default": 0.5,
         "title": "Text Unit Prop",
         "type": "number"
      },
      "community_prop": {
         "default": 0.25,
         "title": "Community Prop",
         "type": "number"
      },
      "include_entity_rank": {
         "default": true,
         "title": "Include Entity Rank",
         "type": "boolean"
      },
      "rank_description": {
         "default": "number of relationships",
         "title": "Rank Description",
         "type": "string"
      },
      "include_relationship_weight": {
         "default": true,
         "title": "Include Relationship Weight",
         "type": "boolean"
      },
      "relationship_ranking_attribute": {
         "default": "rank",
         "title": "Relationship Ranking Attribute",
         "type": "string"
      }
   }
}

```

```python
field community_prop: float = 0.25#
```

```python
field include_entity_rank: bool = True#
```

```python
field include_relationship_weight: bool = True#
```

```python
field rank_description: str = 'number of relationships'#
```

```python
field relationship_ranking_attribute: str = 'rank'#
```

```python
field text_unit_prop: float = 0.5#
```

```python
pydantic model LocalDataConfig[source]#
```

【中文翻译】Bases: DataConfig

Show JSON schema{
   "title": "LocalDataConfig",
   "type": "object",
   "properties": {
      "input_dir": {
         "title": "Input Dir",
         "type": "string"
      },
      "entity_table": {
         "default": "create_final_nodes",
         "title": "Entity Table",
         "type": "string"
      },
      "entity_embedding_table": {
         "default": "create_final_entities",
         "title": "Entity Embedding Table",
         "type": "string"
      },
      "community_level": {
         "default": 2,
         "title": "Community Level",
         "type": "integer"
      },
      "relationship_table": {
         "default": "create_final_relationships",
         "title": "Relationship Table",
         "type": "string"
      },
      "text_unit_table": {
         "default": "create_final_text_units",
         "title": "Text Unit Table",
         "type": "string"
      }
   },
   "required": [
      "input_dir"
   ]
}



Fields:

relationship_table (str)
text_unit_table (str)





field relationship_table: str = 'create_final_relationships'#



field text_unit_table: str = 'create_final_text_units'#

**示例**:
```python
{
   "title": "LocalDataConfig",
   "type": "object",
   "properties": {
      "input_dir": {
         "title": "Input Dir",
         "type": "string"
      },
      "entity_table": {
         "default": "create_final_nodes",
         "title": "Entity Table",
         "type": "string"
      },
      "entity_embedding_table": {
         "default": "create_final_entities",
         "title": "Entity Embedding Table",
         "type": "string"
      },
      "community_level": {
         "default": 2,
         "title": "Community Level",
         "type": "integer"
      },
      "relationship_table": {
         "default": "create_final_relationships",
         "title": "Relationship Table",
         "type": "string"
      },
      "text_unit_table": {
         "default": "create_final_text_units",
         "title": "Text Unit Table",
         "type": "string"
      }
   },
   "required": [
      "input_dir"
   ]
}

```

```python
field relationship_table: str = 'create_final_relationships'#
```

```python
field text_unit_table: str = 'create_final_text_units'#
```

```python
class LocalSearchTool(token_encoder: Encoding, llm: BaseLLM, embedder: BaseTextEmbedding, data_config: LocalDataConfig, context_config: LocalContextConfig = _default_context_config, search_config: SearchConfig = _default_search_config)[source]#
```

【中文翻译】Bases: BaseTool[LocalSearchToolArgs, LocalSearchToolReturn]
Enables running GraphRAG local search queries as an AutoGen tool.
This tool allows you to perform semantic search over a corpus of documents using the GraphRAG framework.
The search combines local document context with semantic embeddings to find relevant information.

Note
This tool requires the graphrag extra for the autogen-ext package.
To install:
pip install -U "autogen-agentchat" "autogen-ext[graphrag]"


Before using this tool, you must complete the GraphRAG setup and indexing process:

Follow the GraphRAG documentation to initialize your project and settings
Configure and tune your prompts for the specific use case
Run the indexing process to generate the required data files
Ensure you have the settings.yaml file from the setup process

Please refer to the [GraphRAG documentation](https://microsoft.github.io/graphrag/)
for detailed instructions on completing these prerequisite steps.

Example usage with AssistantAgent:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_ext.tools.graphrag import LocalSearchTool
from autogen_agentchat.agents import AssistantAgent


async def main():
    # Initialize the OpenAI client
    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="<api-key>",
    )

    # Set up local search tool
    local_tool = LocalSearchTool.from_settings(settings_path="./settings.yaml")

    # Create assistant agent with the local search tool
    assistant_agent = AssistantAgent(
        name="search_assistant",
        tools=[local_tool],
        model_client=openai_client,
        system_message=(
            "You are a tool selector AI assistant using the GraphRAG framework. "
            "Your primary task is to determine the appropriate search tool to call based on the user's query. "
            "For specific, detailed information about particular entities or relationships, call the 'local_search' function."
        ),
    )

    # Run a sample query
    query = "What does the station-master say about Dr. Becher?"
    await Console(assistant_agent.run_stream(task=query))


if __name__ == "__main__":
    asyncio.run(main())



Parameters:

token_encoder (tiktoken.Encoding) – The tokenizer used for text encoding
llm (BaseLLM) – The language model to use for search
embedder (BaseTextEmbedding) – The text embedding model to use
data_config (DataConfig) – Configuration for data source locations and settings
context_config (LocalContextConfig, optional) – Configuration for context building. Defaults to default config.
search_config (SearchConfig, optional) – Configuration for search operations. Defaults to default config.





classmethod from_settings(settings_path: str | Path) → LocalSearchTool[source]#
Create a LocalSearchTool instance from GraphRAG settings file.

Parameters:
settings_path – Path to the GraphRAG settings.yaml file

Returns:
An initialized LocalSearchTool instance





async run(args: LocalSearchToolArgs, cancellation_token: CancellationToken) → LocalSearchToolReturn[source]#

**示例**:
```python
pip install -U "autogen-agentchat" "autogen-ext[graphrag]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_ext.tools.graphrag import LocalSearchTool
from autogen_agentchat.agents import AssistantAgent


async def main():
    # Initialize the OpenAI client
    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="<api-key>",
    )

    # Set up local search tool
    local_tool = LocalSearchTool.from_settings(settings_path="./settings.yaml")

    # Create assistant agent with the local search tool
    assistant_agent = AssistantAgent(
        name="search_assistant",
        tools=[local_tool],
        model_client=openai_client,
        system_message=(
            "You are a tool selector AI assistant using the GraphRAG framework. "
            "Your primary task is to determine the appropriate search tool to call based on the user's query. "
            "For specific, detailed information about particular entities or relationships, call the 'local_search' function."
        ),
    )

    # Run a sample query
    query = "What does the station-master say about Dr. Becher?"
    await Console(assistant_agent.run_stream(task=query))


if __name__ == "__main__":
    asyncio.run(main())

```

```python
classmethod from_settings(settings_path: str | Path) → LocalSearchTool[source]#
```

【中文翻译】Create a LocalSearchTool instance from GraphRAG settings file.

Parameters:
settings_path – Path to the GraphRAG settings.yaml file

Returns:
An initialized LocalSearchTool instance

```python
async run(args: LocalSearchToolArgs, cancellation_token: CancellationToken) → LocalSearchToolReturn[source]#
```

```python
pydantic model LocalSearchToolArgs[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "LocalSearchToolArgs",
   "type": "object",
   "properties": {
      "query": {
         "description": "The user query to perform local search on.",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}



Fields:

query (str)





field query: str [Required]#
The user query to perform local search on.

**示例**:
```python
{
   "title": "LocalSearchToolArgs",
   "type": "object",
   "properties": {
      "query": {
         "description": "The user query to perform local search on.",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}

```

```python
field query: str [Required]#
```

【中文翻译】The user query to perform local search on.

```python
pydantic model LocalSearchToolReturn[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "LocalSearchToolReturn",
   "type": "object",
   "properties": {
      "answer": {
         "description": "The answer to the user query.",
         "title": "Answer",
         "type": "string"
      }
   },
   "required": [
      "answer"
   ]
}



Fields:

answer (str)





field answer: str [Required]#
The answer to the user query.

**示例**:
```python
{
   "title": "LocalSearchToolReturn",
   "type": "object",
   "properties": {
      "answer": {
         "description": "The answer to the user query.",
         "title": "Answer",
         "type": "string"
      }
   },
   "required": [
      "answer"
   ]
}

```

```python
field answer: str [Required]#
```

【中文翻译】The answer to the user query.

```python
pydantic model MapReduceConfig[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "MapReduceConfig",
   "type": "object",
   "properties": {
      "map_max_tokens": {
         "default": 1000,
         "title": "Map Max Tokens",
         "type": "integer"
      },
      "map_temperature": {
         "default": 0.0,
         "title": "Map Temperature",
         "type": "number"
      },
      "reduce_max_tokens": {
         "default": 2000,
         "title": "Reduce Max Tokens",
         "type": "integer"
      },
      "reduce_temperature": {
         "default": 0.0,
         "title": "Reduce Temperature",
         "type": "number"
      },
      "allow_general_knowledge": {
         "default": false,
         "title": "Allow General Knowledge",
         "type": "boolean"
      },
      "json_mode": {
         "default": false,
         "title": "Json Mode",
         "type": "boolean"
      },
      "response_type": {
         "default": "multiple paragraphs",
         "title": "Response Type",
         "type": "string"
      }
   }
}



Fields:

allow_general_knowledge (bool)
json_mode (bool)
map_max_tokens (int)
map_temperature (float)
reduce_max_tokens (int)
reduce_temperature (float)
response_type (str)





field allow_general_knowledge: bool = False#



field json_mode: bool = False#



field map_max_tokens: int = 1000#



field map_temperature: float = 0.0#



field reduce_max_tokens: int = 2000#



field reduce_temperature: float = 0.0#



field response_type: str = 'multiple paragraphs'#

**示例**:
```python
{
   "title": "MapReduceConfig",
   "type": "object",
   "properties": {
      "map_max_tokens": {
         "default": 1000,
         "title": "Map Max Tokens",
         "type": "integer"
      },
      "map_temperature": {
         "default": 0.0,
         "title": "Map Temperature",
         "type": "number"
      },
      "reduce_max_tokens": {
         "default": 2000,
         "title": "Reduce Max Tokens",
         "type": "integer"
      },
      "reduce_temperature": {
         "default": 0.0,
         "title": "Reduce Temperature",
         "type": "number"
      },
      "allow_general_knowledge": {
         "default": false,
         "title": "Allow General Knowledge",
         "type": "boolean"
      },
      "json_mode": {
         "default": false,
         "title": "Json Mode",
         "type": "boolean"
      },
      "response_type": {
         "default": "multiple paragraphs",
         "title": "Response Type",
         "type": "string"
      }
   }
}

```

```python
field allow_general_knowledge: bool = False#
```

```python
field json_mode: bool = False#
```

```python
field map_max_tokens: int = 1000#
```

```python
field map_temperature: float = 0.0#
```

```python
field reduce_max_tokens: int = 2000#
```

```python
field reduce_temperature: float = 0.0#
```

```python
field response_type: str = 'multiple paragraphs'#
```

```python
pydantic model SearchConfig[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "SearchConfig",
   "type": "object",
   "properties": {
      "max_tokens": {
         "default": 1500,
         "title": "Max Tokens",
         "type": "integer"
      },
      "temperature": {
         "default": 0.0,
         "title": "Temperature",
         "type": "number"
      },
      "response_type": {
         "default": "multiple paragraphs",
         "title": "Response Type",
         "type": "string"
      }
   }
}



Fields:

max_tokens (int)
response_type (str)
temperature (float)





field max_tokens: int = 1500#



field response_type: str = 'multiple paragraphs'#



field temperature: float = 0.0#

**示例**:
```python
{
   "title": "SearchConfig",
   "type": "object",
   "properties": {
      "max_tokens": {
         "default": 1500,
         "title": "Max Tokens",
         "type": "integer"
      },
      "temperature": {
         "default": 0.0,
         "title": "Temperature",
         "type": "number"
      },
      "response_type": {
         "default": "multiple paragraphs",
         "title": "Response Type",
         "type": "string"
      }
   }
}

```

```python
field max_tokens: int = 1500#
```

```python
field response_type: str = 'multiple paragraphs'#
```

```python
field temperature: float = 0.0#
```

【中文翻译】previous

【中文翻译】autogen_ext.tools.code_execution

【中文翻译】next

【中文翻译】autogen_ext.tools.http

### autogen_ext.tools.graphrag {autogen_exttoolsgraphrag}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html)

```python
pydantic model GlobalContextConfig[source]#
```

【中文翻译】Bases: ContextConfig

Show JSON schema{
   "title": "GlobalContextConfig",
   "type": "object",
   "properties": {
      "max_data_tokens": {
         "default": 12000,
         "title": "Max Data Tokens",
         "type": "integer"
      },
      "use_community_summary": {
         "default": false,
         "title": "Use Community Summary",
         "type": "boolean"
      },
      "shuffle_data": {
         "default": true,
         "title": "Shuffle Data",
         "type": "boolean"
      },
      "include_community_rank": {
         "default": true,
         "title": "Include Community Rank",
         "type": "boolean"
      },
      "min_community_rank": {
         "default": 0,
         "title": "Min Community Rank",
         "type": "integer"
      },
      "community_rank_name": {
         "default": "rank",
         "title": "Community Rank Name",
         "type": "string"
      },
      "include_community_weight": {
         "default": true,
         "title": "Include Community Weight",
         "type": "boolean"
      },
      "community_weight_name": {
         "default": "occurrence weight",
         "title": "Community Weight Name",
         "type": "string"
      },
      "normalize_community_weight": {
         "default": true,
         "title": "Normalize Community Weight",
         "type": "boolean"
      }
   }
}



Fields:

community_rank_name (str)
community_weight_name (str)
include_community_rank (bool)
include_community_weight (bool)
max_data_tokens (int)
min_community_rank (int)
normalize_community_weight (bool)
shuffle_data (bool)
use_community_summary (bool)





field community_rank_name: str = 'rank'#



field community_weight_name: str = 'occurrence weight'#



field include_community_rank: bool = True#



field include_community_weight: bool = True#



field max_data_tokens: int = 12000#



field min_community_rank: int = 0#



field normalize_community_weight: bool = True#



field shuffle_data: bool = True#



field use_community_summary: bool = False#

**示例**:
```python
{
   "title": "GlobalContextConfig",
   "type": "object",
   "properties": {
      "max_data_tokens": {
         "default": 12000,
         "title": "Max Data Tokens",
         "type": "integer"
      },
      "use_community_summary": {
         "default": false,
         "title": "Use Community Summary",
         "type": "boolean"
      },
      "shuffle_data": {
         "default": true,
         "title": "Shuffle Data",
         "type": "boolean"
      },
      "include_community_rank": {
         "default": true,
         "title": "Include Community Rank",
         "type": "boolean"
      },
      "min_community_rank": {
         "default": 0,
         "title": "Min Community Rank",
         "type": "integer"
      },
      "community_rank_name": {
         "default": "rank",
         "title": "Community Rank Name",
         "type": "string"
      },
      "include_community_weight": {
         "default": true,
         "title": "Include Community Weight",
         "type": "boolean"
      },
      "community_weight_name": {
         "default": "occurrence weight",
         "title": "Community Weight Name",
         "type": "string"
      },
      "normalize_community_weight": {
         "default": true,
         "title": "Normalize Community Weight",
         "type": "boolean"
      }
   }
}

```

```python
field community_rank_name: str = 'rank'#
```

```python
field community_weight_name: str = 'occurrence weight'#
```

```python
field include_community_rank: bool = True#
```

```python
field include_community_weight: bool = True#
```

```python
field max_data_tokens: int = 12000#
```

```python
field min_community_rank: int = 0#
```

```python
field normalize_community_weight: bool = True#
```

```python
field shuffle_data: bool = True#
```

```python
field use_community_summary: bool = False#
```

```python
pydantic model GlobalDataConfig[source]#
```

【中文翻译】Bases: DataConfig

Show JSON schema{
   "title": "GlobalDataConfig",
   "type": "object",
   "properties": {
      "input_dir": {
         "title": "Input Dir",
         "type": "string"
      },
      "entity_table": {
         "default": "create_final_nodes",
         "title": "Entity Table",
         "type": "string"
      },
      "entity_embedding_table": {
         "default": "create_final_entities",
         "title": "Entity Embedding Table",
         "type": "string"
      },
      "community_level": {
         "default": 2,
         "title": "Community Level",
         "type": "integer"
      },
      "community_table": {
         "default": "create_final_communities",
         "title": "Community Table",
         "type": "string"
      },
      "community_report_table": {
         "default": "create_final_community_reports",
         "title": "Community Report Table",
         "type": "string"
      }
   },
   "required": [
      "input_dir"
   ]
}



Fields:

community_report_table (str)
community_table (str)





field community_report_table: str = 'create_final_community_reports'#



field community_table: str = 'create_final_communities'#

**示例**:
```python
{
   "title": "GlobalDataConfig",
   "type": "object",
   "properties": {
      "input_dir": {
         "title": "Input Dir",
         "type": "string"
      },
      "entity_table": {
         "default": "create_final_nodes",
         "title": "Entity Table",
         "type": "string"
      },
      "entity_embedding_table": {
         "default": "create_final_entities",
         "title": "Entity Embedding Table",
         "type": "string"
      },
      "community_level": {
         "default": 2,
         "title": "Community Level",
         "type": "integer"
      },
      "community_table": {
         "default": "create_final_communities",
         "title": "Community Table",
         "type": "string"
      },
      "community_report_table": {
         "default": "create_final_community_reports",
         "title": "Community Report Table",
         "type": "string"
      }
   },
   "required": [
      "input_dir"
   ]
}

```

```python
field community_report_table: str = 'create_final_community_reports'#
```

```python
field community_table: str = 'create_final_communities'#
```

```python
class GlobalSearchTool(token_encoder: Encoding, llm: BaseLLM, data_config: GlobalDataConfig, context_config: GlobalContextConfig = _default_context_config, mapreduce_config: MapReduceConfig = _default_mapreduce_config)[source]#
```

【中文翻译】Bases: BaseTool[GlobalSearchToolArgs, GlobalSearchToolReturn]
Enables running GraphRAG global search queries as an AutoGen tool.
This tool allows you to perform semantic search over a corpus of documents using the GraphRAG framework.
The search combines graph-based document relationships with semantic embeddings to find relevant information.

Note
This tool requires the graphrag extra for the autogen-ext package.
To install:
pip install -U "autogen-agentchat" "autogen-ext[graphrag]"


Before using this tool, you must complete the GraphRAG setup and indexing process:

Follow the GraphRAG documentation to initialize your project and settings
Configure and tune your prompts for the specific use case
Run the indexing process to generate the required data files
Ensure you have the settings.yaml file from the setup process

Please refer to the [GraphRAG documentation](https://microsoft.github.io/graphrag/)
for detailed instructions on completing these prerequisite steps.

Example usage with AssistantAgent:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_ext.tools.graphrag import GlobalSearchTool
from autogen_agentchat.agents import AssistantAgent


async def main():
    # Initialize the OpenAI client
    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="<api-key>",
    )

    # Set up global search tool
    global_tool = GlobalSearchTool.from_settings(settings_path="./settings.yaml")

    # Create assistant agent with the global search tool
    assistant_agent = AssistantAgent(
        name="search_assistant",
        tools=[global_tool],
        model_client=openai_client,
        system_message=(
            "You are a tool selector AI assistant using the GraphRAG framework. "
            "Your primary task is to determine the appropriate search tool to call based on the user's query. "
            "For broader, abstract questions requiring a comprehensive understanding of the dataset, call the 'global_search' function."
        ),
    )

    # Run a sample query
    query = "What is the overall sentiment of the community reports?"
    await Console(assistant_agent.run_stream(task=query))


if __name__ == "__main__":
    asyncio.run(main())




classmethod from_settings(settings_path: str | Path) → GlobalSearchTool[source]#
Create a GlobalSearchTool instance from GraphRAG settings file.

Parameters:
settings_path – Path to the GraphRAG settings.yaml file

Returns:
An initialized GlobalSearchTool instance





async run(args: GlobalSearchToolArgs, cancellation_token: CancellationToken) → GlobalSearchToolReturn[source]#

**示例**:
```python
pip install -U "autogen-agentchat" "autogen-ext[graphrag]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_ext.tools.graphrag import GlobalSearchTool
from autogen_agentchat.agents import AssistantAgent


async def main():
    # Initialize the OpenAI client
    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="<api-key>",
    )

    # Set up global search tool
    global_tool = GlobalSearchTool.from_settings(settings_path="./settings.yaml")

    # Create assistant agent with the global search tool
    assistant_agent = AssistantAgent(
        name="search_assistant",
        tools=[global_tool],
        model_client=openai_client,
        system_message=(
            "You are a tool selector AI assistant using the GraphRAG framework. "
            "Your primary task is to determine the appropriate search tool to call based on the user's query. "
            "For broader, abstract questions requiring a comprehensive understanding of the dataset, call the 'global_search' function."
        ),
    )

    # Run a sample query
    query = "What is the overall sentiment of the community reports?"
    await Console(assistant_agent.run_stream(task=query))


if __name__ == "__main__":
    asyncio.run(main())

```

```python
classmethod from_settings(settings_path: str | Path) → GlobalSearchTool[source]#
```

【中文翻译】Create a GlobalSearchTool instance from GraphRAG settings file.

Parameters:
settings_path – Path to the GraphRAG settings.yaml file

Returns:
An initialized GlobalSearchTool instance

```python
async run(args: GlobalSearchToolArgs, cancellation_token: CancellationToken) → GlobalSearchToolReturn[source]#
```

```python
pydantic model GlobalSearchToolArgs[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "GlobalSearchToolArgs",
   "type": "object",
   "properties": {
      "query": {
         "description": "The user query to perform global search on.",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}



Fields:

query (str)





field query: str [Required]#
The user query to perform global search on.

**示例**:
```python
{
   "title": "GlobalSearchToolArgs",
   "type": "object",
   "properties": {
      "query": {
         "description": "The user query to perform global search on.",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}

```

```python
field query: str [Required]#
```

【中文翻译】The user query to perform global search on.

```python
pydantic model GlobalSearchToolReturn[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "GlobalSearchToolReturn",
   "type": "object",
   "properties": {
      "answer": {
         "title": "Answer",
         "type": "string"
      }
   },
   "required": [
      "answer"
   ]
}



Fields:

answer (str)





field answer: str [Required]#

**示例**:
```python
{
   "title": "GlobalSearchToolReturn",
   "type": "object",
   "properties": {
      "answer": {
         "title": "Answer",
         "type": "string"
      }
   },
   "required": [
      "answer"
   ]
}

```

```python
field answer: str [Required]#
```

```python
pydantic model LocalContextConfig[source]#
```

【中文翻译】Bases: ContextConfig

Show JSON schema{
   "title": "LocalContextConfig",
   "type": "object",
   "properties": {
      "max_data_tokens": {
         "default": 8000,
         "title": "Max Data Tokens",
         "type": "integer"
      },
      "text_unit_prop": {
         "default": 0.5,
         "title": "Text Unit Prop",
         "type": "number"
      },
      "community_prop": {
         "default": 0.25,
         "title": "Community Prop",
         "type": "number"
      },
      "include_entity_rank": {
         "default": true,
         "title": "Include Entity Rank",
         "type": "boolean"
      },
      "rank_description": {
         "default": "number of relationships",
         "title": "Rank Description",
         "type": "string"
      },
      "include_relationship_weight": {
         "default": true,
         "title": "Include Relationship Weight",
         "type": "boolean"
      },
      "relationship_ranking_attribute": {
         "default": "rank",
         "title": "Relationship Ranking Attribute",
         "type": "string"
      }
   }
}



Fields:

community_prop (float)
include_entity_rank (bool)
include_relationship_weight (bool)
rank_description (str)
relationship_ranking_attribute (str)
text_unit_prop (float)





field community_prop: float = 0.25#



field include_entity_rank: bool = True#



field include_relationship_weight: bool = True#



field rank_description: str = 'number of relationships'#



field relationship_ranking_attribute: str = 'rank'#



field text_unit_prop: float = 0.5#

**示例**:
```python
{
   "title": "LocalContextConfig",
   "type": "object",
   "properties": {
      "max_data_tokens": {
         "default": 8000,
         "title": "Max Data Tokens",
         "type": "integer"
      },
      "text_unit_prop": {
         "default": 0.5,
         "title": "Text Unit Prop",
         "type": "number"
      },
      "community_prop": {
         "default": 0.25,
         "title": "Community Prop",
         "type": "number"
      },
      "include_entity_rank": {
         "default": true,
         "title": "Include Entity Rank",
         "type": "boolean"
      },
      "rank_description": {
         "default": "number of relationships",
         "title": "Rank Description",
         "type": "string"
      },
      "include_relationship_weight": {
         "default": true,
         "title": "Include Relationship Weight",
         "type": "boolean"
      },
      "relationship_ranking_attribute": {
         "default": "rank",
         "title": "Relationship Ranking Attribute",
         "type": "string"
      }
   }
}

```

```python
field community_prop: float = 0.25#
```

```python
field include_entity_rank: bool = True#
```

```python
field include_relationship_weight: bool = True#
```

```python
field rank_description: str = 'number of relationships'#
```

```python
field relationship_ranking_attribute: str = 'rank'#
```

```python
field text_unit_prop: float = 0.5#
```

```python
pydantic model LocalDataConfig[source]#
```

【中文翻译】Bases: DataConfig

Show JSON schema{
   "title": "LocalDataConfig",
   "type": "object",
   "properties": {
      "input_dir": {
         "title": "Input Dir",
         "type": "string"
      },
      "entity_table": {
         "default": "create_final_nodes",
         "title": "Entity Table",
         "type": "string"
      },
      "entity_embedding_table": {
         "default": "create_final_entities",
         "title": "Entity Embedding Table",
         "type": "string"
      },
      "community_level": {
         "default": 2,
         "title": "Community Level",
         "type": "integer"
      },
      "relationship_table": {
         "default": "create_final_relationships",
         "title": "Relationship Table",
         "type": "string"
      },
      "text_unit_table": {
         "default": "create_final_text_units",
         "title": "Text Unit Table",
         "type": "string"
      }
   },
   "required": [
      "input_dir"
   ]
}



Fields:

relationship_table (str)
text_unit_table (str)





field relationship_table: str = 'create_final_relationships'#



field text_unit_table: str = 'create_final_text_units'#

**示例**:
```python
{
   "title": "LocalDataConfig",
   "type": "object",
   "properties": {
      "input_dir": {
         "title": "Input Dir",
         "type": "string"
      },
      "entity_table": {
         "default": "create_final_nodes",
         "title": "Entity Table",
         "type": "string"
      },
      "entity_embedding_table": {
         "default": "create_final_entities",
         "title": "Entity Embedding Table",
         "type": "string"
      },
      "community_level": {
         "default": 2,
         "title": "Community Level",
         "type": "integer"
      },
      "relationship_table": {
         "default": "create_final_relationships",
         "title": "Relationship Table",
         "type": "string"
      },
      "text_unit_table": {
         "default": "create_final_text_units",
         "title": "Text Unit Table",
         "type": "string"
      }
   },
   "required": [
      "input_dir"
   ]
}

```

```python
field relationship_table: str = 'create_final_relationships'#
```

```python
field text_unit_table: str = 'create_final_text_units'#
```

```python
class LocalSearchTool(token_encoder: Encoding, llm: BaseLLM, embedder: BaseTextEmbedding, data_config: LocalDataConfig, context_config: LocalContextConfig = _default_context_config, search_config: SearchConfig = _default_search_config)[source]#
```

【中文翻译】Bases: BaseTool[LocalSearchToolArgs, LocalSearchToolReturn]
Enables running GraphRAG local search queries as an AutoGen tool.
This tool allows you to perform semantic search over a corpus of documents using the GraphRAG framework.
The search combines local document context with semantic embeddings to find relevant information.

Note
This tool requires the graphrag extra for the autogen-ext package.
To install:
pip install -U "autogen-agentchat" "autogen-ext[graphrag]"


Before using this tool, you must complete the GraphRAG setup and indexing process:

Follow the GraphRAG documentation to initialize your project and settings
Configure and tune your prompts for the specific use case
Run the indexing process to generate the required data files
Ensure you have the settings.yaml file from the setup process

Please refer to the [GraphRAG documentation](https://microsoft.github.io/graphrag/)
for detailed instructions on completing these prerequisite steps.

Example usage with AssistantAgent:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_ext.tools.graphrag import LocalSearchTool
from autogen_agentchat.agents import AssistantAgent


async def main():
    # Initialize the OpenAI client
    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="<api-key>",
    )

    # Set up local search tool
    local_tool = LocalSearchTool.from_settings(settings_path="./settings.yaml")

    # Create assistant agent with the local search tool
    assistant_agent = AssistantAgent(
        name="search_assistant",
        tools=[local_tool],
        model_client=openai_client,
        system_message=(
            "You are a tool selector AI assistant using the GraphRAG framework. "
            "Your primary task is to determine the appropriate search tool to call based on the user's query. "
            "For specific, detailed information about particular entities or relationships, call the 'local_search' function."
        ),
    )

    # Run a sample query
    query = "What does the station-master say about Dr. Becher?"
    await Console(assistant_agent.run_stream(task=query))


if __name__ == "__main__":
    asyncio.run(main())



Parameters:

token_encoder (tiktoken.Encoding) – The tokenizer used for text encoding
llm (BaseLLM) – The language model to use for search
embedder (BaseTextEmbedding) – The text embedding model to use
data_config (DataConfig) – Configuration for data source locations and settings
context_config (LocalContextConfig, optional) – Configuration for context building. Defaults to default config.
search_config (SearchConfig, optional) – Configuration for search operations. Defaults to default config.





classmethod from_settings(settings_path: str | Path) → LocalSearchTool[source]#
Create a LocalSearchTool instance from GraphRAG settings file.

Parameters:
settings_path – Path to the GraphRAG settings.yaml file

Returns:
An initialized LocalSearchTool instance





async run(args: LocalSearchToolArgs, cancellation_token: CancellationToken) → LocalSearchToolReturn[source]#

**示例**:
```python
pip install -U "autogen-agentchat" "autogen-ext[graphrag]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.ui import Console
from autogen_ext.tools.graphrag import LocalSearchTool
from autogen_agentchat.agents import AssistantAgent


async def main():
    # Initialize the OpenAI client
    openai_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini",
        api_key="<api-key>",
    )

    # Set up local search tool
    local_tool = LocalSearchTool.from_settings(settings_path="./settings.yaml")

    # Create assistant agent with the local search tool
    assistant_agent = AssistantAgent(
        name="search_assistant",
        tools=[local_tool],
        model_client=openai_client,
        system_message=(
            "You are a tool selector AI assistant using the GraphRAG framework. "
            "Your primary task is to determine the appropriate search tool to call based on the user's query. "
            "For specific, detailed information about particular entities or relationships, call the 'local_search' function."
        ),
    )

    # Run a sample query
    query = "What does the station-master say about Dr. Becher?"
    await Console(assistant_agent.run_stream(task=query))


if __name__ == "__main__":
    asyncio.run(main())

```

```python
classmethod from_settings(settings_path: str | Path) → LocalSearchTool[source]#
```

【中文翻译】Create a LocalSearchTool instance from GraphRAG settings file.

Parameters:
settings_path – Path to the GraphRAG settings.yaml file

Returns:
An initialized LocalSearchTool instance

```python
async run(args: LocalSearchToolArgs, cancellation_token: CancellationToken) → LocalSearchToolReturn[source]#
```

```python
pydantic model LocalSearchToolArgs[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "LocalSearchToolArgs",
   "type": "object",
   "properties": {
      "query": {
         "description": "The user query to perform local search on.",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}



Fields:

query (str)





field query: str [Required]#
The user query to perform local search on.

**示例**:
```python
{
   "title": "LocalSearchToolArgs",
   "type": "object",
   "properties": {
      "query": {
         "description": "The user query to perform local search on.",
         "title": "Query",
         "type": "string"
      }
   },
   "required": [
      "query"
   ]
}

```

```python
field query: str [Required]#
```

【中文翻译】The user query to perform local search on.

```python
pydantic model LocalSearchToolReturn[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "LocalSearchToolReturn",
   "type": "object",
   "properties": {
      "answer": {
         "description": "The answer to the user query.",
         "title": "Answer",
         "type": "string"
      }
   },
   "required": [
      "answer"
   ]
}



Fields:

answer (str)





field answer: str [Required]#
The answer to the user query.

**示例**:
```python
{
   "title": "LocalSearchToolReturn",
   "type": "object",
   "properties": {
      "answer": {
         "description": "The answer to the user query.",
         "title": "Answer",
         "type": "string"
      }
   },
   "required": [
      "answer"
   ]
}

```

```python
field answer: str [Required]#
```

【中文翻译】The answer to the user query.

```python
pydantic model MapReduceConfig[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "MapReduceConfig",
   "type": "object",
   "properties": {
      "map_max_tokens": {
         "default": 1000,
         "title": "Map Max Tokens",
         "type": "integer"
      },
      "map_temperature": {
         "default": 0.0,
         "title": "Map Temperature",
         "type": "number"
      },
      "reduce_max_tokens": {
         "default": 2000,
         "title": "Reduce Max Tokens",
         "type": "integer"
      },
      "reduce_temperature": {
         "default": 0.0,
         "title": "Reduce Temperature",
         "type": "number"
      },
      "allow_general_knowledge": {
         "default": false,
         "title": "Allow General Knowledge",
         "type": "boolean"
      },
      "json_mode": {
         "default": false,
         "title": "Json Mode",
         "type": "boolean"
      },
      "response_type": {
         "default": "multiple paragraphs",
         "title": "Response Type",
         "type": "string"
      }
   }
}



Fields:

allow_general_knowledge (bool)
json_mode (bool)
map_max_tokens (int)
map_temperature (float)
reduce_max_tokens (int)
reduce_temperature (float)
response_type (str)





field allow_general_knowledge: bool = False#



field json_mode: bool = False#



field map_max_tokens: int = 1000#



field map_temperature: float = 0.0#



field reduce_max_tokens: int = 2000#



field reduce_temperature: float = 0.0#



field response_type: str = 'multiple paragraphs'#

**示例**:
```python
{
   "title": "MapReduceConfig",
   "type": "object",
   "properties": {
      "map_max_tokens": {
         "default": 1000,
         "title": "Map Max Tokens",
         "type": "integer"
      },
      "map_temperature": {
         "default": 0.0,
         "title": "Map Temperature",
         "type": "number"
      },
      "reduce_max_tokens": {
         "default": 2000,
         "title": "Reduce Max Tokens",
         "type": "integer"
      },
      "reduce_temperature": {
         "default": 0.0,
         "title": "Reduce Temperature",
         "type": "number"
      },
      "allow_general_knowledge": {
         "default": false,
         "title": "Allow General Knowledge",
         "type": "boolean"
      },
      "json_mode": {
         "default": false,
         "title": "Json Mode",
         "type": "boolean"
      },
      "response_type": {
         "default": "multiple paragraphs",
         "title": "Response Type",
         "type": "string"
      }
   }
}

```

```python
field allow_general_knowledge: bool = False#
```

```python
field json_mode: bool = False#
```

```python
field map_max_tokens: int = 1000#
```

```python
field map_temperature: float = 0.0#
```

```python
field reduce_max_tokens: int = 2000#
```

```python
field reduce_temperature: float = 0.0#
```

```python
field response_type: str = 'multiple paragraphs'#
```

```python
pydantic model SearchConfig[source]#
```

【中文翻译】Bases: BaseModel

Show JSON schema{
   "title": "SearchConfig",
   "type": "object",
   "properties": {
      "max_tokens": {
         "default": 1500,
         "title": "Max Tokens",
         "type": "integer"
      },
      "temperature": {
         "default": 0.0,
         "title": "Temperature",
         "type": "number"
      },
      "response_type": {
         "default": "multiple paragraphs",
         "title": "Response Type",
         "type": "string"
      }
   }
}



Fields:

max_tokens (int)
response_type (str)
temperature (float)





field max_tokens: int = 1500#



field response_type: str = 'multiple paragraphs'#



field temperature: float = 0.0#

**示例**:
```python
{
   "title": "SearchConfig",
   "type": "object",
   "properties": {
      "max_tokens": {
         "default": 1500,
         "title": "Max Tokens",
         "type": "integer"
      },
      "temperature": {
         "default": 0.0,
         "title": "Temperature",
         "type": "number"
      },
      "response_type": {
         "default": "multiple paragraphs",
         "title": "Response Type",
         "type": "string"
      }
   }
}

```

```python
field max_tokens: int = 1500#
```

```python
field response_type: str = 'multiple paragraphs'#
```

```python
field temperature: float = 0.0#
```

【中文翻译】previous

【中文翻译】autogen_ext.tools.code_execution

【中文翻译】next

【中文翻译】autogen_ext.tools.http

### autogen_ext.tools.http {autogen_exttoolshttp}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html)

```python
class HttpTool(name: str, host: str, port: int, json_schema: dict[str, Any], headers: dict[str, Any] | None = None, description: str = 'HTTP tool', path: str = '/', scheme: Literal['http', 'https'] = 'http', method: Literal['GET', 'POST', 'PUT', 'DELETE', 'PATCH'] = 'POST', return_type: Literal['text', 'json'] = 'text')[source]#
```

【中文翻译】Bases: BaseTool[BaseModel, Any], Component[HttpToolConfig]
A wrapper for using an HTTP server as a tool.

Parameters:

name (str) – The name of the tool.
description (str, optional) – A description of the tool.
scheme (str) – The scheme to use for the request. Must be either “http” or “https”.
host (str) – The host to send the request to.
port (int) – The port to send the request to.
path (str, optional) – The path to send the request to. Defaults to “/”.
Can include path parameters like “/{param1}/{param2}” which will be templated from input args.
method (str, optional) – The HTTP method to use, will default to POST if not provided.
Must be one of “GET”, “POST”, “PUT”, “DELETE”, “PATCH”.
headers (dict[str, Any], optional) – A dictionary of headers to send with the request.
json_schema (dict[str, Any]) – A JSON Schema object defining the expected parameters for the tool.
Path parameters must also be included in the schema and must be strings.
return_type (Literal["text", "json"], optional) – The type of response to return from the tool.
Defaults to “text”.




Note
This tool requires the http-tool extra for the autogen-ext package.
To install:
pip install -U "autogen-agentchat" "autogen-ext[http-tool]"



Example
Simple use case:
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.http import HttpTool

# Define a JSON schema for a base64 decode tool
base64_schema = {
    "type": "object",
    "properties": {
        "value": {"type": "string", "description": "The base64 value to decode"},
    },
    "required": ["value"],
}

# Create an HTTP tool for the httpbin API
base64_tool = HttpTool(
    name="base64_decode",
    description="base64 decode a value",
    scheme="https",
    host="httpbin.org",
    port=443,
    path="/base64/{value}",
    method="GET",
    json_schema=base64_schema,
)


async def main():
    # Create an assistant with the base64 tool
    model = OpenAIChatCompletionClient(model="gpt-4")
    assistant = AssistantAgent("base64_assistant", model_client=model, tools=[base64_tool])

    # The assistant can now use the base64 tool to decode the string
    response = await assistant.on_messages(
        [TextMessage(content="Can you base64 decode the value 'YWJjZGU=', please?", source="user")],
        CancellationToken(),
    )
    print(response.chat_message)


asyncio.run(main())




classmethod _from_config(config: HttpToolConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → HttpToolConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of HttpToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.http.HttpTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'tool'#
The logical type of the component.



async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#
Execute the HTTP tool with the given arguments.

Parameters:

args – The validated input arguments
cancellation_token – Token for cancelling the operation


Returns:
The response body from the HTTP call in JSON format

Raises:
Exception – If tool execution fails

**示例**:
```python
pip install -U "autogen-agentchat" "autogen-ext[http-tool]"

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.http import HttpTool

# Define a JSON schema for a base64 decode tool
base64_schema = {
    "type": "object",
    "properties": {
        "value": {"type": "string", "description": "The base64 value to decode"},
    },
    "required": ["value"],
}

# Create an HTTP tool for the httpbin API
base64_tool = HttpTool(
    name="base64_decode",
    description="base64 decode a value",
    scheme="https",
    host="httpbin.org",
    port=443,
    path="/base64/{value}",
    method="GET",
    json_schema=base64_schema,
)


async def main():
    # Create an assistant with the base64 tool
    model = OpenAIChatCompletionClient(model="gpt-4")
    assistant = AssistantAgent("base64_assistant", model_client=model, tools=[base64_tool])

    # The assistant can now use the base64 tool to decode the string
    response = await assistant.on_messages(
        [TextMessage(content="Can you base64 decode the value 'YWJjZGU=', please?", source="user")],
        CancellationToken(),
    )
    print(response.chat_message)


asyncio.run(main())

```

```python
classmethod _from_config(config: HttpToolConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → HttpToolConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of HttpToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.http.HttpTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'tool'#
```

【中文翻译】The logical type of the component.

```python
async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#
```

【中文翻译】Execute the HTTP tool with the given arguments.

Parameters:

args – The validated input arguments
cancellation_token – Token for cancelling the operation


Returns:
The response body from the HTTP call in JSON format

Raises:
Exception – If tool execution fails

【中文翻译】previous

【中文翻译】autogen_ext.tools.graphrag

【中文翻译】next

【中文翻译】autogen_ext.tools.langchain

### autogen_ext.tools.http {autogen_exttoolshttp}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html)

```python
class HttpTool(name: str, host: str, port: int, json_schema: dict[str, Any], headers: dict[str, Any] | None = None, description: str = 'HTTP tool', path: str = '/', scheme: Literal['http', 'https'] = 'http', method: Literal['GET', 'POST', 'PUT', 'DELETE', 'PATCH'] = 'POST', return_type: Literal['text', 'json'] = 'text')[source]#
```

【中文翻译】Bases: BaseTool[BaseModel, Any], Component[HttpToolConfig]
A wrapper for using an HTTP server as a tool.

Parameters:

name (str) – The name of the tool.
description (str, optional) – A description of the tool.
scheme (str) – The scheme to use for the request. Must be either “http” or “https”.
host (str) – The host to send the request to.
port (int) – The port to send the request to.
path (str, optional) – The path to send the request to. Defaults to “/”.
Can include path parameters like “/{param1}/{param2}” which will be templated from input args.
method (str, optional) – The HTTP method to use, will default to POST if not provided.
Must be one of “GET”, “POST”, “PUT”, “DELETE”, “PATCH”.
headers (dict[str, Any], optional) – A dictionary of headers to send with the request.
json_schema (dict[str, Any]) – A JSON Schema object defining the expected parameters for the tool.
Path parameters must also be included in the schema and must be strings.
return_type (Literal["text", "json"], optional) – The type of response to return from the tool.
Defaults to “text”.




Note
This tool requires the http-tool extra for the autogen-ext package.
To install:
pip install -U "autogen-agentchat" "autogen-ext[http-tool]"



Example
Simple use case:
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.http import HttpTool

# Define a JSON schema for a base64 decode tool
base64_schema = {
    "type": "object",
    "properties": {
        "value": {"type": "string", "description": "The base64 value to decode"},
    },
    "required": ["value"],
}

# Create an HTTP tool for the httpbin API
base64_tool = HttpTool(
    name="base64_decode",
    description="base64 decode a value",
    scheme="https",
    host="httpbin.org",
    port=443,
    path="/base64/{value}",
    method="GET",
    json_schema=base64_schema,
)


async def main():
    # Create an assistant with the base64 tool
    model = OpenAIChatCompletionClient(model="gpt-4")
    assistant = AssistantAgent("base64_assistant", model_client=model, tools=[base64_tool])

    # The assistant can now use the base64 tool to decode the string
    response = await assistant.on_messages(
        [TextMessage(content="Can you base64 decode the value 'YWJjZGU=', please?", source="user")],
        CancellationToken(),
    )
    print(response.chat_message)


asyncio.run(main())




classmethod _from_config(config: HttpToolConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → HttpToolConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





component_config_schema#
alias of HttpToolConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.http.HttpTool'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'tool'#
The logical type of the component.



async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#
Execute the HTTP tool with the given arguments.

Parameters:

args – The validated input arguments
cancellation_token – Token for cancelling the operation


Returns:
The response body from the HTTP call in JSON format

Raises:
Exception – If tool execution fails

**示例**:
```python
pip install -U "autogen-agentchat" "autogen-ext[http-tool]"

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.http import HttpTool

# Define a JSON schema for a base64 decode tool
base64_schema = {
    "type": "object",
    "properties": {
        "value": {"type": "string", "description": "The base64 value to decode"},
    },
    "required": ["value"],
}

# Create an HTTP tool for the httpbin API
base64_tool = HttpTool(
    name="base64_decode",
    description="base64 decode a value",
    scheme="https",
    host="httpbin.org",
    port=443,
    path="/base64/{value}",
    method="GET",
    json_schema=base64_schema,
)


async def main():
    # Create an assistant with the base64 tool
    model = OpenAIChatCompletionClient(model="gpt-4")
    assistant = AssistantAgent("base64_assistant", model_client=model, tools=[base64_tool])

    # The assistant can now use the base64 tool to decode the string
    response = await assistant.on_messages(
        [TextMessage(content="Can you base64 decode the value 'YWJjZGU=', please?", source="user")],
        CancellationToken(),
    )
    print(response.chat_message)


asyncio.run(main())

```

```python
classmethod _from_config(config: HttpToolConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → HttpToolConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
component_config_schema#
```

【中文翻译】alias of HttpToolConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.http.HttpTool'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'tool'#
```

【中文翻译】The logical type of the component.

```python
async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#
```

【中文翻译】Execute the HTTP tool with the given arguments.

Parameters:

args – The validated input arguments
cancellation_token – Token for cancelling the operation


Returns:
The response body from the HTTP call in JSON format

Raises:
Exception – If tool execution fails

【中文翻译】previous

【中文翻译】autogen_ext.tools.graphrag

【中文翻译】next

【中文翻译】autogen_ext.tools.langchain

### autogen_ext.tools.langchain {autogen_exttoolslangchain}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html)

```python
class LangChainToolAdapter(langchain_tool: LangChainTool)[source]#
```

【中文翻译】Bases: BaseTool[BaseModel, Any]
Allows you to wrap a LangChain tool and make it available to AutoGen.

Note
This class requires the langchain extra for the autogen-ext package.
pip install -U "autogen-ext[langchain]"




Parameters:
langchain_tool (LangChainTool) – A LangChain tool to wrap


Examples
Use the PythonAstREPLTool from the langchain_experimental package to
create a tool that allows you to interact with a Pandas DataFrame.
import asyncio
import pandas as pd
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from autogen_ext.tools.langchain import LangChainToolAdapter
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken


async def main() -> None:
    df = pd.read_csv("https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv")  # type: ignore
    tool = LangChainToolAdapter(PythonAstREPLTool(locals={"df": df}))
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(
        "assistant",
        tools=[tool],
        model_client=model_client,
        system_message="Use the `df` variable to access the dataset.",
    )
    await Console(
        agent.on_messages_stream(
            [TextMessage(content="What's the average age of the passengers?", source="user")], CancellationToken()
        )
    )


asyncio.run(main())


This example demonstrates how to use the SQLDatabaseToolkit from the langchain_community
package to interact with an SQLite database.
It uses the RoundRobinGroupChat to iterate the single agent over multiple steps.
If you want to one step at a time, you can just call run_stream method of the
AssistantAgent class directly.
import asyncio
import sqlite3

import requests
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.langchain import LangChainToolAdapter
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_openai import ChatOpenAI
from sqlalchemy import Engine, create_engine
from sqlalchemy.pool import StaticPool


def get_engine_for_chinook_db() -> Engine:
    url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
    response = requests.get(url)
    sql_script = response.text
    connection = sqlite3.connect(":memory:", check_same_thread=False)
    connection.executescript(sql_script)
    return create_engine(
        "sqlite://",
        creator=lambda: connection,
        poolclass=StaticPool,
        connect_args={"check_same_thread": False},
    )


async def main() -> None:
    # Create the engine and database wrapper.
    engine = get_engine_for_chinook_db()
    db = SQLDatabase(engine)

    # Create the toolkit.
    llm = ChatOpenAI(temperature=0)
    toolkit = SQLDatabaseToolkit(db=db, llm=llm)

    # Create the LangChain tool adapter for every tool in the toolkit.
    tools = [LangChainToolAdapter(tool) for tool in toolkit.get_tools()]

    # Create the chat completion client.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # Create the assistant agent.
    agent = AssistantAgent(
        "assistant",
        model_client=model_client,
        tools=tools,  # type: ignore
        model_client_stream=True,
        system_message="Respond with 'TERMINATE' if the task is completed.",
    )

    # Create termination condition.
    termination = TextMentionTermination("TERMINATE")

    # Create a round-robin group chat to iterate the single agent over multiple steps.
    chat = RoundRobinGroupChat([agent], termination_condition=termination)

    # Run the chat.
    await Console(chat.run_stream(task="Show some tables in the database"))


if __name__ == "__main__":
    asyncio.run(main())




async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#

**示例**:
```python
pip install -U "autogen-ext[langchain]"

```

**示例**:
```python
import asyncio
import pandas as pd
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from autogen_ext.tools.langchain import LangChainToolAdapter
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken


async def main() -> None:
    df = pd.read_csv("https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv")  # type: ignore
    tool = LangChainToolAdapter(PythonAstREPLTool(locals={"df": df}))
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(
        "assistant",
        tools=[tool],
        model_client=model_client,
        system_message="Use the `df` variable to access the dataset.",
    )
    await Console(
        agent.on_messages_stream(
            [TextMessage(content="What's the average age of the passengers?", source="user")], CancellationToken()
        )
    )


asyncio.run(main())

```

**示例**:
```python
import asyncio
import sqlite3

import requests
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.langchain import LangChainToolAdapter
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_openai import ChatOpenAI
from sqlalchemy import Engine, create_engine
from sqlalchemy.pool import StaticPool


def get_engine_for_chinook_db() -> Engine:
    url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
    response = requests.get(url)
    sql_script = response.text
    connection = sqlite3.connect(":memory:", check_same_thread=False)
    connection.executescript(sql_script)
    return create_engine(
        "sqlite://",
        creator=lambda: connection,
        poolclass=StaticPool,
        connect_args={"check_same_thread": False},
    )


async def main() -> None:
    # Create the engine and database wrapper.
    engine = get_engine_for_chinook_db()
    db = SQLDatabase(engine)

    # Create the toolkit.
    llm = ChatOpenAI(temperature=0)
    toolkit = SQLDatabaseToolkit(db=db, llm=llm)

    # Create the LangChain tool adapter for every tool in the toolkit.
    tools = [LangChainToolAdapter(tool) for tool in toolkit.get_tools()]

    # Create the chat completion client.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # Create the assistant agent.
    agent = AssistantAgent(
        "assistant",
        model_client=model_client,
        tools=tools,  # type: ignore
        model_client_stream=True,
        system_message="Respond with 'TERMINATE' if the task is completed.",
    )

    # Create termination condition.
    termination = TextMentionTermination("TERMINATE")

    # Create a round-robin group chat to iterate the single agent over multiple steps.
    chat = RoundRobinGroupChat([agent], termination_condition=termination)

    # Run the chat.
    await Console(chat.run_stream(task="Show some tables in the database"))


if __name__ == "__main__":
    asyncio.run(main())

```

```python
async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.tools.http

【中文翻译】next

【中文翻译】autogen_ext.tools.mcp

### autogen_ext.tools.langchain {autogen_exttoolslangchain}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html)

```python
class LangChainToolAdapter(langchain_tool: LangChainTool)[source]#
```

【中文翻译】Bases: BaseTool[BaseModel, Any]
Allows you to wrap a LangChain tool and make it available to AutoGen.

Note
This class requires the langchain extra for the autogen-ext package.
pip install -U "autogen-ext[langchain]"




Parameters:
langchain_tool (LangChainTool) – A LangChain tool to wrap


Examples
Use the PythonAstREPLTool from the langchain_experimental package to
create a tool that allows you to interact with a Pandas DataFrame.
import asyncio
import pandas as pd
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from autogen_ext.tools.langchain import LangChainToolAdapter
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken


async def main() -> None:
    df = pd.read_csv("https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv")  # type: ignore
    tool = LangChainToolAdapter(PythonAstREPLTool(locals={"df": df}))
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(
        "assistant",
        tools=[tool],
        model_client=model_client,
        system_message="Use the `df` variable to access the dataset.",
    )
    await Console(
        agent.on_messages_stream(
            [TextMessage(content="What's the average age of the passengers?", source="user")], CancellationToken()
        )
    )


asyncio.run(main())


This example demonstrates how to use the SQLDatabaseToolkit from the langchain_community
package to interact with an SQLite database.
It uses the RoundRobinGroupChat to iterate the single agent over multiple steps.
If you want to one step at a time, you can just call run_stream method of the
AssistantAgent class directly.
import asyncio
import sqlite3

import requests
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.langchain import LangChainToolAdapter
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_openai import ChatOpenAI
from sqlalchemy import Engine, create_engine
from sqlalchemy.pool import StaticPool


def get_engine_for_chinook_db() -> Engine:
    url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
    response = requests.get(url)
    sql_script = response.text
    connection = sqlite3.connect(":memory:", check_same_thread=False)
    connection.executescript(sql_script)
    return create_engine(
        "sqlite://",
        creator=lambda: connection,
        poolclass=StaticPool,
        connect_args={"check_same_thread": False},
    )


async def main() -> None:
    # Create the engine and database wrapper.
    engine = get_engine_for_chinook_db()
    db = SQLDatabase(engine)

    # Create the toolkit.
    llm = ChatOpenAI(temperature=0)
    toolkit = SQLDatabaseToolkit(db=db, llm=llm)

    # Create the LangChain tool adapter for every tool in the toolkit.
    tools = [LangChainToolAdapter(tool) for tool in toolkit.get_tools()]

    # Create the chat completion client.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # Create the assistant agent.
    agent = AssistantAgent(
        "assistant",
        model_client=model_client,
        tools=tools,  # type: ignore
        model_client_stream=True,
        system_message="Respond with 'TERMINATE' if the task is completed.",
    )

    # Create termination condition.
    termination = TextMentionTermination("TERMINATE")

    # Create a round-robin group chat to iterate the single agent over multiple steps.
    chat = RoundRobinGroupChat([agent], termination_condition=termination)

    # Run the chat.
    await Console(chat.run_stream(task="Show some tables in the database"))


if __name__ == "__main__":
    asyncio.run(main())




async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#

**示例**:
```python
pip install -U "autogen-ext[langchain]"

```

**示例**:
```python
import asyncio
import pandas as pd
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from autogen_ext.tools.langchain import LangChainToolAdapter
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken


async def main() -> None:
    df = pd.read_csv("https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv")  # type: ignore
    tool = LangChainToolAdapter(PythonAstREPLTool(locals={"df": df}))
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(
        "assistant",
        tools=[tool],
        model_client=model_client,
        system_message="Use the `df` variable to access the dataset.",
    )
    await Console(
        agent.on_messages_stream(
            [TextMessage(content="What's the average age of the passengers?", source="user")], CancellationToken()
        )
    )


asyncio.run(main())

```

**示例**:
```python
import asyncio
import sqlite3

import requests
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.langchain import LangChainToolAdapter
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_openai import ChatOpenAI
from sqlalchemy import Engine, create_engine
from sqlalchemy.pool import StaticPool


def get_engine_for_chinook_db() -> Engine:
    url = "https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql"
    response = requests.get(url)
    sql_script = response.text
    connection = sqlite3.connect(":memory:", check_same_thread=False)
    connection.executescript(sql_script)
    return create_engine(
        "sqlite://",
        creator=lambda: connection,
        poolclass=StaticPool,
        connect_args={"check_same_thread": False},
    )


async def main() -> None:
    # Create the engine and database wrapper.
    engine = get_engine_for_chinook_db()
    db = SQLDatabase(engine)

    # Create the toolkit.
    llm = ChatOpenAI(temperature=0)
    toolkit = SQLDatabaseToolkit(db=db, llm=llm)

    # Create the LangChain tool adapter for every tool in the toolkit.
    tools = [LangChainToolAdapter(tool) for tool in toolkit.get_tools()]

    # Create the chat completion client.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    # Create the assistant agent.
    agent = AssistantAgent(
        "assistant",
        model_client=model_client,
        tools=tools,  # type: ignore
        model_client_stream=True,
        system_message="Respond with 'TERMINATE' if the task is completed.",
    )

    # Create termination condition.
    termination = TextMentionTermination("TERMINATE")

    # Create a round-robin group chat to iterate the single agent over multiple steps.
    chat = RoundRobinGroupChat([agent], termination_condition=termination)

    # Run the chat.
    await Console(chat.run_stream(task="Show some tables in the database"))


if __name__ == "__main__":
    asyncio.run(main())

```

```python
async run(args: BaseModel, cancellation_token: CancellationToken) → Any[source]#
```

【中文翻译】previous

【中文翻译】autogen_ext.tools.http

【中文翻译】next

【中文翻译】autogen_ext.tools.mcp

### autogen_ext.tools.mcp {autogen_exttoolsmcp}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html)

```python
class McpSessionActor(server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')])[source]#
```

【中文翻译】Bases: ComponentBase[BaseModel], Component[McpSessionActorConfig]


async call(type: str, args: McpActorArgs | None = None) → Future[Coroutine[Any, Any, ListToolsResult] | Coroutine[Any, Any, CallToolResult]][source]#



async close() → None[source]#



component_config_schema#
alias of McpSessionActorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.McpSessionActor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'mcp_session_actor'#
The logical type of the component.



async initialize() → None[source]#



server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]#

```python
async call(type: str, args: McpActorArgs | None = None) → Future[Coroutine[Any, Any, ListToolsResult] | Coroutine[Any, Any, CallToolResult]][source]#
```

```python
async close() → None[source]#
```

```python
component_config_schema#
```

【中文翻译】alias of McpSessionActorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.McpSessionActor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'mcp_session_actor'#
```

【中文翻译】The logical type of the component.

```python
async initialize() → None[source]#
```

```python
server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]#
```

```python
class McpWorkbench(server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')])[source]#
```

【中文翻译】Bases: Workbench, Component[McpWorkbenchConfig]
A workbench that wraps an MCP server and provides an interface
to list and call tools provided by the server.

Parameters:
server_params (McpServerParams) – The parameters to connect to the MCP server.
This can be either a StdioServerParams or SseServerParams.


Examples
Here is a simple example of how to use the workbench with a mcp-server-fetch server:
import asyncio

from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    params = StdioServerParams(
        command="uvx",
        args=["mcp-server-fetch"],
        read_timeout_seconds=60,
    )

    # You can also use `start()` and `stop()` to manage the session.
    async with McpWorkbench(server_params=params) as workbench:
        tools = await workbench.list_tools()
        print(tools)
        result = await workbench.call_tool(tools[0]["name"], {"url": "https://github.com/"})
        print(result)


asyncio.run(main())


Example of using the workbench with the GitHub MCP Server:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    server_params = StdioServerParams(
        command="docker",
        args=[
            "run",
            "-i",
            "--rm",
            "-e",
            "GITHUB_PERSONAL_ACCESS_TOKEN",
            "ghcr.io/github/github-mcp-server",
        ],
        env={
            "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
        },
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "github_assistant",
            model_client=model_client,
            workbench=mcp,
            reflect_on_tool_use=True,
            model_client_stream=True,
        )
        await Console(agent.run_stream(task="Is there a repository named Autogen"))


asyncio.run(main())


Example of using the workbench with the Playwright MCP Server:
# First run `npm install -g @playwright/mcp@latest` to install the MCP server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    server_params = StdioServerParams(
        command="npx",
        args=[
            "@playwright/mcp@latest",
            "--headless",
        ],
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "web_browsing_assistant",
            model_client=model_client,
            workbench=mcp,
            model_client_stream=True,
        )
        team = RoundRobinGroupChat(
            [agent],
            termination_condition=TextMessageTermination(source="web_browsing_assistant"),
        )
        await Console(team.run_stream(task="Find out how many contributors for the microsoft/autogen repository"))


asyncio.run(main())




classmethod _from_config(config: McpWorkbenchConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → McpWorkbenchConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.





component_config_schema#
alias of McpWorkbenchConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.McpWorkbench'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async list_tools() → List[ToolSchema][source]#
List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.



async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.





async reset() → None[source]#
Reset the workbench to its initialized, started state.



async save_state() → Mapping[str, Any][source]#
Save the state of the workbench.
This method should be called to persist the state of the workbench.



property server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]#



async start() → None[source]#
Start the workbench and initialize any resources.
This method should be called before using the workbench.



async stop() → None[source]#
Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

**示例**:
```python
import asyncio

from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    params = StdioServerParams(
        command="uvx",
        args=["mcp-server-fetch"],
        read_timeout_seconds=60,
    )

    # You can also use `start()` and `stop()` to manage the session.
    async with McpWorkbench(server_params=params) as workbench:
        tools = await workbench.list_tools()
        print(tools)
        result = await workbench.call_tool(tools[0]["name"], {"url": "https://github.com/"})
        print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    server_params = StdioServerParams(
        command="docker",
        args=[
            "run",
            "-i",
            "--rm",
            "-e",
            "GITHUB_PERSONAL_ACCESS_TOKEN",
            "ghcr.io/github/github-mcp-server",
        ],
        env={
            "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
        },
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "github_assistant",
            model_client=model_client,
            workbench=mcp,
            reflect_on_tool_use=True,
            model_client_stream=True,
        )
        await Console(agent.run_stream(task="Is there a repository named Autogen"))


asyncio.run(main())

```

**示例**:
```python
# First run `npm install -g @playwright/mcp@latest` to install the MCP server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    server_params = StdioServerParams(
        command="npx",
        args=[
            "@playwright/mcp@latest",
            "--headless",
        ],
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "web_browsing_assistant",
            model_client=model_client,
            workbench=mcp,
            model_client_stream=True,
        )
        team = RoundRobinGroupChat(
            [agent],
            termination_condition=TextMessageTermination(source="web_browsing_assistant"),
        )
        await Console(team.run_stream(task="Find out how many contributors for the microsoft/autogen repository"))


asyncio.run(main())

```

```python
classmethod _from_config(config: McpWorkbenchConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → McpWorkbenchConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
```

【中文翻译】Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.

```python
component_config_schema#
```

【中文翻译】alias of McpWorkbenchConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.McpWorkbench'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async list_tools() → List[ToolSchema][source]#
```

【中文翻译】List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.

```python
async reset() → None[source]#
```

【中文翻译】Reset the workbench to its initialized, started state.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the workbench.
This method should be called to persist the state of the workbench.

```python
property server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]#
```

```python
async start() → None[source]#
```

【中文翻译】Start the workbench and initialize any resources.
This method should be called before using the workbench.

```python
async stop() → None[source]#
```

【中文翻译】Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

```python
class SseMcpToolAdapter(server_params: SseServerParams, tool: Tool, session: ClientSession | None = None)[source]#
```

【中文翻译】Bases: McpToolAdapter[SseServerParams], Component[SseMcpToolAdapterConfig]
Allows you to wrap an MCP tool running over Server-Sent Events (SSE) and make it available to AutoGen.
This adapter enables using MCP-compatible tools that communicate over HTTP with SSE
with AutoGen agents. Common use cases include integrating with remote MCP services,
cloud-based tools, and web APIs that implement the Model Context Protocol (MCP).

Note
To use this class, you need to install mcp extra for the autogen-ext package.
pip install -U "autogen-ext[mcp]"




Parameters:

server_params (SseServerParameters) – Parameters for the MCP server connection,
including URL, headers, and timeouts.
tool (Tool) – The MCP tool to wrap.
session (ClientSession, optional) – The MCP client session to use. If not provided,
it will create a new session. This is useful for testing or when you want to
manage the session lifecycle yourself.



Examples
Use a remote translation service that implements MCP over SSE to create tools
that allow AutoGen agents to perform translations:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken


async def main() -> None:
    # Create server params for the remote MCP service
    server_params = SseServerParams(
        url="https://api.example.com/mcp",
        headers={"Authorization": "Bearer your-api-key", "Content-Type": "application/json"},
        timeout=30,  # Connection timeout in seconds
    )

    # Get the translation tool from the server
    adapter = await SseMcpToolAdapter.from_server_params(server_params, "translate")

    # Create an agent that can use the translation tool
    model_client = OpenAIChatCompletionClient(model="gpt-4")
    agent = AssistantAgent(
        name="translator",
        model_client=model_client,
        tools=[adapter],
        system_message="You are a helpful translation assistant.",
    )

    # Let the agent translate some text
    await Console(
        agent.run_stream(task="Translate 'Hello, how are you?' to Spanish", cancellation_token=CancellationToken())
    )


if __name__ == "__main__":
    asyncio.run(main())




component_config_schema#
alias of SseMcpToolAdapterConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.SseMcpToolAdapter'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
pip install -U "autogen-ext[mcp]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken


async def main() -> None:
    # Create server params for the remote MCP service
    server_params = SseServerParams(
        url="https://api.example.com/mcp",
        headers={"Authorization": "Bearer your-api-key", "Content-Type": "application/json"},
        timeout=30,  # Connection timeout in seconds
    )

    # Get the translation tool from the server
    adapter = await SseMcpToolAdapter.from_server_params(server_params, "translate")

    # Create an agent that can use the translation tool
    model_client = OpenAIChatCompletionClient(model="gpt-4")
    agent = AssistantAgent(
        name="translator",
        model_client=model_client,
        tools=[adapter],
        system_message="You are a helpful translation assistant.",
    )

    # Let the agent translate some text
    await Console(
        agent.run_stream(task="Translate 'Hello, how are you?' to Spanish", cancellation_token=CancellationToken())
    )


if __name__ == "__main__":
    asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of SseMcpToolAdapterConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.SseMcpToolAdapter'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
pydantic model SseServerParams[source]#
```

【中文翻译】Bases: BaseModel
Parameters for connecting to an MCP server over SSE.

Show JSON schema{
   "title": "SseServerParams",
   "description": "Parameters for connecting to an MCP server over SSE.",
   "type": "object",
   "properties": {
      "type": {
         "const": "SseServerParams",
         "default": "SseServerParams",
         "title": "Type",
         "type": "string"
      },
      "url": {
         "title": "Url",
         "type": "string"
      },
      "headers": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Headers"
      },
      "timeout": {
         "default": 5,
         "title": "Timeout",
         "type": "number"
      },
      "sse_read_timeout": {
         "default": 300,
         "title": "Sse Read Timeout",
         "type": "number"
      }
   },
   "required": [
      "url"
   ]
}



Fields:

headers (dict[str, Any] | None)
sse_read_timeout (float)
timeout (float)
type (Literal['SseServerParams'])
url (str)





field headers: dict[str, Any] | None = None#



field sse_read_timeout: float = 300#



field timeout: float = 5#



field type: Literal['SseServerParams'] = 'SseServerParams'#



field url: str [Required]#

**示例**:
```python
{
   "title": "SseServerParams",
   "description": "Parameters for connecting to an MCP server over SSE.",
   "type": "object",
   "properties": {
      "type": {
         "const": "SseServerParams",
         "default": "SseServerParams",
         "title": "Type",
         "type": "string"
      },
      "url": {
         "title": "Url",
         "type": "string"
      },
      "headers": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Headers"
      },
      "timeout": {
         "default": 5,
         "title": "Timeout",
         "type": "number"
      },
      "sse_read_timeout": {
         "default": 300,
         "title": "Sse Read Timeout",
         "type": "number"
      }
   },
   "required": [
      "url"
   ]
}

```

```python
field headers: dict[str, Any] | None = None#
```

```python
field sse_read_timeout: float = 300#
```

```python
field timeout: float = 5#
```

```python
field type: Literal['SseServerParams'] = 'SseServerParams'#
```

```python
field url: str [Required]#
```

```python
class StdioMcpToolAdapter(server_params: StdioServerParams, tool: Tool, session: ClientSession | None = None)[source]#
```

【中文翻译】Bases: McpToolAdapter[StdioServerParams], Component[StdioMcpToolAdapterConfig]
Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.
This adapter enables using MCP-compatible tools that communicate over standard input/output
with AutoGen agents. Common use cases include wrapping command-line tools and local services
that implement the Model Context Protocol (MCP).

Note
To use this class, you need to install mcp extra for the autogen-ext package.
pip install -U "autogen-ext[mcp]"




Parameters:

server_params (StdioServerParams) – Parameters for the MCP server connection,
including command to run and its arguments
tool (Tool) – The MCP tool to wrap
session (ClientSession, optional) – The MCP client session to use. If not provided,
a new session will be created. This is useful for testing or when you want to
manage the session lifecycle yourself.



See mcp_server_tools() for examples.


component_config_schema#
alias of StdioMcpToolAdapterConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.StdioMcpToolAdapter'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
pip install -U "autogen-ext[mcp]"

```

```python
component_config_schema#
```

【中文翻译】alias of StdioMcpToolAdapterConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.StdioMcpToolAdapter'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
pydantic model StdioServerParams[source]#
```

【中文翻译】Bases: StdioServerParameters
Parameters for connecting to an MCP server over STDIO.

Show JSON schema{
   "title": "StdioServerParams",
   "description": "Parameters for connecting to an MCP server over STDIO.",
   "type": "object",
   "properties": {
      "command": {
         "title": "Command",
         "type": "string"
      },
      "args": {
         "items": {
            "type": "string"
         },
         "title": "Args",
         "type": "array"
      },
      "env": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Env"
      },
      "cwd": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "format": "path",
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Cwd"
      },
      "encoding": {
         "default": "utf-8",
         "title": "Encoding",
         "type": "string"
      },
      "encoding_error_handler": {
         "default": "strict",
         "enum": [
            "strict",
            "ignore",
            "replace"
         ],
         "title": "Encoding Error Handler",
         "type": "string"
      },
      "type": {
         "const": "StdioServerParams",
         "default": "StdioServerParams",
         "title": "Type",
         "type": "string"
      },
      "read_timeout_seconds": {
         "default": 5,
         "title": "Read Timeout Seconds",
         "type": "number"
      }
   },
   "required": [
      "command"
   ]
}



Fields:

read_timeout_seconds (float)
type (Literal['StdioServerParams'])





field read_timeout_seconds: float = 5#



field type: Literal['StdioServerParams'] = 'StdioServerParams'#

**示例**:
```python
{
   "title": "StdioServerParams",
   "description": "Parameters for connecting to an MCP server over STDIO.",
   "type": "object",
   "properties": {
      "command": {
         "title": "Command",
         "type": "string"
      },
      "args": {
         "items": {
            "type": "string"
         },
         "title": "Args",
         "type": "array"
      },
      "env": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Env"
      },
      "cwd": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "format": "path",
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Cwd"
      },
      "encoding": {
         "default": "utf-8",
         "title": "Encoding",
         "type": "string"
      },
      "encoding_error_handler": {
         "default": "strict",
         "enum": [
            "strict",
            "ignore",
            "replace"
         ],
         "title": "Encoding Error Handler",
         "type": "string"
      },
      "type": {
         "const": "StdioServerParams",
         "default": "StdioServerParams",
         "title": "Type",
         "type": "string"
      },
      "read_timeout_seconds": {
         "default": 5,
         "title": "Read Timeout Seconds",
         "type": "number"
      }
   },
   "required": [
      "command"
   ]
}

```

```python
field read_timeout_seconds: float = 5#
```

```python
field type: Literal['StdioServerParams'] = 'StdioServerParams'#
```

```python
create_mcp_server_session(server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]) → AsyncGenerator[ClientSession, None][source]#
```

【中文翻译】Create an MCP client session for the given server parameters.

```python
async mcp_server_tools(server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')], session: ClientSession | None = None) → list[StdioMcpToolAdapter | SseMcpToolAdapter][source]#
```

【中文翻译】Creates a list of MCP tool adapters that can be used with AutoGen agents.
This factory function connects to an MCP server and returns adapters for all available tools.
The adapters can be directly assigned to an AutoGen agent’s tools list.

Note
To use this function, you need to install mcp extra for the autogen-ext package.
pip install -U "autogen-ext[mcp]"




Parameters:

server_params (McpServerParams) – Connection parameters for the MCP server.
Can be either StdioServerParams for command-line tools or
SseServerParams for HTTP/SSE services.
session (ClientSession | None) – Optional existing session to use. This is used
when you want to reuse an existing connection to the MCP server. The session
will be reused when creating the MCP tool adapters.


Returns:
list[StdioMcpToolAdapter | SseMcpToolAdapter] – A list of tool adapters ready to use
with AutoGen agents.


Examples
Local file system MCP service over standard I/O example:
Install the filesystem server package from npm (requires Node.js 16+ and npm).
npm install -g @modelcontextprotocol/server-filesystem


Create an agent that can use all tools from the local filesystem MCP server.
import asyncio
from pathlib import Path
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken


async def main() -> None:
    # Setup server params for local filesystem access
    desktop = str(Path.home() / "Desktop")
    server_params = StdioServerParams(
        command="npx.cmd", args=["-y", "@modelcontextprotocol/server-filesystem", desktop]
    )

    # Get all available tools from the server
    tools = await mcp_server_tools(server_params)

    # Create an agent that can use all the tools
    agent = AssistantAgent(
        name="file_manager",
        model_client=OpenAIChatCompletionClient(model="gpt-4"),
        tools=tools,  # type: ignore
    )

    # The agent can now use any of the filesystem tools
    await agent.run(task="Create a file called test.txt with some content", cancellation_token=CancellationToken())


if __name__ == "__main__":
    asyncio.run(main())


Local fetch MCP service over standard I/O example:
Install the mcp-server-fetch package.
pip install mcp-server-fetch


Create an agent that can use the fetch tool from the local MCP server.
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools


async def main() -> None:
    # Get the fetch tool from mcp-server-fetch.
    fetch_mcp_server = StdioServerParams(command="uvx", args=["mcp-server-fetch"])
    tools = await mcp_server_tools(fetch_mcp_server)

    # Create an agent that can use the fetch tool.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(name="fetcher", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore

    # Let the agent fetch the content of a URL and summarize it.
    result = await agent.run(task="Summarize the content of https://en.wikipedia.org/wiki/Seattle")
    print(result.messages[-1])


asyncio.run(main())


Sharing an MCP client session across multiple tools:
You can create a single MCP client session and share it across multiple tools.
This is sometimes required when the server maintains a session state
(e.g., a browser state) that should be reused for multiple requests.
The following example show how to create a single MCP client session
to a local Playwright
server and share it across multiple tools.
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, create_mcp_server_session, mcp_server_tools


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", parallel_tool_calls=False)  # type: ignore
    params = StdioServerParams(
        command="npx",
        args=["@playwright/mcp@latest"],
        read_timeout_seconds=60,
    )
    async with create_mcp_server_session(params) as session:
        await session.initialize()
        tools = await mcp_server_tools(server_params=params, session=session)
        print(f"Tools: {[tool.name for tool in tools]}")

        agent = AssistantAgent(
            name="Assistant",
            model_client=model_client,
            tools=tools,  # type: ignore
        )

        termination = TextMentionTermination("TERMINATE")
        team = RoundRobinGroupChat([agent], termination_condition=termination)
        await Console(
            team.run_stream(
                task="Go to https://ekzhu.com/, visit the first link in the page, then tell me about the linked page."
            )
        )


asyncio.run(main())


Remote MCP service over SSE example:
from autogen_ext.tools.mcp import SseServerParams, mcp_server_tools


async def main() -> None:
    # Setup server params for remote service
    server_params = SseServerParams(url="https://api.example.com/mcp", headers={"Authorization": "Bearer token"})

    # Get all available tools
    tools = await mcp_server_tools(server_params)

    # Create an agent with all tools
    agent = AssistantAgent(name="tool_user", model_client=OpenAIChatCompletionClient(model="gpt-4"), tools=tools)  # type: ignore


For more examples and detailed usage, see the samples directory in the package repository.

**示例**:
```python
pip install -U "autogen-ext[mcp]"

```

**示例**:
```python
npm install -g @modelcontextprotocol/server-filesystem

```

**示例**:
```python
import asyncio
from pathlib import Path
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken


async def main() -> None:
    # Setup server params for local filesystem access
    desktop = str(Path.home() / "Desktop")
    server_params = StdioServerParams(
        command="npx.cmd", args=["-y", "@modelcontextprotocol/server-filesystem", desktop]
    )

    # Get all available tools from the server
    tools = await mcp_server_tools(server_params)

    # Create an agent that can use all the tools
    agent = AssistantAgent(
        name="file_manager",
        model_client=OpenAIChatCompletionClient(model="gpt-4"),
        tools=tools,  # type: ignore
    )

    # The agent can now use any of the filesystem tools
    await agent.run(task="Create a file called test.txt with some content", cancellation_token=CancellationToken())


if __name__ == "__main__":
    asyncio.run(main())

```

**示例**:
```python
pip install mcp-server-fetch

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools


async def main() -> None:
    # Get the fetch tool from mcp-server-fetch.
    fetch_mcp_server = StdioServerParams(command="uvx", args=["mcp-server-fetch"])
    tools = await mcp_server_tools(fetch_mcp_server)

    # Create an agent that can use the fetch tool.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(name="fetcher", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore

    # Let the agent fetch the content of a URL and summarize it.
    result = await agent.run(task="Summarize the content of https://en.wikipedia.org/wiki/Seattle")
    print(result.messages[-1])


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, create_mcp_server_session, mcp_server_tools


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", parallel_tool_calls=False)  # type: ignore
    params = StdioServerParams(
        command="npx",
        args=["@playwright/mcp@latest"],
        read_timeout_seconds=60,
    )
    async with create_mcp_server_session(params) as session:
        await session.initialize()
        tools = await mcp_server_tools(server_params=params, session=session)
        print(f"Tools: {[tool.name for tool in tools]}")

        agent = AssistantAgent(
            name="Assistant",
            model_client=model_client,
            tools=tools,  # type: ignore
        )

        termination = TextMentionTermination("TERMINATE")
        team = RoundRobinGroupChat([agent], termination_condition=termination)
        await Console(
            team.run_stream(
                task="Go to https://ekzhu.com/, visit the first link in the page, then tell me about the linked page."
            )
        )


asyncio.run(main())

```

**示例**:
```python
from autogen_ext.tools.mcp import SseServerParams, mcp_server_tools


async def main() -> None:
    # Setup server params for remote service
    server_params = SseServerParams(url="https://api.example.com/mcp", headers={"Authorization": "Bearer token"})

    # Get all available tools
    tools = await mcp_server_tools(server_params)

    # Create an agent with all tools
    agent = AssistantAgent(name="tool_user", model_client=OpenAIChatCompletionClient(model="gpt-4"), tools=tools)  # type: ignore

```

【中文翻译】previous

【中文翻译】autogen_ext.tools.langchain

【中文翻译】next

【中文翻译】autogen_ext.memory.canvas

### autogen_ext.tools.mcp {autogen_exttoolsmcp}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html)

```python
class McpSessionActor(server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')])[source]#
```

【中文翻译】Bases: ComponentBase[BaseModel], Component[McpSessionActorConfig]


async call(type: str, args: McpActorArgs | None = None) → Future[Coroutine[Any, Any, ListToolsResult] | Coroutine[Any, Any, CallToolResult]][source]#



async close() → None[source]#



component_config_schema#
alias of McpSessionActorConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.McpSessionActor'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



component_type: ClassVar[ComponentType] = 'mcp_session_actor'#
The logical type of the component.



async initialize() → None[source]#



server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]#

```python
async call(type: str, args: McpActorArgs | None = None) → Future[Coroutine[Any, Any, ListToolsResult] | Coroutine[Any, Any, CallToolResult]][source]#
```

```python
async close() → None[source]#
```

```python
component_config_schema#
```

【中文翻译】alias of McpSessionActorConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.McpSessionActor'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
component_type: ClassVar[ComponentType] = 'mcp_session_actor'#
```

【中文翻译】The logical type of the component.

```python
async initialize() → None[source]#
```

```python
server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]#
```

```python
class McpWorkbench(server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')])[source]#
```

【中文翻译】Bases: Workbench, Component[McpWorkbenchConfig]
A workbench that wraps an MCP server and provides an interface
to list and call tools provided by the server.

Parameters:
server_params (McpServerParams) – The parameters to connect to the MCP server.
This can be either a StdioServerParams or SseServerParams.


Examples
Here is a simple example of how to use the workbench with a mcp-server-fetch server:
import asyncio

from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    params = StdioServerParams(
        command="uvx",
        args=["mcp-server-fetch"],
        read_timeout_seconds=60,
    )

    # You can also use `start()` and `stop()` to manage the session.
    async with McpWorkbench(server_params=params) as workbench:
        tools = await workbench.list_tools()
        print(tools)
        result = await workbench.call_tool(tools[0]["name"], {"url": "https://github.com/"})
        print(result)


asyncio.run(main())


Example of using the workbench with the GitHub MCP Server:
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    server_params = StdioServerParams(
        command="docker",
        args=[
            "run",
            "-i",
            "--rm",
            "-e",
            "GITHUB_PERSONAL_ACCESS_TOKEN",
            "ghcr.io/github/github-mcp-server",
        ],
        env={
            "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
        },
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "github_assistant",
            model_client=model_client,
            workbench=mcp,
            reflect_on_tool_use=True,
            model_client_stream=True,
        )
        await Console(agent.run_stream(task="Is there a repository named Autogen"))


asyncio.run(main())


Example of using the workbench with the Playwright MCP Server:
# First run `npm install -g @playwright/mcp@latest` to install the MCP server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    server_params = StdioServerParams(
        command="npx",
        args=[
            "@playwright/mcp@latest",
            "--headless",
        ],
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "web_browsing_assistant",
            model_client=model_client,
            workbench=mcp,
            model_client_stream=True,
        )
        team = RoundRobinGroupChat(
            [agent],
            termination_condition=TextMessageTermination(source="web_browsing_assistant"),
        )
        await Console(team.run_stream(task="Find out how many contributors for the microsoft/autogen repository"))


asyncio.run(main())




classmethod _from_config(config: McpWorkbenchConfig) → Self[source]#
Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.





_to_config() → McpWorkbenchConfig[source]#
Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.





async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.





component_config_schema#
alias of McpWorkbenchConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.McpWorkbench'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.



async list_tools() → List[ToolSchema][source]#
List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.



async load_state(state: Mapping[str, Any]) → None[source]#
Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.





async reset() → None[source]#
Reset the workbench to its initialized, started state.



async save_state() → Mapping[str, Any][source]#
Save the state of the workbench.
This method should be called to persist the state of the workbench.



property server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]#



async start() → None[source]#
Start the workbench and initialize any resources.
This method should be called before using the workbench.



async stop() → None[source]#
Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

**示例**:
```python
import asyncio

from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    params = StdioServerParams(
        command="uvx",
        args=["mcp-server-fetch"],
        read_timeout_seconds=60,
    )

    # You can also use `start()` and `stop()` to manage the session.
    async with McpWorkbench(server_params=params) as workbench:
        tools = await workbench.list_tools()
        print(tools)
        result = await workbench.call_tool(tools[0]["name"], {"url": "https://github.com/"})
        print(result)


asyncio.run(main())

```

**示例**:
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    server_params = StdioServerParams(
        command="docker",
        args=[
            "run",
            "-i",
            "--rm",
            "-e",
            "GITHUB_PERSONAL_ACCESS_TOKEN",
            "ghcr.io/github/github-mcp-server",
        ],
        env={
            "GITHUB_PERSONAL_ACCESS_TOKEN": "ghp_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
        },
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "github_assistant",
            model_client=model_client,
            workbench=mcp,
            reflect_on_tool_use=True,
            model_client_stream=True,
        )
        await Console(agent.run_stream(task="Is there a repository named Autogen"))


asyncio.run(main())

```

**示例**:
```python
# First run `npm install -g @playwright/mcp@latest` to install the MCP server.
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import McpWorkbench, StdioServerParams


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano")
    server_params = StdioServerParams(
        command="npx",
        args=[
            "@playwright/mcp@latest",
            "--headless",
        ],
    )
    async with McpWorkbench(server_params) as mcp:
        agent = AssistantAgent(
            "web_browsing_assistant",
            model_client=model_client,
            workbench=mcp,
            model_client_stream=True,
        )
        team = RoundRobinGroupChat(
            [agent],
            termination_condition=TextMessageTermination(source="web_browsing_assistant"),
        )
        await Console(team.run_stream(task="Find out how many contributors for the microsoft/autogen repository"))


asyncio.run(main())

```

```python
classmethod _from_config(config: McpWorkbenchConfig) → Self[source]#
```

【中文翻译】Create a new instance of the component from a configuration object.

Parameters:
config (T) – The configuration object.

Returns:
Self – The new instance of the component.

```python
_to_config() → McpWorkbenchConfig[source]#
```

【中文翻译】Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

Returns:
T – The configuration of the component.

```python
async call_tool(name: str, arguments: Mapping[str, Any] | None = None, cancellation_token: CancellationToken | None = None) → ToolResult[source]#
```

【中文翻译】Call a tool in the workbench.

Parameters:

name (str) – The name of the tool to call.
arguments (Mapping[str, Any] | None) – The arguments to pass to the tool.
If None, the tool will be called with no arguments.
cancellation_token (CancellationToken | None) – An optional cancellation token
to cancel the tool execution.


Returns:
ToolResult – The result of the tool execution.

```python
component_config_schema#
```

【中文翻译】alias of McpWorkbenchConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.McpWorkbench'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
async list_tools() → List[ToolSchema][source]#
```

【中文翻译】List the currently available tools in the workbench as ToolSchema
objects.
The list of tools may be dynamic, and their content may change after
tool execution.

```python
async load_state(state: Mapping[str, Any]) → None[source]#
```

【中文翻译】Load the state of the workbench.

Parameters:
state (Mapping[str, Any]) – The state to load into the workbench.

```python
async reset() → None[source]#
```

【中文翻译】Reset the workbench to its initialized, started state.

```python
async save_state() → Mapping[str, Any][source]#
```

【中文翻译】Save the state of the workbench.
This method should be called to persist the state of the workbench.

```python
property server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]#
```

```python
async start() → None[source]#
```

【中文翻译】Start the workbench and initialize any resources.
This method should be called before using the workbench.

```python
async stop() → None[source]#
```

【中文翻译】Stop the workbench and release any resources.
This method should be called when the workbench is no longer needed.

```python
class SseMcpToolAdapter(server_params: SseServerParams, tool: Tool, session: ClientSession | None = None)[source]#
```

【中文翻译】Bases: McpToolAdapter[SseServerParams], Component[SseMcpToolAdapterConfig]
Allows you to wrap an MCP tool running over Server-Sent Events (SSE) and make it available to AutoGen.
This adapter enables using MCP-compatible tools that communicate over HTTP with SSE
with AutoGen agents. Common use cases include integrating with remote MCP services,
cloud-based tools, and web APIs that implement the Model Context Protocol (MCP).

Note
To use this class, you need to install mcp extra for the autogen-ext package.
pip install -U "autogen-ext[mcp]"




Parameters:

server_params (SseServerParameters) – Parameters for the MCP server connection,
including URL, headers, and timeouts.
tool (Tool) – The MCP tool to wrap.
session (ClientSession, optional) – The MCP client session to use. If not provided,
it will create a new session. This is useful for testing or when you want to
manage the session lifecycle yourself.



Examples
Use a remote translation service that implements MCP over SSE to create tools
that allow AutoGen agents to perform translations:
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken


async def main() -> None:
    # Create server params for the remote MCP service
    server_params = SseServerParams(
        url="https://api.example.com/mcp",
        headers={"Authorization": "Bearer your-api-key", "Content-Type": "application/json"},
        timeout=30,  # Connection timeout in seconds
    )

    # Get the translation tool from the server
    adapter = await SseMcpToolAdapter.from_server_params(server_params, "translate")

    # Create an agent that can use the translation tool
    model_client = OpenAIChatCompletionClient(model="gpt-4")
    agent = AssistantAgent(
        name="translator",
        model_client=model_client,
        tools=[adapter],
        system_message="You are a helpful translation assistant.",
    )

    # Let the agent translate some text
    await Console(
        agent.run_stream(task="Translate 'Hello, how are you?' to Spanish", cancellation_token=CancellationToken())
    )


if __name__ == "__main__":
    asyncio.run(main())




component_config_schema#
alias of SseMcpToolAdapterConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.SseMcpToolAdapter'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
pip install -U "autogen-ext[mcp]"

```

**示例**:
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken


async def main() -> None:
    # Create server params for the remote MCP service
    server_params = SseServerParams(
        url="https://api.example.com/mcp",
        headers={"Authorization": "Bearer your-api-key", "Content-Type": "application/json"},
        timeout=30,  # Connection timeout in seconds
    )

    # Get the translation tool from the server
    adapter = await SseMcpToolAdapter.from_server_params(server_params, "translate")

    # Create an agent that can use the translation tool
    model_client = OpenAIChatCompletionClient(model="gpt-4")
    agent = AssistantAgent(
        name="translator",
        model_client=model_client,
        tools=[adapter],
        system_message="You are a helpful translation assistant.",
    )

    # Let the agent translate some text
    await Console(
        agent.run_stream(task="Translate 'Hello, how are you?' to Spanish", cancellation_token=CancellationToken())
    )


if __name__ == "__main__":
    asyncio.run(main())

```

```python
component_config_schema#
```

【中文翻译】alias of SseMcpToolAdapterConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.SseMcpToolAdapter'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
pydantic model SseServerParams[source]#
```

【中文翻译】Bases: BaseModel
Parameters for connecting to an MCP server over SSE.

Show JSON schema{
   "title": "SseServerParams",
   "description": "Parameters for connecting to an MCP server over SSE.",
   "type": "object",
   "properties": {
      "type": {
         "const": "SseServerParams",
         "default": "SseServerParams",
         "title": "Type",
         "type": "string"
      },
      "url": {
         "title": "Url",
         "type": "string"
      },
      "headers": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Headers"
      },
      "timeout": {
         "default": 5,
         "title": "Timeout",
         "type": "number"
      },
      "sse_read_timeout": {
         "default": 300,
         "title": "Sse Read Timeout",
         "type": "number"
      }
   },
   "required": [
      "url"
   ]
}



Fields:

headers (dict[str, Any] | None)
sse_read_timeout (float)
timeout (float)
type (Literal['SseServerParams'])
url (str)





field headers: dict[str, Any] | None = None#



field sse_read_timeout: float = 300#



field timeout: float = 5#



field type: Literal['SseServerParams'] = 'SseServerParams'#



field url: str [Required]#

**示例**:
```python
{
   "title": "SseServerParams",
   "description": "Parameters for connecting to an MCP server over SSE.",
   "type": "object",
   "properties": {
      "type": {
         "const": "SseServerParams",
         "default": "SseServerParams",
         "title": "Type",
         "type": "string"
      },
      "url": {
         "title": "Url",
         "type": "string"
      },
      "headers": {
         "anyOf": [
            {
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Headers"
      },
      "timeout": {
         "default": 5,
         "title": "Timeout",
         "type": "number"
      },
      "sse_read_timeout": {
         "default": 300,
         "title": "Sse Read Timeout",
         "type": "number"
      }
   },
   "required": [
      "url"
   ]
}

```

```python
field headers: dict[str, Any] | None = None#
```

```python
field sse_read_timeout: float = 300#
```

```python
field timeout: float = 5#
```

```python
field type: Literal['SseServerParams'] = 'SseServerParams'#
```

```python
field url: str [Required]#
```

```python
class StdioMcpToolAdapter(server_params: StdioServerParams, tool: Tool, session: ClientSession | None = None)[source]#
```

【中文翻译】Bases: McpToolAdapter[StdioServerParams], Component[StdioMcpToolAdapterConfig]
Allows you to wrap an MCP tool running over STDIO and make it available to AutoGen.
This adapter enables using MCP-compatible tools that communicate over standard input/output
with AutoGen agents. Common use cases include wrapping command-line tools and local services
that implement the Model Context Protocol (MCP).

Note
To use this class, you need to install mcp extra for the autogen-ext package.
pip install -U "autogen-ext[mcp]"




Parameters:

server_params (StdioServerParams) – Parameters for the MCP server connection,
including command to run and its arguments
tool (Tool) – The MCP tool to wrap
session (ClientSession, optional) – The MCP client session to use. If not provided,
a new session will be created. This is useful for testing or when you want to
manage the session lifecycle yourself.



See mcp_server_tools() for examples.


component_config_schema#
alias of StdioMcpToolAdapterConfig



component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.StdioMcpToolAdapter'#
Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

**示例**:
```python
pip install -U "autogen-ext[mcp]"

```

```python
component_config_schema#
```

【中文翻译】alias of StdioMcpToolAdapterConfig

```python
component_provider_override: ClassVar[str | None] = 'autogen_ext.tools.mcp.StdioMcpToolAdapter'#
```

【中文翻译】Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

```python
pydantic model StdioServerParams[source]#
```

【中文翻译】Bases: StdioServerParameters
Parameters for connecting to an MCP server over STDIO.

Show JSON schema{
   "title": "StdioServerParams",
   "description": "Parameters for connecting to an MCP server over STDIO.",
   "type": "object",
   "properties": {
      "command": {
         "title": "Command",
         "type": "string"
      },
      "args": {
         "items": {
            "type": "string"
         },
         "title": "Args",
         "type": "array"
      },
      "env": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Env"
      },
      "cwd": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "format": "path",
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Cwd"
      },
      "encoding": {
         "default": "utf-8",
         "title": "Encoding",
         "type": "string"
      },
      "encoding_error_handler": {
         "default": "strict",
         "enum": [
            "strict",
            "ignore",
            "replace"
         ],
         "title": "Encoding Error Handler",
         "type": "string"
      },
      "type": {
         "const": "StdioServerParams",
         "default": "StdioServerParams",
         "title": "Type",
         "type": "string"
      },
      "read_timeout_seconds": {
         "default": 5,
         "title": "Read Timeout Seconds",
         "type": "number"
      }
   },
   "required": [
      "command"
   ]
}



Fields:

read_timeout_seconds (float)
type (Literal['StdioServerParams'])





field read_timeout_seconds: float = 5#



field type: Literal['StdioServerParams'] = 'StdioServerParams'#

**示例**:
```python
{
   "title": "StdioServerParams",
   "description": "Parameters for connecting to an MCP server over STDIO.",
   "type": "object",
   "properties": {
      "command": {
         "title": "Command",
         "type": "string"
      },
      "args": {
         "items": {
            "type": "string"
         },
         "title": "Args",
         "type": "array"
      },
      "env": {
         "anyOf": [
            {
               "additionalProperties": {
                  "type": "string"
               },
               "type": "object"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Env"
      },
      "cwd": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "format": "path",
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Cwd"
      },
      "encoding": {
         "default": "utf-8",
         "title": "Encoding",
         "type": "string"
      },
      "encoding_error_handler": {
         "default": "strict",
         "enum": [
            "strict",
            "ignore",
            "replace"
         ],
         "title": "Encoding Error Handler",
         "type": "string"
      },
      "type": {
         "const": "StdioServerParams",
         "default": "StdioServerParams",
         "title": "Type",
         "type": "string"
      },
      "read_timeout_seconds": {
         "default": 5,
         "title": "Read Timeout Seconds",
         "type": "number"
      }
   },
   "required": [
      "command"
   ]
}

```

```python
field read_timeout_seconds: float = 5#
```

```python
field type: Literal['StdioServerParams'] = 'StdioServerParams'#
```

```python
create_mcp_server_session(server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')]) → AsyncGenerator[ClientSession, None][source]#
```

【中文翻译】Create an MCP client session for the given server parameters.

```python
async mcp_server_tools(server_params: Annotated[StdioServerParams | SseServerParams, FieldInfo(annotation=NoneType, required=True, discriminator='type')], session: ClientSession | None = None) → list[StdioMcpToolAdapter | SseMcpToolAdapter][source]#
```

【中文翻译】Creates a list of MCP tool adapters that can be used with AutoGen agents.
This factory function connects to an MCP server and returns adapters for all available tools.
The adapters can be directly assigned to an AutoGen agent’s tools list.

Note
To use this function, you need to install mcp extra for the autogen-ext package.
pip install -U "autogen-ext[mcp]"




Parameters:

server_params (McpServerParams) – Connection parameters for the MCP server.
Can be either StdioServerParams for command-line tools or
SseServerParams for HTTP/SSE services.
session (ClientSession | None) – Optional existing session to use. This is used
when you want to reuse an existing connection to the MCP server. The session
will be reused when creating the MCP tool adapters.


Returns:
list[StdioMcpToolAdapter | SseMcpToolAdapter] – A list of tool adapters ready to use
with AutoGen agents.


Examples
Local file system MCP service over standard I/O example:
Install the filesystem server package from npm (requires Node.js 16+ and npm).
npm install -g @modelcontextprotocol/server-filesystem


Create an agent that can use all tools from the local filesystem MCP server.
import asyncio
from pathlib import Path
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken


async def main() -> None:
    # Setup server params for local filesystem access
    desktop = str(Path.home() / "Desktop")
    server_params = StdioServerParams(
        command="npx.cmd", args=["-y", "@modelcontextprotocol/server-filesystem", desktop]
    )

    # Get all available tools from the server
    tools = await mcp_server_tools(server_params)

    # Create an agent that can use all the tools
    agent = AssistantAgent(
        name="file_manager",
        model_client=OpenAIChatCompletionClient(model="gpt-4"),
        tools=tools,  # type: ignore
    )

    # The agent can now use any of the filesystem tools
    await agent.run(task="Create a file called test.txt with some content", cancellation_token=CancellationToken())


if __name__ == "__main__":
    asyncio.run(main())


Local fetch MCP service over standard I/O example:
Install the mcp-server-fetch package.
pip install mcp-server-fetch


Create an agent that can use the fetch tool from the local MCP server.
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools


async def main() -> None:
    # Get the fetch tool from mcp-server-fetch.
    fetch_mcp_server = StdioServerParams(command="uvx", args=["mcp-server-fetch"])
    tools = await mcp_server_tools(fetch_mcp_server)

    # Create an agent that can use the fetch tool.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(name="fetcher", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore

    # Let the agent fetch the content of a URL and summarize it.
    result = await agent.run(task="Summarize the content of https://en.wikipedia.org/wiki/Seattle")
    print(result.messages[-1])


asyncio.run(main())


Sharing an MCP client session across multiple tools:
You can create a single MCP client session and share it across multiple tools.
This is sometimes required when the server maintains a session state
(e.g., a browser state) that should be reused for multiple requests.
The following example show how to create a single MCP client session
to a local Playwright
server and share it across multiple tools.
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, create_mcp_server_session, mcp_server_tools


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", parallel_tool_calls=False)  # type: ignore
    params = StdioServerParams(
        command="npx",
        args=["@playwright/mcp@latest"],
        read_timeout_seconds=60,
    )
    async with create_mcp_server_session(params) as session:
        await session.initialize()
        tools = await mcp_server_tools(server_params=params, session=session)
        print(f"Tools: {[tool.name for tool in tools]}")

        agent = AssistantAgent(
            name="Assistant",
            model_client=model_client,
            tools=tools,  # type: ignore
        )

        termination = TextMentionTermination("TERMINATE")
        team = RoundRobinGroupChat([agent], termination_condition=termination)
        await Console(
            team.run_stream(
                task="Go to https://ekzhu.com/, visit the first link in the page, then tell me about the linked page."
            )
        )


asyncio.run(main())


Remote MCP service over SSE example:
from autogen_ext.tools.mcp import SseServerParams, mcp_server_tools


async def main() -> None:
    # Setup server params for remote service
    server_params = SseServerParams(url="https://api.example.com/mcp", headers={"Authorization": "Bearer token"})

    # Get all available tools
    tools = await mcp_server_tools(server_params)

    # Create an agent with all tools
    agent = AssistantAgent(name="tool_user", model_client=OpenAIChatCompletionClient(model="gpt-4"), tools=tools)  # type: ignore


For more examples and detailed usage, see the samples directory in the package repository.

**示例**:
```python
pip install -U "autogen-ext[mcp]"

```

**示例**:
```python
npm install -g @modelcontextprotocol/server-filesystem

```

**示例**:
```python
import asyncio
from pathlib import Path
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken


async def main() -> None:
    # Setup server params for local filesystem access
    desktop = str(Path.home() / "Desktop")
    server_params = StdioServerParams(
        command="npx.cmd", args=["-y", "@modelcontextprotocol/server-filesystem", desktop]
    )

    # Get all available tools from the server
    tools = await mcp_server_tools(server_params)

    # Create an agent that can use all the tools
    agent = AssistantAgent(
        name="file_manager",
        model_client=OpenAIChatCompletionClient(model="gpt-4"),
        tools=tools,  # type: ignore
    )

    # The agent can now use any of the filesystem tools
    await agent.run(task="Create a file called test.txt with some content", cancellation_token=CancellationToken())


if __name__ == "__main__":
    asyncio.run(main())

```

**示例**:
```python
pip install mcp-server-fetch

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools


async def main() -> None:
    # Get the fetch tool from mcp-server-fetch.
    fetch_mcp_server = StdioServerParams(command="uvx", args=["mcp-server-fetch"])
    tools = await mcp_server_tools(fetch_mcp_server)

    # Create an agent that can use the fetch tool.
    model_client = OpenAIChatCompletionClient(model="gpt-4o")
    agent = AssistantAgent(name="fetcher", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore

    # Let the agent fetch the content of a URL and summarize it.
    result = await agent.run(task="Summarize the content of https://en.wikipedia.org/wiki/Seattle")
    print(result.messages[-1])


asyncio.run(main())

```

**示例**:
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, create_mcp_server_session, mcp_server_tools


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", parallel_tool_calls=False)  # type: ignore
    params = StdioServerParams(
        command="npx",
        args=["@playwright/mcp@latest"],
        read_timeout_seconds=60,
    )
    async with create_mcp_server_session(params) as session:
        await session.initialize()
        tools = await mcp_server_tools(server_params=params, session=session)
        print(f"Tools: {[tool.name for tool in tools]}")

        agent = AssistantAgent(
            name="Assistant",
            model_client=model_client,
            tools=tools,  # type: ignore
        )

        termination = TextMentionTermination("TERMINATE")
        team = RoundRobinGroupChat([agent], termination_condition=termination)
        await Console(
            team.run_stream(
                task="Go to https://ekzhu.com/, visit the first link in the page, then tell me about the linked page."
            )
        )


asyncio.run(main())

```

**示例**:
```python
from autogen_ext.tools.mcp import SseServerParams, mcp_server_tools


async def main() -> None:
    # Setup server params for remote service
    server_params = SseServerParams(url="https://api.example.com/mcp", headers={"Authorization": "Bearer token"})

    # Get all available tools
    tools = await mcp_server_tools(server_params)

    # Create an agent with all tools
    agent = AssistantAgent(name="tool_user", model_client=OpenAIChatCompletionClient(model="gpt-4"), tools=tools)  # type: ignore

```

【中文翻译】previous

【中文翻译】autogen_ext.tools.langchain

【中文翻译】next

【中文翻译】autogen_ext.memory.canvas

### autogen_ext.tools.semantic_kernel {autogen_exttoolssemantic_kernel}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html)

```python
pydantic model KernelFunctionFromTool[source]#
```

【中文翻译】Bases: KernelFunctionFromMethod

Show JSON schema{
   "title": "KernelFunctionFromTool",
   "type": "object",
   "properties": {
      "metadata": {
         "$ref": "#/$defs/KernelFunctionMetadata"
      },
      "invocation_duration_histogram": {
         "default": null,
         "title": "Invocation Duration Histogram"
      },
      "streaming_duration_histogram": {
         "default": null,
         "title": "Streaming Duration Histogram"
      },
      "method": {
         "default": null,
         "title": "Method"
      },
      "stream_method": {
         "default": null,
         "title": "Stream Method"
      }
   },
   "$defs": {
      "KernelFunctionMetadata": {
         "description": "The kernel function metadata.",
         "properties": {
            "name": {
               "pattern": "^[0-9A-Za-z_]+$",
               "title": "Name",
               "type": "string"
            },
            "plugin_name": {
               "anyOf": [
                  {
                     "pattern": "^[0-9A-Za-z_]+$",
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Plugin Name"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "parameters": {
               "items": {
                  "$ref": "#/$defs/KernelParameterMetadata"
               },
               "title": "Parameters",
               "type": "array"
            },
            "is_prompt": {
               "title": "Is Prompt",
               "type": "boolean"
            },
            "is_asynchronous": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": true,
               "title": "Is Asynchronous"
            },
            "return_parameter": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/KernelParameterMetadata"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "additional_properties": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Additional Properties"
            }
         },
         "required": [
            "name",
            "is_prompt"
         ],
         "title": "KernelFunctionMetadata",
         "type": "object"
      },
      "KernelParameterMetadata": {
         "description": "The kernel parameter metadata.",
         "properties": {
            "name": {
               "anyOf": [
                  {
                     "pattern": "^[0-9A-Za-z_]+$",
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Name"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "default_value": {
               "anyOf": [
                  {},
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Default Value"
            },
            "type": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": "str",
               "title": "Type"
            },
            "is_required": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": false,
               "title": "Is Required"
            },
            "type_object": {
               "anyOf": [
                  {},
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Type Object"
            },
            "schema_data": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Schema Data"
            },
            "include_in_function_choices": {
               "default": true,
               "title": "Include In Function Choices",
               "type": "boolean"
            }
         },
         "required": [
            "name"
         ],
         "title": "KernelParameterMetadata",
         "type": "object"
      }
   },
   "required": [
      "metadata"
   ]
}



Fields:

**示例**:
```python
{
   "title": "KernelFunctionFromTool",
   "type": "object",
   "properties": {
      "metadata": {
         "$ref": "#/$defs/KernelFunctionMetadata"
      },
      "invocation_duration_histogram": {
         "default": null,
         "title": "Invocation Duration Histogram"
      },
      "streaming_duration_histogram": {
         "default": null,
         "title": "Streaming Duration Histogram"
      },
      "method": {
         "default": null,
         "title": "Method"
      },
      "stream_method": {
         "default": null,
         "title": "Stream Method"
      }
   },
   "$defs": {
      "KernelFunctionMetadata": {
         "description": "The kernel function metadata.",
         "properties": {
            "name": {
               "pattern": "^[0-9A-Za-z_]+$",
               "title": "Name",
               "type": "string"
            },
            "plugin_name": {
               "anyOf": [
                  {
                     "pattern": "^[0-9A-Za-z_]+$",
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Plugin Name"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "parameters": {
               "items": {
                  "$ref": "#/$defs/KernelParameterMetadata"
               },
               "title": "Parameters",
               "type": "array"
            },
            "is_prompt": {
               "title": "Is Prompt",
               "type": "boolean"
            },
            "is_asynchronous": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": true,
               "title": "Is Asynchronous"
            },
            "return_parameter": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/KernelParameterMetadata"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "additional_properties": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Additional Properties"
            }
         },
         "required": [
            "name",
            "is_prompt"
         ],
         "title": "KernelFunctionMetadata",
         "type": "object"
      },
      "KernelParameterMetadata": {
         "description": "The kernel parameter metadata.",
         "properties": {
            "name": {
               "anyOf": [
                  {
                     "pattern": "^[0-9A-Za-z_]+$",
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Name"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "default_value": {
               "anyOf": [
                  {},
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Default Value"
            },
            "type": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": "str",
               "title": "Type"
            },
            "is_required": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": false,
               "title": "Is Required"
            },
            "type_object": {
               "anyOf": [
                  {},
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Type Object"
            },
            "schema_data": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Schema Data"
            },
            "include_in_function_choices": {
               "default": true,
               "title": "Include In Function Choices",
               "type": "boolean"
            }
         },
         "required": [
            "name"
         ],
         "title": "KernelParameterMetadata",
         "type": "object"
      }
   },
   "required": [
      "metadata"
   ]
}

```

【中文翻译】previous

【中文翻译】autogen_ext.memory.canvas

【中文翻译】next

【中文翻译】autogen_ext.code_executors.local

### autogen_ext.tools.semantic_kernel {autogen_exttoolssemantic_kernel}

**链接**: [https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.semantic_kernel.html)

```python
pydantic model KernelFunctionFromTool[source]#
```

【中文翻译】Bases: KernelFunctionFromMethod

Show JSON schema{
   "title": "KernelFunctionFromTool",
   "type": "object",
   "properties": {
      "metadata": {
         "$ref": "#/$defs/KernelFunctionMetadata"
      },
      "invocation_duration_histogram": {
         "default": null,
         "title": "Invocation Duration Histogram"
      },
      "streaming_duration_histogram": {
         "default": null,
         "title": "Streaming Duration Histogram"
      },
      "method": {
         "default": null,
         "title": "Method"
      },
      "stream_method": {
         "default": null,
         "title": "Stream Method"
      }
   },
   "$defs": {
      "KernelFunctionMetadata": {
         "description": "The kernel function metadata.",
         "properties": {
            "name": {
               "pattern": "^[0-9A-Za-z_]+$",
               "title": "Name",
               "type": "string"
            },
            "plugin_name": {
               "anyOf": [
                  {
                     "pattern": "^[0-9A-Za-z_]+$",
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Plugin Name"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "parameters": {
               "items": {
                  "$ref": "#/$defs/KernelParameterMetadata"
               },
               "title": "Parameters",
               "type": "array"
            },
            "is_prompt": {
               "title": "Is Prompt",
               "type": "boolean"
            },
            "is_asynchronous": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": true,
               "title": "Is Asynchronous"
            },
            "return_parameter": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/KernelParameterMetadata"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "additional_properties": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Additional Properties"
            }
         },
         "required": [
            "name",
            "is_prompt"
         ],
         "title": "KernelFunctionMetadata",
         "type": "object"
      },
      "KernelParameterMetadata": {
         "description": "The kernel parameter metadata.",
         "properties": {
            "name": {
               "anyOf": [
                  {
                     "pattern": "^[0-9A-Za-z_]+$",
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Name"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "default_value": {
               "anyOf": [
                  {},
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Default Value"
            },
            "type": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": "str",
               "title": "Type"
            },
            "is_required": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": false,
               "title": "Is Required"
            },
            "type_object": {
               "anyOf": [
                  {},
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Type Object"
            },
            "schema_data": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Schema Data"
            },
            "include_in_function_choices": {
               "default": true,
               "title": "Include In Function Choices",
               "type": "boolean"
            }
         },
         "required": [
            "name"
         ],
         "title": "KernelParameterMetadata",
         "type": "object"
      }
   },
   "required": [
      "metadata"
   ]
}



Fields:

**示例**:
```python
{
   "title": "KernelFunctionFromTool",
   "type": "object",
   "properties": {
      "metadata": {
         "$ref": "#/$defs/KernelFunctionMetadata"
      },
      "invocation_duration_histogram": {
         "default": null,
         "title": "Invocation Duration Histogram"
      },
      "streaming_duration_histogram": {
         "default": null,
         "title": "Streaming Duration Histogram"
      },
      "method": {
         "default": null,
         "title": "Method"
      },
      "stream_method": {
         "default": null,
         "title": "Stream Method"
      }
   },
   "$defs": {
      "KernelFunctionMetadata": {
         "description": "The kernel function metadata.",
         "properties": {
            "name": {
               "pattern": "^[0-9A-Za-z_]+$",
               "title": "Name",
               "type": "string"
            },
            "plugin_name": {
               "anyOf": [
                  {
                     "pattern": "^[0-9A-Za-z_]+$",
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Plugin Name"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "parameters": {
               "items": {
                  "$ref": "#/$defs/KernelParameterMetadata"
               },
               "title": "Parameters",
               "type": "array"
            },
            "is_prompt": {
               "title": "Is Prompt",
               "type": "boolean"
            },
            "is_asynchronous": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": true,
               "title": "Is Asynchronous"
            },
            "return_parameter": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/KernelParameterMetadata"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "additional_properties": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Additional Properties"
            }
         },
         "required": [
            "name",
            "is_prompt"
         ],
         "title": "KernelFunctionMetadata",
         "type": "object"
      },
      "KernelParameterMetadata": {
         "description": "The kernel parameter metadata.",
         "properties": {
            "name": {
               "anyOf": [
                  {
                     "pattern": "^[0-9A-Za-z_]+$",
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "title": "Name"
            },
            "description": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Description"
            },
            "default_value": {
               "anyOf": [
                  {},
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Default Value"
            },
            "type": {
               "anyOf": [
                  {
                     "type": "string"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": "str",
               "title": "Type"
            },
            "is_required": {
               "anyOf": [
                  {
                     "type": "boolean"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": false,
               "title": "Is Required"
            },
            "type_object": {
               "anyOf": [
                  {},
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Type Object"
            },
            "schema_data": {
               "anyOf": [
                  {
                     "type": "object"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null,
               "title": "Schema Data"
            },
            "include_in_function_choices": {
               "default": true,
               "title": "Include In Function Choices",
               "type": "boolean"
            }
         },
         "required": [
            "name"
         ],
         "title": "KernelParameterMetadata",
         "type": "object"
      }
   },
   "required": [
      "metadata"
   ]
}

```

【中文翻译】previous

【中文翻译】autogen_ext.memory.canvas

【中文翻译】next

【中文翻译】autogen_ext.code_executors.local

