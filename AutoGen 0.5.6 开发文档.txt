AutoGen 0.5.6 深度开发指南
1. AutoGen 0.5.6 简介
1.1. AutoGen 框架概览
AutoGen 是一个由微软开发的开源框架，旨在简化和赋能基于大型语言模型（LLM）的多智能体应用的开发 。它提供了一套可定制、可对话的智能体，这些智能体可以集成 LLM、人工输入和各种工具，通过自动化的智能体间对话来协同完成复杂任务 3。AutoGen 的核心设计理念是通过多智能体对话来编排、自动化和优化复杂的 LLM 工作流，从而最大限度地发挥 LLM 的性能，弥补其固有缺陷，并以最小的努力构建下一代 LLM 应用 2。
AutoGen 0.5.6 版本在之前版本的基础上，进一步增强了框架的灵活性、可扩展性和易用性，特别是在工作流定义和扩展能力方面。该框架采用分层和可扩展的设计，允许开发者在不同抽象级别上使用框架，从高级 API 到低级组件 2。
1.2. 本文档的目标读者与范围
本文档旨在为开发者提供一份针对 AutoGen 0.5.6 版本的超级详细且完整的最终开发指南。目标读者包括但不限于：
●希望利用 AutoGen 构建复杂多智能体应用的 AI 应用开发者。
●对多智能体系统、LLM 编排和自动化感兴趣的研究人员。
●寻求将 AutoGen 集成到现有系统或扩展其功能的软件工程师。
本文档将深入探讨 AutoGen 0.5.6 的核心概念、AgentChat API、全新的 GraphFlow 工作流、关键的 AutoGen 扩展以及 AutoGen Studio 的使用。内容将覆盖从安装配置、基础概念、智能体与团队构建，到高级功能如工具使用、代码执行、人机回圈、状态管理和自定义扩展开发等各个方面。
1.3. AutoGen 0.5.6 的主要特性与更新
AutoGen 0.5.6 版本在继承了先前版本强大功能的基础上，引入了若干重要的新特性和改进 4：
●GraphFlow：基于有向图的自定义工作流：这是 0.5.6 版本中最引人注目的新特性。作为 AgentChat 的一部分，GraphFlow 引入了一个新的实验性 API，允许用户使用有向图创建高度可定制的工作流。它支持并发智能体，并且比之前的 SelectorGroupChat 更为强大。官方提供了用户指南和扇出扇入（fan-out-fan-in）工作流示例 1。此功能由 @abhinav-aegis 在 #6333 中添加，文档在 #6417 中提供 4。
●Azure AI Agent 改进：增加了对 Bing grounding 引用 URL 的支持 4。
●新增示例：添加了一个多智能体 PostgreSQL 数据管理示例 4。
●错误修复：
○修复了 DockerCommandLineCodeExecutor 中的多事件循环感知问题 4。
○修复了 GraphFlow 的序列化/反序列化问题并添加了测试 4。
○修复了在 Gemini 中使用 OpenAI SDK 时 MultiModalMessage 的错误 4。
○修复了与 McpWorkbench_errors_properties_and_grace_shutdown 相关的问题 4。
○修复了 AssistantAgent 反序列化过程中 workbench 和 tools 之间的冲突问题 4。
●开发改进：显著加快了 Docker 执行器单元测试的速度 4。
●其他 Python 相关变更：包括对 Anthropic 模型在 Bedrock 上的聊天完成支持、文档中缺失依赖的澄清等 4。
这些更新表明 AutoGen 团队致力于提升框架的灵活性、稳定性和开发者体验，特别是通过 GraphFlow 引入更强大的工作流编排能力。
1.4. 如何使用本文档
本文档按照逻辑结构组织，从基础概念逐步深入到高级应用和扩展开发。建议读者：
1.顺序阅读：特别是对于初次接触 AutoGen 的开发者，建议从安装设置开始，逐步了解核心概念和 AgentChat API。
2.查阅 API 参考：本文档会引用官方 API 参考页面 (microsoft.github.io/autogen/stable/reference/index.html)。在学习特定组件时，请结合 API 参考获取最详细的参数和方法说明。
3.实践代码示例：文档中会包含代码示例，动手实践是掌握 AutoGen 的最佳途径。
4.关注 GraphFlow 和 Extensions：这两部分是用户查询的重点，本文档会给予特别详细的介绍。
5.版本注意：本文档主要基于 AutoGen 0.5.6 版本。由于 AutoGen 仍在快速发展，请留意官方文档以获取最新信息。
2. 安装与设置
2.1. 系统与 Python 版本要求
AutoGen 要求 Python 版本为 3.10 或更高版本 2。建议使用最新的稳定 Python 版本以获得最佳兼容性和性能。
2.2. 创建虚拟环境 (推荐)
为了避免与系统中已有的 Python 包产生冲突，强烈建议在虚拟环境中安装和使用 AutoGen 6。
●使用 venv：
Bash
python3 -m venv autogen_env
# 激活虚拟环境
# On Linux/macOS
source autogen_env/bin/activate
# On Windows
# autogen_env\Scripts\activate
6
●使用 conda：
Bash
conda create -n autogen_env python=3.10  # 或 3.11, 3.12
conda activate autogen_env
6
2.3. 安装 AutoGen 核心库与扩展
AutoGen 的安装通过 pip 完成。根据需要安装的核心组件和扩展，命令会有所不同。
●安装核心 AgentChat 和 OpenAI 客户端扩展：
这是开始使用 AutoGen AgentChat 并与 OpenAI 模型交互的基础安装。
Bash
pip install -U "autogen-agentchat[openai]"

或者，更细致地，可以分别安装 autogen-agentchat 和 autogen-ext 的 OpenAI 部分：
Bash
pip install -U "autogen-agentchat" "autogen-ext[openai]"

2
autogen-core 是 AutoGen 的核心基础库，提供了事件驱动的智能体和运行时。autogen-agentchat 是构建在 autogen-core 之上的更高级 API，用于快速原型设计。autogen-ext 包含了对各种 LLM 客户端（如 OpenAI）和能力的具体实现 2。
●安装其他扩展：
AutoGen 提供了丰富的扩展，可以按需安装。例如，要使用 Web Surfer 功能：
Bash
pip install -U "autogen-ext[web-surfer]"

2 (提及 autogen-ext[openai,web-surfer])
要使用 Docker 进行代码执行：
Bash
pip install -U "autogen-ext[docker]"

6 (提及 autogen-ext[openai] 和 autogen-ext[azure], Docker 安装是独立步骤)
其他扩展如 Anthropic, Ollama, Llama.cpp, Semantic Kernel 等，也遵循类似的安装模式，例如 pip install -U "autogen-ext[anthropic]" 10。详细的扩展安装说明见后续章节。
●安装 AutoGen Studio：
如果希望使用 AutoGen Studio (一个用于构建多智能体应用的低代码/无代码图形用户界面)：
Bash
pip install -U autogenstudio

2
2.4. 基础配置 (API 密钥)
大多数 AutoGen 应用都需要与 LLM 服务交互，这通常需要配置 API 密钥。
●OpenAI API 密钥：
最常见的方式是设置环境变量 OPENAI_API_KEY。AutoGen 会自动检测并使用此密钥 15。
Bash
export OPENAI_API_KEY="sk-your_openai_api_key"

可选地，也可以设置 OPENAI_ORG_ID。
●Azure OpenAI API 密钥：
Azure OpenAI 服务需要更详细的配置，可以通过环境变量或在代码中直接配置 AzureOpenAIChatCompletionClient 16。
所需信息通常包括：
○API 密钥 (e.g., AZURE_OPENAI_API_KEY)
○Azure OpenAI Endpoint (e.g., AZURE_OPENAI_ENDPOINT 或 AZURE_OPENAI_API_BASE)
○API 版本 (e.g., AZURE_OPENAI_API_VERSION)
○部署名称 (e.g., AZURE_OPENAI_DEPLOYMENT_NAME) Azure OpenAI 也支持基于 Azure Active Directory (AAD) 令牌的身份验证，这将在 AzureTokenProvider 扩展部分详细介绍 16。
●其他模型提供商：
对于 Anthropic、Ollama、Llama.cpp 等其他模型或本地模型，其配置方法将在各自的扩展章节中详细说明。通常涉及在初始化相应的模型客户端时传入 API 密钥或模型路径等参数。
3. AutoGen 核心概念深度解析 (v0.5.6)
AutoGen 的核心 (autogen_core) 提供了一套基础构建块，用于创建事件驱动的、可扩展的多智能体系统。理解这些核心概念对于有效利用 AutoGen 至关重要。
3.1. autogen_core.Agent 接口与 BaseAgent
autogen_core.Agent 是 AutoGen 框架中智能体定义的核心。它并非一个具体的类，而是一个协议 (Python typing.Protocol)，规定了任何希望在 AutoGen 生态系统中扮演智能体角色的实体必须遵循的契约 17。
●关键属性：
○id: AgentId：智能体实例的唯一标识符。AgentId 由 type (与创建智能体的工厂关联) 和 key (运行时的数据相关实例键) 组成，对于在智能体运行时（尤其是在分布式环境中）寻址和管理至关重要 17。
○metadata: AgentMetadata：包含关于智能体的描述性信息，如其类型、键以及对其能力的文本描述 17。
●关键方法：
○async on_message(self, message: Any, ctx: MessageContext) -> Any：这是智能体最核心的消息处理方法。当智能体接收到消息时，运行时会调用此异步方法。智能体的具体实现需要定义如何处理传入的 message (可以是任意类型) 以及在给定的 MessageContext 下生成何种响应 17。
○async save_state(self) -> Mapping[str, Any]：允许智能体持久化其内部状态。返回的状态必须是 JSON 可序列化的 17。
○async load_state(self, state: Mapping[str, Any]) -> None：从先前保存的状态映射中恢复智能体的状态 17。
○async close(self) -> None：当运行时关闭时调用，允许智能体执行任何必要的清理操作 17。
autogen_core.BaseAgent 是一个实现了 Agent 协议的抽象基类 (ABC)。它为一些通用方法提供了默认实现，并引入了 async on_message_impl(self, message: Any, ctx: MessageContext) -> Any 这个抽象方法，具体的智能体子类必须覆盖此方法以定义其特定的消息处理逻辑 17。
智能体作为状态化的、消息驱动的行动者
AutoGen 中的智能体被设计为响应消息并管理其内部状态的实体。框架通过 AgentRuntime (后续章节详述) 提供了通信和生命周期管理的基础设施 19。这种关注点分离（智能体逻辑 vs. 运行时管理）类似于行动者模型（Actor Model），简化了并发和分布式多智能体系统的开发。开发者可以专注于智能体的核心反应逻辑和状态维护，而将消息传递、智能体创建和销毁等任务交给运行时处理。这种设计使得智能体能够跨交互保持上下文、记忆或学习到的信息，这对于完成复杂任务和实现持续学习至关重要。
3.2. 消息传递: autogen_core.models 与 autogen_agentchat.messages
消息传递是 AutoGen 中智能体间通信的主要方式。AutoGen 定义了清晰的消息结构，以支持丰富和可靠的交互。
●核心消息基元 (autogen_core.models) 20：
此模块定义了与 LLM 交互的基础消息类型，通常作为 Pydantic 模型实现，提供了数据验证和结构化。
○UserMessage(content: str | List[Union[str, Image]], source: str, type: Literal['UserMessage'])：代表来自用户或扮演用户角色的智能体的输入。内容可以是文本字符串，也可以是包含文本和 autogen_core.Image 对象的列表，以支持多模态输入。source 字段标识消息发送者。
○AssistantMessage(content: str | List[FunctionCall], thought: Optional[str], source: str, type: Literal['AssistantMessage'])：代表由 LLM 驱动的助手生成的消息。内容可以是文本，也可以是 FunctionCall 对象列表（表示工具调用请求）。可选的 thought 字段可用于记录模型的思考过程。
○SystemMessage(content: str, type: Literal)：由开发者提供给 LLM 的指令或信息，用于设置上下文、角色或行为准则。
○FunctionCall(id: str, arguments: str, name: str)：(定义于 autogen_core._types，常用于 AssistantMessage) 代表 LLM 请求调用特定函数或工具。id 是调用标识，arguments 是 JSON 格式的参数字符串，name 是函数名。
○FunctionExecutionResult(call_id: str, content: str, is_error: Optional[bool], name: str)：代表单个函数调用的输出结果。is_error 标识执行是否出错。
○FunctionExecutionResultMessage(content: List, type: Literal)：代表执行一个或多个函数调用后的结果集合。
●AgentChat 消息类型 (autogen_agentchat.messages) 21：
autogen_agentchat 模块在核心消息类型的基础上，针对多智能体对话场景定义了更专门化的消息类型。这些类型通常继承自 BaseChatMessage 或 BaseAgentEvent。
○TextMessage: 用于传递基本文本内容。
○MultiModalMessage: 用于包含文本和图像等多种模态内容的消息。
○ToolCallRequestEvent: 表示智能体请求执行工具调用的事件。
○ToolCallExecutionEvent: 表示工具调用执行完成的事件，通常包含执行结果。
○StructuredMessage: 其内容是一个 Pydantic 模型实例，用于传递结构化数据。
○HandoffMessage: 用于在智能体之间显式传递控制权。
○StopMessage: 用于指示对话或任务的终止。
○AgentEvent: 智能体内部事件的基类，如 ThoughtEvent, MemoryQueryEvent 等，用于观测智能体内部状态和行为，而非直接用于智能体间通信。
●通用消息字段：
大多数消息包含 content 字段，以及一个表示发送者或消息类型的 source (对于 autogen_core.models 中的消息) 或 role 字段。metadata 字段也常用于携带额外信息。
●MessageContext (autogen_core) 17：
为每条被处理的消息提供关键的上下文信息，包括：
○sender: Optional[AgentId]：发送消息的智能体 ID。
○topic_id: Optional：消息发布到的主题 ID (用于发布/订阅模式)。
○is_rpc: bool：指示消息是否为远程过程调用 (RPC)。
○cancellation_token: Optional：用于取消操作的令牌。
○message_id: Optional[str]：消息的唯一标识符。
类型化与结构化消息传递的优势
AutoGen 对消息类型的精心设计，特别是广泛使用 Pydantic 模型，为多智能体通信带来了显著优势。简单的字符串通信不足以支撑需要交换结构化数据、工具调用、多模态内容或控制信号的复杂系统。通过定义特定的消息类，AutoGen 强制了通信的模式，确保消息具有预期的字段和数据类型。这种结构化方法带来了：
1.清晰性：消息的意图和内容更加明确。
2.验证：Pydantic 模型自动进行数据验证，减少了因格式错误导致的问题。
3.路由与处理：智能体和运行时可以更容易地根据消息类型决定如何处理消息。
4.可扩展性：可以通过定义新的消息类型来引入新的交互模式和功能。 这种设计使得开发者能够构建更健壮、更易于维护的多智能体系统，实现比简单文本交换更丰富的交互，并为复杂通信协议提供了坚实的基础。
3.3. 模型交互: autogen_core.models.ChatCompletionClient 接口
autogen_core.models.ChatCompletionClient 是一个抽象基类 (ABC)，它定义了与聊天完成模型 (Chat Completion Models) 交互的统一接口 20。所有具体的模型客户端实现（例如 OpenAI、Azure OpenAI、Anthropic 等的客户端）都应遵循此接口。
●核心方法 20：
○async create(*messages: Sequence[LLMMessage], tools: Sequence] =, json_output: Union, None] = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: Optional = None) -> CreateResult： 此异步方法用于从模型生成单个响应。它接收一系列 LLMMessage 对象（代表对话历史）、可选的 tools 列表（用于函数调用）、json_output 参数（用于控制是否以及如何生成 JSON 格式的输出，可以是一个布尔值或一个 Pydantic 模型类型），以及其他特定于模型的参数 (extra_create_args) 和一个取消令牌。它返回一个 CreateResult 对象。
○async create_stream(*messages: Sequence[LLMMessage], tools: Sequence] =, json_output: Union, None] = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: Optional = None) -> AsyncGenerator, None]： 此异步方法用于从模型创建响应流。它允许逐步接收和处理模型生成的内容，而不是等待完整的响应。参数与 create 方法类似。它返回一个异步生成器，可以逐块产生字符串响应或最终的 CreateResult。
○async count_tokens(*messages: Sequence[LLMMessage], tools: Sequence] =) -> int： 此异步方法用于计算给定消息序列和工具列表所占用的 token 数量。这对于管理上下文长度和估算成本非常重要。
○async actual_usage() -> RequestUsage：获取上次 API 调用的实际 token 使用情况。
○async total_usage() -> RequestUsage：获取当前会话或客户端实例的总 token 使用情况。
○async remaining_tokens(*messages: Sequence[LLMMessage], tools: Sequence] =) -> int：估算给定上下文剩余可用 token 的数量。
○async close() -> None：关闭客户端及相关资源。
●关键属性 20：
○capabilities: ModelCapabilities：返回底层模型能力的抽象属性（例如，是否支持函数调用、JSON 输出、视觉等）。
○model_info: ModelInfo：返回模型详细信息的抽象属性，包括其家族和支持的特性。
●CreateResult 对象 20：
create 或 create_stream 的最终结果。包含：
○finish_reason: Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown']：模型停止生成的原因。
○content: Union[str, List[FunctionCall]]：模型生成的内容，可以是文本或函数调用列表。
○usage: RequestUsage：本次调用的 token 使用情况 (prompt_tokens, completion_tokens)。
○cached: bool：响应是否来自缓存。
○logprobs: Optional]：token 的对数概率（如果可用）。
○thought: Optional[str]：模型的思考过程文本（如果模型提供）。
●RequestUsage 对象 20：
存储 API 请求的 token 使用情况，包含 prompt_tokens 和 completion_tokens。
通过抽象出 ChatCompletionClient 接口，AutoGen 使得更换或集成新的 LLM 服务变得更加容易，开发者可以编写与特定模型实现解耦的智能体逻辑。具体的客户端实现在 autogen_ext.models 包中提供，例如 OpenAIChatCompletionClient 16、AzureOpenAIChatCompletionClient 16 等。
3.4. 工具定义与使用: autogen_core.tools.BaseTool 与 FunctionTool
工具 (Tools) 是 AutoGen 智能体执行动作、与外部世界交互或执行特定计算的关键机制。autogen_core.tools 模块提供了定义和使用工具的基础。
●BaseTool 21：
所有工具类的抽象基类。它定义了工具的基本结构和元数据。
○name: str：工具的唯一名称。
○description: str：对工具功能的描述，供 LLM 理解何时使用该工具。
○schema: ToolSchema：工具的 JSON Schema，描述了工具的输入参数。这是 LLM 生成正确函数调用的关键。
○async run(self, args: P, cancellation_token: Optional = None) -> R：执行工具的异步方法。P 是输入参数类型，R 是返回类型。
○async run_json(self, args: Mapping[str, Any], cancellation_token: Optional = None) -> Any：以 JSON (字典) 形式接收参数并执行工具。
●FunctionTool 21：
一个便捷的类，用于将普通的 Python 函数（同步或异步）包装成 AutoGen 工具。
○构造函数参数：
■func: Callable]]：要包装的 Python 函数。
■description: Optional[str]：工具的描述。如果未提供，会尝试从函数的文档字符串中提取。
■name: Optional[str]：工具的名称。如果未提供，默认为函数名。
■strict: bool = False：如果为 True，工具模式将只包含函数签名中明确定义的参数，不允许默认值。当模型使用结构化输出模式时，必须设置为 True 23。
■global_imports: Optional]]: 全局导入，确保函数执行环境的依赖。
○模式生成：FunctionTool 会自动根据函数的类型注解 (type hints) 和文档字符串 (docstring) 生成 JSON Schema。类型注解（尤其是 typing_extensions.Annotated）对于精确描述参数至关重要 22。例如，Annotated 可以指导 LLM 参数的格式。
○用法示例 (get_stock_price 函数) 22：
Python
from autogen_core.tools import FunctionTool
from typing_extensions import Annotated
import random

async def get_stock_price(ticker: str, date: Annotated) -> float:
    """Get the stock price for a given ticker and date."""
    return random.uniform(100, 200)

stock_price_tool = FunctionTool(
    func=get_stock_price,
    description="Fetches the stock price for a specific ticker on a given date."
)
# stock_price_tool.schema 可以获取生成的 JSON schema

●工具模式 (Schema)：
工具的 schema (通常是 JSON Schema 格式) 描述了工具的名称、功能以及最重要的——它期望的输入参数（名称、类型、是否必需、描述等）。LLM 使用此 schema 来决定：
1.在特定情境下是否应该调用某个工具。
2.如果调用，如何构造传递给工具的参数。 AutoGen 的模型客户端（如 OpenAIChatCompletionClient）会将工具 schema 发送给 LLM，LLM 则根据 schema 生成结构化的函数调用请求 22。
●工具执行流程 22：
1.智能体（通常是 AssistantAgent）将用户消息和可用工具的 schema 发送给 LLM。
2.LLM 如果认为需要使用工具，会生成一个或多个 FunctionCall 对象，包含工具名和参数。
3.AutoGen 框架或智能体本身捕获这些 FunctionCall。
4.框架查找并执行相应的 FunctionTool（或 BaseTool）的 run 或 run_json 方法。
5.工具执行的结果（通常包装在 FunctionExecutionResult 或 FunctionExecutionResultMessage 中）返回给 LLM。
6.LLM 参考工具执行结果，生成最终回复或决定下一步行动（可能调用更多工具）。
通过这种机制，AutoGen 智能体可以利用外部函数和 API 来扩展其能力，完成信息检索、数据操作、与外部系统交互等任务。
3.5. 代码执行: autogen_core.code_executor.CodeExecutor 接口
AutoGen 提供了强大的代码执行能力，允许智能体生成代码并执行它以解决问题或与环境交互。autogen_core.code_executor.CodeExecutor 是代码执行器的基础接口 19。
●CodeExecutor 接口：
这是一个抽象基类，定义了代码执行器必须实现的方法。
○async execute_code_blocks(self, code_blocks: List, cancellation_token: Optional = None) -> CodeResult： 核心方法，接收一个 CodeBlock 对象列表并异步执行它们。返回一个 CodeResult 对象，其中包含执行的退出码 (exit_code) 和输出 (output)。
○async start(self) -> None 和 async stop(self) -> None：实验性方法，用于启动和停止执行器，管理其生命周期，例如启动或关闭 Docker 容器或 Jupyter 内核 4。
○work_dir: Path：执行器的工作目录。
○timeout: int：执行超时时间。
●CodeBlock 数据类 19：
用于表示一个代码块，包含两个字段：
○code: str：要执行的代码字符串。
○language: str：代码的语言（例如 "python", "sh", "bash"）。
●CodeResult 数据类 21：
表示代码执行的结果，包含：
○exit_code: int：执行的退出码（0 通常表示成功）。
○output: str：代码执行的标准输出和标准错误合并的字符串。
○某些执行器（如 JupyterCodeExecutor）可能会返回更具体的子类，如 JupyterCodeResult，其中可能包含额外的输出文件路径等信息 26。
●代码执行器实现：
AutoGen 在 autogen_ext.code_executors 包中提供了多种 CodeExecutor 的实现，以适应不同的安全和功能需求：
○LocalCommandLineCodeExecutor (autogen_ext.code_executors.local)：在本地机器的命令行中执行代码。强烈建议谨慎使用此执行器，因为它直接在宿主环境执行 LLM 生成的代码，存在潜在的安全风险 19。
○DockerCommandLineCodeExecutor (autogen_ext.code_executors.docker)：在 Docker 容器内执行代码。这是推荐的方式，因为它提供了沙箱环境，增强了安全性 19。可以指定 Docker 镜像、工作目录、端口映射等。
○JupyterCodeExecutor (autogen_ext.code_executors.jupyter)：在 Jupyter 内核中执行代码，支持状态保持（变量和导入在连续的代码块执行之间保持有效）21。
○DockerJupyterCodeExecutor (autogen_ext.code_executors.docker_jupyter)：结合了 Docker 的隔离性和 Jupyter 内核的状态保持能力 21。
○ACADynamicSessionsCodeExecutor (autogen_ext.code_executors.azure)：在 Azure Container Apps 动态会话中执行代码，提供无服务器、安全的执行环境 21。
代码执行流程与安全考量
1.代码生成：一个智能体（如 AssistantAgent）根据任务需求生成代码块。
2.执行请求：该智能体或另一个专门的执行智能体（如 CodeExecutorAgent）将代码块传递给配置好的 CodeExecutor。
3.沙箱执行：强烈建议使用如 DockerCommandLineCodeExecutor 这样的沙箱化执行器，以隔离 LLM 生成的代码，防止潜在的恶意行为或意外的系统修改 19。LocalCommandLineCodeExecutor 风险较高，应避免在生产环境或处理不可信代码时使用。
4.结果返回：执行器返回 CodeResult，包含执行的输出和状态。
5.迭代与调试：智能体可以根据执行结果（例如错误信息）来调试和修改代码，形成一个代码生成-执行-反馈的循环 19。
AutoGen 的代码执行框架设计灵活，允许开发者根据应用场景的安全需求和功能需求选择合适的执行器。
3.6. autogen_core.AgentRuntime：智能体生命周期与通信管理
AgentRuntime 是 AutoGen 核心框架的支柱，负责管理智能体的生命周期、消息传递以及其他运行时服务 17。它为智能体提供了一个运行环境，使它们能够相互通信并与外部世界فاعل。
●核心职责：
○智能体生命周期管理：
■注册与创建：通过 register_factory 方法注册智能体类型及其工厂函数。运行时使用这些工厂函数按需创建智能体实例 17。BaseAgent 类提供了 register 类方法以简化此过程。
■状态管理：AgentRuntime 提供了保存和加载整个运行时状态（包括所有托管智能体）的方法 (save_state, load_state)，以及针对单个智能体的状态管理方法 (agent_save_state, agent_load_state) 17。
■关闭：在运行时关闭时，会调用每个智能体的 close 方法进行清理 17。
○消息传递：
■点对点发送：send_message(message, recipient, sender=None,...) 方法允许一个智能体向另一个特定的智能体发送消息并等待响应（如果适用）17。
■发布/订阅：publish_message(message, topic_id, sender=None,...) 方法允许智能体将消息发布到一个主题 (TopicId)，所有订阅了该主题的智能体都会收到此消息 17。
■订阅管理：通过 add_subscription 和 remove_subscription 管理智能体对特定主题的订阅。Subscription 协议（及其实现如 TypeSubscription, DefaultSubscription）定义了消息如何匹配到订阅者 17。
○消息序列化：通过 add_message_serializer 支持自定义消息序列化器 (MessageSerializer)，以处理不同内容类型的消息负载 17。
●关键组件交互：
○AgentId：作为智能体在运行时的唯一地址，用于消息路由 17。
○MessageContext：在消息传递给智能体的 on_message 方法时，由运行时填充并提供，包含发送者、主题等上下文信息 17。
○CancellationToken：用于在异步操作（如消息处理）中传递取消信号 17。
●具体实现：
○SingleThreadedAgentRuntime：一个具体的 AgentRuntime 实现，它在单个 asyncio 队列上处理所有消息。适用于开发和不需要高并发的独立应用 17。
○GrpcWorkerAgentRuntime (autogen_ext.runtimes.grpc)：允许将智能体作为 gRPC 服务运行，支持分布式部署和跨语言通信 21。
AgentRuntime 的设计将智能体的业务逻辑与其运行环境的复杂性（如消息分发、状态持久化、并发控制）分离开来。这使得开发者可以专注于智能体本身的行为，同时 AutoGen 框架负责底层的通信和管理。这种抽象对于构建可扩展和可维护的多智能体系统至关重要。
4. AutoGen AgentChat (v0.5.6)
autogen_agentchat 是构建在 autogen_core 之上的一个更高级、更具主见性的 API 层，专为快速原型设计和构建交互式多智能体应用而设计 2。它封装了许多核心API的复杂性，提供了更易于使用的组件，如特定类型的智能体和团队管理机制。
4.1. AgentChat 概览
AgentChat API 旨在简化常见多智能体模式（如双智能体聊天、群聊）的实现。它提供了预构建的智能体类型和团队结构，使得开发者可以快速搭建能够进行对话、使用工具和执行代码的智能体系统。
●核心组件：
○Agents (autogen_agentchat.agents)：如 AssistantAgent, UserProxyAgent, CodeExecutorAgent。
○Teams (autogen_agentchat.teams)：如 RoundRobinGroupChat, SelectorGroupChat, 以及新增的 GraphFlow。
○Messages (autogen_agentchat.messages)：对话中使用的特定消息类型。
○Tools (autogen_agentchat.tools)：与 AgentChat 智能体集成的工具。
○Model Clients: 通过 autogen_ext.models 模块与 LLM 交互。
○Code Executors: 通过 autogen_ext.code_executors 模块执行代码。
○Base Components (autogen_agentchat.base)：定义了如 ChatAgent, Team, TerminationCondition 等基础抽象。
○Conditions (autogen_agentchat.conditions)：预定义的对话终止条件。
○UI (autogen_agentchat.ui)：如 Console，用于在控制台显示对话流。
○State (autogen_agentchat.state)：用于管理智能体和团队状态的类。
21
4.2. ConversableAgent (历史角色与 v0.5.6 中的对应)
在 AutoGen 的早期版本 (如 v0.2) 中，ConversableAgent 是一个核心类，作为可对话智能体的通用基类，能够配置为助手或用户代理 3。它具备发送和接收消息、自动回复、集成 LLM、执行代码以及处理人类输入等多种功能 3。
●v0.2 中的关键参数 35：
○name: str
○system_message: str
○llm_config: dict：用于配置 LLM。
○human_input_mode: str：控制人类输入的模式 ("ALWAYS", "TERMINATE", "NEVER")。
○code_execution_config: dict：配置代码执行。
○max_consecutive_auto_reply: int：最大连续自动回复次数。
●v0.2 中的重要方法 35：
○initiate_chat(recipient, message,...)：发起与另一个智能体的聊天。
○send(message, recipient,...)：向另一个智能体发送消息。
○receive(message, sender,...)：接收来自另一个智能体的消息。
○generate_reply(messages, sender,...)：生成回复。
○register_reply(trigger, reply_func,...)：注册自定义回复函数。
v0.4 及更高版本 (包括 0.5.6) 中的演变 35：
从 AutoGen v0.4 开始，ConversableAgent 的功能被重构并分散到更专门化的组件中。AgentChat API 提供了新的抽象和实现：
●基础智能体抽象：autogen_agentchat.base.ChatAgent 成为了 AgentChat 中可对话智能体的基础协议 (Protocol) 或抽象基类 39。它定义了如 on_messages, on_reset, save_state, load_state 等核心方法。
●具体智能体实现：
○AssistantAgent 和 UserProxyAgent 现在直接或间接地基于 ChatAgent (或其内部的 BaseChatAgent) 构建，分别承担了 LLM 驱动的助手和人类用户代理的角色。
○llm_config 被 model_client (一个 ChatCompletionClient 实例) 取代 35。
○human_input_mode 的功能主要体现在 UserProxyAgent 的设计和 input_func 参数上 29。
○代码执行能力被更明确地分离到 CodeExecutorAgent 或通过为 AssistantAgent 配置 code_executor 来实现 29。
○register_reply 的灵活自定义回复逻辑，现在可以通过创建自定义智能体并重写其 on_messages 方法来实现 35。
○对话的发起和管理更多地由团队（如 RoundRobinGroupChat, SelectorGroupChat, GraphFlow）的 run() 或 run_stream() 方法处理。
因此，在 AutoGen 0.5.6 的语境下，虽然 ConversableAgent 的概念（即可进行对话的智能体）依然核心，但其具体的 API 和实现已经演变为以 autogen_agentchat.base.ChatAgent 为基础，并通过 AssistantAgent, UserProxyAgent 等具体类以及团队机制来体现。开发者应主要关注这些新的组件。
4.3. UserProxyAgent
UserProxyAgent 在 AgentChat 中扮演人类用户的代理角色 29。它不直接与 LLM 交互生成回复，而是用于从人类用户处获取输入，或在特定条件下执行代码。
●核心目的：
○模拟人类输入：允许人类用户参与到多智能体对话中，提供指令、反馈或数据。
○代码执行（可选）：在某些配置下（主要在旧版本或特定场景中），它可以执行收到的代码块。在 v0.4+ 中，代码执行更倾向于由 CodeExecutorAgent 或 AssistantAgent 配置代码执行器来处理。
●关键参数 (autogen_agentchat.agents.UserProxyAgent) 29：
○name: str：智能体的名称。
○description: str = "A human user"：智能体的描述。
○input_func: Callable], Awaitable[str]] | Callable[[str], str] | None = None： 一个函数，用于获取用户输入。它接收一个提示字符串 (prompt) 和一个可选的 CancellationToken，并应返回用户输入的字符串。如果未提供，默认行为可能是从控制台读取输入。这是实现人机回圈的关键。
■人机回圈 (HIL)：input_func 的设计直接关系到 HIL 的实现。它可以是一个简单的 input() 调用，也可以是一个更复杂的函数，例如等待来自 Web UI 的输入 41。
○code_execution_config (在 v0.2 中重要，v0.4+ 中行为有所变化)：在旧版本中，此参数用于配置代码执行器。在较新版本中，UserProxyAgent 本身通常不直接配置为执行代码，而是依赖其他智能体或机制。
○human_input_mode (在 v0.2 中重要，v0.4+ 中通过 input_func 和团队逻辑控制)：旧版本中用于控制何时请求人类输入。在新版本中，UserProxyAgent 是否以及何时被调用以获取输入，更多地由其在团队（如 RoundRobinGroupChat 或 SelectorGroupChat）中的角色和团队的控制逻辑决定 41。
●重要方法 (autogen_agentchat.agents.UserProxyAgent) 29：
○async on_messages(self, messages: Sequence, cancellation_token: CancellationToken) -> Response： 处理传入消息。对于 UserProxyAgent，这通常意味着向用户显示收到的消息（或其摘要），然后调用 input_func 来获取用户的回复。返回一个包含用户输入的 Response 对象（通常是 TextMessage）。
○async on_messages_stream(self, messages: Sequence, cancellation_token: CancellationToken) -> AsyncGenerator, None]： 与 on_messages 类似，但以异步生成器的方式流式处理用户交互和响应。
○async on_reset(self, cancellation_token: Optional = None) -> None：重置智能体状态。
●使用模式：
○在团队中作为人类接口：将 UserProxyAgent 添加到智能体团队中，使其能够在对话流程的特定点（由团队逻辑决定）暂停并请求人类输入 41。
○阻塞行为：当 UserProxyAgent 被调用以获取输入时，它会阻塞团队的执行，直到用户提供输入或操作被取消（例如通过 CancellationToken）41。因此，对于需要长时间等待人类响应的场景，建议使用允许团队终止并稍后从保存的状态恢复的模式，而不是让 UserProxyAgent 长时间阻塞。
UserProxyAgent 是实现人机协作和监督的关键组件，使得复杂的 AI 系统能够接受人类的指导和干预。
4.4. AssistantAgent
AssistantAgent 是 AgentChat 中核心的 LLM 驱动的智能体，设计用于提供帮助、执行任务，并且能够使用工具 29。它是构建能够与 LLM 交互并执行复杂操作的智能应用的基础。
●核心能力：
○LLM 交互：通过配置的 model_client 与大型语言模型通信，生成文本回复、进行推理。
○工具使用 (Function Calling)：可以注册和使用工具（Python 函数或 BaseTool 实例）来执行特定操作，如信息检索、API 调用、代码执行等。
○结构化输出：能够生成结构化的输出（例如 JSON），而不仅仅是文本，通过配置 output_content_type 为 Pydantic 模型实现。
○状态保持：AssistantAgent 是有状态的，它会维护对话历史（通过 model_context），并在多次调用之间保持其状态。
○流式响应：支持通过 model_client_stream 参数和 on_messages_stream 方法实现流式输出模型响应。
●关键参数 (autogen_agentchat.agents.AssistantAgent) 29：
○name: str：智能体的名称。
○model_client: ChatCompletionClient：用于与 LLM 交互的模型客户端实例 (例如 OpenAIChatCompletionClient)。
○tools: Optional]]] = None：注册给智能体使用的工具列表。可以是 BaseTool 实例或 Python 函数。
○workbench: Optional[Workbench] = None：一个可选的工作台，用于更结构化的工具使用。如果设置了 workbench，则不能同时使用 tools 参数。
○handoffs: Optional[List[Union[Handoff, str]]] = None：配置智能体在特定条件下将对话移交给其他智能体。
○model_context: Optional[ChatCompletionContext] = None：用于管理发送给 LLM 的消息历史的上下文对象。例如 BufferedChatCompletionContext 或 TokenLimitedChatCompletionContext 用于限制上下文大小。
○description: str = "An agent that provides assistance with ability to use tools."：智能体的描述。
○system_message: Optional[str] = "You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed."：发送给 LLM 的系统消息，用于设定其角色和行为。
○model_client_stream: bool = False：如果为 True，模型客户端将以流式模式工作。
○reflect_on_tool_use: Optional[bool] = None：如果为 True（当设置了 output_content_type 时默认为 True），智能体在使用工具后会进行一次额外的模型推理以优化响应。
○tool_call_summary_format: str = "{result}"：用于格式化工具调用结果摘要的字符串模板。
○output_content_type: Optional] = None：指定结构化输出的 Pydantic 模型类型。
○memory: Optional] = None：用于跨交互保留信息的内存存储。
○metadata: Optional] = None：可选的元数据。
●重要方法 (autogen_agentchat.agents.AssistantAgent) 29：
○async on_messages(self, messages: Sequence, cancellation_token: CancellationToken) -> Response： 处理传入消息序列。核心逻辑包括：将消息添加到 model_context，调用 model_client.create() 获取 LLM 响应。如果 LLM 返回工具调用请求 (FunctionCall)，则执行这些工具。如果配置了 reflect_on_tool_use，则在工具执行后再次调用 LLM。最终返回一个 Response 对象，其中 chat_message 包含最终的回复（可能是 TextMessage, StructuredMessage, 或 ToolCallSummaryMessage）。
○async on_messages_stream(self, messages: Sequence, cancellation_token: CancellationToken) -> AsyncGenerator, None]： 与 on_messages 功能类似，但以异步生成器方式流式返回内部事件、消息和最终响应。如果 model_client_stream 为 True，还会产生 ModelClientStreamingChunkEvent。
○async run(self, *, task: Union, None] = None, cancellation_token: Optional = None) -> TaskResult： 便捷方法，调用 on_messages 并包装结果为 TaskResult。
○async run_stream(self, *, task: Union, None] = None, cancellation_token: Optional = None) -> AsyncGenerator, None]： 便捷方法，调用 on_messages_stream 并包装结果为 TaskResult。
○async on_reset(self, cancellation_token: Optional = None) -> None：重置智能体状态，主要是其 model_context。
○async save_state(self) -> Mapping[str, Any] 和 async load_state(self, state: Mapping[str, Any]) -> None：保存和加载智能体状态，主要涉及 model_context。
●使用模式与注意事项：
○状态性：AssistantAgent 是有状态的。在连续调用其方法时，应只传递新的消息，而不是整个对话历史，因为历史已由 model_context 维护。
○非并发安全：AssistantAgent 不是线程安全或协程安全的，不应在多个任务间共享或并发调用其方法。
○工具执行：默认情况下，工具调用结果会作为响应返回。如果工具不返回格式良好的自然语言字符串，可以设置 reflect_on_tool_use=True 让模型对工具输出进行总结 42。
○移交行为 (Handoff)：如果触发了移交，将返回 HandoffMessage。在移交前，工具调用会被执行，其结果会通过上下文传递给目标智能体 29。
AssistantAgent 是 AutoGen 中构建能够执行复杂任务、利用外部知识和能力的 AI 助手的核心。通过灵活配置其模型客户端、工具、系统消息和上下文管理，开发者可以创建出功能强大的智能体。
4.5. CodeExecutorAgent
CodeExecutorAgent 是 AgentChat 中专门用于执行代码的智能体 21。它可以接收包含代码块的消息，执行这些代码，并返回执行结果。它也可以配置为使用 LLM 来生成代码或在代码执行出错时进行反思和重试。
●核心能力：
○代码执行：核心功能是执行在消息中找到的代码块（如 Python、shell 脚本）。
○代码生成 (可选)：如果配置了 model_client，它可以利用 LLM 生成代码以响应任务。
○错误反思与重试 (可选)：当配置了 model_client 且模型支持结构化输出时，如果代码执行失败，它可以进行多次重试，并在每次失败后可能让 LLM 反思错误并生成修正后的代码 4。
●关键参数 (autogen_agentchat.agents.CodeExecutorAgent) 29：
○name: str：智能体的名称。
○code_executor: CodeExecutor：负责执行代码的代码执行器实例。强烈推荐使用 DockerCommandLineCodeExecutor 以确保安全 29。
○model_client: Optional[ChatCompletionClient] = None：用于代码生成和错误反思的模型客户端。如果未提供，智能体仅执行在输入消息中找到的代码块。
○model_client_stream: bool = False：如果为 True 且配置了 model_client，则以流式模式使用模型客户端。
○description: Optional[str] = DEFAULT_AGENT_DESCRIPTION：智能体的描述。
○system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE：用于模型客户端的系统消息，仅在提供了 model_client 时使用。
○sources: Optional] = None：一个字符串序列，指定只检查来自这些源智能体的消息中的代码块进行执行。如果未提供 model_client，则检查所有消息。
○max_retries_on_error: int = 1：代码执行出错时的最大重试次数。仅在提供了 model_client 时生效。
●重要方法 (autogen_agentchat.agents.CodeExecutorAgent)：
继承自 BaseChatAgent，其核心逻辑主要在 on_messages 和 on_messages_stream 中实现。
○当接收到消息时，如果配置了 model_client，它可能会先调用 LLM 生成代码。
○然后，它会从消息中提取代码块（根据 sources 参数过滤，如果 model_client 未配置）。
○使用提供的 code_executor 执行代码块。
○如果执行成功，返回包含代码执行结果的 ToolCallExecutionEvent。
○如果执行失败且配置了 model_client 和 max_retries_on_error > 0，它会尝试让 LLM 反思错误并生成新代码进行重试，直到达到最大重试次数或成功。
●使用模式：
○作为专用代码执行者：在多智能体团队中，CodeExecutorAgent 可以充当一个专门的服务，负责安全地执行由其他智能体（如 AssistantAgent）生成的代码。
○结合代码生成：通过配置 model_client，使其不仅能执行代码，还能根据任务描述生成代码。
○安全执行环境：务必为其配置一个安全的 CodeExecutor，如 DockerCommandLineCodeExecutor，以防止 LLM 生成的潜在恶意代码对宿主系统造成损害 19。
CodeExecutorAgent 通过将代码执行能力封装在一个专门的智能体中，增强了 AutoGen 应用的安全性和模块化。
5. 多智能体协作与 AgentChat
AutoGen 的核心优势之一在于其支持复杂的多智能体协作。AgentChat API 提供了多种机制来组织和管理智能体团队，实现协同任务解决。
5.1. GroupChat 与 GroupChatManager 概念 (历史与当前)
在 AutoGen 的早期版本 (v0.2) 中，GroupChat 和 GroupChatManager 是实现多智能体群聊的核心类 44。GroupChat 通常包含一组智能体，而 GroupChatManager (通常是 ConversableAgent 的一个实例) 负责协调对话流程，例如选择下一个发言者，并将消息广播给其他成员 44。
在 AutoGen v0.4 及更高版本 (包括 0.5.6) 中，这些概念演变成了更专门化和结构化的团队类，它们构建于 autogen_agentchat.base.Team 和 autogen_agentchat.base.ChatAgent 之上 35。虽然底层的管理和协调逻辑依然存在，但 API 层面提供了更直接的团队抽象。
●autogen_agentchat.base.BaseGroupChat 46： 这是 AgentChat 中所有群聊团队的基类。它定义了群聊团队的通用接口和功能，如参与者管理、终止条件、运行和流式处理对话等。
○参数：participants (智能体列表), termination_condition, max_turns, custom_message_types, emit_team_events 等。
○核心方法：run, run_stream, reset, save_state, load_state。
5.2. RoundRobinGroupChat
RoundRobinGroupChat 是一种简单的群聊团队，其中参与的智能体按照预定义的顺序轮流发言 2。
●角色：实现一个智能体按顺序发言的群聊。如果只有一个参与者，则该参与者将是唯一的发言者。
●参数 (autogen_agentchat.teams.RoundRobinGroupChat) 46：
○继承自 BaseGroupChat 的参数，如 participants (一个 BaseChatAgent 列表), termination_condition, max_turns。
○custom_message_types: 可用于群聊的自定义消息类型列表。
○emit_team_events: 是否通过 run_stream() 发出团队事件。
●工作机制：当团队通过 run() 或 run_stream() 接收到任务时，它会按顺序提示每个参与的智能体做出响应，并将响应广播给所有其他参与者，直到满足终止条件。
●适用场景：适用于需要每个智能体按固定顺序贡献的简单协作任务。
5.3. SelectorGroupChat
SelectorGroupChat 是一种更高级的群聊团队，它使用一个聊天完成模型 (LLM) 或自定义函数来动态选择下一位发言者 1。
●角色：实现一个动态的、上下文感知的群聊，其中发言顺序由模型根据对话历史和智能体描述智能决定。
●关键特性 47：
○基于模型的发言者选择。
○可配置的参与者角色和描述 (这些描述对于模型选择至关重要)。
○可选择阻止同一发言者连续发言。
○可自定义选择提示 (selector_prompt)。
○可自定义选择函数 (selector_func) 以覆盖默认的基于模型的选择。
○可自定义候选函数 (candidate_func) 以在模型选择前筛选候选发言者。
●参数 (autogen_agentchat.teams.SelectorGroupChat) 46：
○继承自 BaseGroupChat 的参数。
○model_client: ChatCompletionClient：用于选择下一位发言者的 LLM 客户端。
○selector_prompt: str：指导模型如何选择发言者的提示模板。可以包含占位符如 {roles}, {history}, {participants}。
○allow_repeated_speaker: bool = False：是否允许同一发言者连续发言。
○max_selector_attempts: int = 3：模型选择发言者的最大尝试次数。
○selector_func: Optional], Optional[str]]]] = None：自定义发言者选择函数。
○candidate_func: Optional], List[str]]]] = None：自定义候选发言者筛选函数。
○model_client_streaming: bool = False：发言者选择时是否使用流式模型客户端。
●工作机制 47：
1.团队分析当前对话上下文（历史消息、智能体名称和描述）。
2.使用 model_client 和 selector_prompt (或 selector_func) 来确定下一位发言者。
3.提示选定的智能体提供响应。
4.将响应广播给所有其他参与者。
5.检查终止条件，如果未满足则重复步骤1。
●与 GraphFlow 的关系：GraphFlow 在 AutoGen 0.5.6 中被引入，作为一种更强大的、基于有向图的工作流定义方式，可以看作是 SelectorGroupChat 的一种演进，其中选择逻辑由图结构和边条件明确定义，而不是完全依赖 LLM 或单个选择函数 1。
5.4. GraphFlow (Workflows) - AutoGen 0.5.6 新特性
GraphFlow 是 AutoGen 0.5.6 中引入的一项实验性特性，它允许开发者使用有向图来定义和控制多智能体工作流的执行顺序和条件 1。这为构建结构化、可预测且高度可控的多智能体交互提供了强大的机制。
●核心概念 5：
○GraphFlow 类：代表多智能体工作流的团队。它接收参与智能体的列表和一个定义执行流程的 DiGraph 对象。
○DiGraph (有向图)：图中的每个节点代表一个智能体，边代表智能体之间的允许执行路径。
○DiGraphBuilder：一个辅助类，用于以编程方式构建 DiGraph。它提供了添加节点（智能体）和边（执行路径，可带条件）的流畅接口。
○结构化执行：与传统的群聊相比，GraphFlow 提供了对智能体交互顺序的严格控制。
○支持的流程模式：顺序链、并行扇出、条件分支和带安全退出条件的循环 5。
●设计与实现 5：
1.智能体创建：首先创建将参与工作流的各个 AssistantAgent 或自定义智能体实例。
2.图构建 (DiGraphBuilder)：
■builder.add_node(agent)：将智能体添加为图中的节点。
■builder.add_edge(source_agent, target_agent, condition: Optional[str] = None)：在智能体之间添加有向边。condition 是一个可选字符串，如果提供，则只有当源智能体的最后一条消息内容包含此字符串时，流程才会沿着这条边进行。
■builder.set_entry_point(agent)：如果图中没有源节点（没有入边的节点），则需要设置一个入口点。
3.图的最终确定：调用 graph = builder.build() 来生成并验证图结构。
4.GraphFlow 实例化：创建 GraphFlow 对象，传入参与者列表和构建好的 graph：flow = GraphFlow(participants=builder.get_participants(), graph=graph)。
5.运行工作流：使用 flow.run_stream(task="...") 启动工作流。此方法返回一个异步事件流。可以使用 autogen_agentchat.ui.Console 来更好地在控制台显示输出。
●节点类型：
在 GraphFlow 中，图的节点主要是参与工作流的智能体 5。
●边条件：
边可以附带条件，这些条件基于源智能体产生的消息内容。例如，一条边的条件可能是 "APPROVE"，这意味着只有当源智能体的消息中包含 "APPROVE" 时，执行流才会沿着这条边传递到目标智能体 5。
●状态管理：
虽然文档没有明确详述 GraphFlow 专用的状态管理机制，但执行流本身由 DiGraph 管理。状态通过智能体之间传递的消息序列隐式维护，并根据图结构和边条件进行演进 5。对于更复杂的状态需求，可能需要智能体自身管理或与外部状态存储交互。
●GraphFlow 的优势与适用场景 5：
○明确控制：当需要严格控制智能体行动顺序，或者不同结果需要导致不同后续步骤时，GraphFlow 非常有用。
○确定性：相比于依赖 LLM 进行动态选择的 SelectorGroupChat，GraphFlow 提供了更具确定性的控制流。
○复杂流程：适用于处理具有条件分支、并行处理或循环的复杂多步骤过程。
○可观测性：图结构使得工作流的逻辑更易于理解和调试。
●示例 5：
○顺序流：例如，writer 智能体起草段落，然后 reviewer 智能体提供反馈。DiGraphBuilder 用于添加这两个节点及从 writer 到 reviewer 的边。
Python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient
import asyncio

async def sequential_flow_example():
    client = OpenAIChatCompletionClient(model="gpt-4o-mini") # 替换为您的模型配置
    writer = AssistantAgent("writer", model_client=client, system_message="Draft a short paragraph on climate change.")
    reviewer = AssistantAgent("reviewer", model_client=client, system_message="Review the draft and suggest improvements.")

    builder = DiGraphBuilder()
    builder.add_node(writer).add_node(reviewer)
    builder.add_edge(writer, reviewer)
    graph = builder.build()

    flow = GraphFlow(participants=builder.get_participants(), graph=graph)
    # await Console(flow.run_stream(task="Write a short paragraph about climate change."))
# asyncio.run(sequential_flow_example())

○并行扇出与汇合 (Fan-out-Fan-in)：writer 起草段落，然后两个 editor 智能体并行编辑（一个负责语法，一个负责风格），最后由 final_reviewer 智能体合并编辑。图的边从 writer 分叉到两个 editor，再从两个 editor 汇合到 final_reviewer。
Python
#... (imports and client setup as above)
# async def parallel_flow_example():
#     writer = AssistantAgent("writer", model_client=client, system_message="Draft a short paragraph on climate change.")
#     editor1 = AssistantAgent("editor1", model_client=client, system_message="Edit the paragraph for grammar.")
#     editor2 = AssistantAgent("editor2", model_client=client, system_message="Edit the paragraph for style.")
#     final_reviewer = AssistantAgent(
#         "final_reviewer", model_client=client,
#         system_message="Consolidate the grammar and style edits into a final version."
#     )

#     builder = DiGraphBuilder()
#     builder.add_node(writer).add_node(editor1).add_node(editor2).add_node(final_reviewer)
#     builder.add_edge(writer, editor1)
#     builder.add_edge(writer, editor2)
#     builder.add_edge(editor1, final_reviewer)
#     builder.add_edge(editor2, final_reviewer)
#     graph = builder.build()

#     flow = GraphFlow(participants=builder.get_participants(), graph=graph)
#     # await Console(flow.run_stream(task="Write a short paragraph about climate change and get it edited."))
# asyncio.run(parallel_flow_example())

○条件循环与过滤摘要：generator 智能体产生想法，reviewer 智能体审核。如果 reviewer 回复 "REVISE"，流程返回 generator；如果回复 "APPROVE"，流程进入 summarizer。summarizer 可以配置为只接收初始用户输入和 reviewer 的最终消息（通过 MessageFilterAgent）5。
Python
from autogen_agentchat.agents import MessageFilterAgent, MessageFilterConfig, PerSourceFilter
#... (other imports and client setup)
# async def conditional_loop_example():
#     generator = AssistantAgent("generator", model_client=client, system_message="Generate a list of creative ideas.")
#     reviewer = AssistantAgent(
#         "reviewer", model_client=client,
#         system_message="Review ideas. Respond with 'REVISE' and feedback, or 'APPROVE' for final approval."
#     )
#     summarizer_core = AssistantAgent(
#         "summary_core", model_client=client, system_message="Summarize the user request and the final feedback."
#     )
#     filtered_summarizer = MessageFilterAgent(
#         name="summary", wrapped_agent=summarizer_core,
#         filter=MessageFilterConfig(
#             per_source=
#         )
#     )

#     builder = DiGraphBuilder()
#     builder.add_node(generator).add_node(reviewer).add_node(filtered_summarizer)
#     builder.add_edge(generator, reviewer)
#     builder.add_edge(reviewer, generator, condition="REVISE")
#     builder.add_edge(reviewer, filtered_summarizer, condition="APPROVE")
#     builder.set_entry_point(generator) # Important if no natural source node exists
#     graph = builder.build()

#     flow = GraphFlow(participants=builder.get_participants(), graph=graph)
#     # await Console(flow.run_stream(task="Brainstorm ways to reduce plastic waste."))
# asyncio.run(conditional_loop_example())

●实验性状态：需要注意 GraphFlow 目前仍是实验性功能，其 API 和行为在未来版本中可能会发生变化 5。
GraphFlow 的引入是 AutoGen 在工作流编排方面迈出的重要一步，它使得开发者能够以更结构化和声明式的方式设计复杂的多智能体交互逻辑，从而构建出更可靠、可控和可理解的 AI 应用。
6. 关键 AgentChat 功能
除了核心的智能体和团队结构，AgentChat 还提供了一些关键功能来增强用户交互、控制对话流程和管理状态。
6.1. 人机回圈 (Human-in-the-Loop, HIL)
在许多实际应用中，完全自主的智能体系统可能无法达到预期效果，或者需要人类的监督和干预。AutoGen AgentChat 提供了强大的人机回圈 (HIL) 功能，允许人类用户在智能体对话的不同阶段参与进来 41。
●HIL 的两种主要模式 41：
1.在团队运行期间提供反馈：通过 UserProxyAgent 实现。当对话流程轮到 UserProxyAgent 时，它会暂停团队执行，等待人类用户输入。这种模式适用于需要即时反馈的短交互，如批准/否决或需要立即关注的警报。
2.在一次运行终止后为下一次运行提供反馈：团队运行至某个终止条件（如达到最大轮次或特定智能体发出移交请求），然后人类用户提供输入，团队基于此输入开始新的运行。这种模式适用于持久化会话和异步通信。
●配置 UserProxyAgent 以获取人类输入 29：
○input_func 参数：可以自定义获取用户输入的函数。默认情况下，它可能从控制台读取。开发者可以将其替换为与 Web UI（如 FastAPI, ChainLit, Streamlit）或其他输入源集成的函数。
○UserProxyAgent 在团队中的角色：在 RoundRobinGroupChat 中，UserProxyAgent 按其在参与者列表中的顺序被调用。在 SelectorGroupChat 中，则由选择器提示或选择函数决定何时调用 UserProxyAgent。
●通过最大轮次 (max_turns) 实现 HIL 41：
可以在团队构造函数中设置 max_turns 参数，使团队在达到指定轮次后自动停止，从而为人类输入创造机会。例如，在聊天机器人场景中，可以设置为每当某个智能体响应后就暂停以获取用户反馈。团队状态（如对话历史）在暂停和恢复之间会得到保留。
●通过终止条件实现 HIL 41：
○TextMentionTermination: 当特定文本（如 "TERMINATE", "APPROVE"）在消息中被提及。
○HandoffTermination: 当智能体发出 HandoffMessage 请求移交给用户或其他智能体时，团队可以暂停。这允许智能体在遇到无法独立解决的问题或需要人类决策时主动请求帮助。
●示例 41：
○即时反馈：在诗歌生成的例子中，UserProxyAgent 在 AssistantAgent 生成诗歌后提示用户输入 "APPROVE" 来结束对话。
○异步反馈 (max_turns)：团队配置为 max_turns=1，在 AssistantAgent 生成第一版诗歌后暂停。用户输入反馈（例如 "Can you make it about a person..."），团队基于此反馈继续运行。
○异步反馈 (HandoffTermination)：AssistantAgent 在无法获取天气信息时，配置为向用户移交。团队因 HandoffTermination 而暂停，用户随后提供天气信息作为新任务输入，团队继续处理。
HIL 机制使得 AutoGen 应用更加灵活和实用，能够将人类的智慧和判断力融入到自动化流程中。
6.2. 终止条件 (Termination Conditions)
控制对话何时结束是多智能体系统设计的关键部分。AgentChat 提供了一个灵活的终止条件框架 47。
●TerminationCondition 基类 (autogen_agentchat.base.TerminationCondition) 39：
所有终止条件的抽象基类。它是一个可调用对象，接收自上次调用以来的消息序列，如果满足终止条件则返回 StopMessage，否则返回 None。终止条件是状态化的，在触发后需要通过 reset() 方法重置才能再次使用，但在每次团队运行 (run() 或 run_stream()) 结束后会自动重置。
●内置终止条件 (autogen_agentchat.conditions) 21：
○MaxMessageTermination(max_messages: int)：在达到指定数量的消息后终止。
○TextMentionTermination(texts: Union[str, List[str]], case_sensitive: bool = False)：当消息中提及指定的一个或多个文本字符串时终止。
○TokenUsageTermination(max_prompt_tokens: Optional[int] = None, max_completion_tokens: Optional[int] = None, max_total_tokens: Optional[int] = None)：当提示、完成或总 token 使用量达到阈值时终止（需要智能体报告 token 使用情况）。
○TimeoutTermination(timeout: float)：在指定的秒数后终止。
○HandoffTermination(target_agent_name: str)：当向特定目标智能体发出移交请求时终止。
○SourceMatchTermination(source_agent_name: str)：当特定源智能体响应后终止。
○ExternalTermination()：允许从运行外部以编程方式控制终止（例如，通过 UI 按钮）。
○StopMessageTermination()：当智能体产生 StopMessage 时终止。
○TextMessageTermination()：当智能体产生 TextMessage 时终止。
○FunctionCallTermination(function_name: str)：当智能体产生包含特定名称的函数执行结果 (FunctionExecutionResult) 的 ToolCallExecutionEvent 时终止。
○FunctionalTermination(condition_func: Callable]], bool])：当一个自定义的函数表达式在最新的消息序列上评估为 True 时终止。
●组合终止条件 39：
可以使用逻辑与 (&) 和逻辑或 (|) 操作符组合多个终止条件，以创建更复杂的终止逻辑。例如，MaxMessageTermination(10) | TextMentionTermination("APPROVED") 表示在 10 条消息后或当 "APPROVED" 被提及时终止。
●在团队中的应用 48：
对于群聊团队（如 RoundRobinGroupChat, SelectorGroupChat），终止条件在每个智能体响应后被调用一次（即使响应包含多个内部消息）。
●示例 48：
○MaxMessageTermination：限制俳句生成对话在3条消息后停止。
○组合条件：俳句对话在达到10条消息或评论家提及 "APPROVE" 时停止。
○自定义 FunctionCallTermination：当评论家调用名为 "approve" 的工具函数时，对话停止。
通过灵活运用这些终止条件，开发者可以精确控制多智能体对话的生命周期，确保其在适当的时候结束，避免无限循环或资源浪费。
6.3. 状态管理 (Managing State)
在许多应用中，尤其是在需要持久化会话（如 Web 应用）或从中断处恢复的场景中，保存和加载智能体及团队的状态至关重要。AutoGen AgentChat 为此提供了内置支持 41。
●保存和加载智能体状态 49：
○AssistantAgent 状态：其状态主要由 model_context (即与 LLM 的消息历史) 组成。调用 assistant_agent.save_state() 方法返回一个包含类型、版本和 llm_messages 的字典。
○加载状态：创建一个新的 AssistantAgent 实例，然后调用 new_assistant_agent.load_state(saved_agent_state) 来恢复之前的对话历史。
○自定义智能体状态：对于自定义智能体，默认的 save_state() 和 load_state() 方法保存和加载空状态。开发者需要重写这些方法以定义哪些内部数据应被持久化以及如何恢复。
●保存和加载团队状态 49：
○调用团队实例（如 RoundRobinGroupChat）的 save_state() 方法会保存团队中所有智能体的状态以及团队自身的管理状态（如当前轮次、下一发言者等）。返回一个包含团队类型、版本以及各智能体状态（以其唯一 ID 为键）的字典。
○加载团队状态时，先创建具有相同初始智能体配置的新团队实例，然后调用 new_team.load_state(saved_team_state)。
●持久化状态到文件或数据库 49：
○save_state() 返回的字典是标准的 Python 字典，可以轻松序列化为 JSON 等格式并存储到文件或数据库中。
○JSON 文件示例：
Python
import json
#... (team_state has been obtained from team.save_state())
# with open("team_state.json", "w") as f:
#     json.dump(team_state, f)

# with open("team_state.json", "r") as f:
#     loaded_team_state = json.load(f)
# new_team.load_state(loaded_team_state)

●状态管理最佳实践 49：
○明确需要持久化的关键状态。
○为自定义智能体正确实现 save_state 和 load_state。
○根据应用需求选择合适的存储机制（文件、数据库）。
○注意状态中敏感信息的安全存储。
○注意 AutoGen 版本升级可能带来的状态格式变化（状态字典中的 version 字段有助于此）。
○在 Web 应用中，请求开始时加载状态，结束时保存状态，符合无状态端点的设计。
○实现健壮的错误处理机制。
通过有效的状态管理，AutoGen 应用可以实现对话的持久化、中断恢复和跨会话的上下文保持。
7. AutoGen 扩展 (autogen-ext)
AutoGen 的核心设计理念之一是可扩展性。autogen-ext 包是 AutoGen 官方维护的内置组件实现集合，它极大地扩展了 AutoGen 的核心功能，提供了与各种外部服务、模型和工具的集成 9。
7.1. 扩展概述与安装
autogen-ext 包含了多种类型的组件，例如 9：
●Agents：如 MultimodalWebSurfer、FileSurfer、VideoSurfer、OpenAIAssistantAgent。
●Models：用于连接各种托管和本地模型的客户端，如 OpenAIChatCompletionClient、AzureOpenAIChatCompletionClient、AnthropicChatCompletionClient、OllamaChatCompletionClient、LlamaCppChatCompletionClient、SKChatCompletionAdapter。
●Tools：如 GraphRAG 的 LocalSearchTool 和 GlobalSearchTool、HttpTool、LangChainToolAdapter、mcp_server_tools()。
●Code Executors：如 DockerCommandLineCodeExecutor、LocalCommandLineCodeExecutor、JupyterCodeExecutor、ACADynamicSessionsCodeExecutor。
●Cache Stores：如 DiskCacheStore、RedisStore。
●Runtimes：如 GrpcWorkerAgentRuntime。
●Authentication：如 AzureTokenProvider。
●Memory：如 TextCanvasMemory、实验性的 TaskCentricMemory。
安装扩展：
通常，autogen-ext 的核心部分会随 autogen-agentchat 一起安装，或者可以单独安装 pip install autogen-ext。许多扩展功能以 "extras" 的形式提供，需要在使用时显式安装。例如：

Bash


# 安装 OpenAI 模型客户端支持
pip install "autogen-ext[openai]"

# 安装 Docker 代码执行器支持
pip install "autogen-ext[docker]"

# 安装 Web Surfer Agent 支持
pip install "autogen-ext[web-surfer]"

# 安装 File Surfer Agent 支持
pip install "autogen-ext[file-surfer]"

# 安装 Video Surfer Agent 支持
pip install "autogen-ext[video-surfer]"

# 安装 Anthropic 模型客户端支持
pip install "autogen-ext[anthropic]"

# 安装 Ollama 模型客户端支持
pip install "autogen-ext[ollama]"

# 安装 Llama.cpp 模型客户端支持
pip install "autogen-ext[llama-cpp]"

# 安装 Semantic Kernel 模型适配器支持 (具体 extras 可能依赖于 SK 后端)
pip install "autogen-ext[semantic-kernel-anthropic]" # 示例

# 安装 MCP 工具支持
pip install "autogen-ext[mcp]"

# 安装 LangChain 工具适配器支持
pip install "autogen-ext[langchain]"

# 安装 DiskCache 存储支持
pip install "autogen-ext[diskcache]"

# 安装 Redis 存储支持
pip install "autogen-ext[redis]"

6
具体的 extras 名称请查阅相应扩展的文档。
7.2. 模型客户端扩展
这些扩展使得 AutoGen 能够与多种 LLM 服务和本地模型进行交互。
7.2.1. OpenAI 与 Azure OpenAI (OpenAIChatCompletionClient, AzureOpenAIChatCompletionClient)
●目的：提供与 OpenAI API 和 Azure OpenAI 服务交互的客户端。
●OpenAIChatCompletionClient 16：
○用于 OpenAI 托管模型 (如 "gpt-4o", "gpt-3.5-turbo")。
○参数：model, api_key (或 OPENAI_API_KEY 环境变量), organization, base_url (用于兼容 OpenAI 的其他端点), timeout, max_retries, model_info, 以及控制模型行为的参数如 temperature, max_tokens, top_p, tools, response_format (或 json_output) 等。
○支持流式响应 (create_stream) 和结构化输出。
●AzureOpenAIChatCompletionClient 16：
○专用于 Azure OpenAI 服务托管的模型。
○参数：除了 OpenAIClient 的通用参数外，还需要 Azure 特定的参数：azure_endpoint (必需), azure_deployment (必需), api_version (必需)。
○身份验证：支持 API 密钥（api_key 或 AZURE_OPENAI_API_KEY 环境变量）和 Azure Active Directory (AAD) 令牌（通过 azure_ad_token 或 azure_ad_token_provider，后者可与 AzureTokenProvider 扩展配合使用）。
●通用功能：两个客户端都实现了 ChatCompletionClient 接口，提供 create, create_stream, count_tokens 等方法。它们能够处理函数/工具调用，并将工具 schema 传递给模型。
●配置示例 (OpenAIChatCompletionClient) 16：
Python
from autogen_ext.models.openai import OpenAIChatCompletionClient
# client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06", api_key="YOUR_OPENAI_API_KEY")

●配置示例 (AzureOpenAIChatCompletionClient with AAD) 16：
Python
# from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
# from autogen_ext.auth.azure import AzureTokenProvider # 假设已安装并导入
# from azure.identity import DefaultAzureCredential

# token_provider = AzureTokenProvider(credential=DefaultAzureCredential())
# azure_client = AzureOpenAIChatCompletionClient(
#     azure_deployment="your-deployment-name",
#     azure_endpoint="https://your-resource-name.openai.azure.com/",
#     api_version="2024-02-01", # 替换为您的 API 版本
#     azure_ad_token_provider=token_provider,
#     model="gpt-4o" # 确保模型与部署匹配
# )

7.2.2. Anthropic 模型 (AnthropicChatCompletionClient)
●目的：提供与 Anthropic Claude 模型交互的客户端 10。
●安装：pip install "autogen-ext[anthropic]" 10。
●AnthropicChatCompletionClient 10：
○参数：model (如 "claude-3-sonnet-20240229"), api_key (或 ANTHROPIC_API_KEY 环境变量), base_url, max_tokens, temperature, tools (用于实验性工具使用支持) 等。
○支持通过 Amazon Bedrock 使用 Anthropic 模型，此时需要配置 AnthropicBedrockClientConfiguration，包含 bedrock_info (AWS 访问密钥、区域等)。
●使用示例 10：
Python
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
# from autogen_core.models import UserMessage
# import asyncio

# async def anthropic_example():
#     client = AnthropicChatCompletionClient(
#         model="claude-3-opus-20240229", 
#         api_key="YOUR_ANTHROPIC_API_KEY"
#     )
#     # response = await client.create([UserMessage(content="Hello, Claude!", source="user")])
#     # print(response)
# asyncio.run(anthropic_example())

7.2.3. 本地模型与 Ollama (OllamaChatCompletionClient)
●目的：允许 AutoGen 使用通过 Ollama 在本地运行的 LLM 12。
●设置：需要安装 Ollama 并拉取所需模型 (如 ollama pull llama3)。安装 AutoGen 扩展：pip install "autogen-ext[ollama]" 12。
●OllamaChatCompletionClient 12：
○参数：model (必需, 如 "llama3"), host (Ollama 服务地址，默认为本地), response_format (可指定 Pydantic 模型以获取结构化 JSON 输出), options (传递给 Ollama 的额外参数如 temperature), model_info (如果模型不在预定义列表中，则必需，用于声明模型能力)。
○工具使用注意：Ollama 对工具属性的 schema 要求比 OpenAI 更严格，属性需要明确的 type 和 description 12。
●使用示例 12：
Python
from autogen_ext.models.ollama import OllamaChatCompletionClient
# ollama_client = OllamaChatCompletionClient(model="llama3")
# #... 然后可以将 ollama_client 用于 AssistantAgent 的 llm_config

7.2.4. 本地模型与 Llama.cpp (LlamaCppChatCompletionClient)
●目的：允许 AutoGen 使用通过 llama-cpp-python 库在本地运行的 GGUF 格式 LLM 13。
●设置：安装 AutoGen 扩展：pip install "autogen-ext[llama-cpp]"。需要有 GGUF 格式的模型文件，可以本地提供或从 Hugging Face Hub 下载 13。
●LlamaCppChatCompletionClient 13：
○参数：model_path (本地模型文件路径) 或 (repo_id 和 filename 从 Hugging Face Hub 下载)，n_gpu_layers (卸载到 GPU 的层数), n_ctx (上下文大小), verbose 等，以及传递给底层 Llama 类的其他参数。
○model_info：用于声明模型能力，默认为支持函数调用和 JSON 输出。
●使用示例 (本地模型) 13：
Python
from autogen_ext.models.llama_cpp import LlamaCppChatCompletionClient
# llama_client = LlamaCppChatCompletionClient(model_path="/path/to/your/model.gguf")

●使用示例 (Hugging Face Hub) 13：
Python
# llama_client_hf = LlamaCppChatCompletionClient(
#     repo_id="NousResearch/Hermes-2-Pro-Mistral-7B-GGUF", 
#     filename="Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf", # 选择一个具体文件
#     n_gpu_layers=-1 # 尝试卸载所有层到 GPU
# )

7.2.5. Semantic Kernel 集成 (SKChatCompletionAdapter)
●目的：将 Semantic Kernel 的 ChatCompletionClientBase 连接器适配为 AutoGen 的 ChatCompletionClient，从而允许 AutoGen 使用 Semantic Kernel 支持的各种模型后端（如 Azure OpenAI, Google Gemini, Ollama, Anthropic 等）11。
●安装：基础 pip install autogen-ext，然后根据具体 SK 后端安装额外依赖，如 pip install "autogen-ext[semantic-kernel-google]" 11。
●SKChatCompletionAdapter 11：
○参数：sk_client (必需, Semantic Kernel 的 ChatCompletionClientBase 实例), kernel (可选的 SK Kernel 实例), prompt_settings (可选的 SK PromptExecutionSettings), model_info (可选)。
●使用模式：初始化相应的 SK 客户端 (如 GoogleAIChatCompletion)，然后用 SKChatCompletionAdapter 包装它，再将此适配器用作 AutoGen 智能体的 model_client 11。
7.2.6. 响应缓存 (ChatCompletionCache)
●目的：包装一个 ChatCompletionClient 以缓存其响应，从而节省成本并提高后续相同请求的速度 21。
●ChatCompletionCache 54：
○参数：client (要包装的 ChatCompletionClient 实例), store (一个 CacheStore 对象，如 DiskCacheStore 或 RedisStore；默认为内存缓存)。
○方法：实现了 ChatCompletionClient 接口。create 和 create_stream 方法会首先检查缓存，如果命中则直接返回缓存结果。
●使用示例 (配合 DiskCacheStore) 54：
Python
# import tempfile
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# from autogen_ext.models.cache import ChatCompletionCache
# from autogen_ext.cache_store.diskcache import DiskCacheStore

# with tempfile.TemporaryDirectory() as tmpdirname:
#     openai_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
#     disk_store = DiskCacheStore(tmpdirname) # CHAT_CACHE_VALUE_TYPE is inferred
#     cached_client = ChatCompletionCache(client=openai_client, store=disk_store)
#     # response1 = await cached_client.create(...) # 第一次调用，会请求 OpenAI 并缓存
#     # response2 = await cached_client.create(...) # 第二次相同调用，会从磁盘缓存读取

7.2.7. API 调用回放 (ReplayChatCompletionClient)
●目的：用于回放先前记录的 LLM API 调用，主要用于测试、调试和演示，而无需实际调用 LLM 服务，从而节省成本和时间 21。
●工作方式：需要一个包含先前 API 调用记录的存储（通常是 JSON 文件或目录）。当客户端收到与记录中匹配的请求时，它会返回记录的响应。
●参数与方法：具体的参数会涉及记录的路径、匹配方式等。方法会模拟 ChatCompletionClient 的接口。 59
7.3. 特定功能智能体扩展
这些扩展提供了具有特定内置能力的智能体。
7.3.1. 网页浏览 (MultimodalWebSurfer)
●目的：使智能体能够浏览网页、与网页内容（包括文本和视觉元素）交互，并提取信息 21。
●组件：
○MultimodalWebSurfer Agent：利用多模态模型 (如 GPT-4o) 理解网页内容，执行点击、滚动、填写表单等操作。
○PlaywrightController：一个辅助类，使用 Playwright 库与浏览器进行实际交互。
●设置：
○安装：pip install "autogen-ext[web-surfer]" 51。
○Playwright 会自动安装浏览器驱动（默认为 Chromium）。
●参数 (MultimodalWebSurfer) 51：
○name, model_client (必须是支持函数/工具调用的多模态模型), downloads_folder, description, debug_dir, headless (是否无头浏览器), start_page, to_save_screenshots, use_ocr 等。
●方法 (MultimodalWebSurfer) 51：
○async close()：关闭浏览器。
○async on_messages(...) / async on_messages_stream(...)：处理消息，与网页交互。
●使用示例 51：
Python
# import asyncio
# from autogen_agentchat.ui import Console
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# from autogen_ext.agents.web_surfer import MultimodalWebSurfer

# async def web_surfer_example():
#     model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06", api_key="YOUR_OPENAI_API_KEY")
#     web_surfer = MultimodalWebSurfer(
#         name="WebSurfer",
#         model_client=model_client,
#         headless=False, # 设为 False 以便观察浏览器操作
#         to_save_screenshots=True,
#         debug_dir="web_surfer_debug"
#     )
#     task = "Search for 'AutoGen framework' and summarize the key features from the official GitHub page."
#     # await Console(web_surfer.run_stream(task=task))
#     await web_surfer.close()
# asyncio.run(web_surfer_example())

7.3.2. 文件系统交互 (FileSurfer)
●目的：使 MagenticOne 框架内的智能体能够预览本地文件内容和导航本地文件系统层次结构 21。
●FileSurfer Agent 52：
○参数：name, model_client (必须支持工具使用), description, base_path (文件浏览的起始目录，默认为当前工作目录)。
○能力：读取常见文件类型，列出目录内容。
●安装：pip install "autogen-ext[file-surfer]" 52。
●使用场景：用于需要从本地文档、配置文件等获取信息的任务。
7.3.3. 视频内容分析 (VideoSurfer)
●目的：使智能体能够回答关于本地视频文件内容的问题，通过提取和处理视频信息实现 21。
●VideoSurfer Agent 53：
○能力：获取视频时长、在特定时间戳截屏、转录音频。
○内置工具：extract_audio(), get_video_length(), transcribe_audio_with_timestamps(), get_screenshot_at(), save_screenshot(), transcribe_video_screenshot()。
○参数：name, model_client, tools (可自定义使用的工具列表), description, system_message。
●安装：pip install "autogen-ext[video-surfer]" 53。
●使用示例 53：
Python
# import asyncio
# from autogen_agentchat.ui import Console
# from autogen_agentchat.teams import RoundRobinGroupChat
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# from autogen_ext.agents.video_surfer import VideoSurfer

# async def video_surfer_example():
#     # 假设已有一个名为 "example_video.mp4" 的视频文件
#     model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06", api_key="YOUR_OPENAI_API_KEY")
#     video_agent = VideoSurfer(name="VideoExpert", model_client=model_client)
#     # team = RoundRobinGroupChat([video_agent], termination_condition=TextMentionTermination("TERMINATE"))
#     # await Console(team.run_stream(task="What is discussed around timestamp 1:30 in example_video.mp4?"))
# asyncio.run(video_surfer_example())

7.3.4. OpenAI Assistants API 集成 (OpenAIAssistantAgent)
●目的：提供一个直接利用 OpenAI Assistants API 功能的智能体实现，包括代码解释器、文件搜索和自定义函数调用等内置能力 21。
●OpenAIAssistantAgent 60：
○与标准 AssistantAgent 的区别：OpenAIAssistantAgent 直接使用 Assistants API，状态（线程、运行）由 OpenAI 服务器管理；而标准 AssistantAgent 通常使用 Chat Completions API，状态和工具执行更多在 AutoGen 框架内管理。
○参数：name, description, client (一个 AsyncOpenAI 或 AsyncAzureOpenAI 实例), model, instructions, tools (可以是 "code_interpreter", "file_search" 或自定义函数), assistant_id (使用现有助手), thread_id (使用现有线程) 等。
○方法：包括处理消息 (on_messages), 文件上传 (on_upload_for_code_interpreter, on_upload_for_file_search), 删除助手/文件等。
●使用场景：需要 Assistants API 提供的代码解释器、持久化线程和文件处理等高级功能的场景。
●示例 (使用代码解释器分析 CSV) 60：
Python
# from openai import AsyncOpenAI
# from autogen_core import CancellationToken
# import asyncio
# from autogen_ext.agents.openai import OpenAIAssistantAgent
# from autogen_agentchat.messages import TextMessage

# async def openai_assistant_example():
#     # cancellation_token = CancellationToken()
#     # open_ai_client = AsyncOpenAI(api_key="YOUR_OPENAI_API_KEY")
#     # assistant = OpenAIAssistantAgent(
#     #     name="DataAnalyzer",
#     #     client=open_ai_client,
#     #     model="gpt-4-turbo",
#     #     instructions="You are a data analyst. Analyze the provided CSV file.",
#     #     tools=["code_interpreter"],
#     # )
#     # # 假设已有一个名为 "my_data.csv" 的文件
#     # await assistant.on_upload_for_code_interpreter("my_data.csv", cancellation_token)
#     # response = await assistant.on_messages(
#     #    ,
#     #     cancellation_token
#     # )
#     # print(response)
#     # await assistant.delete_uploaded_files(cancellation_token)
#     # await assistant.delete_assistant(cancellation_token)
# asyncio.run(openai_assistant_example())

7.3.5. Azure AI Agent (AzureAIAgent)
●目的：与 Azure AI 服务集成，具体功能可能包括使用 Azure特定的AI能力，如 Bing grounding URL 支持 4。
●能力：0.5.6 版本中提到增加了对 Bing grounding 引用 URL 的支持 4。 61
7.3.6. MagenticOne Coder Agent (MagenticOneCoderAgent)
●目的：在 MagenticOne 平台内提供编码辅助，使用 LLM 客户端帮助完成与代码相关的任务 21。
●MagenticOneCoderAgent 62：
○继承自 AssistantAgent。
○其提示和描述是“密封的”，以复制原始 MagenticOne 设置。
○参数：name, model_client (ChatCompletionClient)。
●集成：专为 MagenticOne 框架设计，确保其行为与 MagenticOne 对代码相关任务的要求一致。
7.4. 专用工具扩展
这些扩展为智能体提供了特定的工具能力。
7.4.1. Python 代码执行工具 (PythonCodeExecutionTool)
●目的：使智能体能够执行 Python 代码片段 21。
●集成：通常与一个 CodeExecutor (如 LocalCommandLineCodeExecutor, DockerCommandLineCodeExecutor, JupyterCodeExecutor) 配合使用。PythonCodeExecutionTool 本身定义了工具接口，而实际的执行由配置的 CodeExecutor 完成。
●参数：会接收一个 CodeExecutor 实例。可能还包括如 timeout 等参数，或从其包装的 CodeExecutor 继承这些设置。
●使用示例 (配合 JupyterCodeExecutor) 26：
Python
# import asyncio
# from autogen_agentchat.agents import AssistantAgent
# from autogen_ext.code_executors.jupyter import JupyterCodeExecutor
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# from autogen_ext.tools.code_execution import PythonCodeExecutionTool # 假设路径

# async def python_tool_example():
#     # async with JupyterCodeExecutor() as executor:
#     #     python_tool = PythonCodeExecutionTool(code_executor=executor)
#     #     model_client = OpenAIChatCompletionClient(model="gpt-4o-mini", api_key="YOUR_API_KEY")
#     #     coder_agent = AssistantAgent(
#     #         name="PythonCoder",
#     #         model_client=model_client,
#     #         tools=[python_tool],
#     #         system_message="You write python code to solve tasks. Execute the code using the provided tool."
#     #     )
#         # result = await coder_agent.run(task="Calculate 25 * 4 and print the result.")
#         # print(result.messages[-1])
# asyncio.run(python_tool_example())
26
7.4.2. GraphRAG 工具 (GlobalSearchTool, LocalSearchTool)
●目的：提供基于知识图谱的检索增强生成 (RAG)能力，通过利用文档关系和语义嵌入进行语义搜索 21。
●组件 64：
○GlobalSearchTool: 用于对整个索引数据集进行广泛查询。
○LocalSearchTool: 用于对数据的特定子集（如图中的特定节点或邻域）进行聚焦查询。
●配置 64：
○需要 GraphRAG 项目的 settings.yaml 文件进行初始化 (GlobalSearchTool.from_settings(...))。
○涉及 GlobalContextConfig, GlobalDataConfig, LocalContextConfig, LocalDataConfig, SearchConfig 等 Pydantic 模型来定义数据源、上下文构建和搜索行为。
●参数 (GlobalSearchTool, LocalSearchTool) 64：
○token_encoder (tiktoken.Encoding), llm (BaseLLM), data_config, context_config 等。
●方法 (run) 64：接收包含 query 的参数对象 (GlobalSearchToolArgs 或 LocalSearchToolArgs)，返回包含 answer 的结果对象。
●使用示例 (GlobalSearchTool 与 AssistantAgent) 64：
Python
# import asyncio
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# from autogen_agentchat.ui import Console
# from autogen_ext.tools.graphrag import GlobalSearchTool
# from autogen_agentchat.agents import AssistantAgent

# async def graphrag_global_example():
#     # openai_client = OpenAIChatCompletionClient(model="gpt-4o-mini", api_key="YOUR_API_KEY")
#     # # 假设./settings.yaml 已根据 GraphRAG 文档配置好
#     # global_tool = GlobalSearchTool.from_settings(settings_path="./settings.yaml")
#     # assistant_agent = AssistantAgent(
#     #     name="GraphRAG_Assistant",
#     #     tools=[global_tool],
#     #     model_client=openai_client,
#     #     system_message="Use global_search for broad queries about the dataset."
#     # )
#     # query = "What is the overall sentiment of the community reports?"
#     # await Console(assistant_agent.run_stream(task=query))
# asyncio.run(graphrag_global_example())
本地搜索工具 LocalSearchTool 的使用方式类似，但适用于更具体的查询。
7.4.3. HTTP 工具 (HttpTool)
●目的：使智能体能够通过发送 HTTP 请求与 Web API 交互 21。
●HttpTool 55：
○参数：name, description, scheme ("http" 或 "https"), host, port, path (可包含路径参数如 /{param}), method ("GET", "POST", "PUT", "DELETE", "PATCH"), headers, json_schema (定义输入参数的 JSON Schema), return_type ("text" 或 "json")。
●安全考量 55：
○安全处理凭证（API 密钥等）。
○输入验证。
○优先使用 HTTPS。
○处理 API 错误和速率限制。
●使用示例 (解码 Base64) 55：
Python
# from autogen_ext.tools.http import HttpTool
# from autogen_agentchat.agents import AssistantAgent
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# import asyncio

# async def http_tool_example():
#     # base64_schema = {
#     #     "type": "object",
#     #     "properties": {"value": {"type": "string", "description": "Base64 string to decode"}},
#     #     "required": ["value"],
#     # }
#     # base64_tool = HttpTool(
#     #     name="base64_decode",
#     #     description="Decodes a base64 string using httpbin.org.",
#     #     scheme="https", host="httpbin.org", port=443,
#     #     path="/base64/{value}", method="GET",
#     #     json_schema=base64_schema,
#     # )
#     # model_client = OpenAIChatCompletionClient(model="gpt-4o-mini", api_key="YOUR_API_KEY")
#     # assistant = AssistantAgent(
#     #     name="DecoderAssistant",
#     #     model_client=model_client,
#     #     tools=[base64_tool]
#     # )
#     # result = await assistant.run(task="Decode the base64 value 'SGVsbG8gV29ybGQ='") # Hello World
#     # print(result.messages[-1])
# asyncio.run(http_tool_example())

7.4.4. LangChain 工具适配器 (LangChainToolAdapter)
●目的：允许在 AutoGen 智能体工作流中使用 LangChain框架中开发的工具 21。
●LangChainToolAdapter 56：
○参数：langchain_tool (一个 LangChain 工具实例)。
○方法：async run(*args: BaseModel, cancellation_token: CancellationToken) -> Any，执行包装的 LangChain 工具。
●安装：pip install -U "autogen-ext[langchain]" 56。
●使用示例 (使用 LangChain 的 PythonAstREPLTool 操作 Pandas DataFrame) 56：
Python
# import asyncio
# import pandas as pd
# from langchain_experimental.tools.python.tool import PythonAstREPLTool
# from autogen_ext.tools.langchain import LangChainToolAdapter
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# from autogen_agentchat.agents import AssistantAgent
# #... (other imports)

# async def langchain_adapter_example():
#     # df = pd.DataFrame({'col1': , 'col2': })
#     # python_repl_tool = PythonAstREPLTool(locals={"df": df})
#     # autogen_compatible_tool = LangChainToolAdapter(python_repl_tool)

#     # model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="YOUR_API_KEY")
#     # agent = AssistantAgent(
#     #     name="PandasAssistant",
#     #     tools=[autogen_compatible_tool],
#     #     model_client=model_client,
#     #     system_message="Use the provided tool to interact with the pandas DataFrame 'df'."
#     # )
#     # result = await agent.run(task="What is the sum of col1 in df?")
#     # print(result.messages[-1])
# asyncio.run(langchain_adapter_example())

7.4.5. MCP 工具 (SseMcpToolAdapter, StdioMcpToolAdapter, mcp_server_tools)
●目的：使 AutoGen 智能体能够与实现了模型上下文协议 (Model Context Protocol, MCP) 的外部工具和服务进行交互 21。
●组件 57：
○SseMcpToolAdapter: 包装通过 HTTP 服务器发送事件 (SSE) 通信的 MCP 工具。参数包括 SseServerParams (URL, headers, timeout) 和工具对象。
○StdioMcpToolAdapter: 包装通过标准输入/输出 (STDIO) 通信的 MCP 工具（例如命令行工具）。参数包括 StdioServerParams (命令和参数) 和工具对象。
○mcp_server_tools(): 工厂函数，连接到 MCP 服务器并为所有可用工具创建相应的适配器列表。
●安装：pip install -U "autogen-ext[mcp]" 57。
●使用示例 (本地文件系统 MCP 服务) 57：
Python
# import asyncio
# from pathlib import Path
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools
# from autogen_agentchat.agents import AssistantAgent

# async def mcp_stdio_example():
#     # desktop_path = str(Path.home() / "Desktop")
#     # # 假设 @modelcontextprotocol/server-filesystem 已通过 npx 可用
#     # server_params = StdioServerParams(
#     #     command="npx", # 在 Windows 上可能是 "npx.cmd"
#     #     args=["-y", "@modelcontextprotocol/server-filesystem", desktop_path]
#     # )
#     # file_tools = await mcp_server_tools(server_params)

#     # model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="YOUR_API_KEY")
#     # file_agent = AssistantAgent(
#     #     name="FileManager",
#     #     model_client=model_client,
#     #     tools=file_tools, # type: ignore
#     #     system_message="You can manage files on the desktop."
#     # )
#     # result = await file_agent.run(task="Create a file named 'autogen_mcp_test.txt' on the desktop with content 'Hello MCP!'")
#     # print(result.messages[-1])
# asyncio.run(mcp_stdio_example())

7.4.6. Azure AI 搜索工具 (AzureAISearchTool)
●目的：将 Azure AI 搜索功能集成到 AutoGen 智能体中，用于检索增强生成 (RAG) 等场景 21。
●组件：AzureAISearchTool，配置对象 AzureAISearchConfig。 65
7.4.7. 文件工具 (独立于 FileSurfer)
●目的：提供用于读取、写入和管理文件的独立工具，可能比 FileSurfer 智能体更原子化。 66
7.4.8. Web 工具 (独立于 WebSurfer)
●目的：提供用于 Web 交互的独立工具（如获取内容、搜索），可能比 WebSurfer 智能体更原子化。 67
7.5. 代码执行器扩展
这些扩展提供了不同的代码执行环境。
Works cited
1.AutoGen v0.5.6 released : r/AutoGenAI - Reddit, accessed May 9, 2025, https://www.reddit.com/r/AutoGenAI/comments/1kdu20x/autogen_v056_released/
2.README.md - microsoft/autogen - GitHub, accessed May 9, 2025, https://github.com/microsoft/autogen/blob/main/README.md
3.Multi-agent Conversation Framework | AutoGen 0.2, accessed May 10, 2025, https://microsoft.github.io/autogen/0.2/docs/Use-Cases/agent_chat/
4.Releases · microsoft/autogen - GitHub, accessed May 10, 2025, https://github.com/microsoft/autogen/releases
5.GraphFlow (Workflows) — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/graph-flow.html
6.Installation — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/installation.html
7.Installation — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html
8.Installation — AutoGen - Microsoft Open Source, accessed May 9, 2025, https://microsoft.github.io/autogen/0.4.2/user-guide/autogenstudio-user-guide/installation.html
9.Extensions — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html
10.autogen_ext.models.anthropic — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.anthropic.html
11.autogen_ext.models.semantic_kernel — AutoGen, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.semantic_kernel.html
12.autogen_ext.models.ollama — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.ollama.html
13.autogen_ext.models.llama_cpp — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.llama_cpp.html
14.autogen/python/packages/autogen-studio/README.md at main - GitHub, accessed May 9, 2025, https://github.com/microsoft/autogen/blob/main/python/packages/autogen-studio/README.md
15.Usage — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html
16.autogen_ext.models.openai — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html
17.autogen_core — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_core.html
18.FAQs — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/faqs.html
19.Code Execution — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/code-execution-groupchat.html
20.autogen_core.models — AutoGen - Microsoft Open Source, accessed May 9, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html
21.API Reference — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/index.html
22.Tools — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/tools.html
23.autogen_core.tools — AutoGen - Microsoft Open Source, accessed May 9, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tools.html
24.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_core.tool.html
25.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_core.code_executor.html
26.autogen_ext.code_executors.jupyter — AutoGen, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.jupyter.html
27.autogen_ext.code_executors.docker — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html
28.Command Line Code Executors — AutoGen - Microsoft Open Source, accessed May 9, 2025, https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/components/command-line-code-executors.html
29.autogen_agentchat.agents — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html
30.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker_jupyter.html
31.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.azure.html
32.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/
33.autogen_ext.runtimes.grpc — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.runtimes.grpc.html
34.autogen_agentchat — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html
35.Migration Guide for v0.2 to v0.4 — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html
36.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/
37.autogen.ConversableAgent - AG2 docs, accessed May 10, 2025, https://docs.ag2.ai/docs/api-reference/autogen/ConversableAgent
38.agentchat.conversable_agent | AutoGen 0.2, accessed May 10, 2025, https://microsoft.github.io/autogen/0.2/docs/reference/agentchat/conversable_agent/
39.autogen_agentchat.base — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html
40.Migration Guide for v0.2 to v0.4 — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable//user-guide/agentchat-user-guide/migration-guide.html
41.Human-in-the-Loop — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html
42.Agents — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html
43.Agents — AutoGen - Microsoft Open Source, accessed May 9, 2025, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html
44.GroupChatManager - AG2, accessed May 9, 2025, https://docs.ag2.ai/docs/api-reference/autogen/GroupChatManager/
45.Group Chat — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/design-patterns/group-chat.html
46.autogen_agentchat.teams — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html
47.Selector Group Chat — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html
48.Termination — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html
49.Managing State — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/state.html
50.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/managing-state.html
51.autogen_ext.agents.web_surfer — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.web_surfer.html
52.autogen_ext.agents.file_surfer — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.file_surfer.html
53.autogen_ext.agents.video_surfer — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.video_surfer.html
54.autogen_ext.models.cache — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.cache.html
55.autogen_ext.tools.http — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.http.html
56.autogen_ext.tools.langchain — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.langchain.html
57.autogen_ext.tools.mcp — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.mcp.html
58.autogen_ext.cache_store.redis — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.cache_store.redis.html
59.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.replay.html
60.autogen_ext.agents.openai — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.openai.html
61.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.azure.html
62.autogen_ext.agents.magentic_one — AutoGen, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.agents.magentic_one.html
63.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html
64.autogen_ext.tools.graphrag — AutoGen - Microsoft Open Source, accessed May 10, 2025, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.graphrag.html
65.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.azure.html
66.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.file.html
67.accessed January 1, 1970, https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.web.html